{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/josh/opt/anaconda3/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV1klEQVR4nO3df6zd9X3f8ecrhBCU4AbEBTm2F7PM6QZIcYrlUaFtWciKS7KadGMybQCpTM4Y0cjWroJIU5NJ3lKt+THUweY0CLNmYZaSFC8/mlKSLItG4lyYAxiHxgsOOPbwza/GbBKtnff+OB+Uo8ux77n2vecm9/N8SEfne97fz+d8P19Zft2vPud7zidVhSSpDy9Z6gFIkibH0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihr2UvyYEkbx6zbSX5a6d4nFPuK02KoS8tsSS/luTbSf5vkj9Kct5Sj0nLl6EvLaEklwD/CbgeuBD4f8CdSzooLWuGvrqSZGOSh5L8MMnhJL+f5GWzml2d5FtJvpvk3yV5yVD/30iyL8kPknwuyWtOcJyrkzyR5GiS7yT5rRMM6deB/1ZVX6qq54B/BfxqknMW5ISlWQx99eY48M+B84FfBK4E/umsNm8DNgC/AGwGfgMgyTXAu4FfBaaA/wF87ATH+Qjwjqo6B7gU+PwJ2l0CfP2FF1X1v4G/AF43v9OSxmPoqytV9XBVfaWqjlXVAQZTK39nVrPfrarvV9XTwIeA61r9HcC/rap9VXUM+DfA+hNc7f8lcHGSFVX1g6p65ARDeiXw57Nqfw54pa9FYeirK0lel+RTSf5Pkh8xCO7zZzV7Zmj728Cr2/ZrgH/fpoZ+CHwfCLBqxKH+AXA18O0k/z3JL55gSM8BK2bVVgBHxz0naT4MffXmLuAbwLqqWsFguiaz2qwZ2v4rwKG2/QyDKZtXDT3Orqr/OfsgVfW1qtoMXAD8EbDzBOPZC7z+hRdJ/ipwFvBn8z4zaQyGvnpzDvAj4Lkkfx24eUSbf5nk3CRrgFuB/9rq/xG4vd1xQ5KfS3Lt7M5JXpbk15P8XFX9ZTve8ROM56PA30/yt5K8AvjXwCeqyit9LQpDX735LeDXGEyffJifBPqw+4GHgT3Apxl8KEtVfRL4XeC+NjX0OPDLJzjO9cCB1u6fAG8f1aiq9rb9HwWOMPijNPuDZWnBxEVUJKkfXulLUkcMfUnqiKEvSR0x9CWpIy9d6gHM5fzzz6+1a9cu9TAk6WfKww8//N2qmppd/6kP/bVr1zI9Pb3Uw5CknylJvj2q7vSOJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZOxv5CY5A5gGvlNVb01yHoMFKNYCB4B/VFU/aG1vB25isFrQP6uqz7X6ZcA9wNnAZ4Bbyx/0l7RMrb3t06fc98D73rKAI/mJ+Vzp3wrsG3p9G/BgVa0DHmyvSXIxsAW4BNgE3Nn+YMBgfdKtwLr22HRao5ckzctYoZ9kNfAW4A+GypuBHW17B3DNUP2+qnq+qp4C9gMbk6wEVlTVQ+3q/t6hPpKkCRj3Sv9DwG8DPx6qXVhVhwHa8wWtvgp4ZqjdwVZb1bZn118kydYk00mmZ2ZmxhyiJGkuc4Z+krcCR6rq4THfMyNqdZL6i4tV26tqQ1VtmJp60S+DSpJO0Tgf5F4B/EqSq4GXAyuS/CHwbJKVVXW4Td0cae0PAmuG+q8GDrX66hF1SdKEzHmlX1W3V9XqqlrL4APaz1fV24FdwI2t2Y3A/W17F7AlyVlJLmLwge3uNgV0NMnlSQLcMNRHkjQBp7OIyvuAnUluAp4GrgWoqr1JdgJPAMeAW6rqeOtzMz+5ZfOz7SFJmpB5hX5VfRH4Ytv+HnDlCdptA7aNqE8Dl853kJKkheE3ciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjc4Z+kpcn2Z3k60n2Jnlvq78nyXeS7GmPq4f63J5kf5Ink1w1VL8syWNt3x1trVxJ0oSMs1zi88Cbquq5JGcCX07ywtq2H6yq3xtunORiBguoXwK8GvjTJK9r6+TeBWwFvgJ8BtiE6+RK0sTMeaVfA8+1l2e2R52ky2bgvqp6vqqeAvYDG5OsBFZU1UNVVcC9wDWnNXpJ0ryMNaef5Iwke4AjwANV9dW2651JHk1yd5JzW20V8MxQ94Ottqptz66POt7WJNNJpmdmZsY/G0nSSY0V+lV1vKrWA6sZXLVfymCq5rXAeuAw8P7WfNQ8fZ2kPup426tqQ1VtmJqaGmeIkqQxzOvunar6IfBFYFNVPdv+GPwY+DCwsTU7CKwZ6rYaONTqq0fUJUkTMs7dO1NJXtW2zwbeDHyjzdG/4G3A4217F7AlyVlJLgLWAbur6jBwNMnl7a6dG4D7F+5UJElzGefunZXAjiRnMPgjsbOqPpXkPydZz2CK5gDwDoCq2ptkJ/AEcAy4pd25A3AzcA9wNoO7drxzR5ImaM7Qr6pHgTeMqF9/kj7bgG0j6tPApfMcoyRpgfiNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR8ZZLvHlSXYn+XqSvUne2+rnJXkgyTfb87lDfW5Psj/Jk0muGqpfluSxtu+OtmyiJGlCxrnSfx54U1W9HlgPbEpyOXAb8GBVrQMebK9JcjGwBbgE2ATc2ZZaBLgL2Mpg3dx1bb8kaULmDP0aeK69PLM9CtgM7Gj1HcA1bXszcF9VPV9VTwH7gY1tIfUVVfVQVRVw71AfSdIEjDWnn+SMJHuAI8ADVfVV4MKqOgzQni9ozVcBzwx1P9hqq9r27Pqo421NMp1kemZmZh6nI0k6mbFCv6qOV9V6YDWDq/aTLW4+ap6+TlIfdbztVbWhqjZMTU2NM0RJ0hjmdfdOVf0Q+CKDufhn25QN7flIa3YQWDPUbTVwqNVXj6hLkiZknLt3ppK8qm2fDbwZ+AawC7ixNbsRuL9t7wK2JDkryUUMPrDd3aaAjia5vN21c8NQH0nSBLx0jDYrgR3tDpyXADur6lNJHgJ2JrkJeBq4FqCq9ibZCTwBHANuqarj7b1uBu4BzgY+2x6SpAmZM/Sr6lHgDSPq3wOuPEGfbcC2EfVp4GSfB0iSFpHfyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjLNG7pokX0iyL8neJLe2+nuSfCfJnva4eqjP7Un2J3kyyVVD9cuSPNb23dHWypUkTcg4a+QeA36zqh5Jcg7wcJIH2r4PVtXvDTdOcjGwBbgEeDXwp0le19bJvQvYCnwF+AywCdfJlaSJmfNKv6oOV9UjbfsosA9YdZIum4H7qur5qnoK2A9sTLISWFFVD1VVAfcC15zuCUiSxjevOf0kaxkskv7VVnpnkkeT3J3k3FZbBTwz1O1gq61q27Pro46zNcl0kumZmZn5DFGSdBJjh36SVwIfB95VVT9iMFXzWmA9cBh4/wtNR3Svk9RfXKzaXlUbqmrD1NTUuEOUJM1hrNBPciaDwP9oVX0CoKqerarjVfVj4MPAxtb8ILBmqPtq4FCrrx5RlyRNyDh37wT4CLCvqj4wVF851OxtwONtexewJclZSS4C1gG7q+owcDTJ5e09bwDuX6DzkCSNYZy7d64ArgceS7Kn1d4NXJdkPYMpmgPAOwCqam+SncATDO78uaXduQNwM3APcDaDu3a8c0eSJmjO0K+qLzN6Pv4zJ+mzDdg2oj4NXDqfAUqSFo7fyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWSc5RLXJPlCkn1J9ia5tdXPS/JAkm+253OH+tyeZH+SJ5NcNVS/LMljbd8dbdlESdKEjHOlfwz4zar6G8DlwC1JLgZuAx6sqnXAg+01bd8W4BJgE3BnkjPae90FbGWwbu66tl+SNCFzhn5VHa6qR9r2UWAfsArYDOxozXYA17TtzcB9VfV8VT0F7Ac2toXUV1TVQ1VVwL1DfSRJEzCvOf0ka4E3AF8FLqyqwzD4wwBc0JqtAp4Z6naw1Va17dn1UcfZmmQ6yfTMzMx8hihJOomxQz/JK4GPA++qqh+drOmIWp2k/uJi1faq2lBVG6ampsYdoiRpDmOFfpIzGQT+R6vqE638bJuyoT0fafWDwJqh7quBQ62+ekRdkjQh49y9E+AjwL6q+sDQrl3AjW37RuD+ofqWJGcluYjBB7a72xTQ0SSXt/e8YaiPJGkCXjpGmyuA64HHkuxptXcD7wN2JrkJeBq4FqCq9ibZCTzB4M6fW6rqeOt3M3APcDbw2faQJE3InKFfVV9m9Hw8wJUn6LMN2DaiPg1cOp8BSpIWjt/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZJzlEu9OciTJ40O19yT5TpI97XH10L7bk+xP8mSSq4bqlyV5rO27oy2ZKEmaoHGu9O8BNo2of7Cq1rfHZwCSXAxsAS5pfe5MckZrfxewlcGauetO8J6SpEU0Z+hX1ZeA74/5fpuB+6rq+ap6CtgPbEyyElhRVQ9VVQH3Atec4pglSafodOb035nk0Tb9c26rrQKeGWpzsNVWte3ZdUnSBJ1q6N8FvBZYDxwG3t/qo+bp6yT1kZJsTTKdZHpmZuYUhyhJmu2UQr+qnq2q41X1Y+DDwMa26yCwZqjpauBQq68eUT/R+2+vqg1VtWFqaupUhihJGuGUQr/N0b/gbcALd/bsArYkOSvJRQw+sN1dVYeBo0kub3ft3ADcfxrjliSdgpfO1SDJx4A3AucnOQj8DvDGJOsZTNEcAN4BUFV7k+wEngCOAbdU1fH2VjczuBPobOCz7SFJmqA5Q7+qrhtR/shJ2m8Dto2oTwOXzmt0kqQF5TdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNzhn6Su5McSfL4UO28JA8k+WZ7Pndo3+1J9id5MslVQ/XLkjzW9t3R1sqVJE3QOFf69wCbZtVuAx6sqnXAg+01SS4GtgCXtD53Jjmj9bkL2MpgsfR1I95TkrTI5gz9qvoS8P1Z5c3Ajra9A7hmqH5fVT1fVU8B+4GNSVYCK6rqoaoq4N6hPpKkCTnVOf0Lq+owQHu+oNVXAc8MtTvYaqva9uz6SEm2JplOMj0zM3OKQ5QkzbbQH+SOmqevk9RHqqrtVbWhqjZMTU0t2OAkqXenGvrPtikb2vORVj8IrBlqtxo41OqrR9QlSRN0qqG/C7ixbd8I3D9U35LkrCQXMfjAdnebAjqa5PJ2184NQ30kSRPy0rkaJPkY8Ebg/CQHgd8B3gfsTHIT8DRwLUBV7U2yE3gCOAbcUlXH21vdzOBOoLOBz7aHJGmC5gz9qrruBLuuPEH7bcC2EfVp4NJ5jU6StKD8Rq4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOnFfpJDiR5LMmeJNOtdl6SB5J8sz2fO9T+9iT7kzyZ5KrTHbwkaX4W4kr/71bV+qra0F7fBjxYVeuAB9trklwMbAEuATYBdyY5YwGOL0ka02JM72wGdrTtHcA1Q/X7qur5qnoK2A9sXITjS5JO4HRDv4A/SfJwkq2tdmFVHQZozxe0+irgmaG+B1vtRZJsTTKdZHpmZuY0hyhJesGcC6PP4YqqOpTkAuCBJN84SduMqNWohlW1HdgOsGHDhpFtJEnzd1pX+lV1qD0fAT7JYLrm2SQrAdrzkdb8ILBmqPtq4NDpHF+SND+nHPpJXpHknBe2gV8CHgd2ATe2ZjcC97ftXcCWJGcluQhYB+w+1eNLkubvdKZ3LgQ+meSF9/kvVfXHSb4G7ExyE/A0cC1AVe1NshN4AjgG3FJVx09r9JKkeTnl0K+qbwGvH1H/HnDlCfpsA7ad6jElSafHb+RKUkcMfUnqiKEvSR0x9CWpI4a+JHXkdL+Rq58ya2/79Cn3PfC+tyzgSCT9NPJKX5I6sqyv9L3qlZaH0/m/DP5/HuaVviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTioZ9kU5Ink+xPctukjy9JPZto6Cc5A/gPwC8DFwPXJbl4kmOQpJ5N+kp/I7C/qr5VVX8B3AdsnvAYJKlbqarJHSz5h8CmqvrH7fX1wN+sqnfOarcV2Npe/jzw5Cke8nzgu6fY92eV59yH3s65t/OF0z/n11TV1OzipH9lMyNqL/qrU1Xbge2nfbBkuqo2nO77/CzxnPvQ2zn3dr6weOc86emdg8CaodergUMTHoMkdWvSof81YF2Si5K8DNgC7JrwGCSpWxOd3qmqY0neCXwOOAO4u6r2LuIhT3uK6GeQ59yH3s65t/OFRTrniX6QK0laWn4jV5I6YuhLUkeWZej3+FMPSe5OciTJ40s9lklIsibJF5LsS7I3ya1LPabFluTlSXYn+Xo75/cu9ZgmJckZSf5Xkk8t9VgmIcmBJI8l2ZNkekHfe7nN6befevgz4O8xuEX0a8B1VfXEkg5skSX528BzwL1VdelSj2exJVkJrKyqR5KcAzwMXLOc/52TBHhFVT2X5Ezgy8CtVfWVJR7aokvyL4ANwIqqeutSj2exJTkAbKiqBf9C2nK80u/ypx6q6kvA95d6HJNSVYer6pG2fRTYB6xa2lEtrhp4rr08sz2W11XbCElWA28B/mCpx7IcLMfQXwU8M/T6IMs8DHqXZC3wBuCrSzyURdemOfYAR4AHqmrZnzPwIeC3gR8v8TgmqYA/SfJw+1maBbMcQ3+sn3rQ8pDklcDHgXdV1Y+WejyLraqOV9V6Bt9m35hkWU/lJXkrcKSqHl7qsUzYFVX1Cwx+kfiWNn27IJZj6PtTD51o89ofBz5aVZ9Y6vFMUlX9EPgisGlpR7LorgB+pc1x3we8KckfLu2QFl9VHWrPR4BPMpi2XhDLMfT9qYcOtA81PwLsq6oPLPV4JiHJVJJXte2zgTcD31jSQS2yqrq9qlZX1VoG/5c/X1VvX+JhLaokr2g3J5DkFcAvAQt2V96yC/2qOga88FMP+4Cdi/xTDz8VknwMeAj4+SQHk9y01GNaZFcA1zO48tvTHlcv9aAW2UrgC0keZXBx80BVdXELY2cuBL6c5OvAbuDTVfXHC/Xmy+6WTUnSiS27K31J0okZ+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj/x+Xg4qYnQnAzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOWklEQVR4nO3dcahe9X3H8ffHaLXMDpVcXUiyxcFlLClU5ZI5hOFmmZkdi39USGEuDEfYsGBhMJL+sdI/Av5VxmAywirLWFcJtM7g2m0hq5TBZnp1Wo0x8652ekkwt5bWyoYj2Xd/3CM83tyb5+Te5zH3+e39gss553d+5znfX37cT07O8zwnqSokSW256koXIEkaPcNdkhpkuEtSgwx3SWqQ4S5JDbr6ShcAsHHjxtq2bduVLkOSJspzzz33g6qaWm7fugj3bdu2MTs7e6XLkKSJkuQ/V9rnbRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQuviG6lpt2/93qz72+49+aoSVtM8/68mwlnkC5+pyrcffC6/cJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qFe5Jvp/kpSQvJJnt2m5KcizJa93yxoH+B5LMJTmd5N5xFS9JWt7lXLn/alXdVlUz3fZ+4HhVTQPHu22SbAf2ADuAXcBjSTaMsGZJ0hBruS2zGzjcrR8G7h9of6Kq3quq14E5YOcaziNJukx9w72Af0zyXJJ9XdstVXUWoFve3LVvBt4cOHa+a/uAJPuSzCaZXVhYWF31kqRl9X1w2F1VdSbJzcCxJK9eom+WaauLGqoOAYcAZmZmLtovSVq9XlfuVXWmW54DnmTxNstbSTYBdMtzXfd5YOvA4VuAM6MqWJI03NBwT/JTST72/jrw68DLwFFgb9dtL/BUt34U2JPk2iS3AtPAiVEXLklaWZ/bMrcATyZ5v//fVNXfJ/kOcCTJQ8AbwAMAVXUyyRHgFeA88HBVXRhL9ZKkZQ0N96r6HvCJZdrfBu5Z4ZiDwME1VydJWhW/oSpJDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIa1Dvck2xI8m9Jnu62b0pyLMlr3fLGgb4HkswlOZ3k3nEULkla2eVcuT8CnBrY3g8cr6pp4Hi3TZLtwB5gB7ALeCzJhtGUK0nqo1e4J9kCfAr4i4Hm3cDhbv0wcP9A+xNV9V5VvQ7MATtHUq0kqZe+V+5/AvwR8L8DbbdU1VmAbnlz174ZeHOg33zX9gFJ9iWZTTK7sLBwuXVLki5haLgn+U3gXFU91/M1s0xbXdRQdaiqZqpqZmpqqudLS5L6uLpHn7uA30pyH3Ad8NNJ/hp4K8mmqjqbZBNwrus/D2wdOH4LcGaURUuSLm3olXtVHaiqLVW1jcU3Sv+pqn4bOArs7brtBZ7q1o8Ce5Jcm+RWYBo4MfLKJUkr6nPlvpJHgSNJHgLeAB4AqKqTSY4ArwDngYer6sKaK5Uk9XZZ4V5VzwDPdOtvA/es0O8gcHCNtUmSVslvqEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGDQ33JNclOZHkxSQnk3yxa78pybEkr3XLGweOOZBkLsnpJPeOcwCSpIv1uXJ/D/i1qvoEcBuwK8mdwH7geFVNA8e7bZJsB/YAO4BdwGNJNoyhdknSCoaGey16t9u8pvspYDdwuGs/DNzfre8Gnqiq96rqdWAO2DnKoiVJl9brnnuSDUleAM4Bx6rqWeCWqjoL0C1v7rpvBt4cOHy+a1v6mvuSzCaZXVhYWMMQJElL9Qr3qrpQVbcBW4CdST5+ie5Z7iWWec1DVTVTVTNTU1O9ipUk9XNZn5apqh8Bz7B4L/2tJJsAuuW5rts8sHXgsC3AmbUWKknqr8+nZaaS3NCtfxT4JPAqcBTY23XbCzzVrR8F9iS5NsmtwDRwYsR1S5Iu4eoefTYBh7tPvFwFHKmqp5P8C3AkyUPAG8ADAFV1MskR4BXgPPBwVV0YT/mSpOUMDfeq+i5w+zLtbwP3rHDMQeDgmquTJK2K31CVpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoa7km2JvlWklNJTiZ5pGu/KcmxJK91yxsHjjmQZC7J6ST3jnMAkqSL9blyPw/8YVX9InAn8HCS7cB+4HhVTQPHu226fXuAHcAu4LEkG8ZRvCRpeUPDvarOVtXz3fpPgFPAZmA3cLjrdhi4v1vfDTxRVe9V1evAHLBzxHVLki7hsu65J9kG3A48C9xSVWdh8S8A4Oau22bgzYHD5ru2pa+1L8lsktmFhYVVlC5JWknvcE9yPfA14HNV9c6lui7TVhc1VB2qqpmqmpmamupbhiSph17hnuQaFoP9K1X19a75rSSbuv2bgHNd+zywdeDwLcCZ0ZQrSeqjz6dlAnwZOFVVXxrYdRTY263vBZ4aaN+T5NoktwLTwInRlSxJGubqHn3uAh4EXkryQtf2eeBR4EiSh4A3gAcAqupkkiPAKyx+0ubhqrow6sIlSSsbGu5V9c8sfx8d4J4VjjkIHFxDXZKkNfAbqpLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBQ8M9yeNJziV5eaDtpiTHkrzWLW8c2HcgyVyS00nuHVfhkqSV9bly/0tg15K2/cDxqpoGjnfbJNkO7AF2dMc8lmTDyKqVJPUyNNyr6tvAD5c07wYOd+uHgfsH2p+oqveq6nVgDtg5mlIlSX2t9p77LVV1FqBb3ty1bwbeHOg337VdJMm+JLNJZhcWFlZZhiRpOaN+QzXLtNVyHavqUFXNVNXM1NTUiMuQpP/fVhvubyXZBNAtz3Xt88DWgX5bgDOrL0+StBqrDfejwN5ufS/w1ED7niTXJrkVmAZOrK1ESdLlunpYhyRfBe4GNiaZB74APAocSfIQ8AbwAEBVnUxyBHgFOA88XFUXxlS7JGkFQ8O9qj6zwq57Vuh/EDi4lqIkSWvjN1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg8YW7kl2JTmdZC7J/nGdR5J0sbGEe5INwJ8BvwFsBz6TZPs4ziVJuti4rtx3AnNV9b2q+h/gCWD3mM4lSVoiVTX6F00+Deyqqt/rth8EfqmqPjvQZx+wr9v8BeD0Gk65EfjBGo5fL1oZBziW9aiVcYBjed/PVdXUcjuuXn09l5Rl2j7wt0hVHQIOjeRkyWxVzYzita6kVsYBjmU9amUc4Fj6GNdtmXlg68D2FuDMmM4lSVpiXOH+HWA6ya1JPgLsAY6O6VySpCXGclumqs4n+SzwD8AG4PGqOjmOc3VGcntnHWhlHOBY1qNWxgGOZaixvKEqSbqy/IaqJDXIcJekBk1MuA97nEEW/Wm3/7tJ7rgSdfbRYyx3J/lxkhe6nz++EnUOk+TxJOeSvLzC/kmak2FjmZQ52ZrkW0lOJTmZ5JFl+kzEvPQcy6TMy3VJTiR5sRvLF5fpM9p5qap1/8Pim7L/Afw88BHgRWD7kj73Ad9k8TP2dwLPXum61zCWu4Gnr3StPcbyK8AdwMsr7J+IOek5lkmZk03AHd36x4B/n+DflT5jmZR5CXB9t34N8Cxw5zjnZVKu3Ps8zmA38Fe16F+BG5Js+rAL7aGZRzNU1beBH16iy6TMSZ+xTISqOltVz3frPwFOAZuXdJuIeek5lonQ/Vm/221e0/0s/TTLSOdlUsJ9M/DmwPY8F09ynz7rQd86f7n7J9w3k+z4cEobuUmZk74mak6SbANuZ/EqcdDEzcslxgITMi9JNiR5ATgHHKuqsc7LuB4/MGpDH2fQs8960KfO51l8ZsS7Se4D/haYHndhYzApc9LHRM1JkuuBrwGfq6p3lu5e5pB1Oy9DxjIx81JVF4DbktwAPJnk41U1+B7PSOdlUq7c+zzOYFIeeTC0zqp65/1/wlXVN4Brkmz88EocmUmZk6EmaU6SXMNiGH6lqr6+TJeJmZdhY5mkeXlfVf0IeAbYtWTXSOdlUsK9z+MMjgK/073jfCfw46o6+2EX2sPQsST5mSTp1neyOE9vf+iVrt2kzMlQkzInXY1fBk5V1ZdW6DYR89JnLBM0L1PdFTtJPgp8Enh1SbeRzstE3JapFR5nkOT3u/1/DnyDxXeb54D/An73StV7KT3H8mngD5KcB/4b2FPd2+nrSZKvsvhphY1J5oEvsPhG0UTNCfQay0TMCXAX8CDwUnd/F+DzwM/CxM1Ln7FMyrxsAg5n8T8yugo4UlVPjzPDfPyAJDVoUm7LSJIug+EuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGvR/82ENSdA3DKAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.0, 0.0), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABdeElEQVR4nO29S4xlXXbX+Vt7n3PvjcjMr8r1sF22aTCSW2rTE9wIaIEQEkJtLCQzoYWREC1ZYgJqkBhQ4AEjS4aBhwwsYdEDsLEEUntgyQILhJCANo0M+IHtst3YBUXZVd8jMyPi3nvO3qsHe6991jn33Ij4qvKrvIlzSZERee957LPP2uvxX48tqspbekuPofC6B/CW3hx6yyxv6dH0llne0qPpLbO8pUfTW2Z5S4+mt8zylh5NHxmziMh3isgvisjnROSzH9V93tLXjuSjwFlEJAK/BPxx4PPATwPfo6o//8pv9pa+ZvRRSZbfD3xOVX9VVY/AjwLf/RHd6y19jaj7iK77zcBvuP9/HvgD5w7eyE6vwlNQxeSciIDU/3jhZ5+3z7T8bRJy+b1q+eyxJPUfu8ZS8kr7Z35/mI9heZinDyPMl3Oycvt2b/+ca+fZuWqHqPtP+fWC976kqp9eG8pHxSxr0zSbIhH5C8BfANjJE/7g9Z8sD5hS+b7rkBjLiSm1yZC+gxghK2iGXM+p5xEjEmOZiJQgZwihTGQI5f/nVK9IuWe9LymV6+Rc/h9CmXj73u5/5l6ywqQztW/XXSMba0poKsfZs/vrtnvX8TeqYxd7bjcmO0dTavOouYzrn6Z/+J/PDemjYpbPA7/D/f9bgP/qD1DVHwJ+COBj8VNtBtvDnaOsIB9iaZ67lmOAemMIoUzkOM6PcedoCIhmkMV1Y5xelr9nkDbuGeOt0VIyhVA+WzCYLp9p7b6srFh/ri0MyagKInXxpfPD+6iY5aeBbxORbwX+C/BngD/74Fl+1dqLW5CqlpdlFOTkAc8a7SZVcp5WnUkDm8S6mqfrP2zWtZVukvBEdQUgnUqf+aBXr6kicwm1kHT2+0SK9fXVLpl0eW6MSEooCYgwnH/Oj4RZVHUUkb8E/GQZAT+sqj/3qJOXL2dtJVaR2VbtORKZdLmJ9XPHUV5QMVO03CPIfHXKJCUIK9cKMo3tVVAQUDcf3i4xBrE5kFAkw2y8AUJGMkWiuOdv52ZFH2nTfVSSBVX9CeAnPuRJZfWtrTpbkaYKxL0YE9dGy9VjFEKTGs32qMf4lSkiaPAGpaweV67pGMhWcR2DhgBJEamfm6pak1ZnbCnp6ity9pHEUMce3RxlUJnZck3kBkEoC2F1wbQx38/oHxmzfMXkX/TyxeQ8E/WzFxfj6kScGINLgy+a+tGZ+lgyxWwV1+OXku3EJqkMoN7g7buTl3JWbUoAY4yskBOSUlEdfT+p7JxBQzGE8zgfSwiIxCYlzzKMMdk9dHnM4mkhdttnVfromq72560Zg04ci1MrbXLvAylN1NsKd3M780rKxadr2ed9Nz++HSpVClWpFyP0XWGIrivnqcKYivFdP1P7HJAxIccB1Qzj4qVrhjyfB1Ut0sY/79JoX9DlMIs6lbL8fOn2mkq67zr2glIqxzqX0xhIVWFIM6MXnEeWc7uXN6ztOK9yVg1P+679XaTRjLGiUyVGMSKbDWw36KZHN30Z1zCW82JEtx3aTx6YDInw4g6Ow/QMTfI4leTJSTiJ6w6Fp8thljVynorEUEToMNRVnecTvSJaGz4Bzssq+tvjMw13yFpxljMGn1cfSym0JtrXpF6eqypTb5JBbYybHnZb9GpL3vbkXXlN4RiRMaN9JG/Kj1ng4ZCQ44jcBnRgHQsy8ovFxhnCWQzR6HKYxSZ2+VDeA/G0pl4W1xPqhDQX857pkIDExTGPcJvL2Lwr79Sl/38I5doSCrgmUozXroOuvNgGWG968q5HNx1p15G31U7bBGRUch8YryLaCZKUMCoyZDTGcs3tdpKidSFIygV8S3kume/DfRZ0OcziXbsVLGJmsLnjvE3SjFAp+l/t2HO2Tb1vkTxM916OYY1plmjwmsu/NKQlQNch202xOTY92ld1EgUNZfwaBETIfSBtA3kT0ABhCEhS8iYwXgVSD91e6W/qmKPApnfzIEi1dXQcYRyRrKdG7iODyZfBLDJhHKu0FPn3rXipK5gKNj3AKM1VP2dnuM+9TTOzV4zMI1OdqcoS5yrjkhhg05Ovd+hVT9pG8jaSO0G7yigKkhUExl0gbcr446BIgrQRxp2Q+wJmx6OgfSBvO2TcFoM4hmbkM4zIEOEgVfWen7776DKYxch5BOde2IkR6Y7TEIprSSwi1wzX6YBp1QWB1CJq66jtWhzG/X4sqWoZlwradeXeXSBtI8OznvFJYYjcCTlCSCB1bGkj5A1IhrwXukNhIlGQBLkTDu8E0ran30a6TUSSolEgCjJkwjER7obyLOMIwzgf4OsG5b5i8rrWUNSF99NWrYl55wGpwdemtjwTqBYbYYl2ek/IAWczbCXrqURZY5rK8DNp5gG6Oh6NQtpFhqehvOwqKXIHYYQwFAmTN5D78hkoIQEKYVTQck56IgxZSBth0xc7JveCRiEcle4u0XWBmDOyP5zCCY+ky2MWTxUf0HA+2n/ywmaS5BGxpTVaqq7HQPizNACn1hzmU5i7uPF505GuIsengePHhXEHeatohHAQur1JjvKDQI6QozQVpQHSBtKuus8ZILTzNArxqC0tQQ6JuN0gw1DjVB9OUl4esyxBorAIpK25eP7lmFpas3EMqPPH2TXt/16ywaSq1ug+43ftHl2HVE8nXXUMTwLHd4TDx5V0rYVZgtI9j4RBiGNhmKgQjlU9ZSV3QtpC2pawRLFxCjON2+KG2yTlWFSZjIF41RGud4XZhgGtTCN5BX1eoctilgqCtRe1ZBSYr2DniUjXFTsFTu2PJTMNw/SdXXMNHFuqmgXTkvP8M+95VW/EcnEkFk+I7YZ81TNeR45PheEZDB/P6JNE3CYQJaUtehOROxpKHAaQsdgquYPxiTBeVZV1KEylQUg7LWoqARkkV/W2FdKuIzzZErIih+Dg//Fh9JqLYZZ1JdMYxb9U+797UVPAL5Sgnbdj7qNlHOrD6vJzx5v7HkEMW3n6BH16xfDxa/Zfv+Xuk5H9J4XjxxR9OtJfDWw2CVW47TdVjVCkhtLUTo7l5Y87SDtFD4KM1QQTyCKIgqZi12RnCOdN9ZiGHnIuiPA40tI8HjB0L4RZmMLxPioMc7xlllS0DAuseE/nxKpFr82I1orcLsMIy7SGpVd2n1sO1X0tuEr+xDOOn7rm7lM9d58K7D8Jx49n8rPUGCWGTFaBqGhXjNSW9SjlpSOQN4J2xfCVGjsULdOn5imFImlAkSToUJgv94HQR6SLLQyCasFf3gzJcobOMQorqQIWBa7QOZzmcJxcJ6UpyCbz8EHzfM5KD89US1VZsZ4a49HrHcdPXnPzmZ7brw/sP6UMn0jEZwO7zchuMxBqfsyQIgQtxmxHkyyqVLxFm9ekEXJUQpRmBGvlL4mFuUSFMGiRSl11zzcR2Uck1jRLmc/bObo8ZvHG5hrV71bBNscwJx7MOQlxhh7jIax6TEEgFhtKn16Tnl0zfN2Om2/quflMYP9pZfzEwJNP3PF0dyCGTBRlyIHD0DEM5ZVorwVfGQUZmCRMfalxX+wRqUaw5HJMqBLGGAqFeKhMFATtAhoDdOH+cMoKXQiz6EmA7cEEIZ8E9ahbuKDZGjNaauV9kmiZuN0+n0S4UGM+uy35Y9ccPn3N3aciN98UuP3GDJ8+8HUfu+Ubn73gaX8gq3DMHe/vr7jJgWGIkIXcK2kjdJURipQApBi7fQK9xQWUirFrGEvaQNpOXpLZPBopSHEUNAbE41fpfmj3QpjlzEpeehrn0gDWyCO0D9FSjSwTp/39zzBnU1vV49HrHeM7W/afiNx9uqgevv7AN37qA77h+gXfcv0+vSQOuecmbTiMHe+zQ5NAUHSj5KTkAaIZrVJ4Q5ISD0CuqqiTykRKGGhcpVHJsYQRcoQQtEiXcGrMP0aSXgyz3BsbgvWAnWEmS7TVX/MeY1dECuIrcuppefISzYcQfKpl3xUc5WpHfuea8Z0dx491DNcFcEtPMs+e7vnMk+d83eaOqzgQKffJCE/6A892G1IKDF3HOESyRFKOSBZUijphnFROc6vrzDV8RSEei9oJFaAbr6v9UqIOxRuyAGN6XOT5YphlldZetE9lmAFvK/YLzq6wWiIPvhnA1/R29XB8lv5C9c3IJI1lte22hVE+dsXx4z2HZ5HxSshb0KvEp57e8E1XH7ANI70Ukb8NI0GUr9t0jBoR4G7TsT/2HOKGVC1W0VAhf4r9YsIvT0yiUt1tCojX50n1jNdSAL5jnavMlHn3EKJd6bKZxdPSK1q+xEfYLk3SLBjmwVoeH6z06ZCWgCUB+h693pGebBmedRyfBsZrSDtIGyVuEx/b3PHJ/oZYRULSQBYhqLKNI7s4cNUPJBXGFBliInUR7QK50+oKU6RHqPxij23MEsrfZvTm6mJrpBUKSKqxtXGcwf4P0WUwizJVHcJkpftMuQUzLNWLLlHbJdTuALzZuWvhAaMzTDK7TyjIrF5tyU+2pOuu5KCYm9qBdhBCZhMTvST6KlmyBva550BHUuF23PDyuOH2sOE4dKQhwjEQDkIYqt0SIasgoSC12k1uM1XK+Bo8McM3F6kSj5lwzMhoNuAjit8qXQazwL2Ia0scgiYyW0WPg9Qbcy2tepMA1aWeRY/t3l7aLNI1z2ffl/wU2fTkTU+67hivYo34mucBuVc2XSagRMnsZGQbhgLAUbCVMUfuxp7bw4bDsWMcOvIhEg6hxIVqhEID0IHWc0uMCAjlmHgoU9QYRsvnYVTiQQkHLYwyTmWxj4H64VKYxb/o+1TDIp90mQZwQs5zWmbj2+dNhS1TIO5Ray2h+4xHZuogbWC8UvJ1ou9HRg18MF6RYiAhZA28SDtepi13qWdIkZQD49CR9gU4C0chjDLZKTKhtC2vpRq8mMdkKopiu8SD0t0p/W2m2yfCcWyAZAMe3xTJ0nhaav2tBeEs2SjlEvNxCdZNnZyLHy0nIaWp8m4tbLCo91H3vaxl/vvEqJSQYSTcjYRd9V5CSVoan2a6ZwPbLnE7bvjC4WNcxYGrcATgLm+4Sz0vx20xZoE8CrKPxNtQJIqTEjNVkyEmD7pRbJkw2SfhAN2dsv0g09+MxJuBsB+RYZzbX2uB1AVdBLPMyIJwIRecxGwSKC98HEvFv8Hp9vIeMoDb9cNqXOesi21eU67JzrHeNzpJlEpQLhxG4qFHciwo6gb0OvHOs1uu+oEhRd47XHMTRzZhB8AxR/ap52bYkGptj6ZA3EtBadMEw4vOJYlkLVHnsTBL2pbclga+CYgq3V7ZvBiItyNyGEq5yDgZtmuwwxpdHrO4OuKZMQqntgjMVNfMFlmrBZrdZyXZmzNM0xjxTGQ6JzgOyG0gXvWE1DUQjQC7fuRTVy/5pqvnvNPd8TJtuRm3fDDseHHc8XLYVBVUuhlIl8k7JaWS6RZUit0xQNxrCSbGisKqNs9X45RtZ5JF63E5BkKU1fj+qopeoYtgFoHyErSqGpcrazU+UF+kdSSQMIFplj1PWXXmGbWoMsxrl418X5MYZ/ebMemyqsBTLeDSwxFJiXC1IRy3eIQxivIt1+/zh9/5JT4ZX/LLh2/kP919hveOV7wcNrx/c1WOi0Xddpux5KuESHwZmxtcUiQrM+yU1AuGwqnQUjCbd6QVkNsKaRcIQyR4dd1SRqvafcDI/cqSMV81LROaZqs6TMFBYyD7/4pbfZJ1r5OUWpaXwMJll1NJM6Nl4M3wiaxwOKD7A7IfCMfc1AOjMOTAVRz4H7p3+d3dB3y6e8E2jIwauTv2HA49x0PPOBa7oesycZNgm8nbjMYiTSRDSDUPF4p9UuF+iwflvnhfubc0B0u9DOQ+nO+YYO1A7qGLkCzAvMjbr2rXOEdq7oXCHKKHOV5QjdsZOuvJGb5N/Ximadd08SDnMc0KuByJFtsl7hP9TaZ/GTk+j3z5/af8++038yzu+ZbNu3z++AneH64Yc0nWCiGDCjkLw1Cg/jQGSCXmkzcl2Sk8kTmGUvNYmpcUKqN0Np4JlLP4kfikJw/GvTEGbsUryLrAOHQSlaH6jN4L8XbKWqCxVujN2ni5ZkGNMZ3HBLjST+fSe6N20Vpjup7CmIh3A/3Lns0HgeFpYH+141flk+zHnq+/flEvIaQciEHpukxKgZyFnAP5GNFjgLF6RxstsZ1cck/CUBDaMJhEhCxSpQxoX1WvpVT2UhBgoWAsh2ONCaU29sdE7y+DWYxW81CqwZvdql6ugiWjlAPbsWJu89I4dQzT+quFWsUH7TOBCdYHWpXWGWxHDon+dqS7i8Q7Id4Ehs2W/xaecTv07LqRbTcy5kAMmW0/MoTIOIbyKFkgCTKWBWKGcnnxziOiBAs1Vntlo+RNrRAYISQD6opHFPfFENdhgHE8Nf4f8Igug1lUK5ayPtiCNGaXCB1OALpGxhAhzKSSMDHYbJJiRFNq/dUa5Xr91gZjbgOdpSrm5ZhLDbJ5RZVSFvZjxzFFRJQoypPtkbEmPx2Csh8DegjFbU7FfTZ0NowU8C0676cvbvN4BXmXy/GHwOa5sPuS8uSLid0X74gf3CEvb4tUcYtyapl2v4F7GcwCU4HYzEqfd4LUvkckltzWBJDmeMrCjQYmm6evk6F57iZWRlJrzeFJc0E5E4h2tX2He/MnMSjBqhvDcWwFYQaQlDxy4TBEsgp9TDzdHnm6OTDkSBeKN3Q89CV/OkEYpcD9R2kZ/kBTOWkDeaukHeTrDNsMd5HuTti+Wxjl+jdeEv7bl9HDkWzqZ1Gp8EblswAN+2gv01pRPKRPFw++KqGc57KKvawlZ9eOSxKkYTatxsYf567VALq7gbjPxGMkjEI6Bg53PSkZEwupLzZLECWrMOZAymFmZys0kC13Tv1ESp6uMcpGYRTkZUdvEuU3i0QJ770gv//BPIwx8+zy6UJZoQeZRUR+GPiTwG+q6v9cP/sE8A+B3wX8f8D/rqrv1e/+OvC9lLX/f6rqTz44Ck/LdElb+faZtd9UZ1hyj951Kguc11MBv7mUWSRRDTKf3EXS0+y+bfwJDkdEhO7uKXHfEe8EJZKGwLDtStVYNWpTCtwdS7OeIpQCOoYaMgB6JdWqxFhVz2SvVBW0yyDQvQj0L4SrLylPvpC4+sIN8b0b9OZ2GuvM09R1b/EMPQZn+XvAdy4++yzwU6r6bcBP1f8jIt9OaWP6e+o5f0dKH//H0Ro877sPBKdarFWXeVLW7FjC/AfasTqOkyG8gvA2RqnXks2mdGByTZS9IX3CoLmoID0c0bs98Wagv1W6W+hfCNv3Av17kfAywj6S95Hjbc/tiy13Nxv2txuO+x4dQrNLcq8Fzb1SxitIV0raacvu1wDalWfoXwhP/qvy7NcHrn/9OeHzv0X+rS+jt3eTNFxCDj4+9ADjPChZVPVfiMjvWnz83cAfrX//X8A/B/5a/fxHVfUA/JqIfI7Sx/9fPXSfWQDw3KAXRpn9lkzxmNbae9qxvgftApybOlHLDJhr5N3sNpbTrpnlPqU7JaMQ9gOb55ndrgBm2pVWGeOT8pN2gnalTEMBqyAkKtpn2NgAgSBoCqU7glIK2LRElfsPitd19SXl+ksj2y/vCS/u0Lt9q76cBQotjcMW5bn+vQv6Sm2Wb1DVL9QbfEFEvr5+/s3Av3bHfb5+9jD5dAEj/+KWno8DzJo6CXnGUM0V90lOMAcAGwPJKcZzzvNZVBi0a9nXISBZkbsj2/eOwIZcW2oMV8JhkFLGkUJxd3slDEI4Fu9neKbwsYG4SeQk5DHAIU56QEBDKR7rbksRff9cuf5SYvtbe8IHt3C3n9IP1qLta8/zAL1qA3c1TrV64KJ3/0nKo6f7quVMnSztj5XzmtoyY9TOhxPG9HaOG/Qp8LccC1RGzsjdge79OyRlchfIm0h3HZGahC2jlBrkrRKOJcocUnGDtcvsro4cjx0jkAeLDNIkkIyweQG7dzPb9xLb9w7E926Ke3w4TovvTEhkKl/hQagfvnJm+aKIfKZKlc8Av1k/f7Bnvxvw1Lu/K737fZRZVVsPff+C2/nGWGur/Aydg+nJuaKZcZVJThjZScFZtNpHxbOiw4Dc3BHHRKg94sLQIVnp9qXV17gr3RCguMoaS/rjcSjGr2ZpIF1hKCl4ywDxDnbvZa6+PLJ5d0+4OSD7Izqmyb46nfdTm+tVeUNn6MeBPw/8QP39f7vP/4GI/CDwTcC3Af/Po6+67CRpK8BD7RYnSum0D8vain9knZF1Z9KF5GhhiJOxBgcAnrr4qoocDgXwOxyRY+k+KUMJB2y7QN5GxquO8SpUSB6GayEcBIbAOFaGSQEZKzC3h/4ldLfK5mVm9+7I5rduCM9vYRhb7zjfg29GS1xqLYf5DD3Gdf4RijH7KRH5PPA3KUzyYyLyvcCvA3+6TtDPiciPAT8PjMBfVNXHdzDz5aepPEALGFoebvOIauRsuX3KOUh/dp91mP5EWrVryGnB2rkApSNNGdKxBu1ySXE4dgQziPuO7mpDuir9WtIuICkyvBDG9zuGqqrCsdgmm+dC/1zZvCiMsnk+0r+3J7z3Er29nZha8+oznlPlr6yZj6p+z5mv/tiZ478f+P5H3b2dRENQzybhLGJEs3JXr5fX4hvLYKP7fBlNXrWb1mwXmHAXvT8aTdYSk8m1T4uNt3ok3ZAI+46464iHDg09MgbG664lPXV7pbvN9HdKd5PoX47Elwfkpno8bdG4cEe7/0o3LJMoj2QUuCQE1xmq7f/uu5mLTH1IYwBTUzKphUaVySQ7S9uL54cirnk+jrUxW9dr86aWxmSzv47HYoh3pZukpA4ZE+yPxL4j3ETiTU84XrF52ZO2pQNCPChhyIQxI8dMvBuQmz1yd0CHoaiedsNqj1SmPIENZsPXSZo/gi6HWRxZG4iGDZzFXXzgTyesZcXGWIPo7z3GM+9qiuUZz23tWTxTpYTmjPRdzektTC7DgMSIHI70YyYctmgfS5v1Y4Ixl2h4zsj+CPsDejze7+I/hiqjvFkpCgsX795sc791TFUtBojBNHEztbDchGFR+nE2wdvlfKyqLEdn832XpIoOY1VFNfKdpaiTUEpY45jqDiK5JFerli7ZOU9pBmtG6ZoxvpbW4dNEl5URZ+hymGURD5rhHo9ZOS2NcvICWg7LAq1cMqJ5Xrpwfdu9JUw5LdBso7bpk93f7nuu7siY25en1Oy/iQbIxYM62ZugpnKQU/F8PPJcx3y2cnI5V37BrOX6rNDlMAtuZTrb5AR+9ytnbcXASaqCWm/bc8ZcZaiZJGp6PE5JVG6ziPmY8kziLKlJoWUi+XKnNGOII5MtITKFMWr/fZ9C6sHI1TLUJYywNPDrMW+WGvJwOTgP49QAayvKMcpJPqyL9ZQNMM94Kh4jsRduL3/Ny1qU0foXcG/9TXKo6RL0q1iON8D1eDy5d9ukwcfPvKdTbuAnYc48Lng6Mf49XuSCLodZjOxh/QZUmRbnmRuq06pbJjTNPKMcykYO544LeUqAgjmjeC/Lk5WknBnP/LjC9POXmoHaBNCK6gw3cnslFTUVpjl5CGT0Xs8yKGtR9qxFfPf9bFE9RJfHLEZLNDQ7qWDiF2EtvbIwRGrgXjlnBWhbTpD3cNq9lNXNMxfjnGXmufQJ89DmyLAARa09xhi+l/JKaa377sQuSammeyzjWh9dbOjVk62CZfaafX2P5b/U1WrnetS1TtzJSjLJ47EYB/atBigdnXhtLhVUY5wY2o852J7KZxjFM+0sCn/KDDNPr+um460mfIGvtPEuU1htbPfQ5TDLkpahf5+8g310z8Pdh4EsI8znaAkU2un35NvMGLd5HTK9WFM/4NSFC16uGOKzYOVaOKINV0/C/u2zc3P1yEY+cEnMcl/OhQ+KnQm3N1pzE72n4ldSM1KdhLJrzAC/PGNWn/Z51lV19y0xIgPkanKU5Hsl1zxftqq6zGn0OyVnF5kqfAQDrKSIPkSXwyxwPkJqFnw7bsIVGkK64rra56veTD3/tNzVMYz9f5nI7a+/Qt5TUzd+NbRVBVVp28ms1kFlnXZDaQHUdMIwxS135z7Cxpk9y9o+2mfospjF00qerMWByt8LaVDprGoy3GLNkDunstaYxE3sDMpfHucL5mburI9hhTZmy9nREMrxa8HINfJI7Mq4ZypzETx8cxHcNVp7iT6PxdFJDMalEzabI83heF1KEW9reKrXOgHllmGHdrwZ0GmG2Nr5047vS2+s7uyeFwV3j9gfqZFnlBXAcpk5CA5XekAqXQ6z+JiLW1mrWW1Ga3aOQeLNdQ0ncaGzGXVLlLWOpzGKn/zHBu/UqTrbEmcRcli1u5zEOlutuQxqLoOl3lhezeFxgOcjsuUuh1mYVrxFnZefN3ooxWANjXxkxtwJbgLzl+C3u/2Q1Jj3nHe1th3xGY/snDRZuWn5bbt/zL575JxUuihmAaYXrWWizm76sIhSz79a19P35624+/jgnj/f0i6HoTCMSYplaABozf3OjGlt34ETY3v64mSX2BNVulSpns4Z/77K4RF0McxyVsyueUgr58yKqBY2xKzGyJ27+nLWVmye3FxSaQmqY8k/KcDbgs4lFK2kNMxe+jkMxdsUD6mLEym7EvuxeJmN1bqPP0AXwyxtsOnM6lrhfpvsE4/EBxmNQRbnnyRDrUSOZ6J/9rchsOelg8yG45je7Agf+/L4iA9iLu/tg4feKJ+pST1lGKPmQY7r3z9AF8MswMxdflAnewN4AZrdB8zdC7FzPlrd7mUu7lqnTGcXLasETu5tY8or2XiLMZ3zCh+kZdTamHTJkI9MUXg8fPdRU0vuSaeSYknLlXeGpvrlcBp38ve9914LQ9tQZHN9zzGnj+3YzwLQm3UGt/POPa8nO+e+sS8ZIE8S+16Vdw9dlmQxylr6zS5dQ/vbpw9AEee+qKqunJlNYn+7SO/MODSXWYTSLVNPpEOb/DPBzlWpuFy9Mc4TrD2thRnuOabZPN5OW6ZMwNzLCq7X3n3o9gpdFrPkzCpSudT5zCH1kldyZqIfWj1rCKzFTVw3b6Ax2rJ/7ix1ojLezE5xjIKpthY4zOvZfmZ7+PEvsZms0MfWlVzdc5/1tJZM4hnkjWgTBs71c9LE6+y11exbi54jm3CvFvzLWXpPjwCnli9CZNpY66zuX76Icy9mTcX6sXvwcm2YZsvVfsKr17f735eyukIXwSxNf1dqqOmaV+LOabgHrEP07vPmpVi4wKuF5Sr2kuRe/MIDdy7wuGobaclMkEXk/IzKamPPecJ0lmqxjnG6R57F0k68MP/cNn7XBOAh9/kimAVo2MQayulFvbc3Zrp/bfLt/8aIFsn10d4YEdtp/rEoL5zoeYncv20fzEME94BnYJJjPvbSNdkZ616dZJ1CC2Z/1Wdc5iuf0KK2/BxdDrOE004JRqscv6Y2FpN99twVWqsDOrm+H0+ap022a5wB+qa6JuZejB/7Odf7ITopr51UzdJ2OjnPkOY3BZQTFiJzjWzifNNjXxR/H1YgBqLNX5CpsrP3KgedxX1mTYSW53uV5o3t7JKmrRRk6VEZtK86qRtP3qA+Exs7l7b52Ez+NboIZmlG59lclNxsgQdjPPfRCjh2bxXhMu7ijm9M9Ig25sbYrTEz0NIrDeS7L/TwyECft/PaAoHFloDVsK1tW2cNBh6QZJfBLDDHCZYGmJGPwC5X/7ku2h4VhnkeiY8f+RXqd2pdrOJGzbaaJn+m8pY2lJNu5biVzP61dIxzAOQScYapkqFdbk19K76dWlNBOa8f7+gymEWczbDECUIByeZdleYM0Lpgr3kTi4kQd5xhKcu4TzlOpu9hFY+ZGY6Jhw1kU5c2NjPkXf+Xe0tf22VWMBJ1wcYz47Xrn1Q9eNV4D10Gs6y2ooOTgi1YIJSn0WVZTtIaeYn1WBF/7jpGefGS1ryyduxCYriXdMIoD9lyMJ8jDwHoginWxjIb4xsByj1gia95Dx4LgfP2Tl39YuothPZypKqbVaPPl2YYrXkTzDPrsLH4RGgPpC2xnOW57jOfDOYrMifpcGZReIZZ4i5rOSxrRvQKXQizVLonuDXLF612x0k+i/c6VnCXVa9h2dN5CVB522AxlhORXg6chwkWDX5m6nYpBZdost2/eYB52jdyOU9rcTQXftCsZWf72bPqtJge4TRcFrO4SSwqJTewaXZM+3t6wJOamnrssvZmRo+o74WF/eSit8ApYy5VoGcsIw/fL5/JyBjOSagTA3oROD0ZR9s+J3Bi/a6o8IfowdkSkd8hIv9MRH5BRH5ORP5y/fwTIvJPROSX6++vc+f8dRH5nIj8ooj8bw+OApkDSBWb0GGs7ubS6F3EhmC+Ovzxy0RrCfPPziReP5iC6YGsJTM3ZNXbJbmopmE4Od5SKSSG1t7Lnk+HkXyoXZ6WXcI9lrKQVGJJ4bWFmvTdTLrNkHIb2wM5Mo9ZWiPwV1X1fwL+IPAXpfTof7X9+61HvzfoTISuuKQnSc7W5XL2dOc+i7MXctL1YK111lpPlnM5JWuMUuNfs2RvD9cbA3vXXnM5x1qWuvs0xvJezkyqVvXSd1M1waL8pKV4mMf41TKLqn5BVf9d/fsF8AuUFuvfTenbT/39p+rf303t36+qvwZY//5XRjO10DAGB3jZipEFRlE3YmAcS2OcyoitvNQDVfM5qODVwpbxPzAxjr24xeSb9JhLH+cCu9U9SYXTF31qe4X5IrNndd+vGtQfos4ZPqTNImXDh98L/Btedf/+tiWMzO0Pg/UtIaoeUwYUJoPUvxivktzEnHgRXmdDixyvqaCTVbeM9rYD9XQLPQv8LV946w8jLRHc2zPSddMzOBWsutiH+Vyg0DO+zacLVczOf4RH9GhmEZGnwD8C/oqqPr9Hp699ccLC4nv3hydr95u2pBOhtcJprvP0sLNAWNPHbhgrXZpWsZBFEvVqpt1KHOrEu6nMK9R41NJ7W61r0hYSaFLlnsDqasL54pgW7Dy9wLrqfBUIroj0FEb5+6r6j+vHX1X/fl327l9teV43pgLkRMSmuXditkzNGis3yRVZXQnBPwKMawzj1Uxw6Zx5nhKwHLeVqa71cJko1QZSDgtZXOvsufeFMur3uvas55DuB+gx3pAAfxf4BVX9QffVj1P69sNp//4/IyJbEflWHtO/X2mJ2m3VmIh2RprvTGC2hrchGqP4TalSnidGew/pzMpdW40iUjwKaxvmN6xaGLp27GzsK9LEkr6W2+aejGtWTF8lpw9lpDzZXUaLSPcJdnNflP4MPUay/CHgzwH/UUR+pn72N3jV/fuXqKcIbsPUU7UA9wNuOA/hvhCAF79VxSzjMyeGqO9vd09mmtUW+e5SJxK0MsxaikYpijuVLF4FFRVsz7ZY+961vw/TeSQ9pnf/v+Rs8OYV9e9fCXo1UA7mht/SNlm9ngu9e+9lxYvw5BlyVS3YS6mqbebKLt1l5/62/QZgZpy3/abngyi/PXC31tOu2ikitXXryvOcXNNJk2Uq6zkp6+miENypKjEhMO+17xlqDcW1kHvKp+fYS1xhjtk1qEiwVwlLWrF/zjJKe7B8okoKxfNAmGPas906YZ4nbOppGtj091LtrBjtD9FFMIsyh5uXSOyJ5FkmHS17ua2dc45OambmKQnNQFzGgNr5i8Cmm/SZi2vplycg4aIeyXs2C4xoNdVyGcBc0prBvAxZrIUpVugimAUA215l1gNl8VALspex6pHcQ7J4CcC6qHfo8Yn0eChDzvAMKAyRmWyw5b5AxmDL345OotruHqvkUXALn4TaDta1fP0wc3chzGJGWmiuqabcdg5dP8XZFr6ZMJyuFJNEa9hKO+Y04KdaYjMSBLX+/TUmcy6t4STDDlybUz/+hbG7DGOsjtF5QGso7Eyd5lOGuee5H9Oc6Cs3jV8lzebsQ7hzi6jpWh7rLGf2w5Ix0Jn+dcDcFvmQ117LbQGmMMZDidVLz6qGJFbpK3n+BcljSyU+ShKR3wJugC+97rF8CPoU/32O93eq6qfXvrgIZgEQkX+rqr/vdY/jsfTbcbyXoYbe0htBb5nlLT2aLolZfuh1D+BD0m+78V6MzfKWLp8uSbK8pQunt8zylh5Nr51ZROQ7axXA50Tks697PAAi8sMi8psi8rPus1dYzfDKx/s1qMDAJRK9hh9KtORXgN8NbIB/D3z76xxTHdcfAb4D+Fn32d8GPlv//izwt+rf317HvQW+tT5P/BqP9zPAd9S/nwG/VMf1Ssf8uiXL7wc+p6q/qqpH4Ecp1QGvlVT1XwDvLj7+bl5TNcNDpF+jCozXzSzfDPyG+//jKgFeD82qGQBfzXAxz3BfBQZf5ZhfN7M8qhLgwulinmFZgXHfoSufPTjm180sj6oEuBD6Yq1i4CupZvio6b4KjPr9Vz3m180sPw18m4h8q4hsKGWvP/6ax3SOXl01wyumr0kFBrxeb6ha5t9Fsd5/Bfi+1z2eOqYfAb4ADJRV+L3AJyk13b9cf3/CHf99dfy/CPyJ1zDeP0xRI/8B+Jn6812vesxv4f639Gj6yNTQJYJtb+mro49EstQWG78E/HGKGP9p4HtU9edf+c3e0teMPirJcpFg21v66uijyu5fA33+gD/Ad1GI0v8vTzafpGT5qxUSMbn+Bguo9caY2liIlO89cqCLP3T2oR/F4rrLj+vns/JVS4rW6UDxJ7oxKiVrXnE9Y6A1EGqXkOl3u6/Ok6zF3cs/93Lg4p/Jz+WSdMr6b71s4Pn+v31Jz+TgflTM8iDoo76LwtVn9H/9nX8eGWszm/q7beRg2fOaSyOerLDpkb6HrgPXAelkoq3MdFmBNx9M8RDcZhOqtcQkZeg65HqHbjdljMehjMOK7JdlIdYoaEwwHMu+zNdXyNWujPNuj+4P5Xm6WkDvmgLpcZjO83VKtWuVdLGc19XyFqvC7CLa1YaHVvJrc+l2i1XVMv5hLPN4dYVebUGEn/xPP/Cfz73Uj4pZPhzoo1peQl7W0izKPddqoVTLhOh4f10QTIVhtRh+1r3goXKOZe00oJUJrZdMa4AM9e88tV7NdZwkdBjaQhAplYpKnlZYkMKg0dX7SJiK8Iw5KzO05/A7gIiTMms1TnYfXyX5mvrgNrAN+C8UsO3Pnj1amV6EF5m+t5tIqd/xnRdtJQ5DWe1qjXPiybmtmtAmzqTYcaD1dAtTS4/TMSriOmd6yaO2YUNTifXeIaBhanSow1CY5jhMBXTtRcbCMCIzKdPIGCC6z63NxnFoPecUSpuP7bZIXms+tHwmcYziGese+kiYRVVHEflLwE9S0hB+WFV/7p4T6qpjqlEW9wKstVZUNJeJRVwbsXEs3RxhOieXFmIqrvlOsz2myVGtbS2k9HtTr75gvtqMma0NmFbbI4GGWCRByo4pQxkzzFSo9aIxlSD2fQ5lPyTP3OokmpyOpfVlqVWZNn7puqKqbCxWkdlUcaj7TPh7vB7Jgqr+BPATjzy6TKQne0D721YrNP0LzJrgiFTx3fd1IikMk5wRpW51xohs+mIHbPpyjXEstkb229O4CR2LGik2i1MLFFtDYpjsKBZltFIMUnEtMiSGaisBVMbD2dhjmuam64q9Us/1HZ+kFrxL/T8SmmoqJbcBkrRrlaZE9busyHF4ULpcRq2zMrXKWKoPLw3qb5E0X1l56mYtXQebvqxwHWBUIKMqpQFxjE1qSNe12mX6blrJwzBNdOsilWGkiPsq9guDbaba7HFE1W3jS55slyrpTOWstw7TqctBoI7lSL7bl/FuNhMjmgqp9k0jJwmb1DIHYNE3eNYmbHi4OP4ymAXOG1fBieRlQx/7vkmKlYY9ML1wz3zUl7K0A0yXO9vHRPXMkAwC/aYwjEhhTBtbSjCGhcpyXsvaM896qfjPQ2FqrV0mlv3yln17z7XtsMVlXl/OxZuCIm1todxDl8EsQZrqmOlVe7AoaAylM4C51NUWkcpIGl1rL/OsoDHHJEWc8Sj1BeYifZqq2/SQ560/igRLRWVtnT0QC1NICqg13BmGiTE0z5jGbBWTMN7QbC8bJjttty3urWeE1gY1lXFnZ6jawuo6x1g6GcPWiEjdArJF9kYwC1LUQBX1fqOCWc8SkeKujmN9WVNTwmIkVmZru84X78kYyhhF7QWrTGI/eeZa4DbjWFdyYRa2G3TTT6NPGYJtVZdgUMqDML0w16u39UmBU5d4Ni0CXZw6dTY1KZCOTlUyLRR3nsZQxlYXj45pLt38Zutxce8VugxmUS0A0drnBqi1rpDqPJG6EvzKGDltTtO8mFxd4JVeJHav5Wc4V7rrmmcls0nPk0GuCqQJUBMzLnMzQku3zDDZWMsO3TO3WSZPyMaXqgGeFcSM4xXykirnYluFvjkCdPFREsXoQpglo4djEcW2qqsbaLtiCNQVcsYQC9VGCEtVVo1bm+jRtcZyamTqr5+nF2FkL9aM4aa6bPyFGSUEtBqbYkxRGUyjztxxCZPXpF0soKR5YiFP8zCmucRRLdLVN1iGdVvNL6j6vG0jCZOwMKG9D9CFMAuTaunmRmpjGDMw3Utq1r5IAcYyk0Gc88xeUJgjxPZCFvciJxjGGTAnVHfcVE/1fGZkHaGg7tVotod5cFKkAMwBNj8Guz+xHLvskWdjTAtmnu2olpu0kGVjn8r0Ghf3XNzjHF0GswRBtpvJnVsAY4rFOcZpRfXd9LJVEVNj5uHAZFg2byjMJ9bc5EoiUiSD2O7sk/qZu5o6ST9rlKzq1JBdv7rTYeEmm5FrnpVhLYYTmQQzFeFUZEFq6pyx0ozZnhkgurG4OZG0cCL8/++hy2AWCXC1K3/b4I3MRTQJYVup2AsLMln6MInbGh7QYZxEdec2wTSDL1c1ZwE9u4ZzM6UrNkbxyHRSXTAxdKD0tT0JYo7AOI/DZErowrazcepXzIsxe8Oe1RjHmD9W6bNEde33OBYQztRZayGfJ5vvQzRthEthliBo9YbE0FkDs4xRfL80e/iurvbkVjjQttpVt2KWPdoaVJ9OkeKuAHfmqs8MTmMGzxTWGNk8CjNscd0uM1O3SlMjiepFFVtHNotYjV/txhSm6jQXw1ZWtv9zY23M32B95zE6IPQxvfwug1nWJGANqCnUFaAOL6m4TNW/kjMyyrSq7WU00C2244tEGafVZUNQnW+6GZjc+OrSt0DiOE6uOUzubzXEVUIxtHEdKn3bVpFTmwe7VyjPqnrKLGZriEnTsXJhaM8wqSDrqLlwlYJMINzaJlz30GUwS50cH9VtLxkT2WXy6aoa2fRFGpkainF6ic1QxEmhCpMfDZwyXMUZexXVbF6C9bL1GyVknbwlyy9xHkZxfesLaXEcmewQ6tpYqgHNRZfVe2lTtTK5uqF6TlCwFmsH6++9ZLAl/uIWhKxJrnvoMphFKa5je9Fzl086FohjdJMj0+o/R+ImdG2ltb/r9zVOpBS1Jj6BCKqdwQSmLZDh9hLWXlzOiFaAzufruBiUucbl/zUSbe55cqizb6tqdpo4aWHOgseQvHpbMsgDLdkvhFl0AuWacejC8i145laKeUCqNbDnVmllAG34ySLy2zbTdGrEsIe+QzcdGgKhTr5B7YLLjnNur1b7SWOELlTXGaRuqyvJqZUxlRduqsrB/csx2bM0BNfmqKmaMNl37tlmYQNj8tGMbHfMbKPRN0aylHjKiaHmPZFNXyanSh0ZSlpgsz+qV2Mv/mS3DaOlLaRVnRiiWZkld6FKeGOUSjYWi1IDGgXtI3nTlb+7gEZBkhKGjAyFQWTMLQpccmfy9OKWtLQnlhLH9mJabDDRrmSMrFpSNGsKQtklLaObfsr9eaNsFmFaPQvdqzkjY5qseqNxuamTsx38C0h5ckehupBdEesW2u87dNOju56868lXXQkKSjUdo6BXmypFCmNorIHDADkGtBNyV8BBjfWZUpEuYewIx0w4JkIXCLmok+LV6Pn0DJgQW/u86+Yv1+NSTlVK1DYP2rnNv5v7rnNPsNzg3td0Icwi85yMCoopFVvJCoPMRaftogq0JGavHmLNK1GdXEf7Wehm3W3I1xvSriNvInnjXWVgC3kbSZsiMRDQUJkj1r8jhUnceZLLT0hKPATCMdDFgKQqGQ1oNGNcq0rqnDE8uORwjyLDPEaWalLWUNSNsmnzJTGWc40x6y6zWmGJZny/EQguMs8pUS0zrxWLqFFftbhMkLknYTm0FXcpUeWS0riEs9XbPjGgMaJXPeOTnvEqFgnRm3qBEIXcB8YngeEqlGFJGXLu6jCjtM8akyQtmFkGstDtlW5fVdqYkUOPpFSAPHPBo0w2iKlcmCRA/dEg1e6Z0FdT21oZqwCRzviGYi8xomOV3JpA5XQPyjN0IcyikzHr9a+JXBulSQWTRE1/y4T8Omv/JFDmsBQzZPNVT7rqGK8j4y6QNoVZcgTJZeuX3MN4LYzXnkEgd4p2oGhlEJAsVf2UWJUoyAjpTkh3igYIx4542yGHWtJhwJ339KC5zbqA/mfb9FYjV5z3BrgQxcLYddKnSWljsgfoMphFoaUSejURZAqjr+lT7SYmMc8nV2DMSZASjNR5zkrfoX1kvO4Lo1xJYZYtpK2Q+4kp0gbSlZJ2St4q2mfYZEKXCTUhezxEOAZkDGDMUn+HsV5zI0Cgu4v0L7qCmVRXujyv84qMOZbqwQKFdkw7L7gKAyaVvLB9dH8oucK77RQGyammn95Pl8EsxgdLpmjA1xROb6twGVH1aqmrGsG7lAsEVDc9aVckyvAkMO6E8QrSTkg7SFvIGyX3kLaK7hKyS/S7kd124Hp7ZBsTfUyMOXBz3HB72DAMkZQCeSw/JIFB6Pqi4iQJ44ui2kIXiwG/xGM83sRCRZj0dGEBrWmiok4CLSPLs0w5t2gkVHX0pqQo1JUOTPrYNqcSgY0Tr22DpoVxN044jeRcordQKvQqc5iNQhdITzYMz3rG68BwLYxPipoZdzQJkreKbjKyTfTbkd1u4Mn2yMe2e97Z7NmERB8SSYV96tmnnrux53bouTv2DCkyjpFhiIyxR6UjHIW0lepNVSPc1GQra10shGVsqkyaWzhuHi2/2HmDLcShCn2PWNKVeUt27gN0McyifTepEou/HI7Fq/GBMFsh6pOPXX6HaqnhaZeu9knfoduCheRtZLyK1WgVxmtheArDEyXtIO8yus3ILrHZTpLk6ebIxzZ3fP3uJR/vbulDoiI2rZ7w5bjl3eEJHxx33I4bDqnj5rjhfblmUGHcR8ZdkSx0oSC0XUSGyiiV6VsA0oUJ2vxYpn6UhxlLtWAsVgJiAKcxTJ0j1NVMnaHLYBZlkhIw2S3enV6GAe69Xg3xhop79B2660i7jrSL9UcYrgJ5Q/np6+9dRq8T8Wpkux3YbQaebY+8s93z8c0t37B9wddvnvPp7gWBTBQlkkk1mHeTt7w3PuHd7RPuUs/NuOXFuOU3QuZLKoz7HcOzwPBOJBw3xH0g3FXjVBWSTHEhqB6PWxxwijk9FOMJXm2FeZjCzrEI9T10IcxScQcxUCsgm75gJ22nUOcxxUjbzregZsU4brrYsJYacNzGwijXHeNVqIasFMO1l8nDCaC9EneJ3W7gajNw1Q+8s93zDbsXfMP2OZ/ZvM83dh/w6e45OxmIVbIMGjkS2eeeT8aXvOh33OQth9zz7vikPqbwm8fI4YMtd5+ISNrQ3UT6GlKQlCFWGSUFE2nqw6VkatfN7Ry/yJafi0xe5cIgnuJiMs+8O0MXwywMY8lkpxizWm0YKoLbylu9tW/2iaUrEibVVCO0uunI26J60jYwXhXVkzeQO88oWvJku0y/GbneHrnuB3bdwDv9nk9vXvA/bL/MN/fv8o3xOZ+OR3YibGsM5zYf2SvsQ+CgLzkSuM1b9trz5fSUrFIM4cOGm4/3HJ4HwhjZBgipI+aMpoK9ADWPRuCYp4Qvq7q0zDtjjsZIbn5mn7sAaos6O5BS9Q3K7ofJHqGIY3N9Zal67GGNlqI0pSJtuojuetJVX9VPlSZ9YZS0EbQruEmuQJskgSSkMTCMkUPIxJA55sig5SdpICFlm2eBnkgUIQZhp5kB5agDB4UnMrLXSC8jv7V5xhc37/B0d+Dlk5Hjx3rCUZAUCMcO0uTem5SVYShSJMhMskzlJYscXUttWVY/LFWTP/ZD0OUwC2BRWSiA2CzuYd/73+Ag/GlCJeciVbYFcEumejYFP8ldZZS+gmx1FiSBDIHx0HEXM7kWil91G27Sltu84f10zU4GNmSiDGwl0bNhK4FeMkmVQRI7zWw1sdPyPB+Ptzzr9zzdHOmfDAzvdISDEI9Ctw+E1CFZSzLdXc0B1tp2g74Z/q1IzUuOGQ6jk4PgerKsxpPUq6Y3SbK0CTAoOtaEpwUt92+GCSIXt5L6jrSNhVGuAuO2uKxpW5gkbSqj9EUFgaGtQj4GhmpcB1Fuug3Phx3vdU/YycguDGwkEUTpGSEcJwkjAlr/JhPJHGXkWdjzNB541u95cn3g/Xd6hkNPtxe620BIWgcEMZt0yfOaqLTAZNYAOnCLKrdC+1b16EtbbQ5FiqR5gC6DWYRWiNXopPtAJb+6vBtpjFJxlbztyZsS/BuuAsOTArg1aRKLRMmxIMcqNeajQBbyMTIKHIJyc9zwpfikYCoUNZQ1cKMb3pUj1+HAExnZSaIX2IkUbUAJbZlbvQsDT/sDH7vas3/Wsz8GjvuOuBcgltwZhTCkgjeN42lC0rLe2ydfGVn5SXUEtAYrJfYTsjtOye+yjGSfoQthFplPiuWZuHxSKnO0gJnl5PqVZm2yGqZSVM94BcM75bcGiqcRKLhWQeCL925241jyYXIIHKXjNiihMmOqqilroB/foa+S5joceCfseRbu+Hg48CyUsQ8KQ83y24aBd7oDH9/eMTwLfHGIDHeBuC/cKiqEIRJvY3Xu8mTYe3Xi0hrER9o9tYQpmVqrwZSHY82M/Py/ps5PH44eAZ2sUi4vvVUbak346QK5C8Xb6SFvqpu81SLFbE7U/QpMai8JIqWKUKMwDJHb0CNSmKaXuczuJfEs7rkORz7RveTT8TmfjDflUkjzirIGupC47o5c9xu224Hbqw3jtRAPQtqX+JFa/bFHdFVaHu4kXaYAaXl8p5KqpCjeYgCdAoWtU1XrRfe46b4QZqmus9ksMHG718mWwL1QsC2+YhOZqUwDOUqRIlpMAJMoRZRU7RUmfrUclIxAL42xcg7sjz0vQ+bL4Qm5OPlAsWveD9d0IfGl7invdk/5dPecIJmsgaNG3k1PeZF23KWeY46MOaAqRcrVgGVTg/V5S98XgPrCLQ0jh5rHy+RFSpFMLYZmRn+oWXljgRVkTFOJi5brtsqGB+yWi2EWrZWBs9YSS9GYmaKrtpqcWhIZSz811ZrBVnEUqUyQ6suplxOt1/TSN5fUgoCSstRFqqQUUIW7oeeDsGvMEhZS5rp7ygfbK97tntBLIkomaeA2b3iZthxzxz71HFMkZ/NEyhhK/osz0i3inKgdpSLad7VZo4tGmzrpIpIV7abcXLUkMPMWqx2kM/tw4WqfoctglkonzQJhijxLXXYL61+gpD3WcL6Ge/JvcYbs4nImjUyykIqHkIfAKEWiSRD2QYmhTHQXMgFFZJrksRo+x9yxCSN99ZoOueMu9RxSx5gDKQdyKikN4QjhAPGgxIMSxkm1zPJZ6gLSjpKWuYT8xwSSG3RAqBWUIZS6KSMnVWafPWDkPgjLfE02lxSmhnmmlytWcNLu1K8Aix9ZH7m+JvzUDHvLVJuistXz6bT8WCpk1TYhTcdLhnAM6D6S9h1pCKQaQT4MHccUOYxdAexyAesAsgr71PH+8YoXw46btOWQO465Y8yRUYv6ySrkXOJC/Uth80LZvMx0LwfC3VCkR5nQKSnKFpNIUUe+BjuXbph6d4fe3CE3d8jdoWbZBdhtS2plAzGrB1XVlj5CsjwGw/t7wHcuPvss8FOq+m2UrUk+W55Lvp3SxvT31HP+Tu3jfz/ZqrGSVHuAplsdeuk6Ms4mri/JRKUkQ5rkCGnyqjXUn2hg3OQVkUHGAsyR6/8HQY4Ch4AeYpEyY+Q4RvaVYY4pMqRIVmlMcDtueFkDiMVG6TikjkMuUqUxzFgM2+4lbF4q/YtE9/KI7IepHw00d9cbsBrdyzYc5jigd3v09rYwzd1dkTbhtCKhZcr5bLllG7MFPcgs+rXaXNJD2D7N0KOPaxLGxK/V45QTq2RRZzAy84ImabN84PqxSZhRkFFqElMgHyPDseMw9EXCjB1DDhzGyhCpKyqmMk5SYciRQQuT5CqBTHVJgnhUYs3+l2HJJHFSuS3vJU+dEKybt5e2MNUK+cXmJIfUtFWxBkUfIYI726hRRPxGjf/aHfd5zmzU6Hv377pnM5RyBji1hJ5KZtDOXOYK1AHsNpXBaLaL2SnmbYgl1VXMpIZ5prEZEq5mFJeDigcbGK258mY6J4ky5lDsmH5ouEzWwKhaVFA2hpH2PWr1RdVWscpMn8NjlHPNe6lQvy91rYFHK4dpJSbjWFSadX7wkfvOGcmvIQd3jT1XFeGsd//2G21mJ/fZM4qXLp55zCNq5Q1lInIt8jK146WKZBqTtM+8baNS1ZYWG1AoMCye8QJ+au3liyh0MDrJcsyx/T7mjiFFUjVwNTuDOlMDiXMJ2Z6zxYFcvrExhC4XmE5SJS0kETSVr9V7OumieYa+Umb5ooh8pkqVr35zyWbJV/tDpox3YxSNU7WdLFaB1Foa3W3Iu468izVvpeTTaiwvJB6tZGPOvzKWpGrvBedYxEkye9CYrDKLUnKcswqpS/R9YtONxc3OgbuxZ5DY1I2qMGpgP3a83G+52/dwF5FEKycBp2r8y/VqyaSN6jzXRf0Cc16Ud6FhAh6bx2n3qNe4h75SZvlxygaNP8DpRo3/QER+EPgmHr9RYylf7XuI3dRl0cPRnmEMM7DJtHqh6y3pqtT/jFehlG9cWVlHcU+9epIKzIVEcV/Hya1Om/oCNzaZMhObqkUypDGgWyEEJW6UKEpSIY3T1Fr0WlU4DB23t1vGlz3xpjCLRb5VYLZ5g0dwQy1VrapDm8rOrtmh1s4NhXHEumuGxfy5FJCG5q7l/i7oQWYRkR8B/ijwKRH5PPA3KUzyYyLyvcCvA3+6jEF/TkR+DPh5SneTv6iqj2gvVMTvahrl0gT3IrkMkFbQ3ke0DxXen6LMLQZUb9WC0zVvKgz1Z9SiZoKUhVc9qdLAsJ6jUhb2KM0Fz8AQlH2XSLl4RWWoc60soiWJ+xCRu0h3V+5bpJZ7nqUXCA5rquQkQeur22qoZX5enTdx4JukuinEI1uEwSOYRVW/58xXf+zM8d8PfP+j7u7Jes0fhymi7Np8iunp41DgfY892Hc+F0hoJaUNSsdpIHv5qdg1uWdyTYGGqFq9PdN5MpYPcifkTa0b0o6XQyB0SoiJEJQQih1T+DnTySSlwgjhKHR30N8ocZ8r+hqRMdYAYp5v2ADzlmgNe5nn1bZCOuPwMU2MYkxo9eA+YPsAKHchCG5ZGWWHiyMeXRRrohd02v0jpZJB5qPVlmLpXeXALG1yKi+VSWoIYJly0b5nageTi5oy9SQjhLFImrQr18yEEt0lknolbSJhWxhGQibGOcpb+tEIYYDuVtm8yHT76SW2Vho5QN+j22KTyZimGFrt0H1S420MIILIONk2MDGK72fXXsHDkNuFMAsL1TINvLnE5T/TsUuXEtCamii1q5ipHIWZR6SxOMtaj1HAuomKV5rmOqPtfKkejF0fqrSqn2eoFQWCBq32Za4SxrnLWmykOBSMRY6lJccM5g9M6sOev9pvAqd5sx5PWaqW5mlW9ZVgamYY5t0UztBlMIsq5FS3SKlw/zhO6qbp5FqQFXW+KmrvfclFlEtWQtIiAUbqBhtSEp2aS6VF7bjLKPVWlcmsThktAcXGbIYEt7pnhSxNmhEVCUqMmb4vVYshZKJ5IsbIuTBnGDJhKIBcCRK6F+02lyj4iGW2TekJOgxVNYoLPrrmhyIlfgZAqhlZOjVdttqsN6LzUzVwLb6jXdl1Q45+t69Q3mI15FoqZYu8Vvc7K+SyasOgxKO03fLKai1Gc1EzRcKUL2kvcNYNoTFX+SnhghJXyrV8hHotVdr1gzFLTGz7BeBl96kqTmrDH2qDIt/4uPSgyWUhbTdYizJrjmj7HarNk+99N3Onw9TlElosqGQdWoT/TWAWkSkoBkXs2ioBfL6L4QezlWBiN2fCcaTbR9IuEPdTikKuNokKUx+VBCpaXWiZwS9qjFGZoWXWhYIeW9pDGCm7q8nEKFJtFTNwgRY7SimU5CrDdlJVM6n2i2s9Z8LUfJEF8FafdVaMBlMnKCg5P1WCzHKT6x5K0k0gpliL2AfoQpglINvt3LOBEvjyPe2hbBvjt4OBtuuFjAk5JOLdSLcJ5H5qtmMZ/E2FVK1hWy42UM7ZIsZY2umUfukdptpiQ6Ue01eR5DwhoMaKYEyRcYjIUNMShqIuxfXcnane2or1JNxhHpE1+bHEMIvC12QmbadMm1tYyeoMFX9EXAguhllkagOa69uyLVBU0WOGw6GmIISJUZaxk5SR40C4K10L8iaQ+kDaVEO14ipZgKAoUhKLTO1YhNqrnE7JHXNGMdPHItUBcpD2gkQmZlGVUuNfi+TTMRIPQhwgVoZp7UBsNzTX1qv1x5O5FLUd3Eo6qGso2GJAcarDMqZY7BnQttJ5ozZ68LSSR9qMsSA1J0OmFbaw/Mu+y2PprpSLijGENkYq/qANU2kvPU2SxUpZWx20dxSkustix5frlHQHbd83qaKC5sD+bsN41xFedHQ3Qrwr0WYZtexU4nvYWkZbDQie7JJqKZfL1+fnwqsXbxjb3KqetpN9gC6EWbzbXCZjBpC5tunqkci2SbjDW1JCDhCGVP3YsvrjsRjHBS+R0rWpnhKs8c64YJStkjZ19VapZNJmytvVafjmXpvXVO2UcQyMtx3hecfmg0D/Aro7bakJYoG+ZoOkYoi6fRBbJwl78dohkprd0lRNdraH29eoSREfrE2T3fIYuhBmYQKKYL4CgFbobmSuYSuHcCI6A1IwizBk4hCqcVp1TelmC1kmsK5GfkXLXJciNCX3im6qzkmUDgex1kSbG27zXNUcCnkUhmOZ2pwD6RAJLzo27wc2z6F/qWxeKt1dJnh8xdthayCZt+egGcHNgPVz5OZQlzaJSWVbbC5n5j66DGbxEnDJKCEUcXq1mz471/feRVlRJe4T3U0o9cTOGG37cidqv1qKjVJDA9bxKfcl9gO0mFDDUgIQtOzbrNQPin7Su45hHyFJ6VB5EPrnwuY5bF4o/U2mf5npX46E22EWcRfZuXmZFoVmM+JlstUapODUlJ3X4H1XxbhsgKRuX8ll3tAKXQazANObWHwcpLTfqMfM+sa2Ux3KaymHSYmH8hJCiqV2mlBxkuIma5wv1BxpFYt5UxmlAmkl2YopsCmFUaS2yMhZmktcoHwpdtKd0O2huynSpL9R+hcjveXaHl0vX5/mCBPw5poVqeuz0ja+6Lu5mskuMplzsYksUr20TdoeiW+K62zkMAGx1dZiGWGC9VfyqVrti1n3tXV5zBlJHTJ2hFGBWBwFrakLlhoABfL3EeAAdNUmsbKNVKVMDugI2hVpIsdA2Aux/UB3V2yT7k7pDko8ZLrbVJKybw4laOrtBq8yGqQfF505zdCathBukWRjlEUoZJbL7MG65aZZb0Yg0dFsYypnjC2s/Hkyd1U/rbdJrsZeRnJX7JfDSDj2pSw1hoKedtSu2PV2FYOJtQ46bUBiBpWybbMakCbFrg1UJi7R43gn9DdVitwUddPdFtzHGDAMiXBzQF7eFXW6SB2V7LweqKrJqZ21eRnGop4W3lAjy4aruS/laycq3zgDF+YIpd9ut5Isdt5o39S2YAZ9A7UhcUkk4lia4MiQSoyk65FcQLvUG2hS1JLp/txB3tkq14bDhGPJyJcBrNN2GKG7LakGmxfK5nli8/6R+PJAeLmHw7HWX5ekXbktGfhkLXs2d1W8GUN4HKQmPXkjtXlOBvkvMJJZ2/ZQUUdjFL9JhGeUN8bAteSnZdt0yy+FonPrb3MjpXtg+Koll2NMDZvpQrFnul0kb0NtsU6rXpQRwiC16XFgHDcFI7wTulshHiDuiyve0hayqZxcjNfnA/HmiNwe4PYOPRyR7aaGKsLcUKqbfc43eygvWmpZ6ok3A/OsflMtpqJsU9Iu1mh4RrVDfLBr6W29Mcyiio6p7Cu06Sdb5WTbu9qXPsSSd2sNlRcxlRNxDdU9LvZMf3ug77uyi8c21hZipT1HtxdSZYzuThhuyrW622p/7KHbF3xELStKKaUcQzGq4yEVUDBNq9lsKqsQnNkhtjssUHY1STOjc1YhWxdACYE4r9DvXeQTspO01rAaM6TgbKTpHPXe1Bm6EGaBtkUtNPFZoO4AlplpeRhQXJe4qXmlYwVI8jTJPjPMKCV0f2gGXtxs0G1P3G6I1xvSNpaNHnoh7QLdXaB/WfJUun3BReI+Ew+lxqegwXXINTVChhKfYhhrWYeLAqdpZVtPlLaPdKNcs/4XMaE2Vzp5Nkv7xLflMIO5hkzY9HX/yTAPQC7zXu6hy2AWqXseWmMey3nsCkqJhmlSU3kRKgXVFJf7AsyK5tcaAkkXJyuWov91GAm3IEMsewV1gXwT6F8WaSMK8ZAIJjFqlBgzcO2eqTLLcSjpFdbuwjVTZJij1W08MOWf2DWt120Le0yoa9saxhvDlnJgG6d7b+mkN1+5lmopnD9pJbZCF8MsLVnbUFiYyjOzIl3ttGCdjAyFnOXNSg3pp2mV2fVhArB8WwooLbnGhNxRjcpArB2wG9Q/jFPxeM2y1ygNh7Ec4MYoh2Nh4CDF7QJ0HKeXYq1XzRPKk+s/G7PZOVo3eLDv115wzX1RcimXMXXjF88MsCsSTY/DvO//GbocZjHVYQ/kcyyk4ghjVUsiWIvxKbO9Zog1lbX0CJhejE2ywyj8irWgW9sI3NIHUi6SoG7mKZ7xbIe1mierwzABZ070q6bWZ2YGwAWZFskiONrIu8we2l+qq6yFYTzKu6RZ0Zqby3voMpilJZDMRfRsCxjT9zEgG1c36hFNmFRV237GFawZU/g4FNB29VKdzmuivbippgZ1LG671ILzGcxembjZSdbpe/Zc7sW14xYv1RaMPW8yNeQgBZffM6uHjlMv4NZKDdq1W030sr45T4HVc3QhzMIkGv2qaS04p5UmXVdzXhf4i1gbc6bvbOUbk8Dc4F1W+FnYoOIa4gzSNsY8lsw424QCigdjWfZL49NewloKgLq0SUvqsmtYW9JqlM4MWytL1YzmOFchFflVdYClrNxbArPoinceztBlMIvS4iCt5ZcVbbvAFzAZbjL1sgemNMQQoafV0JzobHf8dH8XN1lDQNs59wBYXkWsMEbbv9Hff7avs0szANou9NZfxa4TQsObyO6a1iLNx86yMg9onaFHbB8Dl8Is6CyGoRJKimRr6uPSDU0a+NUVo0sbrN6F7X5h5NWMvbjZplbaVI7FoGYSrfZHmRmonmaMUO2PaiO1nT0WPW1nG0/ZdsEwXwTQJI/9SK7PYVLYe0Nm2No4zMYzOEKdpMsLUfJm2CzMrXRNBTwawyRVUipMAXN7ws4TPS2XaC7nQmL4RPC2X2BmJsmc0WvnzVIXl/k1XtUZSZgYxdquwtQlobb6mrGGY57ZtbO6lFJmjGzXar8NzfVVEG1MMj2vb/H+CLoQZpGGqWiW6SFGV1G3aEPR4iHejlCd8BqYT6j3PPwL8J2koHg8x+FUcpiasL5uDV1116oJ1zP14sbcmMT62y4jxHacyNSRc+keL49vRvsUJPSFY60KIkgp/R2GCXJocMWbpIaCtGRs8a7qmCbvxvI58G0mygpR60Gycd7McrX7l+/bqM5WclVrx9QkkKnCBgqeGN/1GlaB4DfKMtAuOJWnzqPxbS6MmQ0sbHPj1Ex7Fpl/ZqCkuu9t8ynv3qsWhhlH5GrXWpW0rMOPqOXGKyettkaJqFYX1HJvs0Mm2wluhVXMpXRnzMW2MK+ouaWLyV3aBVZbnFPLhWGDk1J5fr0FQ5Zu2OM04ctAnTrJ41tpZMesnWMMjw35a6iCysz+afc2MpVpjGIMX6W0Jrepum+MlBdjXtBlMIvSkp00VkMvZoc3KOITmGGaxBgRqb1e+7KSxK9AnOtsL8zcUrMnoKKzNbJtXUBlemGlWpICyJktYCUc4ziVmUIZh0Hx1kiwqoO20YKFMEgT89j5XuLZ+IxBF+jzfNEYE0rt2VIz0MU979VuMpBVyzM8ogwELoZZtG1ORezQLtSM99ItoXgyCyaxF9mKw/v5Xoq2sgLFPhiGeUutUDENa7JcATURKfYPTIimNdEJofWXBVoFoe2V3GqHQyzMZfbWONb0x36G+4gkdMjVoKe0yLCXX4vWG9JrjLIE7mDuqlu4A8q1vRcZAnJ1Nb+Hhw0eoMthFquu65yYj7FuVOUL0E5p5lLD5Go6O6VMiLMnaomsmp1k4t36m8xvMGcUv1nUcqKtCaDtclKvvcwTnlVTNs+nYiutVDVTGglNLnKrd7JkqXrPFipoXT8VatdErQuo9Hqp81Q3etDZNe5nmMtgFqoXEus+OyFMXRvFBwQ9eGY6n2kyHJPNPCGdv0z/olsOrEOQZ90HlvkxOU9STnXKkQ0ytT2HKdYUw5TcNNZUCmcrFHjeVIsxokUvwzQGG2+zcWxsVRpawXybUG8TOVurc16hTK3PVPVBF/pCmKV6NeM4rSKjxcuSlErMwycVVdGutX/a7JG9O2oqKixevv0Nc9e3BSJlYixZZK5Zqcpyq1tnc1iqgI4JQp5qkm18y94otVFP+37JrPaZSajkwL9qE2mWyei1TapCLM/QOc+rXvMxSMuFMEtFGrOW+piQT1VLO7S+PMM5zPA1tHQZy/GQV4xzUeu9Kx++92TGcbNfpCRV23d2XXVQ/RKjMWh/WeDeHmnlGf25y+fxrnNeSEFLOdWVGgjbN/rEE3wgHGCHPXSAiPwOEflnIvILIvJzIvKX6+evrn+/lNqgIoJTwQKWD+RXqvXq3/TQb0p+63Y7BeIMA3ENbwwzka601rLorB4O5WdM83qkurWKoceMYwkeHocpC86rhOaqxplBDDRp0JK1YLquuhhXQ4ijU2/OaDcX2dqF1fRKjzKfNCqsz1L2HKqu8+GAVrylSarayv4+eoxkGYG/qqr/TkSeAf+viPwT4P+g9O//ARH5LKV//19b9O//JuCfisj/eG/XSmGaWMsbiWFSA/bSbeJiwPbcmSbZAU95wSj+JRgZA7R66TBN9KJNGd6LMsPbSz6vKmYvzNlM/njftWlmT7nnm6keM8qtJ8upqzvz3HwnxmWlorVM9bEqf+976DHdKr8AWOv1FyLyC5QW699NaXkKpX//Pwf+Gq5/P/BrImL9+//V+ZvQcIqTzQa8G+zBNR8kXELikcIMx2KsnmTMedS2ueChMYw4g1INpbUyinp/jXEGhM2aC+X6UAYWpjSvRLBjDWj0z+iZHubBTrvPcn78HHgJZ4a6hSjawpiurZJLk8ecv3pm8SQivwv4vcC/4avs3z/r3R+eli6U1iLLqxG/ehcplCfGppuwgk/kUzd46RktmcTbSdnp+JTKGPt+6qA5lrZeqloSovt+xpBtswXNk0dkUW97RpjKYOxzQ3kt7GEMH51qs2fxRnrN0pdMKU6zOanu8sQ0PoyS0HFlO98VejSziMhT4B8Bf0VVn9+Tgrf2xamt5Xv3x0+pWv84nEhdvliYxV1OyhecCNc8hfxPR+Ouaz1km6paeF9SGvepx0qqO69ZJ70fQ2vCM0tQ8tlwDdOomI0ZxZbbK+5lG4NWNVS2f5S5RF0a5HXcakZ4ZRI1ZjEnAMqWiRZzc/1g7qNHMYuI9BRG+fuq+o/rx6+uf7/YbhZOJfi8FSNTRRVUasVjBpqFYg9MxeGciTY79DPIxARrzYOrNBBodpQ1MpYYS1PAxTlCVVMW/VV1G0lNxqpVTLaELygF/K7+WbxktWc4R9aW3Y6rXlyTMMZcIdTuD9Prf8zmVI9pxy7A3wV+QVV/0H3147yq/v1CNWbDaUaZifNa69N0v0WloXhSfVdQTQv9l8GfrpY1w5Jpsk6SnGsGHpb36yoEtJuMZvGdJqHiRdASW23LFgMQRyd5xlLaghn2NvZlHoydA+sAmuq0ta9JN1Nr3m7zRvorhvv/EPDngP8oIj9TP/sbvNL+/TIPqfuQvhfj7oE0ZxiO5W9vlPr+uT5bziVAtcnxqs1afpr9YjfywUaHtdiE665v4xNvcy0TjnwHbFNRVlVoveHIaGLOMN5Y9aWJbSzQ6qTXEGsvmf0iMGb0QOcD9Bhv6F+ybofAq+rfLzWqu9L3rKko3x/Nk2XmD8O0yl122iQhJoPxpPtCGXR1KaFl5Htj1zNui1XlErW1vy2vxJh14ZZqLTaTlNDDkZbWGKQEHw0usOf3Y/MMEMvGFoi0mqdm9/gkdJ+Xa9eoqntmzPs+/vfQZSC4zmKfBfYyc+zByLnXZa/icV4078W3j6skoTTir4whGZGurKrsmEECJ63OfVsyVdBY7AlTO6NL1hKZJJRnGAO97rRJlWYHxVCi0naMqUOY7DALVfgoeLZW9LmFPVgyqquptrnTTT9399+Yduwwc3s11kjrEh/xba6yFhtHdDKMLRfX7I+U17ftFSni21cEWjwmSHF1Ldo7K+/IeMCupUfCfFWqQm1arBU5lS6eHDsBewsQzj+zauk5Z9816VcPMybPrhtDWIxxjax7g08mf4AuhFmqeOycbRACWH1QzlNiNUwvzl6ww0qsR4u96Ladik1GKPU5Pp9WoOy+4ZFam0w/zGWLUZi21M01wtzOt5dUxql7pjokK+/webIwj0p7cG7ZVdKYVCoO4xKvdATqvtMeOmg7m0mYnvE4FEnuvcx76DKYpQAZMxHaqv9r4z31HQkW+rt5UdBeuPraYWMIoOWnwjRBhqbGKVmpuJ2Bky3y7Pjmii6MTkN27eUZoGYv1SLjvrTWvJIa9yn9ayvA59JAm9owb6pCCNTWHpK1LLDK0DNV2IBKKRW+47HUOFtSll9QZ+hh2fM1oYUB51Mnm5GXTw09753MLqdVlei6GD6nmpafPybr3cZkBuXsq4X3lVxCt2eUmZrN82f0rq19Pku8qmP0xWluDtZAu0aWu/OAYdtOP4nuvgYSkd8CboAvve6xfAj6FP99jvd3quqn1764CGYBEJF/q6q/73WP47H023G8F6KG3tKbQG+Z5S09mi6JWX7odQ/gQ9Jvu/FejM3yli6fLkmyvKULp9fOLCLynTWx+3M1l/e1k4j8sIj8poj8rPvs1SWov/rxfvRJ9UDLAHsdP5Qox68Av5tShv7vgW9/nWOq4/ojwHcAP+s++9vAZ+vfnwX+Vv372+u4t8C31ueJX+Pxfgb4jvr3M+CX6rhe6Zhft2T5/cDnVPVXVfUI/Cgl4fu1kqr+C+DdxcffTUlMp/7+U+7zH1XVg6r+GmAJ6l8zUtUvqOq/q3+/AHxS/Ssb8+tmlm8GfsP9fzW5+0JolqAO+AT1i3mG+5Lq+SrH/LqZZS3M+aa5ZxfzDMuk+vsOXfnswTG/bmb58Mndr4++WBPT+aoT1D8Cui+pvn7/VY/5dTPLTwPfJiLfKiIbSiXjj7/mMZ0jS1CH0wT1PyMiWxH5Vh6ToP6K6RFJ9fAqxnwBnsd3Uaz3XwG+73WPp47pRyhVmANlFX4v8Engp4Bfrr8/4Y7/vjr+XwT+xGsY7x+mqJH/APxM/fmuVz3mtwjuW3o0vW419JbeIHrLLG/p0fSWWd7So+kts7ylR9NbZnlLj6a3zPKWHk1vmeUtPZreMstbejT9/8ZShlwmQsrvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5785, loss_val: nan, pos_over_neg: 1.0340266227722168 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.3392, loss_val: nan, pos_over_neg: 2.6888039112091064 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 7.1201, loss_val: nan, pos_over_neg: 2.0475881099700928 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.866, loss_val: nan, pos_over_neg: 8.815142631530762 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.7192, loss_val: nan, pos_over_neg: 20.682579040527344 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.6898, loss_val: nan, pos_over_neg: 19.167526245117188 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.5998, loss_val: nan, pos_over_neg: 20.74757957458496 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.58, loss_val: nan, pos_over_neg: 23.270824432373047 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.5692, loss_val: nan, pos_over_neg: 26.619600296020508 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.4965, loss_val: nan, pos_over_neg: 47.59990310668945 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.4824, loss_val: nan, pos_over_neg: 54.01868438720703 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.4521, loss_val: nan, pos_over_neg: 78.09954071044922 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.4253, loss_val: nan, pos_over_neg: 84.71736907958984 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.4179, loss_val: nan, pos_over_neg: 89.1485824584961 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.4132, loss_val: nan, pos_over_neg: 89.18240356445312 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.3793, loss_val: nan, pos_over_neg: 116.91329956054688 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.3639, loss_val: nan, pos_over_neg: 141.4477996826172 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.3552, loss_val: nan, pos_over_neg: 109.11334991455078 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.3419, loss_val: nan, pos_over_neg: 122.47229766845703 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.3373, loss_val: nan, pos_over_neg: 105.9799575805664 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.3366, loss_val: nan, pos_over_neg: 259.17236328125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.3157, loss_val: nan, pos_over_neg: 270.9415588378906 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.3176, loss_val: nan, pos_over_neg: 256.2096252441406 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.3001, loss_val: nan, pos_over_neg: 215.9441680908203 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.2881, loss_val: nan, pos_over_neg: 122.32212829589844 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.276, loss_val: nan, pos_over_neg: 166.85108947753906 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.2794, loss_val: nan, pos_over_neg: 357.52471923828125 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.2538, loss_val: nan, pos_over_neg: 252.68263244628906 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.2519, loss_val: nan, pos_over_neg: 235.07151794433594 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.2547, loss_val: nan, pos_over_neg: 265.0335693359375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.2513, loss_val: nan, pos_over_neg: 337.862060546875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.2531, loss_val: nan, pos_over_neg: 305.7766418457031 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.2375, loss_val: nan, pos_over_neg: 633.1246337890625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.2357, loss_val: nan, pos_over_neg: 298.6596984863281 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.2334, loss_val: nan, pos_over_neg: 301.9438781738281 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.2287, loss_val: nan, pos_over_neg: 371.3529052734375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.2129, loss_val: nan, pos_over_neg: 652.4263916015625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.1937, loss_val: nan, pos_over_neg: 926.3410034179688 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.1801, loss_val: nan, pos_over_neg: 711.48876953125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.2006, loss_val: nan, pos_over_neg: 13514.1748046875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.1985, loss_val: nan, pos_over_neg: 1290.5953369140625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.1853, loss_val: nan, pos_over_neg: 232.37286376953125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.1912, loss_val: nan, pos_over_neg: 415.76605224609375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.1775, loss_val: nan, pos_over_neg: 372.376220703125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.1815, loss_val: nan, pos_over_neg: 612.8436889648438 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.1672, loss_val: nan, pos_over_neg: 971.1683349609375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.1769, loss_val: nan, pos_over_neg: 758.5227661132812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.1822, loss_val: nan, pos_over_neg: 309.4440612792969 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.1707, loss_val: nan, pos_over_neg: 280.4767150878906 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.1498, loss_val: nan, pos_over_neg: 320.40069580078125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.1543, loss_val: nan, pos_over_neg: 784.5090942382812 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.1805, loss_val: nan, pos_over_neg: 398.21197509765625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.1493, loss_val: nan, pos_over_neg: 472.9237365722656 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.16, loss_val: nan, pos_over_neg: 446.651123046875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.1349, loss_val: nan, pos_over_neg: 335.1882019042969 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 6.1319, loss_val: nan, pos_over_neg: 363.9139709472656 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.1206, loss_val: nan, pos_over_neg: 316.61181640625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.1458, loss_val: nan, pos_over_neg: 440.0192565917969 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.1292, loss_val: nan, pos_over_neg: 735.2374267578125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 6.1488, loss_val: nan, pos_over_neg: 289.18798828125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 6.1403, loss_val: nan, pos_over_neg: 434.7727355957031 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 6.1443, loss_val: nan, pos_over_neg: 366.7488708496094 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 6.1246, loss_val: nan, pos_over_neg: 511.93731689453125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 6.1189, loss_val: nan, pos_over_neg: 575.5982666015625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.139, loss_val: nan, pos_over_neg: 227.02415466308594 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 6.123, loss_val: nan, pos_over_neg: 458.36065673828125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 6.1209, loss_val: nan, pos_over_neg: 232.28961181640625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 6.1267, loss_val: nan, pos_over_neg: 264.15203857421875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 6.124, loss_val: nan, pos_over_neg: 156.27207946777344 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 6.1397, loss_val: nan, pos_over_neg: 286.04754638671875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 6.1171, loss_val: nan, pos_over_neg: 395.7546081542969 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 6.115, loss_val: nan, pos_over_neg: 248.1392364501953 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 6.1051, loss_val: nan, pos_over_neg: 814.9216918945312 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 6.1101, loss_val: nan, pos_over_neg: 271.96697998046875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 6.1088, loss_val: nan, pos_over_neg: 158.92701721191406 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 6.0985, loss_val: nan, pos_over_neg: 247.7693634033203 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 6.1033, loss_val: nan, pos_over_neg: 822.7260131835938 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 6.1031, loss_val: nan, pos_over_neg: 196.8165740966797 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 6.1139, loss_val: nan, pos_over_neg: 250.2802276611328 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 6.1028, loss_val: nan, pos_over_neg: 388.1837463378906 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 6.1156, loss_val: nan, pos_over_neg: 222.47564697265625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 6.1004, loss_val: nan, pos_over_neg: 533.335205078125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 6.1135, loss_val: nan, pos_over_neg: 376.5443420410156 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 6.1041, loss_val: nan, pos_over_neg: 211.04335021972656 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 6.1048, loss_val: nan, pos_over_neg: 245.67474365234375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 6.0755, loss_val: nan, pos_over_neg: 334.8378601074219 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 6.0889, loss_val: nan, pos_over_neg: 273.4922180175781 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 6.0921, loss_val: nan, pos_over_neg: 334.611572265625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 6.0859, loss_val: nan, pos_over_neg: 188.90457153320312 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 6.0991, loss_val: nan, pos_over_neg: 418.66552734375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 6.0855, loss_val: nan, pos_over_neg: 465.7043151855469 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 6.0722, loss_val: nan, pos_over_neg: 386.2148742675781 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 6.089, loss_val: nan, pos_over_neg: 489.72015380859375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 6.0745, loss_val: nan, pos_over_neg: 603.6248779296875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 6.0979, loss_val: nan, pos_over_neg: 252.9903564453125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 6.0765, loss_val: nan, pos_over_neg: 499.3916931152344 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 6.0833, loss_val: nan, pos_over_neg: 368.01971435546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 6.0774, loss_val: nan, pos_over_neg: 289.0410461425781 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 6.0783, loss_val: nan, pos_over_neg: 650.2849731445312 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 6.0774, loss_val: nan, pos_over_neg: 215.2547607421875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 6.0934, loss_val: nan, pos_over_neg: 257.0872497558594 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 6.0707, loss_val: nan, pos_over_neg: 777.4995727539062 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 6.0753, loss_val: nan, pos_over_neg: 444.6756591796875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 6.0793, loss_val: nan, pos_over_neg: 301.9212951660156 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 6.079, loss_val: nan, pos_over_neg: 324.0606689453125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 6.073, loss_val: nan, pos_over_neg: 412.1573791503906 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 6.0884, loss_val: nan, pos_over_neg: 225.4259033203125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 6.0751, loss_val: nan, pos_over_neg: 297.19268798828125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 6.0816, loss_val: nan, pos_over_neg: 470.4563903808594 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 6.0873, loss_val: nan, pos_over_neg: 306.38885498046875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 6.0795, loss_val: nan, pos_over_neg: 352.9260559082031 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 6.0661, loss_val: nan, pos_over_neg: 711.5982666015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 6.0699, loss_val: nan, pos_over_neg: 354.6532897949219 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 6.0655, loss_val: nan, pos_over_neg: 413.16119384765625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 6.0652, loss_val: nan, pos_over_neg: 327.6457824707031 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 6.0676, loss_val: nan, pos_over_neg: 594.8363037109375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 6.0587, loss_val: nan, pos_over_neg: 705.8067016601562 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 6.0521, loss_val: nan, pos_over_neg: 519.8228759765625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 6.0684, loss_val: nan, pos_over_neg: 184.04714965820312 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 6.0523, loss_val: nan, pos_over_neg: 301.35125732421875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 6.0491, loss_val: nan, pos_over_neg: 471.7295227050781 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 6.0428, loss_val: nan, pos_over_neg: 259.9798278808594 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 6.0524, loss_val: nan, pos_over_neg: 400.1877136230469 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 6.0377, loss_val: nan, pos_over_neg: 207.1383056640625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 6.034, loss_val: nan, pos_over_neg: 471.8005676269531 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 6.043, loss_val: nan, pos_over_neg: 351.039794921875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 6.0346, loss_val: nan, pos_over_neg: 231.77362060546875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 6.0487, loss_val: nan, pos_over_neg: 454.43988037109375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 6.0399, loss_val: nan, pos_over_neg: 327.4486083984375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 6.0505, loss_val: nan, pos_over_neg: 1095.07666015625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 6.0447, loss_val: nan, pos_over_neg: 233.69789123535156 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 6.0453, loss_val: nan, pos_over_neg: 452.3835754394531 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 6.037, loss_val: nan, pos_over_neg: 759.9591674804688 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 6.0422, loss_val: nan, pos_over_neg: 281.02911376953125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 6.0373, loss_val: nan, pos_over_neg: 796.0535888671875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 6.0648, loss_val: nan, pos_over_neg: 165.12342834472656 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 6.0314, loss_val: nan, pos_over_neg: 559.9335327148438 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 6.0456, loss_val: nan, pos_over_neg: 730.5363159179688 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 6.0525, loss_val: nan, pos_over_neg: 409.2726745605469 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 6.0353, loss_val: nan, pos_over_neg: 491.0304870605469 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 6.051, loss_val: nan, pos_over_neg: 702.9345703125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 6.0294, loss_val: nan, pos_over_neg: 340.8833923339844 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 6.048, loss_val: nan, pos_over_neg: 280.5970458984375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 6.0387, loss_val: nan, pos_over_neg: 989.1207275390625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 6.041, loss_val: nan, pos_over_neg: 395.1683044433594 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 6.0364, loss_val: nan, pos_over_neg: 619.4559326171875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 6.0288, loss_val: nan, pos_over_neg: 378.05950927734375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 6.0307, loss_val: nan, pos_over_neg: 296.3126220703125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 6.047, loss_val: nan, pos_over_neg: 347.1698303222656 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 6.0356, loss_val: nan, pos_over_neg: 486.1070861816406 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 6.0216, loss_val: nan, pos_over_neg: 330.8927307128906 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 6.0468, loss_val: nan, pos_over_neg: 514.5458984375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 6.0135, loss_val: nan, pos_over_neg: 429.8362731933594 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 6.0214, loss_val: nan, pos_over_neg: 458.36627197265625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 6.0224, loss_val: nan, pos_over_neg: 380.0051574707031 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 6.0206, loss_val: nan, pos_over_neg: 321.4898376464844 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 6.0266, loss_val: nan, pos_over_neg: 297.2198791503906 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 6.038, loss_val: nan, pos_over_neg: 280.510498046875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 6.0369, loss_val: nan, pos_over_neg: 484.8387451171875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 6.0252, loss_val: nan, pos_over_neg: 276.3338623046875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 6.014, loss_val: nan, pos_over_neg: 272.136962890625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 6.0305, loss_val: nan, pos_over_neg: 253.16497802734375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 6.0241, loss_val: nan, pos_over_neg: 312.755859375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 6.0196, loss_val: nan, pos_over_neg: 541.646728515625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 6.0091, loss_val: nan, pos_over_neg: 701.50146484375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 6.0271, loss_val: nan, pos_over_neg: 298.00994873046875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 206.1167449951172 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 6.0313, loss_val: nan, pos_over_neg: 326.91339111328125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 6.0265, loss_val: nan, pos_over_neg: 852.8408203125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 6.0361, loss_val: nan, pos_over_neg: 265.9700012207031 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 270.81365966796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 6.0234, loss_val: nan, pos_over_neg: 316.8646240234375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 6.0223, loss_val: nan, pos_over_neg: 291.8293762207031 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 6.041, loss_val: nan, pos_over_neg: 197.4921112060547 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 6.0172, loss_val: nan, pos_over_neg: 430.2823486328125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 6.0169, loss_val: nan, pos_over_neg: 417.15985107421875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 6.0243, loss_val: nan, pos_over_neg: 289.348876953125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 6.0318, loss_val: nan, pos_over_neg: 275.8428649902344 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 6.0272, loss_val: nan, pos_over_neg: 332.1764221191406 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 6.0283, loss_val: nan, pos_over_neg: 212.54364013671875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 6.0253, loss_val: nan, pos_over_neg: 312.5640563964844 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 6.0206, loss_val: nan, pos_over_neg: 488.9582824707031 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 6.0269, loss_val: nan, pos_over_neg: 295.5270080566406 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 6.0248, loss_val: nan, pos_over_neg: 623.2100830078125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 6.0054, loss_val: nan, pos_over_neg: 441.2593688964844 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 6.0178, loss_val: nan, pos_over_neg: 553.9671630859375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 6.0424, loss_val: nan, pos_over_neg: 285.1591491699219 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 6.0124, loss_val: nan, pos_over_neg: 295.082763671875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 6.017, loss_val: nan, pos_over_neg: 586.639892578125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 6.015, loss_val: nan, pos_over_neg: 741.6808471679688 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 6.0066, loss_val: nan, pos_over_neg: 603.4310913085938 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 6.0156, loss_val: nan, pos_over_neg: 488.43682861328125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 6.0233, loss_val: nan, pos_over_neg: 364.1496887207031 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 6.0127, loss_val: nan, pos_over_neg: 326.8819274902344 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 6.0146, loss_val: nan, pos_over_neg: 522.1526489257812 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 6.0158, loss_val: nan, pos_over_neg: 344.7747497558594 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 6.0057, loss_val: nan, pos_over_neg: 1621.1778564453125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 6.0065, loss_val: nan, pos_over_neg: 816.6354370117188 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 6.0254, loss_val: nan, pos_over_neg: 312.6867980957031 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 6.0099, loss_val: nan, pos_over_neg: 357.34814453125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 6.0162, loss_val: nan, pos_over_neg: 284.3551940917969 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 6.0168, loss_val: nan, pos_over_neg: 791.3878784179688 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 6.029, loss_val: nan, pos_over_neg: 370.4875183105469 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 6.0185, loss_val: nan, pos_over_neg: 1076.20654296875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 6.0077, loss_val: nan, pos_over_neg: 521.6690063476562 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9995, loss_val: nan, pos_over_neg: 472.94195556640625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9981, loss_val: nan, pos_over_neg: 425.32037353515625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 6.0039, loss_val: nan, pos_over_neg: 313.99005126953125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 6.0029, loss_val: nan, pos_over_neg: 699.0155029296875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 797.3381958007812 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 6.0267, loss_val: nan, pos_over_neg: 612.6932983398438 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9983, loss_val: nan, pos_over_neg: 345.137939453125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 6.0135, loss_val: nan, pos_over_neg: 425.4640197753906 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 6.0116, loss_val: nan, pos_over_neg: 727.261474609375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 6.007, loss_val: nan, pos_over_neg: 332.5481872558594 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 6.0006, loss_val: nan, pos_over_neg: 678.33935546875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 6.0005, loss_val: nan, pos_over_neg: 362.1055908203125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 6.0014, loss_val: nan, pos_over_neg: 397.28106689453125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 6.0086, loss_val: nan, pos_over_neg: 188.94125366210938 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 6.0073, loss_val: nan, pos_over_neg: 236.67625427246094 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.998, loss_val: nan, pos_over_neg: 686.1589965820312 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.994, loss_val: nan, pos_over_neg: 2101.10791015625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 224.02447509765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 6.0071, loss_val: nan, pos_over_neg: 458.53680419921875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 6.0098, loss_val: nan, pos_over_neg: 413.7962646484375 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 6.0074, loss_val: nan, pos_over_neg: 251.07623291015625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 6.0094, loss_val: nan, pos_over_neg: 372.53753662109375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 6.0061, loss_val: nan, pos_over_neg: 201.43946838378906 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 6.0053, loss_val: nan, pos_over_neg: 199.40274047851562 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 6.0069, loss_val: nan, pos_over_neg: 595.7850341796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9944, loss_val: nan, pos_over_neg: 261.79534912109375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 6.0142, loss_val: nan, pos_over_neg: 276.9334716796875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9967, loss_val: nan, pos_over_neg: 554.4759521484375 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 6.0126, loss_val: nan, pos_over_neg: 230.75479125976562 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9943, loss_val: nan, pos_over_neg: 364.5345764160156 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 6.0035, loss_val: nan, pos_over_neg: 284.0459899902344 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 6.0013, loss_val: nan, pos_over_neg: 321.671142578125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.983, loss_val: nan, pos_over_neg: 268.1811828613281 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 6.0108, loss_val: nan, pos_over_neg: 368.6581115722656 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9975, loss_val: nan, pos_over_neg: 260.7892150878906 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 6.0078, loss_val: nan, pos_over_neg: 294.9071350097656 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9943, loss_val: nan, pos_over_neg: 424.187255859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9972, loss_val: nan, pos_over_neg: 328.6465759277344 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9988, loss_val: nan, pos_over_neg: 198.1237030029297 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9912, loss_val: nan, pos_over_neg: 421.8459777832031 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9966, loss_val: nan, pos_over_neg: 433.11639404296875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 245.3419647216797 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.9813, loss_val: nan, pos_over_neg: 663.4561157226562 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9923, loss_val: nan, pos_over_neg: 363.6677551269531 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9945, loss_val: nan, pos_over_neg: 182.54562377929688 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 6.0218, loss_val: nan, pos_over_neg: 209.04847717285156 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 439.7975769042969 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9907, loss_val: nan, pos_over_neg: 304.47723388671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9935, loss_val: nan, pos_over_neg: 286.7227783203125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.9972, loss_val: nan, pos_over_neg: 798.4583129882812 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.9877, loss_val: nan, pos_over_neg: 241.68109130859375 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 6.001, loss_val: nan, pos_over_neg: 304.5649719238281 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 6.0041, loss_val: nan, pos_over_neg: 352.1695251464844 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9962, loss_val: nan, pos_over_neg: 143.14974975585938 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9953, loss_val: nan, pos_over_neg: 541.3110961914062 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 6.0006, loss_val: nan, pos_over_neg: 332.83642578125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 6.0049, loss_val: nan, pos_over_neg: 260.8045349121094 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9977, loss_val: nan, pos_over_neg: 534.9609375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 6.0031, loss_val: nan, pos_over_neg: 381.34893798828125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.999, loss_val: nan, pos_over_neg: 526.0549926757812 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9742, loss_val: nan, pos_over_neg: 390.7695617675781 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9819, loss_val: nan, pos_over_neg: 651.6304321289062 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 638.6755981445312 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9919, loss_val: nan, pos_over_neg: 1403.7445068359375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.9947, loss_val: nan, pos_over_neg: 1703.1883544921875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 365.23040771484375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9827, loss_val: nan, pos_over_neg: 323.74676513671875 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9865, loss_val: nan, pos_over_neg: 486.02728271484375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9856, loss_val: nan, pos_over_neg: 358.71612548828125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9956, loss_val: nan, pos_over_neg: 316.3340148925781 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 535.419921875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9902, loss_val: nan, pos_over_neg: 397.31536865234375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 6.0024, loss_val: nan, pos_over_neg: 817.0289916992188 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.9991, loss_val: nan, pos_over_neg: 356.09613037109375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9809, loss_val: nan, pos_over_neg: 689.1744384765625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.996, loss_val: nan, pos_over_neg: 1665.1263427734375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9905, loss_val: nan, pos_over_neg: 435.66302490234375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9885, loss_val: nan, pos_over_neg: 697.3364868164062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9868, loss_val: nan, pos_over_neg: 321.4059753417969 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 387.9443664550781 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9859, loss_val: nan, pos_over_neg: 964.3389282226562 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.9811, loss_val: nan, pos_over_neg: 348.5596923828125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 468.30377197265625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 1055.5089111328125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9761, loss_val: nan, pos_over_neg: 593.3245849609375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9866, loss_val: nan, pos_over_neg: 444.6846923828125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.97, loss_val: nan, pos_over_neg: 432.7095642089844 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 6.0011, loss_val: nan, pos_over_neg: 224.6630859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9829, loss_val: nan, pos_over_neg: 375.142578125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9708, loss_val: nan, pos_over_neg: 1106.371337890625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9828, loss_val: nan, pos_over_neg: 429.140625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9852, loss_val: nan, pos_over_neg: 1109.1390380859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9801, loss_val: nan, pos_over_neg: 744.0438232421875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9883, loss_val: nan, pos_over_neg: 415.6485290527344 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9822, loss_val: nan, pos_over_neg: 295.5062255859375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 357.0909118652344 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9779, loss_val: nan, pos_over_neg: 598.0786743164062 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9848, loss_val: nan, pos_over_neg: 500.034423828125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 1556.2220458984375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.9796, loss_val: nan, pos_over_neg: 1271.0286865234375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9632, loss_val: nan, pos_over_neg: 875.3516845703125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9748, loss_val: nan, pos_over_neg: 350.00946044921875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.9866, loss_val: nan, pos_over_neg: 255.02227783203125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9833, loss_val: nan, pos_over_neg: 466.1702575683594 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9776, loss_val: nan, pos_over_neg: 315.8426818847656 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9853, loss_val: nan, pos_over_neg: 446.5472412109375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9926, loss_val: nan, pos_over_neg: 341.73193359375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.975, loss_val: nan, pos_over_neg: 553.2890014648438 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9974, loss_val: nan, pos_over_neg: 200.58514404296875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9763, loss_val: nan, pos_over_neg: 340.252685546875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9888, loss_val: nan, pos_over_neg: 273.4892272949219 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.9936, loss_val: nan, pos_over_neg: 634.2509155273438 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9794, loss_val: nan, pos_over_neg: 364.9896240234375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9831, loss_val: nan, pos_over_neg: 4079.553466796875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9904, loss_val: nan, pos_over_neg: 310.3891906738281 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9816, loss_val: nan, pos_over_neg: 424.126953125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9856, loss_val: nan, pos_over_neg: 326.917724609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.9786, loss_val: nan, pos_over_neg: 534.4579467773438 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.9894, loss_val: nan, pos_over_neg: 384.82830810546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9923, loss_val: nan, pos_over_neg: 262.1011047363281 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9903, loss_val: nan, pos_over_neg: 360.9793701171875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9847, loss_val: nan, pos_over_neg: 380.3506164550781 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.982, loss_val: nan, pos_over_neg: 345.999267578125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 326.1825256347656 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9909, loss_val: nan, pos_over_neg: 260.21685791015625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.98, loss_val: nan, pos_over_neg: 280.16668701171875 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9754, loss_val: nan, pos_over_neg: 363.1784973144531 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9702, loss_val: nan, pos_over_neg: 1449.3966064453125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 289.8865966796875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 289.4479064941406 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.979, loss_val: nan, pos_over_neg: 914.9896240234375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9736, loss_val: nan, pos_over_neg: 515.9573364257812 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9753, loss_val: nan, pos_over_neg: 352.6732177734375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.9887, loss_val: nan, pos_over_neg: 281.5078125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9643, loss_val: nan, pos_over_neg: 405.97137451171875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9869, loss_val: nan, pos_over_neg: 252.3746795654297 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.974, loss_val: nan, pos_over_neg: 479.5358581542969 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.9686, loss_val: nan, pos_over_neg: 467.404296875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 280.5625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 388.8982238769531 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.968, loss_val: nan, pos_over_neg: 328.3877258300781 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9661, loss_val: nan, pos_over_neg: 348.0988464355469 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 475.9297790527344 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9719, loss_val: nan, pos_over_neg: 533.7483520507812 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 347.9527282714844 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9756, loss_val: nan, pos_over_neg: 401.15167236328125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.9649, loss_val: nan, pos_over_neg: 505.47259521484375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9825, loss_val: nan, pos_over_neg: 285.10888671875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 338.5120849609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 831.5810546875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 444.3211975097656 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 579.236083984375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 513.3201904296875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9711, loss_val: nan, pos_over_neg: 631.256103515625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9727, loss_val: nan, pos_over_neg: 1426.1546630859375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.9802, loss_val: nan, pos_over_neg: 527.7888793945312 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9717, loss_val: nan, pos_over_neg: 980.5802001953125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.985, loss_val: nan, pos_over_neg: 214.9716033935547 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 399.97320556640625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9714, loss_val: nan, pos_over_neg: 802.7356567382812 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9724, loss_val: nan, pos_over_neg: 763.7575073242188 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.9604, loss_val: nan, pos_over_neg: 444.42230224609375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 636.3869018554688 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9835, loss_val: nan, pos_over_neg: 181.58750915527344 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.9652, loss_val: nan, pos_over_neg: 287.7917785644531 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9576, loss_val: nan, pos_over_neg: 524.563232421875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9778, loss_val: nan, pos_over_neg: 1019.3594970703125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9697, loss_val: nan, pos_over_neg: 1689.9288330078125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.9839, loss_val: nan, pos_over_neg: 354.71600341796875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9671, loss_val: nan, pos_over_neg: 590.0376586914062 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 460.851318359375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 397.1617736816406 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 522.3746948242188 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.9533, loss_val: nan, pos_over_neg: 363.359619140625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9703, loss_val: nan, pos_over_neg: 397.63629150390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.9572, loss_val: nan, pos_over_neg: 1056.385009765625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9631, loss_val: nan, pos_over_neg: 337.3144836425781 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.986, loss_val: nan, pos_over_neg: 471.2216491699219 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 814.7364501953125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9725, loss_val: nan, pos_over_neg: 325.99212646484375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 330.8762512207031 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9561, loss_val: nan, pos_over_neg: 561.5689086914062 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.9692, loss_val: nan, pos_over_neg: 403.0609436035156 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 294.8686828613281 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9872, loss_val: nan, pos_over_neg: 227.85549926757812 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.964, loss_val: nan, pos_over_neg: 518.2548828125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9566, loss_val: nan, pos_over_neg: 405.6081848144531 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9562, loss_val: nan, pos_over_neg: 363.4761962890625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9658, loss_val: nan, pos_over_neg: 1350.0311279296875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9688, loss_val: nan, pos_over_neg: 562.5449829101562 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.958, loss_val: nan, pos_over_neg: 527.8424682617188 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9708, loss_val: nan, pos_over_neg: 170.3203582763672 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9701, loss_val: nan, pos_over_neg: 375.41253662109375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.9874, loss_val: nan, pos_over_neg: 491.2532958984375 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9755, loss_val: nan, pos_over_neg: 581.0638427734375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.9556, loss_val: nan, pos_over_neg: 420.9991455078125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.9529, loss_val: nan, pos_over_neg: 647.7720947265625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9637, loss_val: nan, pos_over_neg: 387.8565979003906 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9637, loss_val: nan, pos_over_neg: 602.4642333984375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9673, loss_val: nan, pos_over_neg: 273.7647705078125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.958, loss_val: nan, pos_over_neg: 279.2402648925781 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.9689, loss_val: nan, pos_over_neg: 372.6683349609375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.9648, loss_val: nan, pos_over_neg: 492.689208984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 266.03216552734375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.9748, loss_val: nan, pos_over_neg: 380.1661376953125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 1245.0985107421875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.9718, loss_val: nan, pos_over_neg: 300.683837890625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 400.70025634765625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9596, loss_val: nan, pos_over_neg: 752.4973754882812 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9684, loss_val: nan, pos_over_neg: 711.4688110351562 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 426.71441650390625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 477.8011779785156 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.966, loss_val: nan, pos_over_neg: 788.5437622070312 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9587, loss_val: nan, pos_over_neg: 684.5816040039062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.9573, loss_val: nan, pos_over_neg: 577.9237060546875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.9668, loss_val: nan, pos_over_neg: 647.7173461914062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 304.7799377441406 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.9541, loss_val: nan, pos_over_neg: 628.6376342773438 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9603, loss_val: nan, pos_over_neg: 390.98529052734375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.969, loss_val: nan, pos_over_neg: 528.0709838867188 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.9509, loss_val: nan, pos_over_neg: 753.7571411132812 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.955, loss_val: nan, pos_over_neg: 307.4422912597656 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9487, loss_val: nan, pos_over_neg: 582.3737182617188 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9486, loss_val: nan, pos_over_neg: 459.1286315917969 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 638.9457397460938 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 505.7932434082031 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.9547, loss_val: nan, pos_over_neg: 295.4744873046875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.9597, loss_val: nan, pos_over_neg: 559.9427490234375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 513.8881225585938 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.9585, loss_val: nan, pos_over_neg: 245.14308166503906 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 267.65301513671875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 1109.0467529296875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 570.7171020507812 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.9495, loss_val: nan, pos_over_neg: 387.25482177734375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: 457.1441650390625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 758.5652465820312 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.941, loss_val: nan, pos_over_neg: 506.2863464355469 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.9563, loss_val: nan, pos_over_neg: 212.12261962890625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.9641, loss_val: nan, pos_over_neg: 355.7885437011719 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.9577, loss_val: nan, pos_over_neg: 280.2891540527344 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.9578, loss_val: nan, pos_over_neg: 310.9286804199219 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 497.00518798828125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9617, loss_val: nan, pos_over_neg: 345.716064453125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.9618, loss_val: nan, pos_over_neg: 345.5352478027344 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.9601, loss_val: nan, pos_over_neg: 249.27456665039062 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 336.2367858886719 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 302.8459167480469 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.9583, loss_val: nan, pos_over_neg: 310.4255065917969 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 482.7864685058594 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 526.7140502929688 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.9567, loss_val: nan, pos_over_neg: 787.7354736328125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9559, loss_val: nan, pos_over_neg: 310.3905944824219 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 1130.9930419921875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 361.7742004394531 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.96, loss_val: nan, pos_over_neg: 283.0462341308594 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 456.88134765625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.9522, loss_val: nan, pos_over_neg: 1395.8590087890625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.953, loss_val: nan, pos_over_neg: 351.56744384765625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 404.8465881347656 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9548, loss_val: nan, pos_over_neg: 834.1912231445312 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.9625, loss_val: nan, pos_over_neg: 600.7078857421875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 457.79132080078125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.9602, loss_val: nan, pos_over_neg: 506.59039306640625 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 360.3618469238281 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 331.8295593261719 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1207.607421875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 668.166015625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 562.8670654296875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 455.17596435546875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 467.09222412109375 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 334.7424011230469 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9492, loss_val: nan, pos_over_neg: 379.9796142578125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.9503, loss_val: nan, pos_over_neg: 385.5716247558594 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 669.1920166015625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.9453, loss_val: nan, pos_over_neg: 1418.068359375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 793.9945068359375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.9361, loss_val: nan, pos_over_neg: 1021.8470458984375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 1859.16357421875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9344, loss_val: nan, pos_over_neg: 1273.3258056640625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 448.94073486328125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9551, loss_val: nan, pos_over_neg: 323.3120422363281 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 1719.061279296875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: 1019.7810668945312 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 1878.483154296875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.9592, loss_val: nan, pos_over_neg: 5497.77783203125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.9517, loss_val: nan, pos_over_neg: 761.1292114257812 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.951, loss_val: nan, pos_over_neg: 546.5892944335938 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9468, loss_val: nan, pos_over_neg: 616.3781127929688 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 565.6038818359375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 594.2412109375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 696.5479736328125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.9503, loss_val: nan, pos_over_neg: 442.2668151855469 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 441.2154235839844 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9515, loss_val: nan, pos_over_neg: 274.1280212402344 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9412, loss_val: nan, pos_over_neg: 741.535400390625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 570.3773193359375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.957, loss_val: nan, pos_over_neg: 330.7912902832031 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.9449, loss_val: nan, pos_over_neg: 449.1583557128906 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 272.4512939453125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9407, loss_val: nan, pos_over_neg: 641.9605712890625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.9323, loss_val: nan, pos_over_neg: 419.16583251953125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 472.5428771972656 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 751.854248046875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9537, loss_val: nan, pos_over_neg: 483.9403381347656 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9454, loss_val: nan, pos_over_neg: 478.2812805175781 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 951.941162109375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.9544, loss_val: nan, pos_over_neg: 819.1432495117188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9388, loss_val: nan, pos_over_neg: 530.7918701171875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.9545, loss_val: nan, pos_over_neg: 862.9605712890625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 778.3824462890625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 455.1096496582031 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 1021.1266479492188 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 392.7335205078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.9435, loss_val: nan, pos_over_neg: 367.7323913574219 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 347.4850769042969 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9405, loss_val: nan, pos_over_neg: 417.3964538574219 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.9518, loss_val: nan, pos_over_neg: 433.94805908203125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9432, loss_val: nan, pos_over_neg: 508.242431640625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 914.7380981445312 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.9338, loss_val: nan, pos_over_neg: 567.7254028320312 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: 283.14971923828125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 692.2200317382812 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.9437, loss_val: nan, pos_over_neg: 1078.763427734375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 775.925048828125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.939, loss_val: nan, pos_over_neg: 459.43359375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 1662.10986328125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.9459, loss_val: nan, pos_over_neg: 567.6834716796875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 884.904052734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.9424, loss_val: nan, pos_over_neg: 896.2153930664062 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.9467, loss_val: nan, pos_over_neg: 673.8367919921875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 537.9749755859375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 876.4833374023438 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.9443, loss_val: nan, pos_over_neg: 500.5957336425781 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 1368.1717529296875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.954, loss_val: nan, pos_over_neg: 363.4567565917969 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 402.57684326171875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9381, loss_val: nan, pos_over_neg: 835.8206787109375 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9403, loss_val: nan, pos_over_neg: 471.7430725097656 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 413.8622131347656 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.9574, loss_val: nan, pos_over_neg: 340.1268615722656 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.943, loss_val: nan, pos_over_neg: 2398.147216796875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 418.0240173339844 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.9425, loss_val: nan, pos_over_neg: 457.5960998535156 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.9399, loss_val: nan, pos_over_neg: 543.4943237304688 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9458, loss_val: nan, pos_over_neg: 2201.671142578125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.9571, loss_val: nan, pos_over_neg: 357.6556396484375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9531, loss_val: nan, pos_over_neg: 230.73641967773438 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 393.56536865234375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 603.5847778320312 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 294.6625671386719 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 320.4703369140625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9575, loss_val: nan, pos_over_neg: 340.27728271484375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: 535.7688598632812 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 429.1997375488281 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 567.7555541992188 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9452, loss_val: nan, pos_over_neg: 608.7025756835938 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 1347.41162109375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.94, loss_val: nan, pos_over_neg: 730.1399536132812 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.9493, loss_val: nan, pos_over_neg: 894.108154296875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 510.74896240234375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9582, loss_val: nan, pos_over_neg: 270.753173828125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.9456, loss_val: nan, pos_over_neg: 432.15423583984375 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 374.9728698730469 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 938.7119140625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 7992.9169921875 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 1127.022705078125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.925, loss_val: nan, pos_over_neg: -8376.982421875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.9612, loss_val: nan, pos_over_neg: 781.6016845703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 900.3347778320312 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 396.3359680175781 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.9398, loss_val: nan, pos_over_neg: 520.4799194335938 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.9526, loss_val: nan, pos_over_neg: 440.4091491699219 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 329.45843505859375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.9444, loss_val: nan, pos_over_neg: 639.0092163085938 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.9359, loss_val: nan, pos_over_neg: 1161.7586669921875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.9462, loss_val: nan, pos_over_neg: 569.4191284179688 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.9457, loss_val: nan, pos_over_neg: 234.5037078857422 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 678.8472290039062 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.9408, loss_val: nan, pos_over_neg: 363.5134582519531 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 476.5901184082031 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 478.2394104003906 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.9505, loss_val: nan, pos_over_neg: 365.8416442871094 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 1512.91845703125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 620.2049560546875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 465.924072265625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.9451, loss_val: nan, pos_over_neg: 395.8296813964844 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 611.0557250976562 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.9384, loss_val: nan, pos_over_neg: 434.66326904296875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 424.8110046386719 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.9374, loss_val: nan, pos_over_neg: 532.995849609375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 1089.7967529296875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.9368, loss_val: nan, pos_over_neg: 2213.56201171875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.9429, loss_val: nan, pos_over_neg: 777.2316284179688 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 708.2366333007812 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9516, loss_val: nan, pos_over_neg: 505.4147644042969 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 854.9719848632812 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 310.342529296875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.9378, loss_val: nan, pos_over_neg: 397.71917724609375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.944, loss_val: nan, pos_over_neg: 2046.41357421875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.9427, loss_val: nan, pos_over_neg: 951.3368530273438 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.9278, loss_val: nan, pos_over_neg: 1166.5247802734375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 2506.238525390625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.9389, loss_val: nan, pos_over_neg: 1812.9569091796875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 950.2203369140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9512, loss_val: nan, pos_over_neg: 550.1015014648438 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9434, loss_val: nan, pos_over_neg: 325.3045349121094 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.9258, loss_val: nan, pos_over_neg: 1031.2808837890625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 786.6578369140625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 1049.0975341796875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9508, loss_val: nan, pos_over_neg: 429.4016418457031 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 1069.5794677734375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 559.1255493164062 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.9476, loss_val: nan, pos_over_neg: 344.9280090332031 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 587.106689453125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.9337, loss_val: nan, pos_over_neg: 372.6478271484375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9413, loss_val: nan, pos_over_neg: 326.02288818359375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 635.513671875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 414.38055419921875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9316, loss_val: nan, pos_over_neg: 675.6995849609375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.9265, loss_val: nan, pos_over_neg: 897.6427001953125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 306.1666564941406 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.9251, loss_val: nan, pos_over_neg: 820.5632934570312 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 908.427734375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.9447, loss_val: nan, pos_over_neg: 725.060302734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9354, loss_val: nan, pos_over_neg: 356.1870422363281 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.9318, loss_val: nan, pos_over_neg: 2898.659423828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.9371, loss_val: nan, pos_over_neg: 489.6125793457031 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.9426, loss_val: nan, pos_over_neg: 350.5506591796875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9355, loss_val: nan, pos_over_neg: 819.1332397460938 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 419.6957702636719 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.9329, loss_val: nan, pos_over_neg: 863.8773193359375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 580.7063598632812 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 1461.8211669921875 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1524.8953857421875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.9362, loss_val: nan, pos_over_neg: 495.2906188964844 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.9411, loss_val: nan, pos_over_neg: 389.2862854003906 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.935, loss_val: nan, pos_over_neg: 361.9350280761719 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 562.1588745117188 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 623.1642456054688 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 948.1382446289062 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.9351, loss_val: nan, pos_over_neg: 319.84039306640625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.933, loss_val: nan, pos_over_neg: 780.8867797851562 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.9326, loss_val: nan, pos_over_neg: 743.9932250976562 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 490.29931640625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.9444, loss_val: nan, pos_over_neg: 346.3164978027344 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 384.31744384765625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 897.1698608398438 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 655.0762329101562 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.938, loss_val: nan, pos_over_neg: 801.9020385742188 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 820.3717041015625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.936, loss_val: nan, pos_over_neg: 675.1707763671875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.9423, loss_val: nan, pos_over_neg: 555.9066772460938 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.9386, loss_val: nan, pos_over_neg: 478.86590576171875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 383.6432800292969 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 368.208251953125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 512.9080810546875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 457.1972351074219 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 729.4878540039062 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9479, loss_val: nan, pos_over_neg: 477.44427490234375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.9313, loss_val: nan, pos_over_neg: 1114.987060546875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.9431, loss_val: nan, pos_over_neg: 485.91339111328125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.9291, loss_val: nan, pos_over_neg: 395.8935852050781 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.9268, loss_val: nan, pos_over_neg: 513.986572265625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 479.8262634277344 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.937, loss_val: nan, pos_over_neg: 338.7804870605469 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.9404, loss_val: nan, pos_over_neg: 523.9180297851562 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 1458.919189453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.9186, loss_val: nan, pos_over_neg: 855.93310546875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 495.0271301269531 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.9342, loss_val: nan, pos_over_neg: 383.6399230957031 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 699.5416870117188 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 421.5114440917969 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9385, loss_val: nan, pos_over_neg: 276.98333740234375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.925, loss_val: nan, pos_over_neg: 431.0152587890625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 607.8488159179688 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.9308, loss_val: nan, pos_over_neg: 341.8612365722656 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 342.01171875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.9496, loss_val: nan, pos_over_neg: 286.4357604980469 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9397, loss_val: nan, pos_over_neg: 453.6709289550781 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.9365, loss_val: nan, pos_over_neg: 436.68316650390625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9283, loss_val: nan, pos_over_neg: 512.641845703125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 750.504150390625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 272.3826599121094 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 654.3102416992188 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.9307, loss_val: nan, pos_over_neg: 655.2653198242188 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.9301, loss_val: nan, pos_over_neg: 350.788330078125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9275, loss_val: nan, pos_over_neg: 402.70025634765625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.9347, loss_val: nan, pos_over_neg: 493.63702392578125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 352.0485534667969 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 899.8087158203125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [20:13<101118:24:24, 1213.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.9285, loss_val: nan, pos_over_neg: 424.8764343261719 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.9314, loss_val: nan, pos_over_neg: 352.0105895996094 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 776.2827758789062 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.9262, loss_val: nan, pos_over_neg: 415.7971496582031 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.9469, loss_val: nan, pos_over_neg: 326.228515625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 752.5911865234375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.9346, loss_val: nan, pos_over_neg: 308.29833984375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 526.2825927734375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.9393, loss_val: nan, pos_over_neg: 655.4852905273438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 568.2061157226562 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.9272, loss_val: nan, pos_over_neg: 432.5284729003906 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 622.9012451171875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 867.1334228515625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 596.6976928710938 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.9415, loss_val: nan, pos_over_neg: 227.84722900390625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 543.7960205078125 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.9375, loss_val: nan, pos_over_neg: 414.6020812988281 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.9302, loss_val: nan, pos_over_neg: 403.8240051269531 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.923, loss_val: nan, pos_over_neg: 1930.5277099609375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 959.24853515625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 1408.3936767578125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 6919.19677734375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1237.2860107421875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.9198, loss_val: nan, pos_over_neg: 462.5740966796875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 455.96783447265625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 629.80126953125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9315, loss_val: nan, pos_over_neg: 500.6127014160156 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 1074.6651611328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 717.3128051757812 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 791.0789184570312 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 299.93841552734375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 7008.8291015625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 755.0277099609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.9243, loss_val: nan, pos_over_neg: 570.562255859375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.9271, loss_val: nan, pos_over_neg: 731.1220092773438 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 954.0985717773438 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.931, loss_val: nan, pos_over_neg: 725.6748657226562 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 907.4951171875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 2314.73828125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.9333, loss_val: nan, pos_over_neg: 889.0150756835938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 609.8812866210938 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.9343, loss_val: nan, pos_over_neg: 566.1915893554688 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1075.980712890625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 493.85296630859375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 575.8409423828125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.9236, loss_val: nan, pos_over_neg: 624.1416625976562 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 762.7825317382812 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 682.1261596679688 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 937.0512084960938 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 971.3603515625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.9364, loss_val: nan, pos_over_neg: 777.907470703125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 1106.5830078125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 840.9954223632812 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 655.6151733398438 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 666.9992065429688 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 431.6743469238281 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.9084, loss_val: nan, pos_over_neg: 989.1925048828125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.9387, loss_val: nan, pos_over_neg: 2081.658447265625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.9372, loss_val: nan, pos_over_neg: 463.22802734375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9341, loss_val: nan, pos_over_neg: 375.25946044921875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 352.8844909667969 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1249.7337646484375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 500.2584228515625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 1094.9949951171875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 537.9716186523438 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 733.4186401367188 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 417.273681640625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 277.8742370605469 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9224, loss_val: nan, pos_over_neg: 574.578369140625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 786.2880249023438 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 1512.6676025390625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.921, loss_val: nan, pos_over_neg: 439.541748046875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 562.6143188476562 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 1337.8682861328125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9259, loss_val: nan, pos_over_neg: 396.372314453125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 412.5483093261719 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 513.9580688476562 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9229, loss_val: nan, pos_over_neg: 471.8913879394531 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 445.9011535644531 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.927, loss_val: nan, pos_over_neg: 928.929443359375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 709.7760009765625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9216, loss_val: nan, pos_over_neg: 415.59063720703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 1518.376220703125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 758.2110595703125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 490.2065124511719 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9191, loss_val: nan, pos_over_neg: 394.841796875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9294, loss_val: nan, pos_over_neg: 452.0814208984375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 492.1363525390625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9296, loss_val: nan, pos_over_neg: 553.1380004882812 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1311.2117919921875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 540.048095703125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9138, loss_val: nan, pos_over_neg: 723.7667236328125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 4358.7197265625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.932, loss_val: nan, pos_over_neg: 900.8324584960938 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 664.9981689453125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 361.31146240234375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 849.870361328125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 624.645751953125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9287, loss_val: nan, pos_over_neg: 890.4398803710938 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9245, loss_val: nan, pos_over_neg: 800.6766357421875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9281, loss_val: nan, pos_over_neg: 672.4352416992188 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 1598.2286376953125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 949.953125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.9339, loss_val: nan, pos_over_neg: 768.8717041015625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 954.9060668945312 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9357, loss_val: nan, pos_over_neg: 351.1263122558594 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 935.4710083007812 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 402.9378662109375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 1608.2105712890625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9335, loss_val: nan, pos_over_neg: 723.9801635742188 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9219, loss_val: nan, pos_over_neg: 1345.3309326171875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9277, loss_val: nan, pos_over_neg: 660.1790771484375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 706.4228515625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9348, loss_val: nan, pos_over_neg: 378.8246765136719 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 795.2124633789062 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 1050.305908203125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9367, loss_val: nan, pos_over_neg: 781.268798828125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 463.8302307128906 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 2355.088134765625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 388.83172607421875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9327, loss_val: nan, pos_over_neg: 329.9823913574219 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9383, loss_val: nan, pos_over_neg: 599.0728759765625 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9156, loss_val: nan, pos_over_neg: 755.8533325195312 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 476.5677185058594 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9188, loss_val: nan, pos_over_neg: 430.1590576171875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 640.1522216796875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9356, loss_val: nan, pos_over_neg: 421.2621765136719 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9237, loss_val: nan, pos_over_neg: 276.10382080078125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9269, loss_val: nan, pos_over_neg: 1163.8341064453125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9266, loss_val: nan, pos_over_neg: 557.727294921875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9135, loss_val: nan, pos_over_neg: 581.701416015625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 727.439697265625 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 1395.19189453125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 768.8729248046875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 984.1025390625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 623.7294921875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 1207.782958984375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 773.6776733398438 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 512.533935546875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 652.1782836914062 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9182, loss_val: nan, pos_over_neg: 891.208984375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9244, loss_val: nan, pos_over_neg: 688.7025756835938 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 1811.37841796875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.9263, loss_val: nan, pos_over_neg: 804.9833984375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9293, loss_val: nan, pos_over_neg: 352.3024597167969 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 1735.5928955078125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.9392, loss_val: nan, pos_over_neg: 732.0902709960938 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 753.0628662109375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 2250.4677734375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9133, loss_val: nan, pos_over_neg: 599.2727661132812 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9209, loss_val: nan, pos_over_neg: 865.71484375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9215, loss_val: nan, pos_over_neg: 507.975830078125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 743.6475219726562 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 517.0411987304688 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 1057.1966552734375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.9235, loss_val: nan, pos_over_neg: 836.147705078125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.919, loss_val: nan, pos_over_neg: 1228.302978515625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 1654.596923828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 675.748291015625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 569.6160888671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9197, loss_val: nan, pos_over_neg: 384.6114501953125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9049, loss_val: nan, pos_over_neg: 754.8443603515625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 542.3576049804688 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9274, loss_val: nan, pos_over_neg: 390.20263671875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 443.0591125488281 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 484.8516845703125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 481.1258544921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 1271.296875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.917, loss_val: nan, pos_over_neg: 778.560546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.9304, loss_val: nan, pos_over_neg: 413.9942321777344 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.92, loss_val: nan, pos_over_neg: 1629.0887451171875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.9173, loss_val: nan, pos_over_neg: 1765.54296875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 794.1809692382812 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 564.4993286132812 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 396.4543762207031 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9174, loss_val: nan, pos_over_neg: 425.84893798828125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 659.2305297851562 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.9194, loss_val: nan, pos_over_neg: 787.2550048828125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 1357.591796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 616.2926635742188 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 498.30126953125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: -21379.80078125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 1367.106689453125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 479.3230285644531 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 842.1658325195312 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.9322, loss_val: nan, pos_over_neg: 743.0635375976562 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.9228, loss_val: nan, pos_over_neg: 379.64312744140625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 749.6964111328125 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 324.2972717285156 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.924, loss_val: nan, pos_over_neg: 414.10400390625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.916, loss_val: nan, pos_over_neg: 560.4808349609375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 912.3444213867188 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.9142, loss_val: nan, pos_over_neg: 599.1796264648438 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 511.2461242675781 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.9168, loss_val: nan, pos_over_neg: 1298.6932373046875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 460.8024597167969 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.9126, loss_val: nan, pos_over_neg: 386.5728454589844 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.9221, loss_val: nan, pos_over_neg: 292.20220947265625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.9223, loss_val: nan, pos_over_neg: 296.7071838378906 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 331.0606994628906 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 477.9125061035156 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 542.1668090820312 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 860.1044311523438 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 479.740234375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 723.0365600585938 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.9217, loss_val: nan, pos_over_neg: 660.6371459960938 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 661.9706420898438 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 701.9006958007812 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 9617.4365234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 465.4388732910156 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.9157, loss_val: nan, pos_over_neg: 444.3747863769531 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.9177, loss_val: nan, pos_over_neg: 990.1255493164062 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.9136, loss_val: nan, pos_over_neg: 364.60888671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.9098, loss_val: nan, pos_over_neg: 867.602294921875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 538.0331420898438 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 684.4474487304688 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.93, loss_val: nan, pos_over_neg: 1170.2740478515625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 352.5603332519531 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.9169, loss_val: nan, pos_over_neg: 400.8402099609375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.9108, loss_val: nan, pos_over_neg: 862.7478637695312 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 7784.5849609375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 674.8997192382812 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.9111, loss_val: nan, pos_over_neg: 624.3306884765625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.9206, loss_val: nan, pos_over_neg: 717.5868530273438 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.926, loss_val: nan, pos_over_neg: 566.877197265625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 762.1558227539062 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.9225, loss_val: nan, pos_over_neg: 622.4279174804688 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 1262.171142578125 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 1333.4627685546875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1455.895263671875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 1134.2640380859375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.9163, loss_val: nan, pos_over_neg: 380.3961486816406 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 2644.598876953125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 3574.937744140625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.9249, loss_val: nan, pos_over_neg: 615.5647583007812 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 3137.802001953125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 1774.3443603515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 682.0838012695312 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.9103, loss_val: nan, pos_over_neg: 643.5286865234375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.9253, loss_val: nan, pos_over_neg: 459.42010498046875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 845.272216796875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.9185, loss_val: nan, pos_over_neg: 787.2783813476562 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 1822.3203125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 699.0394897460938 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 2933.888916015625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.9134, loss_val: nan, pos_over_neg: 1440.3134765625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 721.4447021484375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 551.4442749023438 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 497.7862548828125 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.9295, loss_val: nan, pos_over_neg: 451.1436767578125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 695.098876953125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1554.1876220703125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 496.0312194824219 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.9063, loss_val: nan, pos_over_neg: 465.69927978515625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 816.39013671875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 542.8995971679688 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 833.1382446289062 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 798.5576171875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 492.2353515625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.9053, loss_val: nan, pos_over_neg: 727.4207153320312 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.9227, loss_val: nan, pos_over_neg: 1038.9842529296875 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.9105, loss_val: nan, pos_over_neg: 739.5665283203125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.9233, loss_val: nan, pos_over_neg: 764.753662109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.9248, loss_val: nan, pos_over_neg: 328.1366882324219 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9119, loss_val: nan, pos_over_neg: 1452.0855712890625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.9207, loss_val: nan, pos_over_neg: 668.6106567382812 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 591.0809936523438 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.9022, loss_val: nan, pos_over_neg: 839.2102661132812 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 887.6694946289062 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 608.1176147460938 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.9106, loss_val: nan, pos_over_neg: 392.32537841796875 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1127.42431640625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.9317, loss_val: nan, pos_over_neg: 629.4315185546875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.9239, loss_val: nan, pos_over_neg: 490.0606994628906 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 329.33306884765625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 522.8284301757812 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 1526.4073486328125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 650.666259765625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 268.42706298828125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.9201, loss_val: nan, pos_over_neg: 223.41079711914062 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 401.19134521484375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 658.3914794921875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 662.3145751953125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 1966.68115234375 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9144, loss_val: nan, pos_over_neg: 703.2575073242188 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.9035, loss_val: nan, pos_over_neg: 1141.5318603515625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8964, loss_val: nan, pos_over_neg: 1220.74169921875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.9189, loss_val: nan, pos_over_neg: 556.2164306640625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.9279, loss_val: nan, pos_over_neg: 298.7792663574219 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.9161, loss_val: nan, pos_over_neg: 478.2953186035156 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 630.6995239257812 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 413.04205322265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 776.33935546875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1166.9432373046875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1254.9000244140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 637.3419799804688 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.9183, loss_val: nan, pos_over_neg: 1366.5257568359375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.9129, loss_val: nan, pos_over_neg: 858.4903564453125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 869.4415283203125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 646.8131713867188 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 241.65255737304688 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 402.9285888671875 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 393.9948425292969 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 727.0217895507812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 737.4534912109375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.9042, loss_val: nan, pos_over_neg: 1032.1085205078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.9226, loss_val: nan, pos_over_neg: 800.71044921875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 2010.1656494140625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.9192, loss_val: nan, pos_over_neg: 810.3557739257812 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 975.8497924804688 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 415.44879150390625 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 587.4814453125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.9093, loss_val: nan, pos_over_neg: 1393.5565185546875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.9041, loss_val: nan, pos_over_neg: 777.955322265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.9114, loss_val: nan, pos_over_neg: 477.1449279785156 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: 1823.0047607421875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 403.2022705078125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 429.3100891113281 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 848.8464965820312 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 2048.958984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 594.8534545898438 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 958.056396484375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 787.8374633789062 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 761.110107421875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 906.7982788085938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 605.7329711914062 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.9146, loss_val: nan, pos_over_neg: 368.67877197265625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.9099, loss_val: nan, pos_over_neg: 698.219482421875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 3978.905517578125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.9124, loss_val: nan, pos_over_neg: 493.6557312011719 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 397.5929870605469 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 563.4589233398438 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 1265.3052978515625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 464.8829040527344 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 345.4703674316406 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.9083, loss_val: nan, pos_over_neg: 435.4240417480469 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.9122, loss_val: nan, pos_over_neg: 366.90850830078125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 574.5353393554688 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 995.12109375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.9059, loss_val: nan, pos_over_neg: 746.8809814453125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 1151.8966064453125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.9056, loss_val: nan, pos_over_neg: 686.0681762695312 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 529.8804321289062 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 700.13134765625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 1335.20556640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 571.6119995117188 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 981.7589721679688 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 3262.0791015625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.9116, loss_val: nan, pos_over_neg: 493.29132080078125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 560.9379272460938 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 345.8763122558594 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 665.7710571289062 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 859.0939331054688 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 374.172119140625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 935.3250732421875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 1020.2460327148438 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 802.4857788085938 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 682.4148559570312 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.9086, loss_val: nan, pos_over_neg: 609.0037841796875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 417.14764404296875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 471.5537109375 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.9048, loss_val: nan, pos_over_neg: 457.95379638671875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.918, loss_val: nan, pos_over_neg: 809.516845703125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 512.275390625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 786.7392578125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 2769.8994140625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 1536.570556640625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 482.8072814941406 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 647.1224365234375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 752.712890625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 833.7823486328125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 969.054931640625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.9165, loss_val: nan, pos_over_neg: 575.2570190429688 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.914, loss_val: nan, pos_over_neg: 621.2944946289062 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.9195, loss_val: nan, pos_over_neg: 568.9425659179688 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 574.9514770507812 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.911, loss_val: nan, pos_over_neg: 432.90301513671875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 629.7383422851562 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.903, loss_val: nan, pos_over_neg: 610.7008666992188 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.9282, loss_val: nan, pos_over_neg: 282.1377258300781 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 466.5179138183594 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 681.2111206054688 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.9125, loss_val: nan, pos_over_neg: 659.8326416015625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 514.3877563476562 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 1000.3394165039062 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.9171, loss_val: nan, pos_over_neg: 772.6347045898438 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.9109, loss_val: nan, pos_over_neg: 726.9435424804688 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 5004.4296875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1154.6474609375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 695.6729125976562 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 470.05242919921875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 430.40777587890625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.9115, loss_val: nan, pos_over_neg: 829.8978881835938 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 434.4809265136719 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 741.4396362304688 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.9074, loss_val: nan, pos_over_neg: 650.0127563476562 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 691.923828125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.9213, loss_val: nan, pos_over_neg: 395.1574401855469 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 662.97265625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.9184, loss_val: nan, pos_over_neg: 428.87506103515625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 562.1824951171875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 367.92706298828125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.9193, loss_val: nan, pos_over_neg: 434.976318359375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.9164, loss_val: nan, pos_over_neg: 473.7510681152344 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 814.7197875976562 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 392.97052001953125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.912, loss_val: nan, pos_over_neg: 465.2379150390625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8987, loss_val: nan, pos_over_neg: 762.6751708984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 604.1585083007812 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 858.7197875976562 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.913, loss_val: nan, pos_over_neg: 568.0438232421875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 504.9197692871094 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 397.5751037597656 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 570.1532592773438 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 4078.823486328125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: 481.94403076171875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 1105.565673828125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.9166, loss_val: nan, pos_over_neg: 633.3406372070312 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.9055, loss_val: nan, pos_over_neg: 1147.211181640625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 661.660888671875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 684.2202758789062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 688.556640625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8986, loss_val: nan, pos_over_neg: 537.49560546875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 1045.36181640625 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 771.0715942382812 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 1361.0682373046875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.9081, loss_val: nan, pos_over_neg: 392.0140380859375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 553.1638793945312 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.9003, loss_val: nan, pos_over_neg: 763.1806640625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 2151.592529296875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 2213.877685546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 692.33251953125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 1471.2442626953125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.9058, loss_val: nan, pos_over_neg: 710.635986328125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8976, loss_val: nan, pos_over_neg: 706.8296508789062 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 393.6554870605469 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 780.9771728515625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 403.5781555175781 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.901, loss_val: nan, pos_over_neg: 1046.410400390625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 21493.009765625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 3659.58837890625 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 560.8214111328125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 771.736572265625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 411.20599365234375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 622.221435546875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.888, loss_val: nan, pos_over_neg: 550.52001953125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 490.9984436035156 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 754.7598266601562 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 719.0755004882812 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 827.99560546875 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 621.0225830078125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 592.27197265625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.908, loss_val: nan, pos_over_neg: 625.6460571289062 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 822.883056640625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.91, loss_val: nan, pos_over_neg: 494.0376892089844 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 547.7926635742188 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 530.2161865234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.9067, loss_val: nan, pos_over_neg: 492.6903076171875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 627.9617309570312 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 440.0302429199219 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 545.0814819335938 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.902, loss_val: nan, pos_over_neg: 800.7645263671875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.9113, loss_val: nan, pos_over_neg: 1273.47705078125 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 1090.5050048828125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.9076, loss_val: nan, pos_over_neg: 388.7165832519531 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8986, loss_val: nan, pos_over_neg: 636.7608642578125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 754.9019775390625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 761.05419921875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.9038, loss_val: nan, pos_over_neg: 657.5098876953125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.9, loss_val: nan, pos_over_neg: 1944.917724609375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 1154.460693359375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.9167, loss_val: nan, pos_over_neg: 681.1680908203125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.9031, loss_val: nan, pos_over_neg: 636.1129150390625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 1161.7374267578125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 1227.8046875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 688.2974243164062 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.9034, loss_val: nan, pos_over_neg: 660.9989013671875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 475.2289123535156 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 789.9968872070312 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 1384.629638671875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 650.5303955078125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8884, loss_val: nan, pos_over_neg: 664.5061645507812 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 818.7518310546875 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 595.9400634765625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.9028, loss_val: nan, pos_over_neg: 618.3487548828125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 707.2310180664062 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 1508.120361328125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 840.0509033203125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.9027, loss_val: nan, pos_over_neg: 577.3344116210938 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8952, loss_val: nan, pos_over_neg: 550.4493408203125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 1079.1204833984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.9005, loss_val: nan, pos_over_neg: 1255.5460205078125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 531.0729370117188 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 1350.103759765625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 579.6641845703125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.9261, loss_val: nan, pos_over_neg: 361.50469970703125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 675.3211669921875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 1848.1441650390625 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 878.8621826171875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.9025, loss_val: nan, pos_over_neg: 1706.0205078125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 497.0738220214844 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 444.4118347167969 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 560.8355712890625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 1329.5399169921875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1275.892333984375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.892, loss_val: nan, pos_over_neg: 620.2474975585938 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 959.1121826171875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.9069, loss_val: nan, pos_over_neg: 740.6856689453125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 1831.0897216796875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.9036, loss_val: nan, pos_over_neg: 574.9491577148438 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.9082, loss_val: nan, pos_over_neg: 411.7864990234375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 807.741943359375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 475.49798583984375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 551.6948852539062 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 802.6005859375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 1399.3428955078125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 710.7921142578125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: 567.6671752929688 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 580.4749755859375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 1062.8450927734375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.9095, loss_val: nan, pos_over_neg: 830.7635498046875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 952.5997924804688 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 794.0310668945312 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.9008, loss_val: nan, pos_over_neg: 987.2880249023438 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.892, loss_val: nan, pos_over_neg: 956.2935180664062 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 549.2769775390625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 594.0468139648438 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 935.8734130859375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 564.3926391601562 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 923.874267578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.9023, loss_val: nan, pos_over_neg: 621.8273315429688 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 604.8584594726562 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.9016, loss_val: nan, pos_over_neg: 728.1052856445312 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8983, loss_val: nan, pos_over_neg: 3730.660888671875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8993, loss_val: nan, pos_over_neg: 740.9982299804688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 976.6036987304688 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 756.50830078125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 2585.237548828125 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 773.9024047851562 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 562.5455322265625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 943.4089965820312 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 707.5138549804688 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.9015, loss_val: nan, pos_over_neg: 2837.57177734375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 1342.550048828125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 517.4547729492188 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1143.37158203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.9044, loss_val: nan, pos_over_neg: 635.1952514648438 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 797.32080078125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 629.2852172851562 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.9032, loss_val: nan, pos_over_neg: 737.16845703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 822.811279296875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 480.4354248046875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8913, loss_val: nan, pos_over_neg: 561.7122802734375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 605.962890625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 537.5969848632812 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 1395.8603515625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 414.1134033203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 322.6884765625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 663.9384765625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.9143, loss_val: nan, pos_over_neg: 502.47259521484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.9096, loss_val: nan, pos_over_neg: 349.3450927734375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 301.4650573730469 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 569.7728881835938 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 437.3009948730469 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 436.54840087890625 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 421.0770263671875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8947, loss_val: nan, pos_over_neg: 775.7752685546875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 554.0643920898438 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 578.9200439453125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 534.5310668945312 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 595.7489013671875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 1428.1361083984375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 682.9290161132812 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.9062, loss_val: nan, pos_over_neg: 866.0834350585938 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 550.3146362304688 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.904, loss_val: nan, pos_over_neg: 452.232177734375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8927, loss_val: nan, pos_over_neg: 1530.0240478515625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 731.9275512695312 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: 929.6849365234375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 702.6358642578125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8868, loss_val: nan, pos_over_neg: 1001.209716796875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 832.9862670898438 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 1001.83447265625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 329.9217834472656 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 461.45184326171875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 612.1488037109375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.888, loss_val: nan, pos_over_neg: 502.6328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8864, loss_val: nan, pos_over_neg: 391.0682067871094 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 2373.722900390625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.9057, loss_val: nan, pos_over_neg: 1025.6309814453125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 715.3253173828125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 1197.2554931640625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8784, loss_val: nan, pos_over_neg: 646.4188232421875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8974, loss_val: nan, pos_over_neg: 985.7322998046875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 974.3424072265625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 508.6701965332031 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8965, loss_val: nan, pos_over_neg: 520.28271484375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 703.9381103515625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 549.0318603515625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 286.37652587890625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 485.8106689453125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 1104.1549072265625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 870.9422607421875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.906, loss_val: nan, pos_over_neg: 382.1654052734375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8965, loss_val: nan, pos_over_neg: 616.4981689453125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 952.88525390625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 776.9671630859375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8997, loss_val: nan, pos_over_neg: 1392.853759765625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.9011, loss_val: nan, pos_over_neg: 2611.2265625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.9073, loss_val: nan, pos_over_neg: 476.062255859375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.9068, loss_val: nan, pos_over_neg: 413.7775573730469 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8945, loss_val: nan, pos_over_neg: 1291.8350830078125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8923, loss_val: nan, pos_over_neg: 2266.0126953125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 592.1697998046875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.9033, loss_val: nan, pos_over_neg: 633.9154663085938 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8883, loss_val: nan, pos_over_neg: 719.118408203125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 1544.2607421875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8996, loss_val: nan, pos_over_neg: 627.025146484375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 487.36871337890625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8795, loss_val: nan, pos_over_neg: 1300.2442626953125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 963.3120727539062 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 600.5736083984375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8897, loss_val: nan, pos_over_neg: 2265.820556640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.9047, loss_val: nan, pos_over_neg: 834.1187133789062 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8912, loss_val: nan, pos_over_neg: 891.36328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1413.1722412109375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8935, loss_val: nan, pos_over_neg: 1332.1551513671875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 509.89581298828125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 748.930419921875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.9127, loss_val: nan, pos_over_neg: 612.962890625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 815.6377563476562 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 462.8199768066406 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 780.31787109375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 1031.4161376953125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8926, loss_val: nan, pos_over_neg: 802.5473022460938 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 620.5275268554688 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 565.3599853515625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8972, loss_val: nan, pos_over_neg: 991.09619140625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8877, loss_val: nan, pos_over_neg: 455.34185791015625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 912.9921264648438 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.886, loss_val: nan, pos_over_neg: 1123.0775146484375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 870.2911987304688 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.896, loss_val: nan, pos_over_neg: 656.4837646484375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 515.6616821289062 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 1053.904541015625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 441.13873291015625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8874, loss_val: nan, pos_over_neg: 659.2194213867188 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8851, loss_val: nan, pos_over_neg: 1470.7137451171875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 11085.4326171875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 630.7026977539062 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8895, loss_val: nan, pos_over_neg: 664.1300659179688 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 2087.811767578125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8947, loss_val: nan, pos_over_neg: 1291.99267578125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8957, loss_val: nan, pos_over_neg: 659.3645629882812 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8872, loss_val: nan, pos_over_neg: 755.8370971679688 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: 792.8466796875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 1076.447265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 1704.0614013671875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8951, loss_val: nan, pos_over_neg: 466.72064208984375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 1666.85498046875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 993.874755859375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 2736.318359375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8763, loss_val: nan, pos_over_neg: 765.555419921875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.9054, loss_val: nan, pos_over_neg: 784.4601440429688 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 1929.1116943359375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.897, loss_val: nan, pos_over_neg: 741.7984008789062 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 582.343505859375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 835.2476196289062 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8886, loss_val: nan, pos_over_neg: 838.6217041015625 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.9092, loss_val: nan, pos_over_neg: 419.74273681640625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8984, loss_val: nan, pos_over_neg: 576.19482421875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 727.5205078125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 1119.1220703125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8903, loss_val: nan, pos_over_neg: 477.547607421875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 541.6185302734375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8917, loss_val: nan, pos_over_neg: 1935.1448974609375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.9001, loss_val: nan, pos_over_neg: 796.6058959960938 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: 699.5413818359375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 748.1715087890625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: 976.8486328125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8893, loss_val: nan, pos_over_neg: 944.2325439453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8949, loss_val: nan, pos_over_neg: 604.9060668945312 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.894, loss_val: nan, pos_over_neg: 793.8272705078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 813.1654663085938 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8878, loss_val: nan, pos_over_neg: 1210.222412109375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.9091, loss_val: nan, pos_over_neg: 550.3704223632812 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: 2092.582763671875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1040.8548583984375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.9052, loss_val: nan, pos_over_neg: 741.156005859375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8897, loss_val: nan, pos_over_neg: 479.3567810058594 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8905, loss_val: nan, pos_over_neg: 474.69879150390625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.9007, loss_val: nan, pos_over_neg: 958.5037841796875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.892, loss_val: nan, pos_over_neg: 1362.715087890625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 583.0355834960938 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 847.3215942382812 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [40:27<101163:55:03, 1213.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 591.1575927734375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8843, loss_val: nan, pos_over_neg: 1207.9703369140625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 475.2821044921875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1310.626708984375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: 770.9442749023438 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8885, loss_val: nan, pos_over_neg: 704.9644775390625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 1115.5980224609375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8861, loss_val: nan, pos_over_neg: 3196.9541015625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 336.06768798828125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8951, loss_val: nan, pos_over_neg: 353.0799865722656 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 55759.3125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8842, loss_val: nan, pos_over_neg: 851.9600219726562 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8975, loss_val: nan, pos_over_neg: 475.2629699707031 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8989, loss_val: nan, pos_over_neg: 598.2581176757812 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 1618.2608642578125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 1241.736083984375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8832, loss_val: nan, pos_over_neg: 1295.0152587890625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8846, loss_val: nan, pos_over_neg: 1520.8482666015625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.9037, loss_val: nan, pos_over_neg: 451.3163757324219 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 472.5147399902344 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 558.68505859375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 830.3179931640625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 585.4564208984375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 474.7607727050781 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1608.704833984375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 636.7130737304688 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 323.8226013183594 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 337.9325866699219 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 620.9357299804688 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 844.6873168945312 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 704.6504516601562 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 465.11480712890625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 1559.2720947265625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8803, loss_val: nan, pos_over_neg: 2396.36962890625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.883, loss_val: nan, pos_over_neg: 1471.15869140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8903, loss_val: nan, pos_over_neg: 928.5452880859375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 945.475830078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 1429.3367919921875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8823, loss_val: nan, pos_over_neg: 2916.924072265625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8803, loss_val: nan, pos_over_neg: 4402.42724609375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8947, loss_val: nan, pos_over_neg: 1465.154296875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8853, loss_val: nan, pos_over_neg: 675.8198852539062 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 815.7112426757812 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 952.0581665039062 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8853, loss_val: nan, pos_over_neg: 624.8928833007812 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 461.6307067871094 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 674.3177490234375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8971, loss_val: nan, pos_over_neg: 798.6942138671875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 420.1968994140625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 715.2984008789062 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8944, loss_val: nan, pos_over_neg: 520.607421875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: 655.3172607421875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 528.5048828125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 978.0487060546875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8912, loss_val: nan, pos_over_neg: 1665.13623046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.888, loss_val: nan, pos_over_neg: 2728.495361328125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8978, loss_val: nan, pos_over_neg: 531.068603515625 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 807.1033325195312 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8856, loss_val: nan, pos_over_neg: 587.8316040039062 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 720.32568359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 496.4787902832031 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8805, loss_val: nan, pos_over_neg: 563.3102416992188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8897, loss_val: nan, pos_over_neg: 732.419677734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 1215.0662841796875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 2739.145751953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8823, loss_val: nan, pos_over_neg: 5976.52099609375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8848, loss_val: nan, pos_over_neg: 858.8370971679688 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 1476.4544677734375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9006, loss_val: nan, pos_over_neg: 594.7188110351562 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 7124.28515625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8927, loss_val: nan, pos_over_neg: 825.0925903320312 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 1670.9769287109375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 711.3392944335938 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 803.8824462890625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 4172.52392578125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8852, loss_val: nan, pos_over_neg: 2443.054931640625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 437.02423095703125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 762.449462890625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8874, loss_val: nan, pos_over_neg: 1200.1031494140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 522.9215087890625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8948, loss_val: nan, pos_over_neg: 1256.3338623046875 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8922, loss_val: nan, pos_over_neg: 378.3174743652344 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 558.784423828125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9024, loss_val: nan, pos_over_neg: 555.43603515625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 339.09759521484375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 442.0085144042969 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 875.3407592773438 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 443.3166198730469 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8962, loss_val: nan, pos_over_neg: 535.6602172851562 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8817, loss_val: nan, pos_over_neg: 681.1456298828125 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8928, loss_val: nan, pos_over_neg: 547.0733642578125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 542.0989990234375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 542.3333740234375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 578.4869384765625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8956, loss_val: nan, pos_over_neg: 329.2033386230469 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: 349.6807556152344 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.905, loss_val: nan, pos_over_neg: 555.0768432617188 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: 421.1217041015625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 563.5699462890625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: 789.45361328125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9089, loss_val: nan, pos_over_neg: 339.1805419921875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 1104.4564208984375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 584.7283325195312 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 815.6471557617188 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 2073.382080078125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8854, loss_val: nan, pos_over_neg: 1819.400146484375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 1028.5213623046875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 709.6487426757812 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 877.2196044921875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9046, loss_val: nan, pos_over_neg: 1566.4302978515625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 1842.2933349609375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8727, loss_val: nan, pos_over_neg: 921.902587890625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8877, loss_val: nan, pos_over_neg: 362.0251770019531 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8914, loss_val: nan, pos_over_neg: 511.5682678222656 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8886, loss_val: nan, pos_over_neg: 1028.40576171875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.899, loss_val: nan, pos_over_neg: 515.072021484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8891, loss_val: nan, pos_over_neg: 355.3853454589844 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: 906.0855712890625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8986, loss_val: nan, pos_over_neg: 1481.0994873046875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 653.751708984375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 1109.119873046875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 1175.028564453125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8953, loss_val: nan, pos_over_neg: 459.1233215332031 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8836, loss_val: nan, pos_over_neg: 1304.0166015625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 1404.2506103515625 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 632.8764038085938 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 413.6739807128906 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9104, loss_val: nan, pos_over_neg: 641.0140380859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 633.5215454101562 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.894, loss_val: nan, pos_over_neg: 719.3646240234375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8878, loss_val: nan, pos_over_neg: 368.4507141113281 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 742.2710571289062 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 1428.385498046875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9019, loss_val: nan, pos_over_neg: 544.0570678710938 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 624.3466186523438 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8991, loss_val: nan, pos_over_neg: 437.43487548828125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8876, loss_val: nan, pos_over_neg: 1764.5079345703125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 694.8700561523438 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9021, loss_val: nan, pos_over_neg: 533.0789794921875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 477.0314636230469 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 620.751953125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 513.7247314453125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 1693.5361328125 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8903, loss_val: nan, pos_over_neg: 545.6734008789062 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 915.9683837890625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8886, loss_val: nan, pos_over_neg: 492.153564453125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 787.2374267578125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.889, loss_val: nan, pos_over_neg: 667.1587524414062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 545.3497314453125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8856, loss_val: nan, pos_over_neg: 335.5335998535156 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 720.1300659179688 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 684.4397583007812 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8926, loss_val: nan, pos_over_neg: 984.2188110351562 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 766.6302490234375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 350.1231689453125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8946, loss_val: nan, pos_over_neg: 444.7570495605469 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 672.5341796875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8855, loss_val: nan, pos_over_neg: 923.5261840820312 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 829.9495239257812 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 1262.412109375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8883, loss_val: nan, pos_over_neg: 791.7837524414062 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8809, loss_val: nan, pos_over_neg: 420.6230773925781 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 656.8327026367188 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8919, loss_val: nan, pos_over_neg: 516.2469482421875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 503.17608642578125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 754.8318481445312 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 634.169921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 749.1634521484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 1149.7928466796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.877, loss_val: nan, pos_over_neg: 1152.0438232421875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8841, loss_val: nan, pos_over_neg: 810.3162841796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 694.2652587890625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 779.9262084960938 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8995, loss_val: nan, pos_over_neg: 645.9859008789062 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8843, loss_val: nan, pos_over_neg: 521.6163940429688 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 681.1552734375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 1350.8646240234375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 517.0556640625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 961.8121948242188 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8973, loss_val: nan, pos_over_neg: 459.23931884765625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 626.6348266601562 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 837.824462890625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8902, loss_val: nan, pos_over_neg: 1297.8077392578125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 1071.0736083984375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8954, loss_val: nan, pos_over_neg: 1163.797607421875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8905, loss_val: nan, pos_over_neg: 1780.9500732421875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8969, loss_val: nan, pos_over_neg: 3609.92822265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 1008.8475341796875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 749.3157958984375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 514.9794311523438 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 1088.9718017578125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 910.9214477539062 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 405.7536315917969 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 501.1511535644531 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 5774.91015625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 622.646728515625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 492.2016906738281 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 602.0344848632812 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 522.0438232421875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 496.2888488769531 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8839, loss_val: nan, pos_over_neg: 366.5667419433594 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8809, loss_val: nan, pos_over_neg: 491.51300048828125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8961, loss_val: nan, pos_over_neg: 339.4698181152344 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 1580.38916015625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1998.9886474609375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 525.1329956054688 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8917, loss_val: nan, pos_over_neg: 488.3367614746094 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 878.0519409179688 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 1001.4705810546875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 441.8990173339844 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8992, loss_val: nan, pos_over_neg: 354.8352966308594 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.888, loss_val: nan, pos_over_neg: 875.6484985351562 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1180.1900634765625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8836, loss_val: nan, pos_over_neg: 751.259765625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 643.8109130859375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8972, loss_val: nan, pos_over_neg: 357.737060546875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 1218.3758544921875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 569.0009765625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8967, loss_val: nan, pos_over_neg: 372.8734436035156 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8897, loss_val: nan, pos_over_neg: 521.3591918945312 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8851, loss_val: nan, pos_over_neg: 558.4815063476562 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 794.5069580078125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8828, loss_val: nan, pos_over_neg: 639.5894775390625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 582.8029174804688 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8961, loss_val: nan, pos_over_neg: 2286.4736328125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8955, loss_val: nan, pos_over_neg: 586.8466186523438 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8825, loss_val: nan, pos_over_neg: 1087.00537109375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8868, loss_val: nan, pos_over_neg: 1540.8170166015625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8968, loss_val: nan, pos_over_neg: 594.843994140625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 653.19482421875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 713.8545532226562 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 635.72998046875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.883, loss_val: nan, pos_over_neg: 510.7270812988281 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8988, loss_val: nan, pos_over_neg: 891.7315673828125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8835, loss_val: nan, pos_over_neg: 1719.623046875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 728.5487670898438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 545.1341552734375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 1113.5787353515625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8856, loss_val: nan, pos_over_neg: 745.2177124023438 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 714.4779663085938 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8911, loss_val: nan, pos_over_neg: 693.4879760742188 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 673.6158447265625 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 1036.839111328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8769, loss_val: nan, pos_over_neg: 1135.7403564453125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8683, loss_val: nan, pos_over_neg: 2352.0869140625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 1032.62890625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8808, loss_val: nan, pos_over_neg: 484.94818115234375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 1135.082275390625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 949.5924682617188 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 1541.751708984375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 1198.431640625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 1601.9605712890625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8931, loss_val: nan, pos_over_neg: 646.9938354492188 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 790.5704345703125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8839, loss_val: nan, pos_over_neg: 645.93212890625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 479.9963684082031 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 17298.791015625 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.887, loss_val: nan, pos_over_neg: 410.6851806640625 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 742.5859985351562 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8736, loss_val: nan, pos_over_neg: 590.2097778320312 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 1089.50830078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 6115.21875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8832, loss_val: nan, pos_over_neg: 648.4454956054688 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8795, loss_val: nan, pos_over_neg: 1081.1614990234375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.9017, loss_val: nan, pos_over_neg: 562.6530151367188 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 776.7432250976562 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8966, loss_val: nan, pos_over_neg: 514.668212890625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 1041.5316162109375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 909.39404296875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 1083.5291748046875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 494.2930908203125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8933, loss_val: nan, pos_over_neg: 425.5756530761719 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 551.219970703125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8882, loss_val: nan, pos_over_neg: 803.3975830078125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8846, loss_val: nan, pos_over_neg: 628.38037109375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8784, loss_val: nan, pos_over_neg: 640.0546875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 477.156982421875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8875, loss_val: nan, pos_over_neg: 951.4282836914062 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8862, loss_val: nan, pos_over_neg: 745.6373291015625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8895, loss_val: nan, pos_over_neg: 714.5206909179688 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8742, loss_val: nan, pos_over_neg: 984.5045776367188 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8778, loss_val: nan, pos_over_neg: 1030.52978515625 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 1633.663330078125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8894, loss_val: nan, pos_over_neg: 1132.039306640625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 295.4734802246094 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 562.589599609375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8891, loss_val: nan, pos_over_neg: 558.935791015625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.887, loss_val: nan, pos_over_neg: 2246.1630859375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 1045.5765380859375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 636.7228393554688 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 956.124755859375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1446.0882568359375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 1094.006103515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8921, loss_val: nan, pos_over_neg: 527.0897216796875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 550.8555908203125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 887.6765747070312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 1014.8486938476562 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8794, loss_val: nan, pos_over_neg: 575.4788818359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 566.4591064453125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8868, loss_val: nan, pos_over_neg: 598.9849243164062 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1440.974853515625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8883, loss_val: nan, pos_over_neg: 890.9129028320312 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 566.0692749023438 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8832, loss_val: nan, pos_over_neg: 695.3754272460938 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.886, loss_val: nan, pos_over_neg: 2561.826416015625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8735, loss_val: nan, pos_over_neg: -44707.93359375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 1037.996337890625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 508.95599365234375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 1090.87353515625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8808, loss_val: nan, pos_over_neg: 3005.602294921875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 935.3017578125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8883, loss_val: nan, pos_over_neg: 821.2269897460938 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 1329.4700927734375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 3019.240234375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 4462.970703125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8763, loss_val: nan, pos_over_neg: 981.36474609375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 2651.271240234375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 770.3598022460938 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 1560.1304931640625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8878, loss_val: nan, pos_over_neg: -4174.17333984375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 701.211181640625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 516.4819946289062 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 1113.281494140625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 1097.9158935546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 1605.957763671875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8783, loss_val: nan, pos_over_neg: 1134.7347412109375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 1121.3045654296875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 1073.140625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8872, loss_val: nan, pos_over_neg: 790.7368774414062 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8836, loss_val: nan, pos_over_neg: 457.2571105957031 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 493.46319580078125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 890.9318237304688 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 666.211181640625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8929, loss_val: nan, pos_over_neg: 733.9615478515625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 970.5624389648438 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 1292.6767578125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8858, loss_val: nan, pos_over_neg: 735.3096923828125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 1153.019775390625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 823.3137817382812 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 901.8307495117188 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 942.12646484375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 1339.8626708984375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 766.2770385742188 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 3622.47705078125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 829.547607421875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8735, loss_val: nan, pos_over_neg: 764.3671264648438 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 716.2586059570312 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 777.0006713867188 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 2165.209228515625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8786, loss_val: nan, pos_over_neg: 694.7314453125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 1236.6641845703125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 14510.240234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 395.4817810058594 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 780.351806640625 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 645.6394653320312 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 777.11328125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 795.3826293945312 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8861, loss_val: nan, pos_over_neg: 1555.3121337890625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 489.9560852050781 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8817, loss_val: nan, pos_over_neg: 608.2481689453125 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 811.51806640625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 781.5026245117188 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 603.1493530273438 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 1925.0928955078125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8742, loss_val: nan, pos_over_neg: 871.7589721679688 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 768.7649536132812 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8998, loss_val: nan, pos_over_neg: 423.856201171875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 416.5549621582031 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 460.1146545410156 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 1254.6834716796875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8735, loss_val: nan, pos_over_neg: 1261.223876953125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 790.5875244140625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 754.9703979492188 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8832, loss_val: nan, pos_over_neg: 964.3267211914062 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 772.9972534179688 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8906, loss_val: nan, pos_over_neg: 520.5593872070312 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8836, loss_val: nan, pos_over_neg: 878.7791137695312 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8841, loss_val: nan, pos_over_neg: 595.9622802734375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8845, loss_val: nan, pos_over_neg: 743.5711059570312 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 1461.4000244140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 496.7515563964844 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 698.7730712890625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 2480.58447265625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8869, loss_val: nan, pos_over_neg: 1195.5235595703125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 2642.201171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 1902.3345947265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1065.9688720703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 1929.9808349609375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 1046.1029052734375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8943, loss_val: nan, pos_over_neg: 409.6826477050781 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8764, loss_val: nan, pos_over_neg: 466.36358642578125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.893, loss_val: nan, pos_over_neg: 763.0640869140625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8716, loss_val: nan, pos_over_neg: 1492.091552734375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 610.5900268554688 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8904, loss_val: nan, pos_over_neg: 1170.1229248046875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 1968.71533203125 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 451.99871826171875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8901, loss_val: nan, pos_over_neg: 665.8754272460938 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8881, loss_val: nan, pos_over_neg: 451.5975646972656 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 616.250244140625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8778, loss_val: nan, pos_over_neg: 580.2553100585938 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 913.3285522460938 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 594.5746459960938 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 1031.8875732421875 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 405.40093994140625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 350.4618225097656 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 576.0007934570312 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8874, loss_val: nan, pos_over_neg: 479.6439208984375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8831, loss_val: nan, pos_over_neg: 1023.503173828125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 772.7992553710938 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8911, loss_val: nan, pos_over_neg: 579.7166137695312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 2166.1826171875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 1016.5578002929688 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8764, loss_val: nan, pos_over_neg: 780.373779296875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 743.3592529296875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8754, loss_val: nan, pos_over_neg: 1063.2647705078125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8769, loss_val: nan, pos_over_neg: 835.5501098632812 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8861, loss_val: nan, pos_over_neg: 586.2257690429688 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 687.97021484375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: -3012.112548828125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 657.2936401367188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.884, loss_val: nan, pos_over_neg: 440.2434387207031 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 1446.939453125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 1335.20068359375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8812, loss_val: nan, pos_over_neg: 1097.125244140625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 1478.6982421875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 1396.1517333984375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8868, loss_val: nan, pos_over_neg: 834.9288940429688 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 1506.0595703125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 692.2098388671875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 1049.830810546875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 60127.34375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8913, loss_val: nan, pos_over_neg: 2252.122314453125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 757.9573364257812 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8842, loss_val: nan, pos_over_neg: 789.08935546875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 2223.973388671875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8852, loss_val: nan, pos_over_neg: 769.1445922851562 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 617.5262451171875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8813, loss_val: nan, pos_over_neg: 1060.9632568359375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8835, loss_val: nan, pos_over_neg: 487.371826171875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 1667.1630859375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8868, loss_val: nan, pos_over_neg: 724.9708862304688 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 774.3095092773438 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 697.7914428710938 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 498.39044189453125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8806, loss_val: nan, pos_over_neg: 847.8060302734375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 835.5964965820312 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8862, loss_val: nan, pos_over_neg: 312.7602233886719 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 556.72607421875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8786, loss_val: nan, pos_over_neg: 592.966552734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 586.8353881835938 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 3344.592041015625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 578.3924560546875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8693, loss_val: nan, pos_over_neg: 1291.9083251953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 640.3035278320312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 675.20166015625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 696.21923828125 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 2016.207763671875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 618.4938354492188 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 1083.993408203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8788, loss_val: nan, pos_over_neg: 435.3291015625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 834.8101806640625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 415.7804260253906 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 485.6708679199219 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 890.8851318359375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.894, loss_val: nan, pos_over_neg: 430.7198486328125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 1136.433349609375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 835.1781616210938 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 614.4201049804688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8939, loss_val: nan, pos_over_neg: 411.5190124511719 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8788, loss_val: nan, pos_over_neg: 1943.19970703125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8786, loss_val: nan, pos_over_neg: 963.1671142578125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 908.7750244140625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8795, loss_val: nan, pos_over_neg: 1495.5799560546875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8838, loss_val: nan, pos_over_neg: 589.1356201171875 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 660.637451171875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 1093.038330078125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 1350.9490966796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 1134.8372802734375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 1083.753662109375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 1057.379150390625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8804, loss_val: nan, pos_over_neg: 3474.850830078125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 1367.2208251953125 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8813, loss_val: nan, pos_over_neg: 505.1157531738281 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 658.54052734375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 646.701416015625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8806, loss_val: nan, pos_over_neg: 752.0403442382812 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 869.5965576171875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8909, loss_val: nan, pos_over_neg: 1622.5072021484375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 887.3392944335938 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 548.2874755859375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 570.37548828125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 908.9126586914062 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8831, loss_val: nan, pos_over_neg: 556.7711181640625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 523.709228515625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8876, loss_val: nan, pos_over_neg: 446.7493896484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8762, loss_val: nan, pos_over_neg: 578.1092529296875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 1395.4405517578125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 658.7876586914062 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 1498.90771484375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 539.4512939453125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 497.42791748046875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8859, loss_val: nan, pos_over_neg: 504.92724609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 661.6161499023438 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 583.1015014648438 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 644.0794677734375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 900.7996215820312 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8799, loss_val: nan, pos_over_neg: 998.7158813476562 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 1144.8282470703125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 527.4392700195312 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8819, loss_val: nan, pos_over_neg: 636.5350341796875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 883.7391967773438 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 341.1688232421875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8843, loss_val: nan, pos_over_neg: 499.7224426269531 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8716, loss_val: nan, pos_over_neg: 1220.900390625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 1208.5631103515625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 928.4992065429688 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 834.1294555664062 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 1030.3739013671875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 1012.5994262695312 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8857, loss_val: nan, pos_over_neg: 659.5608520507812 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 1037.514892578125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 592.8884887695312 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 774.69091796875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8771, loss_val: nan, pos_over_neg: 788.5719604492188 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 1045.367431640625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8853, loss_val: nan, pos_over_neg: 949.2423706054688 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8725, loss_val: nan, pos_over_neg: 1513.0045166015625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 1255.3929443359375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 857.2841186523438 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 521.552001953125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 685.4611206054688 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 684.6427612304688 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8822, loss_val: nan, pos_over_neg: 802.03173828125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 416.1420593261719 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8812, loss_val: nan, pos_over_neg: 902.6581420898438 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8916, loss_val: nan, pos_over_neg: 866.7811279296875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8809, loss_val: nan, pos_over_neg: 1615.5347900390625 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 677.650390625 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 518.8043823242188 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 847.5595092773438 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 1173.61328125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 735.4575805664062 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 511.1790466308594 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8726, loss_val: nan, pos_over_neg: 420.5030212402344 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 515.8562622070312 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8779, loss_val: nan, pos_over_neg: 607.7361450195312 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 998.930419921875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 1617.5782470703125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 1182.8319091796875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8866, loss_val: nan, pos_over_neg: 752.697021484375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8762, loss_val: nan, pos_over_neg: 1211.912841796875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 1620.480224609375 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8836, loss_val: nan, pos_over_neg: 1896.703857421875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 844.7030029296875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8945, loss_val: nan, pos_over_neg: 1035.9677734375 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 827.7975463867188 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 872.3873901367188 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 3385.21435546875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8829, loss_val: nan, pos_over_neg: 521.6998291015625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 900.6746215820312 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 976.9515380859375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8716, loss_val: nan, pos_over_neg: 1358.191650390625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 1873.5877685546875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.875, loss_val: nan, pos_over_neg: 925.1156616210938 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8763, loss_val: nan, pos_over_neg: 859.5145874023438 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8779, loss_val: nan, pos_over_neg: 518.090576171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 698.54150390625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 583.1675415039062 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 721.9589233398438 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 1140.3966064453125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 1027.3287353515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 729.5177001953125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 654.0177001953125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8817, loss_val: nan, pos_over_neg: 739.8787231445312 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 373.7986145019531 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 1884.716064453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 527.8983154296875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 483.2139892578125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 546.9785766601562 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8707, loss_val: nan, pos_over_neg: 682.0167236328125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 1537.4578857421875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8794, loss_val: nan, pos_over_neg: 761.2460327148438 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 603.7986450195312 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 720.3677368164062 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 1198.104248046875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8721, loss_val: nan, pos_over_neg: 732.8067016601562 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8889, loss_val: nan, pos_over_neg: 435.80755615234375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 1013.7472534179688 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8848, loss_val: nan, pos_over_neg: 2241.10986328125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 656.6207885742188 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 1088.485595703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8805, loss_val: nan, pos_over_neg: 599.6827392578125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8839, loss_val: nan, pos_over_neg: 887.6096801757812 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8977, loss_val: nan, pos_over_neg: 1167.91552734375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8662, loss_val: nan, pos_over_neg: 2340.66650390625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 2420.15673828125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 862.4711303710938 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 593.6859130859375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8936, loss_val: nan, pos_over_neg: 681.8728637695312 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8739, loss_val: nan, pos_over_neg: 923.5479125976562 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 1360.9593505859375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8736, loss_val: nan, pos_over_neg: 1648.7159423828125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 759.2655639648438 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 1086.5457763671875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 1214.4432373046875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 704.3173217773438 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 901.3997192382812 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 853.91943359375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 2759.386962890625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8884, loss_val: nan, pos_over_neg: 435.5224609375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 717.5833740234375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 1542.0455322265625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8871, loss_val: nan, pos_over_neg: 1076.4930419921875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 1346.0736083984375 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 766.791748046875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 1268.6871337890625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8765, loss_val: nan, pos_over_neg: 1132.44189453125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 814.1558837890625 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.877, loss_val: nan, pos_over_neg: 1844.5889892578125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8786, loss_val: nan, pos_over_neg: 778.6900634765625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 1080.357177734375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 1058.2125244140625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 720.4031982421875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 634.36328125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 833.521728515625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.89, loss_val: nan, pos_over_neg: 1418.2235107421875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 640.05029296875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 2277.989990234375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 1005.624267578125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 2195.054931640625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8736, loss_val: nan, pos_over_neg: 2102.78173828125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 735.283935546875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 775.5753173828125 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 2058.017578125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 3263.341064453125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 1246.042724609375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 558.9329833984375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 18697.52734375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 850.0838012695312 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 1982.2255859375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8828, loss_val: nan, pos_over_neg: 1465.7752685546875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.88, loss_val: nan, pos_over_neg: 591.0068969726562 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8717, loss_val: nan, pos_over_neg: 734.5999755859375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8879, loss_val: nan, pos_over_neg: 752.003662109375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 1325.592041015625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 755.5380859375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8812, loss_val: nan, pos_over_neg: 564.00244140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 2884.502685546875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 1538.300537109375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 926.3811645507812 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 714.8719482421875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 1031.7647705078125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8798, loss_val: nan, pos_over_neg: 1076.688720703125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8821, loss_val: nan, pos_over_neg: 686.18310546875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 566.6961669921875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 1637.7393798828125 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8794, loss_val: nan, pos_over_neg: 565.842041015625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 1145.291259765625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8785, loss_val: nan, pos_over_neg: 472.1581115722656 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8746, loss_val: nan, pos_over_neg: 1331.8165283203125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 648.2976684570312 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 431.6482849121094 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.869, loss_val: nan, pos_over_neg: 2480.6533203125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 1147.151123046875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 762.2351684570312 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8834, loss_val: nan, pos_over_neg: 708.7041015625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.884, loss_val: nan, pos_over_neg: 561.5471801757812 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 498.4217224121094 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 586.5606689453125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8816, loss_val: nan, pos_over_neg: 371.1500549316406 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8717, loss_val: nan, pos_over_neg: 713.1551513671875 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8805, loss_val: nan, pos_over_neg: 411.2530517578125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 467.618408203125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 1548.5411376953125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 685.3568725585938 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8711, loss_val: nan, pos_over_neg: 484.6604919433594 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8832, loss_val: nan, pos_over_neg: 856.5543212890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 859.324462890625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 708.947509765625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8651, loss_val: nan, pos_over_neg: 673.3906860351562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 704.9157104492188 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8813, loss_val: nan, pos_over_neg: 547.4663696289062 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 1103.755615234375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 1194.818359375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 784.7821655273438 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 408.1007080078125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 1404.1026611328125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8863, loss_val: nan, pos_over_neg: 697.4628295898438 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 573.8173217773438 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8612, loss_val: nan, pos_over_neg: 659.6509399414062 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 814.7359619140625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.875, loss_val: nan, pos_over_neg: 987.1528930664062 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 1018.0877685546875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 616.4728393554688 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:00:40<101121:59:42, 1213.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 477.6563720703125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 794.0293579101562 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 1719.0823974609375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 1178.047607421875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 2662.276123046875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 848.546630859375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8844, loss_val: nan, pos_over_neg: 2977.937744140625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 1501.81982421875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 2237.7099609375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8673, loss_val: nan, pos_over_neg: 945.9117431640625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8841, loss_val: nan, pos_over_neg: 908.1552734375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 540.1807861328125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 1497.1142578125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8628, loss_val: nan, pos_over_neg: 1067.9400634765625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 614.8486328125 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8711, loss_val: nan, pos_over_neg: 798.4024047851562 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8864, loss_val: nan, pos_over_neg: 764.3685913085938 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8679, loss_val: nan, pos_over_neg: 1241.781982421875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 1062.1063232421875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8739, loss_val: nan, pos_over_neg: 2775.990966796875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 1044.2943115234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8668, loss_val: nan, pos_over_neg: 1389.7735595703125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 963.5203247070312 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 815.2509155273438 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 856.9861450195312 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 3663.374267578125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 1586.939697265625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 2671.59912109375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1167.6185302734375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 578.7720336914062 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8867, loss_val: nan, pos_over_neg: 572.447265625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 1149.1322021484375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 751.4505615234375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 5839.91064453125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 6590.07958984375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 1114.8043212890625 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 593.6715087890625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 970.7536010742188 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 1953.7076416015625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 593.913818359375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8683, loss_val: nan, pos_over_neg: 611.2682495117188 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 866.5001831054688 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 954.5956420898438 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 1496.2611083984375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 746.2994995117188 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 839.170654296875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8662, loss_val: nan, pos_over_neg: 625.3839721679688 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 703.031005859375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 801.759765625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 639.7184448242188 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 1197.33740234375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 1605.1187744140625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8725, loss_val: nan, pos_over_neg: 564.3883666992188 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 557.7178955078125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.872, loss_val: nan, pos_over_neg: 1416.8228759765625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 876.7625732421875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 685.10888671875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8694, loss_val: nan, pos_over_neg: 835.5477294921875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 469.7115173339844 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 2550.854248046875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1079.54833984375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8782, loss_val: nan, pos_over_neg: 764.0303344726562 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8797, loss_val: nan, pos_over_neg: 669.1834106445312 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 2199.968505859375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8818, loss_val: nan, pos_over_neg: 1159.7567138671875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 558.6080932617188 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 507.4098815917969 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 830.8667602539062 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 881.524658203125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 479.4708557128906 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.872, loss_val: nan, pos_over_neg: 786.2526245117188 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8827, loss_val: nan, pos_over_neg: 757.0530395507812 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 684.5350952148438 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 678.0252685546875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8709, loss_val: nan, pos_over_neg: 610.1080322265625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 617.0748291015625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 729.6422729492188 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8814, loss_val: nan, pos_over_neg: 494.8895263671875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 386.9981689453125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 1016.7891845703125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 2037.425048828125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 866.782470703125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8716, loss_val: nan, pos_over_neg: 571.369873046875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8838, loss_val: nan, pos_over_neg: 837.429443359375 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 1212.9505615234375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 475.2962646484375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8886, loss_val: nan, pos_over_neg: 534.6370239257812 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 894.53955078125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 543.5490112304688 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 835.0282592773438 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 1603.3055419921875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 1118.1978759765625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 1289.23193359375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 852.8360595703125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 634.3867797851562 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8769, loss_val: nan, pos_over_neg: 909.3495483398438 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 1081.18212890625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8687, loss_val: nan, pos_over_neg: 410.4725341796875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 593.5872802734375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8718, loss_val: nan, pos_over_neg: 1524.4754638671875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 1553.275390625 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 726.5603637695312 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 1843.21337890625 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 1420.4296875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 623.5288696289062 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 869.1526489257812 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 455.3735046386719 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 442.9191589355469 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 557.091796875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 626.5584716796875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1076.657470703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8865, loss_val: nan, pos_over_neg: 294.14593505859375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8794, loss_val: nan, pos_over_neg: 351.2566833496094 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.869, loss_val: nan, pos_over_neg: 502.9093322753906 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8861, loss_val: nan, pos_over_neg: 534.8460693359375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8771, loss_val: nan, pos_over_neg: 383.1411437988281 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.878, loss_val: nan, pos_over_neg: 514.4044189453125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8711, loss_val: nan, pos_over_neg: 561.9619140625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 1423.6097412109375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 1022.906005859375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 382.1278381347656 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8766, loss_val: nan, pos_over_neg: 1033.6185302734375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8777, loss_val: nan, pos_over_neg: 586.4481201171875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 1262.3438720703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 888.1605834960938 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 774.6720581054688 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 260.92474365234375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 1442.179931640625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1956.637939453125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 2545.144775390625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 1063.2142333984375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 1282.8973388671875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 1470.332275390625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8725, loss_val: nan, pos_over_neg: 809.011962890625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 918.4461669921875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 833.0892333984375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8659, loss_val: nan, pos_over_neg: 1295.94384765625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8671, loss_val: nan, pos_over_neg: 655.2210693359375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 501.5638732910156 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1933.1781005859375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.882, loss_val: nan, pos_over_neg: 566.2686767578125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 1206.968505859375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8733, loss_val: nan, pos_over_neg: 739.9202270507812 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8725, loss_val: nan, pos_over_neg: 1018.89599609375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 798.8308715820312 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 715.2763061523438 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8709, loss_val: nan, pos_over_neg: 574.5341796875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 677.6611328125 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 947.2676391601562 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8888, loss_val: nan, pos_over_neg: 890.0520629882812 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 1788.2813720703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 1113.1873779296875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.868, loss_val: nan, pos_over_neg: 1439.0660400390625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 933.0491333007812 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1981.515625 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 1303.616943359375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 2238.332763671875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 2530.030029296875 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 727.0206909179688 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 936.6585693359375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 1010.4888916015625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 7785.14208984375 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8851, loss_val: nan, pos_over_neg: 1337.366455078125 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 1528.567138671875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 676.5059814453125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 786.450439453125 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 833.9268798828125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: -59461.18359375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 1645.48486328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 833.9556884765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 1122.115966796875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 867.803466796875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 1166.8868408203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 337.2309875488281 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 859.5635375976562 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 484.8981628417969 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 545.6294555664062 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 687.2377319335938 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.866, loss_val: nan, pos_over_neg: 1220.123779296875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8809, loss_val: nan, pos_over_neg: 1203.260498046875 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8679, loss_val: nan, pos_over_neg: 796.4547729492188 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8694, loss_val: nan, pos_over_neg: 612.5957641601562 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1111.21240234375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 1432.000732421875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 684.60888671875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.875, loss_val: nan, pos_over_neg: 500.8390197753906 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 633.2129516601562 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 429.33001708984375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8831, loss_val: nan, pos_over_neg: 770.975341796875 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 643.161376953125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 1347.2919921875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1394.0677490234375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8781, loss_val: nan, pos_over_neg: 754.6333618164062 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1392.9371337890625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8778, loss_val: nan, pos_over_neg: 1405.0782470703125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 696.24365234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 502.4561767578125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8748, loss_val: nan, pos_over_neg: 366.5005798339844 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8761, loss_val: nan, pos_over_neg: 449.7589111328125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8695, loss_val: nan, pos_over_neg: 500.2664489746094 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 1183.5509033203125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 635.6804809570312 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 1418.6212158203125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 652.0280151367188 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 2578.53515625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 824.4312133789062 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 647.6860961914062 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1139.1634521484375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 475.3586730957031 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 491.5115051269531 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 908.6600952148438 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8729, loss_val: nan, pos_over_neg: 901.9602661132812 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8566, loss_val: nan, pos_over_neg: 968.04638671875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 776.8095092773438 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1501.3624267578125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 2677.43408203125 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1562.9471435546875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8681, loss_val: nan, pos_over_neg: 846.8407592773438 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 1843.5836181640625 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8726, loss_val: nan, pos_over_neg: 739.7339477539062 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 422.517333984375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 968.0087890625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 976.1699829101562 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 1336.40673828125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 738.5817260742188 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 1070.813720703125 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8742, loss_val: nan, pos_over_neg: 779.9295654296875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 472.6432189941406 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 741.880859375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 1251.5897216796875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1826.177734375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 711.941650390625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 2698.052978515625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8702, loss_val: nan, pos_over_neg: 1962.7630615234375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8659, loss_val: nan, pos_over_neg: 1977.1195068359375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 1127.425048828125 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 853.52880859375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 670.09326171875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 879.6945190429688 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 558.18798828125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 1209.894775390625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: -18656.80078125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8742, loss_val: nan, pos_over_neg: 1120.5794677734375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.876, loss_val: nan, pos_over_neg: 783.5965576171875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1140.6614990234375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 9741.515625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 1077.34716796875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 690.9925537109375 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 1606.4693603515625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 998.3960571289062 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 2306.722900390625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8744, loss_val: nan, pos_over_neg: 1280.813232421875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 809.7721557617188 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 1451.816650390625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 2812.684326171875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 7150.8046875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 1720.7137451171875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 761.3158569335938 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8753, loss_val: nan, pos_over_neg: 594.2666625976562 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 467.0705871582031 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8602, loss_val: nan, pos_over_neg: 718.747802734375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 691.2517700195312 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8642, loss_val: nan, pos_over_neg: 805.7505493164062 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 849.0347900390625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 1102.896484375 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 1051.614013671875 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 958.0662841796875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 1198.4036865234375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1239.5999755859375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 1597.984375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 1377.4857177734375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 1256.8863525390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8778, loss_val: nan, pos_over_neg: 963.946533203125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8726, loss_val: nan, pos_over_neg: 769.9324951171875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 945.3192749023438 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 657.7780151367188 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 431.3899841308594 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8662, loss_val: nan, pos_over_neg: 913.3431396484375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8745, loss_val: nan, pos_over_neg: 815.9619750976562 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 1250.4283447265625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 1083.3646240234375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 350.61688232421875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 1185.708984375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 1003.5031127929688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 617.37353515625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8791, loss_val: nan, pos_over_neg: 497.71014404296875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1295.2978515625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8769, loss_val: nan, pos_over_neg: 1416.2576904296875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 1603.2210693359375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8758, loss_val: nan, pos_over_neg: 88965.3671875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8638, loss_val: nan, pos_over_neg: 1692.5369873046875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 1770.0592041015625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 503.6297607421875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 945.4568481445312 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 420.9116516113281 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8793, loss_val: nan, pos_over_neg: 523.1163330078125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8774, loss_val: nan, pos_over_neg: 369.6026306152344 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 449.3805847167969 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 1082.1396484375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 933.8807983398438 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8684, loss_val: nan, pos_over_neg: 756.9476928710938 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 2172.026611328125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.866, loss_val: nan, pos_over_neg: 1386.674072265625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 1636.6514892578125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8684, loss_val: nan, pos_over_neg: 1258.5318603515625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 825.0850830078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 559.5691528320312 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 1327.0709228515625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 801.2550048828125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 527.059814453125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8717, loss_val: nan, pos_over_neg: 444.6768493652344 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 1803.70703125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 673.9996337890625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 737.197998046875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8693, loss_val: nan, pos_over_neg: 1863.5009765625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 611.314208984375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8736, loss_val: nan, pos_over_neg: 785.5884399414062 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 805.1036376953125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8726, loss_val: nan, pos_over_neg: 897.2920532226562 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 987.0489501953125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 570.815673828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 1000.1143188476562 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8749, loss_val: nan, pos_over_neg: 794.4368896484375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 1192.51513671875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 725.8656616210938 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 1956.5703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8702, loss_val: nan, pos_over_neg: 1242.8040771484375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 1181.6202392578125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 672.4028930664062 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 2076.508056640625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 683.8614501953125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8709, loss_val: nan, pos_over_neg: 791.3701171875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 1782.3914794921875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8602, loss_val: nan, pos_over_neg: 2239.5908203125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 1429.97802734375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 2156.559326171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1045.8681640625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8659, loss_val: nan, pos_over_neg: 621.3092651367188 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 1000.7239379882812 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 704.0995483398438 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 1440.82177734375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 905.9636840820312 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 441.63543701171875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 1659.7518310546875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 552.98828125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 716.2504272460938 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 1058.012939453125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 2789.0537109375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 668.7394409179688 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 647.5883178710938 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 993.0386962890625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 13885.130859375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 746.6549072265625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 656.4727783203125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 1400.3848876953125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 784.8865356445312 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 446.18511962890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 506.3474426269531 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: -20533.076171875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8702, loss_val: nan, pos_over_neg: 1046.4820556640625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 1083.21435546875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 996.8700561523438 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.869, loss_val: nan, pos_over_neg: 1083.3056640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8651, loss_val: nan, pos_over_neg: 1738.7669677734375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 2873.240966796875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 1734.906005859375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 753.566650390625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8681, loss_val: nan, pos_over_neg: 558.563720703125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 916.1072998046875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 468.1242980957031 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1227.4306640625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.856, loss_val: nan, pos_over_neg: 1237.8170166015625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 2772.85693359375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 3927.457763671875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8694, loss_val: nan, pos_over_neg: 2125.978515625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 2082.35205078125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 1053.1051025390625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 810.9284057617188 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 865.4923706054688 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 1019.4221801757812 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 911.6845092773438 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 1091.4437255859375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8757, loss_val: nan, pos_over_neg: 882.06982421875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 532.4381103515625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 2200.023193359375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 1199.4061279296875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 1282.7333984375 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 532.3038330078125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8762, loss_val: nan, pos_over_neg: 805.1534423828125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 2508.30859375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8681, loss_val: nan, pos_over_neg: 567.3161010742188 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 2864.794677734375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8763, loss_val: nan, pos_over_neg: 969.6642456054688 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 1922.887451171875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 1048.4862060546875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 3226.6904296875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.874, loss_val: nan, pos_over_neg: 1010.3713989257812 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8698, loss_val: nan, pos_over_neg: 1648.681640625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 2097.285400390625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 741.6487426757812 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8727, loss_val: nan, pos_over_neg: 896.4712524414062 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 379.3166198730469 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8811, loss_val: nan, pos_over_neg: 1316.518798828125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 966.327880859375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 2416.502197265625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 729.8995971679688 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 1082.902587890625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 2171.83056640625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 538.0297241210938 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 927.5718383789062 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 415.75909423828125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 624.77685546875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8708, loss_val: nan, pos_over_neg: 2387.4609375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8612, loss_val: nan, pos_over_neg: 975.411865234375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 499.64263916015625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 1172.4501953125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 888.7479858398438 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8679, loss_val: nan, pos_over_neg: 1233.543212890625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8731, loss_val: nan, pos_over_neg: 720.7614135742188 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 729.0682373046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 303.25115966796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 352.18402099609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 2200.026123046875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 1106.09716796875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8692, loss_val: nan, pos_over_neg: 477.59173583984375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8684, loss_val: nan, pos_over_neg: 368.98651123046875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1502.4591064453125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 981.894775390625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8779, loss_val: nan, pos_over_neg: 1431.09375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 1200.4302978515625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 888.7437133789062 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 441.31842041015625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1103.3841552734375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 1439.986083984375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 638.1805419921875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 651.986328125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 683.2112426757812 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 917.3380126953125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 1356.4146728515625 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 927.4094848632812 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 992.440185546875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 898.3355712890625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 687.9768676757812 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1663.0118408203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 3039.781005859375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 2768.551025390625 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1335.0140380859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 1415.3299560546875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.873, loss_val: nan, pos_over_neg: 367.0960998535156 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 938.0399780273438 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 541.017578125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8767, loss_val: nan, pos_over_neg: 449.1734313964844 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 632.268798828125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 574.1275634765625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 899.8978271484375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 668.8199462890625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 1018.5652465820312 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 720.9541015625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8722, loss_val: nan, pos_over_neg: 546.2363891601562 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1572.1632080078125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 1393.6021728515625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 2850.35888671875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.869, loss_val: nan, pos_over_neg: 2163.845458984375 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8612, loss_val: nan, pos_over_neg: 995.3515625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 1102.8865966796875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 1851.6705322265625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 864.6337280273438 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 815.6845092773438 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 969.779541015625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 521.4043579101562 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 400.93939208984375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 640.8057861328125 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 577.8381958007812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 945.1865234375 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 1418.553955078125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 776.2760009765625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 838.0178833007812 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 1618.5225830078125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8629, loss_val: nan, pos_over_neg: 1300.4332275390625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8713, loss_val: nan, pos_over_neg: 908.8656616210938 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 4271.90087890625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 1271.9227294921875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 813.3235473632812 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 2052.381103515625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8671, loss_val: nan, pos_over_neg: 483.8383483886719 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 923.5383911132812 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 2207.21142578125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 935.1229858398438 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 971.35888671875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.866, loss_val: nan, pos_over_neg: 675.25146484375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 899.1268310546875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 714.1681518554688 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 797.4077758789062 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 1263.2825927734375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 623.2567138671875 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 569.8482666015625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8728, loss_val: nan, pos_over_neg: 966.00927734375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 792.13427734375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 484.34564208984375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 1003.8117065429688 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 1080.291015625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 3740.99853515625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2412.263427734375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 962.0171508789062 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1088.31787109375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 5018.31640625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 1000.7982177734375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 515.4277954101562 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 513.6455688476562 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 584.8029174804688 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8629, loss_val: nan, pos_over_neg: 723.0574340820312 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 1383.5950927734375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 1001.5314331054688 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 2611.023681640625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 5964.32861328125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8602, loss_val: nan, pos_over_neg: 1559.3779296875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 738.1897583007812 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 1336.7796630859375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 2590.99951171875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 524.4169311523438 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 611.1683349609375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 1115.2840576171875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 1488.9141845703125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8721, loss_val: nan, pos_over_neg: 654.10302734375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 994.9759521484375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 1108.8602294921875 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 654.64990234375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1520.331298828125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 1100.7301025390625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 506.74334716796875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 680.587646484375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 855.2290649414062 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 1055.3658447265625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 700.3935546875 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 707.18701171875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 3900.685546875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 719.4703979492188 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 2293.4921875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8629, loss_val: nan, pos_over_neg: 1380.30078125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 1652.6954345703125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 561.9990234375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 589.66650390625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 568.291015625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 717.462158203125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 781.8094482421875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8679, loss_val: nan, pos_over_neg: 454.47784423828125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 627.5396728515625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 1719.44921875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 938.8121337890625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8661, loss_val: nan, pos_over_neg: 494.4208679199219 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 767.3173217773438 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8625, loss_val: nan, pos_over_neg: 705.44921875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 822.6527709960938 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 1090.17041015625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 583.8141479492188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 375.07080078125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 452.5231628417969 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8687, loss_val: nan, pos_over_neg: 763.9899291992188 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 978.269775390625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 612.7342529296875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 1050.5047607421875 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8684, loss_val: nan, pos_over_neg: 2340.77490234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 2823.43359375 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 889.4273681640625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1163.0291748046875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1139.4224853515625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 578.0050048828125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 1067.996337890625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 417.0508117675781 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 2332.037109375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 1289.3026123046875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 1288.5421142578125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 1215.908203125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 2849.412353515625 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8687, loss_val: nan, pos_over_neg: 732.064697265625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1947.899658203125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 920.4498901367188 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 574.4063110351562 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8768, loss_val: nan, pos_over_neg: 623.5784301757812 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 816.2552490234375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 1204.3116455078125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 22424.361328125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 909.2352905273438 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 1137.165283203125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 856.8748168945312 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 2577.539794921875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8709, loss_val: nan, pos_over_neg: 749.630126953125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 608.8043823242188 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 1287.2021484375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8695, loss_val: nan, pos_over_neg: 677.831787109375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 641.5275268554688 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 657.8092651367188 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1619.387451171875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1017.8948364257812 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 3153.697021484375 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1009.790771484375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1130.4248046875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.871, loss_val: nan, pos_over_neg: 627.5674438476562 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8735, loss_val: nan, pos_over_neg: 651.3685302734375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 739.046142578125 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 1854.3182373046875 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 997.1016845703125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 923.7322998046875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1158.1951904296875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 2315.627197265625 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 608.8568725585938 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 725.4852294921875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 1172.7200927734375 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.875, loss_val: nan, pos_over_neg: 429.72991943359375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 886.0738525390625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 658.8209228515625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 579.3096923828125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 870.688720703125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 1247.717041015625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1107.8092041015625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2092.38671875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 531.375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 716.9859619140625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 680.7742919921875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1888.4014892578125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 623.0011596679688 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 365.6333312988281 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8659, loss_val: nan, pos_over_neg: 724.37158203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8686, loss_val: nan, pos_over_neg: 696.378173828125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 680.9209594726562 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 584.9679565429688 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 988.3087768554688 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 1091.1458740234375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 3454.091064453125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 849.639404296875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8705, loss_val: nan, pos_over_neg: 555.0194091796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 672.6910400390625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 666.7943725585938 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 1548.7564697265625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 540.287841796875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.87, loss_val: nan, pos_over_neg: 761.3560180664062 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8752, loss_val: nan, pos_over_neg: 637.3497314453125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 935.531005859375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 904.7484741210938 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 709.2093505859375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1040.9075927734375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 1156.2454833984375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1093.614990234375 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 1327.0107421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 816.5105590820312 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8612, loss_val: nan, pos_over_neg: 562.18017578125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8682, loss_val: nan, pos_over_neg: 756.8865356445312 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 977.7499389648438 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1651.7410888671875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 2562.206787109375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 795.7254638671875 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 842.8894653320312 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 1118.6658935546875 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 568.5035400390625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 739.7593994140625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8681, loss_val: nan, pos_over_neg: 828.6812744140625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 673.9986572265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 929.8323364257812 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1397.5504150390625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 8936.7333984375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 2951.4072265625 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 736.4865112304688 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 981.5056762695312 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 2538.6162109375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 483.5758056640625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 543.0457153320312 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 1282.4141845703125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 625.8870849609375 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 880.930419921875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8621, loss_val: nan, pos_over_neg: 738.22021484375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 1819.142822265625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1185.6654052734375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 726.3831787109375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 817.401123046875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 1686.3897705078125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1337.8951416015625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 532.8692626953125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 1091.1907958984375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 1966.8863525390625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 1303.33642578125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8639, loss_val: nan, pos_over_neg: 456.5073547363281 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 1795.2291259765625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 1806.85205078125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1253.56494140625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 1237.6044921875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1209.835205078125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: -40740.015625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1355.306396484375 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 1811.041748046875 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 2237.129150390625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 10405.8427734375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8566, loss_val: nan, pos_over_neg: 2285.734375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 1056.8028564453125 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 1243.845458984375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 5569.49755859375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:20:52<101063:14:32, 1212.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.8567, loss_val: nan, pos_over_neg: 990.2657470703125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 890.4860229492188 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 622.38330078125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: -15714.701171875 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 17419.943359375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 2796.2646484375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 2162.3505859375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.861, loss_val: nan, pos_over_neg: -6631.8994140625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8668, loss_val: nan, pos_over_neg: 1835.092041015625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 2247.31689453125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8704, loss_val: nan, pos_over_neg: 922.8084106445312 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1358.42431640625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 1424.5179443359375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1101.4818115234375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 1280.431640625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 1557.22998046875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 987.06396484375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 682.6641845703125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1324.557373046875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1194.0455322265625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 2300.761474609375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8566, loss_val: nan, pos_over_neg: 1928.83251953125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 2351.580078125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 891.5654296875 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 1276.997314453125 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 2470.8447265625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 9457.8349609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 2648.14111328125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 724.62939453125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 2454.123046875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1177.0797119140625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 565.5756225585938 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 604.1177368164062 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 2075.391845703125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 500.37738037109375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8764, loss_val: nan, pos_over_neg: 373.68695068359375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 560.4662475585938 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 486.7462463378906 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 787.94189453125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 1639.2933349609375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 948.97705078125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 676.072265625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 500.76947021484375 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 1388.4862060546875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 1740.4005126953125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 909.8276977539062 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 706.75390625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 1120.4056396484375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 6762.71435546875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 1208.5843505859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8701, loss_val: nan, pos_over_neg: 586.04638671875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 457.9164123535156 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8647, loss_val: nan, pos_over_neg: 453.6399841308594 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 775.8211669921875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 724.2282104492188 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 2884.8359375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8632, loss_val: nan, pos_over_neg: 621.3596801757812 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 1038.80859375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 2285.31640625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8593, loss_val: nan, pos_over_neg: 1851.77587890625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 1556.615478515625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 648.609619140625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 2206.445556640625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8716, loss_val: nan, pos_over_neg: 860.6570434570312 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 989.5490112304688 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 1122.7076416015625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 764.15380859375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 1281.9549560546875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1489.901123046875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 1116.4940185546875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 720.7067260742188 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 554.144775390625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1472.9820556640625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 915.8121948242188 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 945.3973999023438 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 984.3155517578125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 911.9361572265625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 1005.7189331054688 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 3181.562744140625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 1693.8837890625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8771, loss_val: nan, pos_over_neg: 429.302734375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 957.732177734375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: -14102.3330078125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 3390.089111328125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 994.1663208007812 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 742.1526489257812 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 437.9605407714844 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 3337.7001953125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1524.874267578125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 329.7572021484375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 589.10546875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 1865.274169921875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1341.544189453125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 663.4594116210938 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2494.513916015625 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 715.3324584960938 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2229.806884765625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1334.7022705078125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 632.9441528320312 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 522.7725830078125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 544.484375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 814.6240844726562 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 765.2051391601562 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8687, loss_val: nan, pos_over_neg: 493.98089599609375 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 615.4321899414062 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 1002.0108032226562 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1315.365478515625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 4172.04248046875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 2075.489501953125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 440.1073303222656 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1456.362060546875 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1599.7110595703125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 815.5277709960938 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 1162.514892578125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8789, loss_val: nan, pos_over_neg: 501.4824523925781 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 2639.318603515625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 747.478515625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 727.7275390625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 5353.17236328125 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 2101.966552734375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 1061.3525390625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 652.94677734375 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 881.3696899414062 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 1424.282470703125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 805.7948608398438 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 436.8827209472656 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 574.630126953125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 844.21875 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 2049.929931640625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 1815.197021484375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 736.0621337890625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 868.0919799804688 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1178.9649658203125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 770.17578125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8627, loss_val: nan, pos_over_neg: 923.5775756835938 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 631.648681640625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 900.3924560546875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 1046.41357421875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 1768.7799072265625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 392.7402648925781 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 589.1753540039062 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 692.13916015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 456.0323791503906 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 449.14910888671875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.866, loss_val: nan, pos_over_neg: 715.2093505859375 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8702, loss_val: nan, pos_over_neg: 538.4307861328125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 915.9802856445312 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 815.0776977539062 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 895.3670654296875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 638.8202514648438 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 845.609375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 1015.0300903320312 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 689.7772827148438 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 845.3368530273438 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 817.4949340820312 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 953.0387573242188 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8615, loss_val: nan, pos_over_neg: 585.0661010742188 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 610.0814819335938 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 449.20587158203125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 941.8502807617188 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 658.1934814453125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8642, loss_val: nan, pos_over_neg: 368.8119812011719 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 607.8906860351562 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 749.8304443359375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 2455.469970703125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1452.493896484375 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 1092.357177734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 988.7848510742188 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1961.6651611328125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1345.98876953125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 691.5830688476562 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8551, loss_val: nan, pos_over_neg: 733.298095703125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 736.2885131835938 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1044.256103515625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8628, loss_val: nan, pos_over_neg: 846.3567504882812 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 870.6363525390625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 1264.6885986328125 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8693, loss_val: nan, pos_over_neg: 2022.466552734375 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 1560.0777587890625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 2285.04443359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 699.5908203125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 518.5661010742188 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 656.704833984375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 2189.8779296875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1055.5826416015625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1811.321044921875 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 750.9578247070312 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 2551.486083984375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: -45008.6015625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 852.2757568359375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1180.2880859375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1711.7606201171875 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 825.5591430664062 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 955.0908203125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1485.2183837890625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 1267.8348388671875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 877.2554931640625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1413.013427734375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 2418.66796875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 770.933837890625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 749.245849609375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 2187.589111328125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1101.5457763671875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 614.222412109375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 1356.755126953125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1105.6810302734375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8584, loss_val: nan, pos_over_neg: 907.0166015625 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1767.9674072265625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 953.4549560546875 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1126.2994384765625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1084.0919189453125 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 2024.2928466796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 2423.86083984375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 807.9063720703125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1190.49755859375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 4379.4208984375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 678.1355590820312 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 1013.3665771484375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8642, loss_val: nan, pos_over_neg: 799.6432495117188 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 590.8684692382812 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 1621.7694091796875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 1865.4617919921875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 1083.6517333984375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 843.4537963867188 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 888.4024658203125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 749.3114624023438 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1347.6845703125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 1018.9019165039062 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1949.6077880859375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1523.921142578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 1742.69287109375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8554, loss_val: nan, pos_over_neg: 516.1005249023438 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1134.87548828125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: -27414.873046875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1826.209228515625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8648, loss_val: nan, pos_over_neg: 727.5111694335938 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 1077.93994140625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 1129.1903076171875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 660.2385864257812 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1160.693115234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 626.3446655273438 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 1052.296875 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.856, loss_val: nan, pos_over_neg: 793.43505859375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1231.375244140625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 1845.0048828125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 623.7085571289062 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 884.076416015625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 2489.6025390625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8673, loss_val: nan, pos_over_neg: 835.6025390625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 698.3729248046875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 1195.6483154296875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8666, loss_val: nan, pos_over_neg: 697.0651245117188 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1460.1593017578125 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8714, loss_val: nan, pos_over_neg: 954.5734252929688 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 1012.7747802734375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 683.7314453125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 544.7039794921875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8702, loss_val: nan, pos_over_neg: 705.5345458984375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 610.73291015625 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 976.814208984375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8673, loss_val: nan, pos_over_neg: 481.7931823730469 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 994.4279174804688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 2732.819091796875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 1152.185302734375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 967.8566284179688 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1116.791015625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 855.7813720703125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 831.7191772460938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 677.2368774414062 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 960.8050537109375 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 613.022705078125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 504.8225402832031 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 1293.3199462890625 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1245.035400390625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 579.5342407226562 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 914.92919921875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 906.636474609375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 450.7170715332031 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 621.3795776367188 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 589.8095703125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 700.7158813476562 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 674.7276611328125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 3864.43310546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 1634.8760986328125 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 677.37109375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 977.686767578125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 2596.513671875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 843.9715576171875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 630.2977294921875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 538.6881713867188 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 732.2392578125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 3196.580322265625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 1100.4029541015625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1298.33984375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1787.6795654296875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 888.123779296875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 1334.37451171875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 878.4075317382812 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 1104.996337890625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 913.7185668945312 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 835.3955688476562 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 1304.0380859375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 665.4176025390625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1040.2208251953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 830.7847290039062 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 713.987060546875 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 992.2066040039062 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1673.1983642578125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 770.9854125976562 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 788.46240234375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 830.10107421875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 904.739013671875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1723.6661376953125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.862, loss_val: nan, pos_over_neg: 1366.7747802734375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 1137.745361328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 7025.46337890625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 1325.4068603515625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1666.497314453125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 807.5408935546875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 780.190673828125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 645.4569091796875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1535.0599365234375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 1046.905029296875 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 873.4486694335938 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 2184.81005859375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 917.0508422851562 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 540.6901245117188 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1233.6602783203125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 2275.0107421875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1025.361328125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1107.6087646484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 1465.178466796875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 869.7947998046875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1258.106689453125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 1254.6417236328125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 2070.82958984375 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 735.72509765625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 468.61419677734375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 847.3524780273438 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 3267.460205078125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 1130.797607421875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 971.0393676757812 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 5399.1279296875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 2092.358154296875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1560.8258056640625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 1716.756591796875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 943.6446533203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 767.7544555664062 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 643.0239868164062 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1548.177734375 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 856.85400390625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1139.9248046875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1857.127197265625 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 866.188720703125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 754.0914306640625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 1366.0631103515625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: -17391.919921875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1667.5555419921875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1021.8283081054688 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1667.6214599609375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 578.4743041992188 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 1331.416015625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1084.206298828125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 1013.6088256835938 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 574.3473510742188 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 1214.850830078125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1017.3134765625 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 736.625732421875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 595.0631103515625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8669, loss_val: nan, pos_over_neg: 1810.2081298828125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 3667.264892578125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 776.532958984375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 904.2579956054688 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 1347.3922119140625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8734, loss_val: nan, pos_over_neg: 959.5731811523438 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 692.1729736328125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 903.93798828125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 1412.089599609375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 464.20367431640625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1500.1907958984375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1400.590576171875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1629.8316650390625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1570.043212890625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 979.7550048828125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8657, loss_val: nan, pos_over_neg: 1243.65234375 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 4304.8447265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1019.4351806640625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 883.2548828125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 890.1969604492188 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1526.2083740234375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 998.945068359375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 2862.888427734375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 573.5040283203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 726.838134765625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 589.2571411132812 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 905.3031005859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 7038.14501953125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 527.5514526367188 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 1335.8643798828125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1113.1768798828125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8681, loss_val: nan, pos_over_neg: 2137.440673828125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 2221.603271484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 2817.01416015625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 673.7811889648438 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: -16896.724609375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1861.1068115234375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 1296.914306640625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1327.618896484375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 1116.404541015625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 900.6680908203125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 1216.10693359375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8587, loss_val: nan, pos_over_neg: 708.5369873046875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 1456.1883544921875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 886.2752685546875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1386.5743408203125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 41877.69140625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 2327.8056640625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1869.8380126953125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1073.701904296875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 673.74658203125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 1045.4571533203125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 979.097412109375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 902.7293090820312 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 683.5665283203125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 1381.4583740234375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 759.915771484375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1728.2757568359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8691, loss_val: nan, pos_over_neg: 1717.93408203125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1076.5274658203125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 1543.291015625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8674, loss_val: nan, pos_over_neg: 910.7393188476562 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 1241.01123046875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 795.02734375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8585, loss_val: nan, pos_over_neg: 636.9547729492188 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1071.028076171875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 968.8093872070312 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 1705.0230712890625 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 3220.872314453125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1153.32421875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 568.779541015625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 754.3909912109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1173.4388427734375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 1406.6661376953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 3486.528076171875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 1320.318359375 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8675, loss_val: nan, pos_over_neg: 1715.0076904296875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 2153.482177734375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 1466.7557373046875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 1510.801025390625 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1651.31396484375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 2171.05908203125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 12871.7744140625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1644.7724609375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 1667.16064453125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 4345.62451171875 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 3156.189208984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 2156.570068359375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 1439.8448486328125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8637, loss_val: nan, pos_over_neg: 575.1180419921875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 685.3792724609375 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 503.1434631347656 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 2390.867919921875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 3258.873291015625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 1218.140380859375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 856.9976806640625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1095.9681396484375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 883.5419921875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 804.2744750976562 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2417.8447265625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 628.3271484375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 1155.5721435546875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 744.6858520507812 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 1902.562255859375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 900.0263671875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1112.8970947265625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 700.3580932617188 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 816.1149291992188 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 754.9276733398438 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 878.5677490234375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 2020.7386474609375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 1848.7901611328125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1137.7354736328125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 742.414306640625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1137.6912841796875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 741.1848754882812 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 769.8004760742188 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1175.1656494140625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 1273.06884765625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 716.0300903320312 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 853.6819458007812 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 950.6748657226562 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 785.6874389648438 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 728.8490600585938 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 523.9826049804688 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 801.6768188476562 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 1213.0999755859375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 660.2073974609375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 493.9759216308594 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 808.6143798828125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 76827.1796875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1400.0804443359375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 895.1937255859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1342.33349609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 816.1654663085938 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 926.6953735351562 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 5810.927734375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 1985.663330078125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 440.2073974609375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8642, loss_val: nan, pos_over_neg: 616.0621948242188 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 555.3961791992188 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 1664.4290771484375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 770.3228759765625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1236.659423828125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 542.312255859375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 803.103515625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 912.8467407226562 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 1249.618896484375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8531, loss_val: nan, pos_over_neg: 587.9019165039062 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 788.7669677734375 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 841.5741577148438 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 898.1799926757812 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 1409.0654296875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 753.15380859375 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 829.2469482421875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 711.93115234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 886.7305297851562 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 848.8587036132812 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8667, loss_val: nan, pos_over_neg: 662.9840698242188 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8595, loss_val: nan, pos_over_neg: 950.57470703125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 521.75634765625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 733.8276977539062 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 1011.182861328125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 785.7163696289062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1048.997802734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 722.6449584960938 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1908.20458984375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 616.943359375 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 678.9099731445312 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 690.5809936523438 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 549.7026977539062 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 521.3226318359375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8661, loss_val: nan, pos_over_neg: 783.4555053710938 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8619, loss_val: nan, pos_over_neg: 748.7101440429688 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 477.8246154785156 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8628, loss_val: nan, pos_over_neg: 500.2700500488281 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 4273.07958984375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 575.2681884765625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 464.5142822265625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 846.7391357421875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 1590.2401123046875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 500.3882751464844 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 930.73193359375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8616, loss_val: nan, pos_over_neg: 429.3534851074219 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1760.9927978515625 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 467.7710876464844 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 476.65576171875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 651.609130859375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 1251.1932373046875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 802.3325805664062 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 755.6605834960938 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1071.744140625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 615.1436157226562 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1078.0614013671875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 1142.4486083984375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 887.6180419921875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 720.6259765625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 917.3073120117188 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 823.982666015625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 507.86749267578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 831.6898193359375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 1183.09619140625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 763.401123046875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 923.4296875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1349.2861328125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 853.2504272460938 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 639.119140625 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 878.2909545898438 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 1114.9808349609375 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8631, loss_val: nan, pos_over_neg: 689.60302734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1707.2991943359375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 957.2685546875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1546.1763916015625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 817.8568725585938 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 923.15966796875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8592, loss_val: nan, pos_over_neg: 839.8607177734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 419.7574768066406 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 1627.85791015625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 1075.5621337890625 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8658, loss_val: nan, pos_over_neg: 1405.64404296875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1056.75439453125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 1378.5416259765625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 73725.6875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 1821.4234619140625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1110.8914794921875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 23111.220703125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 3532.031005859375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 1312.5238037109375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 2263.81005859375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1483.1563720703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8614, loss_val: nan, pos_over_neg: 816.73583984375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 756.699462890625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 2464.80029296875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1288.96337890625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1154.556396484375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 951.5519409179688 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1019.5429077148438 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 932.1181030273438 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 786.5353393554688 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 1167.8541259765625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8605, loss_val: nan, pos_over_neg: 2026.33251953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 750.73876953125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 872.7002563476562 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 909.31884765625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 519.416748046875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 902.6983032226562 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1169.57470703125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 568.5205078125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 765.6426391601562 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8643, loss_val: nan, pos_over_neg: 500.1331787109375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 2789.590576171875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 1188.944580078125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8689, loss_val: nan, pos_over_neg: 505.2261962890625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1389.2984619140625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 821.8496704101562 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 842.8239135742188 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8654, loss_val: nan, pos_over_neg: 854.5538940429688 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 772.2390747070312 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 797.6494140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1063.23193359375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 807.9267578125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1412.2197265625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 541.03515625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 811.403076171875 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 724.8040771484375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 1216.0828857421875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 4728.87939453125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 1589.09765625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1028.9986572265625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 7931.35205078125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1541.576416015625 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 777.2091064453125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 2705.1064453125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1902.2960205078125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 692.1539306640625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 624.0276489257812 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 1269.26416015625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 1201.808349609375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 1250.0147705078125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8668, loss_val: nan, pos_over_neg: 1231.618896484375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 853.3692626953125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 683.3096923828125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1210.3548583984375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 2735.858642578125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 1266.1925048828125 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 758.1548461914062 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 639.701416015625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 5716.9404296875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1597.3572998046875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 472.22845458984375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 1107.115966796875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1208.59716796875 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 763.7188720703125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.858, loss_val: nan, pos_over_neg: 1319.5823974609375 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1791.4725341796875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1333.3572998046875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 518.5567626953125 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 756.5975952148438 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1198.8819580078125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1092.571044921875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 732.0809936523438 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1161.90625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 581.3302001953125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 487.4765319824219 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1115.702880859375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 549.1246337890625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.86, loss_val: nan, pos_over_neg: 1152.7681884765625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8593, loss_val: nan, pos_over_neg: 851.3174438476562 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 2139.293212890625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 734.7969360351562 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 490.4494323730469 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1294.958251953125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 871.2943115234375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 1093.8255615234375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 2341.45947265625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8628, loss_val: nan, pos_over_neg: 1552.68408203125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8567, loss_val: nan, pos_over_neg: 3377.61572265625 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 1192.3505859375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 2101.14501953125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1994.596923828125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1497.4541015625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 1518.7774658203125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 880.016845703125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 2709.79150390625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1683.65087890625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 1388.9752197265625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:41:09<101207:28:39, 1214.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1057.6221923828125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 812.8933715820312 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1076.8714599609375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 965.378662109375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1592.301025390625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 3215.29248046875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1064.081298828125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8536, loss_val: nan, pos_over_neg: 574.6121215820312 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 3953.781982421875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 431.45343017578125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1312.7552490234375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 759.8007202148438 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 594.4596557617188 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 791.223388671875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1273.5091552734375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1695.7650146484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 2728.015869140625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 948.2138671875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 3369.8212890625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 3233.977783203125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 2206.16552734375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 3371.642822265625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1416.13818359375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 742.3699340820312 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 1468.26416015625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1657.0089111328125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 542.6722412109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 2071.1123046875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 2056.492431640625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 1624.6624755859375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8528, loss_val: nan, pos_over_neg: 2602.02978515625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 8768.8056640625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1689.5968017578125 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 879.1900024414062 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1285.7149658203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1069.75341796875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 2391.458251953125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1019.5590209960938 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1720.552734375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 2561.069091796875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 1151.271484375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 1660.973876953125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 910.8719482421875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1678.068115234375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1563.2127685546875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1784.3375244140625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1516.356689453125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1194.809326171875 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 985.1759033203125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1420.0277099609375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 1096.1304931640625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 2301.296630859375 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 3868.712646484375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8588, loss_val: nan, pos_over_neg: 1442.8094482421875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 3573.365966796875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 987.8228149414062 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 1473.2742919921875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1011.5458984375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 582.1502685546875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 473.23638916015625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1757.1815185546875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 3908.740478515625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 4049.853759765625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 1007.3551025390625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8577, loss_val: nan, pos_over_neg: 865.2353515625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1428.7115478515625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 1299.5047607421875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 2771.7529296875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 1015.9013061523438 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2966.237548828125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 1926.3133544921875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8607, loss_val: nan, pos_over_neg: 489.4376525878906 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 696.7554931640625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 8160.87353515625 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 1325.343017578125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 968.40966796875 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 949.89892578125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 1438.8724365234375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1338.6080322265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 669.3450927734375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 897.6595458984375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 1359.1275634765625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1465.4713134765625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8542, loss_val: nan, pos_over_neg: 476.0060729980469 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 882.8479614257812 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 2128.934814453125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1696.5609130859375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 462.2732849121094 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 675.8092651367188 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 970.396728515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 1531.4532470703125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 520.7992553710938 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 710.6119995117188 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1722.3104248046875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1209.8192138671875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 582.23193359375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 932.5851440429688 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 836.8814086914062 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 686.6928100585938 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 578.171630859375 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 785.448974609375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 858.1932983398438 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 739.8153686523438 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 2136.96728515625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 1666.7596435546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 7803.82763671875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1283.1453857421875 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 2955.497314453125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1534.3843994140625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 1025.6883544921875 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 3725.1572265625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 2018.323974609375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 438.420166015625 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 559.3363647460938 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 2426.220458984375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 972.989013671875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8597, loss_val: nan, pos_over_neg: 1055.107421875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1377.9400634765625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 2070.943359375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1795.5755615234375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 1938.126953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 1211.9373779296875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 923.3412475585938 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 959.0244750976562 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 630.1237182617188 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1089.387451171875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1007.3682861328125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1126.16552734375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 566.3052368164062 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8638, loss_val: nan, pos_over_neg: 800.3206787109375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1791.05224609375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 6204.82080078125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1161.7135009765625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 869.5006713867188 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8602, loss_val: nan, pos_over_neg: 1814.9241943359375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1996.2132568359375 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 1465.2647705078125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 424.26446533203125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 615.5933227539062 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1079.8631591796875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 980.495849609375 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8559, loss_val: nan, pos_over_neg: 642.5487060546875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1044.8023681640625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1686.26806640625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1796.053466796875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8688, loss_val: nan, pos_over_neg: 783.5614013671875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1117.5738525390625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 4081.212890625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1588.5501708984375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 997.8588256835938 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8649, loss_val: nan, pos_over_neg: 430.5079345703125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 3273.06591796875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 591.4287719726562 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1036.111328125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 1229.64208984375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 754.788818359375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 896.333251953125 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 675.87890625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 1213.676513671875 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1354.7913818359375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 741.5882568359375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 946.8748779296875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1715.1715087890625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1149.1319580078125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 631.0869750976562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 621.5648803710938 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 1812.9053955078125 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1328.2572021484375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 830.0645751953125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8664, loss_val: nan, pos_over_neg: 949.6139526367188 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 2156.6728515625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1640.768798828125 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1659.65185546875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 1169.6544189453125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 925.9622802734375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1546.9847412109375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 938.2635498046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 843.5289916992188 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 1699.4942626953125 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 784.765380859375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 944.63232421875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1116.6229248046875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8633, loss_val: nan, pos_over_neg: 1060.9503173828125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 647.3331909179688 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 3778.91357421875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 957.008056640625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 919.2822875976562 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 1028.7564697265625 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.854, loss_val: nan, pos_over_neg: 912.9506225585938 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1611.7303466796875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 2154.117431640625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 576.1231079101562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 560.7565307617188 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 583.1829223632812 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 940.2366333007812 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 682.7022094726562 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 1092.6939697265625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 582.9950561523438 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 719.7997436523438 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 632.1612548828125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 1709.7452392578125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1463.7921142578125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 5723.30615234375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 837.5090942382812 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 925.1610717773438 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 589.5943603515625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 972.0470581054688 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 1875.8565673828125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1642.5435791015625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 617.8760986328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 3046.87890625 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 4014.678466796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 4099.57275390625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1334.632080078125 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 780.2532348632812 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 1920.63623046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 1532.8682861328125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 2023.518798828125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 1792.99658203125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1492.855224609375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 558.754638671875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 552.5487670898438 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 588.9078369140625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 914.9985961914062 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 769.8800048828125 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8596, loss_val: nan, pos_over_neg: 595.5331420898438 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 610.7822875976562 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 804.3443603515625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 950.3016357421875 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 1050.5029296875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1775.1866455078125 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 984.5164794921875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 662.3372192382812 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 2463.662353515625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1681.6885986328125 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8612, loss_val: nan, pos_over_neg: 514.1181030273438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 530.4693603515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 2263.430908203125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 1707.8367919921875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 2933.358154296875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 580.5614624023438 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 795.2323608398438 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8602, loss_val: nan, pos_over_neg: 1006.0386962890625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 2716.715087890625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 844.5442504882812 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 895.2611083984375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 2034.763916015625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 7608.2841796875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 1724.3485107421875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 746.5127563476562 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8625, loss_val: nan, pos_over_neg: 619.9881591796875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 746.5576171875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8561, loss_val: nan, pos_over_neg: 1188.7763671875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8644, loss_val: nan, pos_over_neg: 1194.098388671875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 805.8406982421875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 1500.52978515625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 987.3504638671875 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1382.546142578125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 5323.97607421875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 835.0402221679688 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1546.0384521484375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1169.6221923828125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 855.9707641601562 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 2318.4560546875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 6654.7470703125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 844.8853759765625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8532, loss_val: nan, pos_over_neg: 3458.082763671875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 994.9100341796875 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1640.5782470703125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1176.1531982421875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 776.1069946289062 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1122.6456298828125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 1476.704833984375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1161.6676025390625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8559, loss_val: nan, pos_over_neg: 1184.859130859375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 3116.924560546875 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 940.096435546875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 4292.91064453125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 4492.697265625 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8528, loss_val: nan, pos_over_neg: 652.409912109375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 2752.603759765625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 3543.845703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 2650.724365234375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 736.9767456054688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 734.1637573242188 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1258.9337158203125 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 2557.57421875 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 4318.38037109375 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1101.5736083984375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 746.7904663085938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 677.2022094726562 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1184.30126953125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 699.7957153320312 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1084.4493408203125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 1103.4259033203125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 1018.4379272460938 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 905.2730102539062 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8566, loss_val: nan, pos_over_neg: 756.2088623046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8603, loss_val: nan, pos_over_neg: 563.5831909179688 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 1212.458251953125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 3146.069091796875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 1607.5401611328125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 10880.927734375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 8058.64111328125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 3538.319091796875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 670.3065795898438 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 10178.375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 705.2197875976562 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 1769.45654296875 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 1602.74169921875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 3248.80126953125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1341.7957763671875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 1674.21337890625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 988.1905517578125 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1245.879150390625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 7308.4013671875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1283.0797119140625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1093.7200927734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1052.3426513671875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 899.9622802734375 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1231.1768798828125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 1187.5489501953125 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 1111.3343505859375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 992.2574462890625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1521.9522705078125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 1113.09423828125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1091.860595703125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 571.2896118164062 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 693.8104248046875 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1456.1539306640625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 2369.964599609375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 852.5890502929688 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1464.86181640625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1263.586181640625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 978.9681396484375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 11319.54296875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1846.134033203125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1299.1622314453125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 981.3809814453125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 671.3886108398438 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 952.177490234375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1283.580322265625 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 1575.5335693359375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 3284.07861328125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1180.566162109375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 4642.9501953125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 1460.199951171875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1374.2994384765625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1365.0484619140625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 657.6150512695312 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 848.6399536132812 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 891.4876708984375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 998.218017578125 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 871.4863891601562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.855, loss_val: nan, pos_over_neg: 2364.6484375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 974.8067016601562 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 2166.02294921875 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 1486.440185546875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 2264.96630859375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 1732.85498046875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1004.4580688476562 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 488.3896789550781 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 1213.3419189453125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 579.3089599609375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1217.8505859375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 1088.819580078125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8628, loss_val: nan, pos_over_neg: 661.5242919921875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1012.1880493164062 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1330.382568359375 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8567, loss_val: nan, pos_over_neg: 1367.55419921875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1070.6173095703125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 819.4644775390625 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 2478.941650390625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 665.8922119140625 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1096.7891845703125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 2787.26513671875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 518.6580200195312 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 955.1065063476562 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 386.07489013671875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 1024.1273193359375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 2114.562744140625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8609, loss_val: nan, pos_over_neg: 664.01318359375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 855.3655395507812 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 702.3399047851562 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 731.8018188476562 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 716.1158447265625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 2651.821533203125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 859.7815551757812 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 759.5833740234375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 925.3743896484375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 1161.9417724609375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 976.998291015625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1126.3114013671875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 7477.30224609375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 708.96875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 2580.00732421875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 884.41357421875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 1316.124755859375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1418.1689453125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1150.92236328125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1005.8331298828125 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 515.1827392578125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 859.2536010742188 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 2011.67333984375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1142.7218017578125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 1007.7396850585938 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 2403.917236328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 2163.53515625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1852.519287109375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1060.402099609375 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 1718.448974609375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 14164.7607421875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1164.5716552734375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 424.1894226074219 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 845.840576171875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1382.846435546875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1214.7398681640625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 420.0793762207031 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 861.9345092773438 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 1473.4490966796875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1120.5084228515625 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 690.4702758789062 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 931.590576171875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1268.2784423828125 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8608, loss_val: nan, pos_over_neg: 604.4370727539062 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1255.5032958984375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 989.9110717773438 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1825.8134765625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 757.93896484375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 811.30810546875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8517, loss_val: nan, pos_over_neg: 1001.1702270507812 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1091.7833251953125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 937.5111694335938 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 1267.912353515625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 896.1553344726562 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8663, loss_val: nan, pos_over_neg: 330.9153747558594 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 384.09100341796875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 892.2227172851562 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 707.5487060546875 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 508.4248962402344 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 661.2335205078125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 1077.508056640625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1170.02587890625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1758.6322021484375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 992.0501098632812 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 687.4091796875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 976.9793701171875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1383.9559326171875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 451.85577392578125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 595.2119140625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 1359.62939453125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1259.4954833984375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 867.721923828125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 478.8237609863281 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 409.6856689453125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 691.949951171875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 701.879638671875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1528.187255859375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1640.5638427734375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 1041.8438720703125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 554.0969848632812 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1422.9473876953125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1577.6962890625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 1030.205078125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 651.4797973632812 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 526.7039184570312 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 4576.84912109375 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1286.2452392578125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 522.7225952148438 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 540.59130859375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 1080.7554931640625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1266.1162109375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1067.041015625 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 945.7678833007812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 871.4325561523438 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 2697.065185546875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1245.6988525390625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1630.6802978515625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1203.2186279296875 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 727.7875366210938 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 8659.716796875 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1469.30419921875 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 1453.174072265625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1131.6903076171875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 647.4750366210938 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 1755.243408203125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 1357.5135498046875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1420.6148681640625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 694.8375244140625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 999.20703125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1080.396728515625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 2413.451416015625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 1544.0191650390625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 2062.038818359375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 783.3958129882812 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1017.4713745117188 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 728.01904296875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 2382.333984375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 531.0665283203125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 498.2035827636719 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 724.2005615234375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 810.3590087890625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 640.7571411132812 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 2080.66650390625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 1174.340087890625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1963.1268310546875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1508.380859375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 742.3226318359375 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 3914.114501953125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 616.2748413085938 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 7176.27783203125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1085.7442626953125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 1308.5296630859375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 769.3526611328125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 571.1839599609375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 2913.80419921875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 685.4906616210938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 3420.19287109375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8563, loss_val: nan, pos_over_neg: 356.4981994628906 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 851.576416015625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 6101.697265625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 3515.239013671875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 620.2879638671875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 905.2802734375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 1068.173583984375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 787.4843139648438 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 792.3358764648438 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1114.34765625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1536.880126953125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 1177.4876708984375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1605.42431640625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 845.3095703125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 1733.21923828125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 962.434814453125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 2141.603759765625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 3007.76318359375 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 962.2841186523438 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 793.9607543945312 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1699.5430908203125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 1632.0062255859375 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 533.4930419921875 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 2343.313232421875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 476.7406311035156 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 647.0963134765625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 939.6144409179688 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 760.118408203125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 442.4911193847656 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 721.8358764648438 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 848.374755859375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1157.37548828125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1122.469482421875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1075.971923828125 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 682.7708129882812 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 1357.33251953125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 2008.1976318359375 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1726.1749267578125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 2633.9541015625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8576, loss_val: nan, pos_over_neg: 358.6419372558594 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 611.6072998046875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 1475.148681640625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 558.4674072265625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 1622.911865234375 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 2204.41845703125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 750.2232055664062 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1320.217529296875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 952.3360595703125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 8763.2451171875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 2588.327392578125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 964.9935302734375 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8516, loss_val: nan, pos_over_neg: 843.2109985351562 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 1826.6502685546875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 1341.2943115234375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 1759.5965576171875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 10243.71484375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1112.192626953125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1042.5574951171875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 1539.734130859375 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1506.998291015625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 830.4540405273438 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1558.7618408203125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1230.3179931640625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1601.521484375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 1893.5054931640625 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 569.376220703125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 562.5411987304688 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1780.992919921875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 1726.3458251953125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 2523.425537109375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1293.2861328125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 674.9627075195312 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 812.7957763671875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 1025.54345703125 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1189.353515625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1703.7623291015625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1066.1710205078125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 849.9887084960938 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 836.8543090820312 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 2601.620849609375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 458.8791809082031 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8494, loss_val: nan, pos_over_neg: 464.3245849609375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 525.2244262695312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 6690.56298828125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 544.5414428710938 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 582.87158203125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1050.1124267578125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 763.643310546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 865.9407348632812 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 323.0368347167969 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 537.7222290039062 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1834.01904296875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 12696.423828125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 527.6814575195312 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 849.3571166992188 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 2936.85400390625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 937.2648315429688 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1016.6365966796875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1199.1383056640625 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 444.30218505859375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 1425.3857421875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1021.7119140625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 2546.08984375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 487.1454772949219 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 656.7208862304688 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 884.4629516601562 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 983.9619140625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1204.2955322265625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1629.68115234375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1265.2557373046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 721.3748779296875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 1301.7562255859375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 574.038330078125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 635.718994140625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 612.931640625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 695.5693359375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 4271.1474609375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8559, loss_val: nan, pos_over_neg: 949.6617431640625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 929.9551391601562 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 678.5994262695312 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 1223.6142578125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 698.7080078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8575, loss_val: nan, pos_over_neg: 457.4453430175781 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8306, loss_val: nan, pos_over_neg: 2121.6689453125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 630.11669921875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1058.529296875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1083.75146484375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1386.825927734375 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 651.1551513671875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 6565.60302734375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 553.31982421875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 667.3247680664062 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 1635.6451416015625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1549.92236328125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 2069.104736328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.852, loss_val: nan, pos_over_neg: 995.7855224609375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 593.0101318359375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 960.1112060546875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 664.3867797851562 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 672.3787231445312 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1250.6260986328125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 3044.76220703125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 1302.038330078125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 643.8697509765625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1329.9010009765625 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1415.2750244140625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 993.4912719726562 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 638.5060424804688 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 3841.705078125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 553.867431640625 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 822.4130859375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1421.8270263671875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 1059.20703125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 726.7360229492188 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 2296.152099609375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 1212.0128173828125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: -9668.1025390625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 5050.65478515625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 935.7396850585938 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 1354.3941650390625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 901.8035278320312 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 3647.38720703125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 1054.5552978515625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1591.4019775390625 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 1576.4493408203125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1329.3931884765625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 768.6630859375 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 1403.884033203125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1030.9154052734375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 2844.110107421875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 7860.52587890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 2695.500244140625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 1400.4161376953125 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 4084.43896484375 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1465.9296875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 2101.703369140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1257.408447265625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:01:31<101394:27:37, 1216.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1743.4515380859375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 739.418701171875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1211.6416015625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1371.131103515625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1261.59716796875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1452.1279296875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 752.4824829101562 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 3965.276123046875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1473.044921875 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 1099.7806396484375 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1310.7889404296875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1576.04443359375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 2325.244873046875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 767.1838989257812 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 726.3505249023438 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 673.0692749023438 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 760.2490844726562 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 702.5913696289062 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 749.71484375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1036.2174072265625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 753.589599609375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 15341.5556640625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1381.2684326171875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 727.257568359375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 4642.4541015625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 1027.220703125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1532.6019287109375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 940.3411254882812 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 4647.45166015625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 2723.567138671875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.851, loss_val: nan, pos_over_neg: -42753.828125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 4926.1904296875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 3998.9013671875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1576.9840087890625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 2921.48095703125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1319.206787109375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 2062.644287109375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 1120.095947265625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 887.1357421875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 838.3795776367188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 937.9559936523438 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 5686.50146484375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8508, loss_val: nan, pos_over_neg: 1947.6141357421875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 919.6779174804688 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 1614.4407958984375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 2285.955078125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 608.7490844726562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1212.878662109375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1722.5191650390625 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1130.6661376953125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 789.9188842773438 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 740.5772705078125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 2224.941162109375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 941.10791015625 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1146.990478515625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1621.4227294921875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1268.8875732421875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1191.1171875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1055.34326171875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 512.4226684570312 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 542.4376220703125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1708.7642822265625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 2854.884033203125 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 2075.23388671875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 862.5538940429688 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 836.7188110351562 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1100.92919921875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 667.2650756835938 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1273.4036865234375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1446.971923828125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1220.95068359375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 900.0015258789062 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 616.5150146484375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 938.601318359375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 3498.697021484375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1212.9439697265625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1405.5660400390625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1380.5538330078125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 2219.158447265625 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1133.0911865234375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 1546.1524658203125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 1368.4000244140625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 673.524169921875 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 933.1458129882812 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 817.0725708007812 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1004.926025390625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 2421.841552734375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 3451.90771484375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1020.2647094726562 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 3706.581787109375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 1787.18896484375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 4501.724609375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1841.7266845703125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 439.12548828125 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1258.6905517578125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 703.1911010742188 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 764.1018676757812 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1024.1953125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 22549.431640625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1350.39794921875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: 763.7725830078125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 2269.145751953125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1853.1497802734375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 399.2395324707031 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 879.17919921875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 941.8917846679688 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1530.1524658203125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 576.2352294921875 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 722.4093627929688 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 1493.439453125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 719.7774047851562 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2341.753173828125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 6598.1083984375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 625.1155395507812 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 2272.41015625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1228.43896484375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 3093.96044921875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 736.4481201171875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1436.93505859375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 2255.443115234375 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 2298.844482421875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 502.3417053222656 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 812.9009399414062 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1700.94140625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1264.452392578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1809.5347900390625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 3679.792236328125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8557, loss_val: nan, pos_over_neg: 1313.05859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 3217.63525390625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 954.2801513671875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 711.7762451171875 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1626.1781005859375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 919.91845703125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 650.5006103515625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 895.6832885742188 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 1183.8465576171875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1474.1923828125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1114.52880859375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 844.5264892578125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1025.6060791015625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 1077.5152587890625 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 599.2474975585938 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 564.968505859375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 4570.6689453125 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1191.8184814453125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 413.50030517578125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 622.1387939453125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 787.0501708984375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8307, loss_val: nan, pos_over_neg: 66001.6171875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 1702.288330078125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 670.6129760742188 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 770.4405517578125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 2602.724853515625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8567, loss_val: nan, pos_over_neg: 837.20849609375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 583.5614624023438 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1780.9071044921875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8525, loss_val: nan, pos_over_neg: 648.497802734375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 1033.4066162109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 1413.0023193359375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 564.0645141601562 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 667.095947265625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 768.2810668945312 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 3840.59326171875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 1238.78271484375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 952.6962280273438 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 784.7505493164062 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 906.1883544921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1657.81005859375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 3998.063720703125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 723.7294311523438 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 711.2236938476562 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 515.0897827148438 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8511, loss_val: nan, pos_over_neg: 565.7615966796875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 508.39398193359375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 869.5609130859375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8556, loss_val: nan, pos_over_neg: 513.3482666015625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 871.5748291015625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 517.8762817382812 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1047.584716796875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 4059.471923828125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 2120.40283203125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 1116.05810546875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 711.1920166015625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1461.0650634765625 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 734.5186157226562 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 1806.5595703125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 501.9801940917969 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 557.2926635742188 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8579, loss_val: nan, pos_over_neg: 657.6535034179688 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 647.7032470703125 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 943.82080078125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 1178.1475830078125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 1235.1241455078125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 5143.330078125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 945.0882568359375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 891.3062744140625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 995.66357421875 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 907.9132690429688 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 772.7760620117188 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 1217.9813232421875 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1064.520751953125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 8076.97900390625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 5318.078125 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 995.0780029296875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1967.4754638671875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 2477.485107421875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 2032.403564453125 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 638.8160400390625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 2187.204345703125 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 887.8002319335938 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 2598.75927734375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 822.0321655273438 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1963.59228515625 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8524, loss_val: nan, pos_over_neg: 675.6785888671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 751.2210693359375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 4104.41796875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 2334.60595703125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 958.9895629882812 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 1352.7923583984375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1691.881103515625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 777.745849609375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1321.0068359375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1873.1171875 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 933.3230590820312 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1023.3056030273438 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 510.6358947753906 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 2711.368408203125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1102.234375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 2924.66162109375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1209.3544921875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 4738.42041015625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 1112.8612060546875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 2784.0498046875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 2380.15234375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1309.3043212890625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 2206.541748046875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 3998.42578125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 1096.1683349609375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1368.53564453125 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 739.1433715820312 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 527.2551879882812 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 3991.280517578125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 3314.78369140625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1148.453369140625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 1625.111572265625 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 51597.640625 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2848.440185546875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 1389.028076171875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 7282.6484375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 3446.6787109375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1164.2916259765625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8458, loss_val: nan, pos_over_neg: 1390.736572265625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1344.116943359375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1754.6783447265625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 2333.006103515625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 1189.7108154296875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1493.84912109375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1786.53466796875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1859.330810546875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1752.4013671875 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 666.562255859375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1360.2720947265625 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 910.2697143554688 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 5645.830078125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 954.6658325195312 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1351.786376953125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 532.7813720703125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 747.6508178710938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 460.47320556640625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 1122.483154296875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1808.469970703125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8552, loss_val: nan, pos_over_neg: 1327.110595703125 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 724.91162109375 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 663.7191162109375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1259.6275634765625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 542.565673828125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 648.2357788085938 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 659.0360717773438 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1094.95849609375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 636.820068359375 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 1109.0537109375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 487.6209411621094 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 417.7064514160156 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1106.883544921875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 1096.256103515625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 2034.0523681640625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1070.9879150390625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 2819.030029296875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 1336.838623046875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 1631.9923095703125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 1175.921142578125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 970.0232543945312 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1371.7669677734375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 4767.27587890625 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 662.9868774414062 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1288.5191650390625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8512, loss_val: nan, pos_over_neg: 403.21978759765625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8417, loss_val: nan, pos_over_neg: 766.4055786132812 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 2028.362548828125 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 2263.22314453125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 2031.2154541015625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1275.856689453125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 993.2902221679688 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1610.70751953125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1149.0511474609375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 724.8844604492188 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 471.57550048828125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 606.5151977539062 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 2019.20556640625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1376.6419677734375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 1082.466796875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 1256.2528076171875 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 1804.797607421875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 1862.8365478515625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 2564.861083984375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 7349.6884765625 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 2257.492431640625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 20123.384765625 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1241.8944091796875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 564.3561401367188 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 800.927490234375 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 957.98583984375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1343.896484375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8467, loss_val: nan, pos_over_neg: 804.6836547851562 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1337.709716796875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 982.8853149414062 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 2056.28564453125 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 968.815673828125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 1384.195068359375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 752.1959838867188 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1760.92822265625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1151.7520751953125 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 682.4662475585938 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 793.277587890625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1085.0401611328125 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 724.0216064453125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1042.4871826171875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1572.233154296875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 1117.2530517578125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 2006.9864501953125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8545, loss_val: nan, pos_over_neg: 476.8416442871094 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 6146.658203125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1927.0418701171875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 960.3693237304688 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 955.9119262695312 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 489.6526794433594 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 2437.941162109375 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1315.515625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 736.5601806640625 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1251.9444580078125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 932.0237426757812 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 1009.2078857421875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 3249.70654296875 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 992.2445068359375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1012.60498046875 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 534.978515625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1104.4378662109375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: -8854.0302734375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1341.0927734375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1153.8631591796875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 751.0879516601562 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1171.0660400390625 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 730.8082885742188 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1881.7454833984375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 903.7540283203125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1024.7467041015625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: -35832.6875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 4492.25927734375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 1381.12744140625 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 841.9917602539062 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1564.9354248046875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 4912.93408203125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 964.7261962890625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 2711.26318359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 909.8963623046875 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1076.0919189453125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1155.2518310546875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1168.660888671875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 5424.521484375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 2381.7099609375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1161.779052734375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1340.1934814453125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 16936.111328125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 4487.5771484375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1103.5439453125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1058.4803466796875 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8475, loss_val: nan, pos_over_neg: 557.5634155273438 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 1565.1771240234375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 786.093994140625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1554.4185791015625 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1143.0406494140625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 826.9202880859375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1645.0274658203125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 726.6498413085938 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 799.4009399414062 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 1588.8109130859375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 985.9862060546875 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1102.2197265625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 3696.49267578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 2920.4130859375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 7683.89599609375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 875.8236694335938 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1293.0294189453125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 651.785400390625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 805.0767211914062 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 556.0234985351562 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1005.0518798828125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8469, loss_val: nan, pos_over_neg: 506.6867980957031 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 358.644287109375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1119.9044189453125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1322.7783203125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 684.631591796875 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 990.3994750976562 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 669.677734375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 2357.02001953125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 672.0300903320312 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 3358.75390625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 776.0903930664062 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 971.7811279296875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 938.0903930664062 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1055.1336669921875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 710.1396484375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 551.3272094726562 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 1678.888671875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1515.125244140625 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8471, loss_val: nan, pos_over_neg: 602.1163940429688 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 551.432373046875 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 726.86767578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1517.1751708984375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1474.066162109375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 11620.623046875 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1240.7362060546875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1294.0538330078125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 912.1707153320312 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 1162.04638671875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 810.626220703125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 572.8607788085938 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1149.3533935546875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 2316.995849609375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 554.171142578125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1421.0538330078125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1770.134765625 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 1439.0228271484375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 978.5258178710938 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 744.1239013671875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 1135.2247314453125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 2456.808349609375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 640.9864501953125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1173.798828125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 1531.2044677734375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8506, loss_val: nan, pos_over_neg: 428.639892578125 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1436.68310546875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 2328.029052734375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 834.802734375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 692.4008178710938 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 1563.226318359375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 1020.8805541992188 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 1120.3634033203125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8547, loss_val: nan, pos_over_neg: 621.0382080078125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1739.0908203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1074.07275390625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1196.01611328125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 1293.294921875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 725.3770141601562 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 860.2698974609375 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 867.3518676757812 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 702.8885498046875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8533, loss_val: nan, pos_over_neg: 439.9183654785156 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 2534.2509765625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 1693.3250732421875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 1068.96142578125 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 620.81982421875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 740.5574340820312 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1671.5545654296875 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1855.2388916015625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 1412.2352294921875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 420.5612487792969 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1121.7440185546875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 725.973876953125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 502.8070373535156 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1259.8740234375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 639.032470703125 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1053.0364990234375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1033.5477294921875 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 775.2239379882812 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1572.24267578125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 2152.72265625 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 10286.7890625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 676.541015625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8526, loss_val: nan, pos_over_neg: 567.3123168945312 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 1256.53564453125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.842, loss_val: nan, pos_over_neg: 1803.94140625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 906.0835571289062 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 814.5889282226562 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 537.3554077148438 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1390.926025390625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 900.9149780273438 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 2715.117919921875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8436, loss_val: nan, pos_over_neg: 2111.61767578125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1077.8077392578125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 482.7659912109375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8457, loss_val: nan, pos_over_neg: 830.8311767578125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1696.382080078125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1371.6787109375 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8483, loss_val: nan, pos_over_neg: 561.6390380859375 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1248.3275146484375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 1362.883544921875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8502, loss_val: nan, pos_over_neg: 513.4497680664062 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1692.98828125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 6742.00830078125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1603.229248046875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1007.4909057617188 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 577.6004028320312 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.829, loss_val: nan, pos_over_neg: 3345.7783203125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 714.6773681640625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 529.6588745117188 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1166.59130859375 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1156.5606689453125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1274.448486328125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 2057.244384765625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1478.33642578125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1782.325439453125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1018.5613403320312 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 599.0192260742188 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 740.2260131835938 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 1127.153564453125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 799.2094116210938 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 745.3617553710938 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 1475.7113037109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 1218.4500732421875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1277.2637939453125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 1452.4813232421875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 1722.2918701171875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 984.8265991210938 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: 1182.8133544921875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 4018.201416015625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 1796.547607421875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 527.8859252929688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 559.2850341796875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 780.0453491210938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1756.1129150390625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 1206.9256591796875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1051.1163330078125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1877.843505859375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 833.9511108398438 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 1705.7230224609375 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 773.8916625976562 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1126.1993408203125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1191.332275390625 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1149.7281494140625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 4108.17431640625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1838.83056640625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1551.0172119140625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1864.0091552734375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 600.8871459960938 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 695.7516479492188 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 569.7971801757812 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 733.6417846679688 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1341.9429931640625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1320.943115234375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1442.031494140625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 1073.5518798828125 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1223.2628173828125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 1991.675537109375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 609.9373168945312 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1097.5234375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 739.952392578125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 810.6028442382812 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 444.4510192871094 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 1462.2484130859375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1082.253662109375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 916.7149658203125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 746.9292602539062 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1351.079833984375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 5246.3212890625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 787.2742309570312 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1515.138916015625 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 585.216796875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 1574.80615234375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 1426.1068115234375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 753.2371215820312 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 791.1181640625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 1309.847412109375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 567.8126220703125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 987.3882446289062 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 901.7326049804688 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1605.9254150390625 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 610.8534545898438 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1869.032958984375 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1084.1666259765625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1757.8238525390625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1109.1800537109375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 734.133544921875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1244.0787353515625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 664.230712890625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 926.7698974609375 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 424.9821472167969 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 589.1773681640625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: 1162.547119140625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 2558.08837890625 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 1900.0303955078125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 2872.786376953125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 2652.37548828125 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1261.874755859375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 2666.878662109375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 1317.7750244140625 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 2173.73828125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 804.8671875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1141.281494140625 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 650.6604614257812 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1249.8096923828125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 31263.626953125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1367.86376953125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 933.5635375976562 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1295.4737548828125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1674.797119140625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 974.2593383789062 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 1031.2010498046875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 975.197998046875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 664.1351928710938 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 2822.88134765625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 4029.37353515625 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 612.0430908203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1726.0194091796875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1313.8797607421875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 1966.5826416015625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 966.6679077148438 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 695.505859375 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 620.0830078125 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1487.78564453125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 2967.538330078125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 1042.9306640625 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 992.3560791015625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8479, loss_val: nan, pos_over_neg: 549.198974609375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 848.54638671875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 858.6788940429688 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1482.7874755859375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 523.3036499023438 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 617.0013427734375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8507, loss_val: nan, pos_over_neg: 649.299560546875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1122.4447021484375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1881.0396728515625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 2865.0546875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1739.9892578125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 855.2999267578125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 4452.46826171875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1575.140380859375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8548, loss_val: nan, pos_over_neg: 738.7228393554688 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 500.8532409667969 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8503, loss_val: nan, pos_over_neg: 437.444091796875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 1454.0986328125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1404.168212890625 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 2010.6798095703125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 818.984375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1225.82666015625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 961.5578002929688 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 2263.427978515625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 1113.524658203125 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1152.212646484375 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 830.2972412109375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1203.829345703125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1216.6207275390625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 1095.538330078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 904.0953369140625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1715.8250732421875 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 831.3125610351562 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1237.41357421875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 2021.137451171875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 2191.95751953125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 747.5057983398438 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 2877.027587890625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 1391.1822509765625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 619.37060546875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 1222.1822509765625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 808.845703125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1575.990234375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1784.39892578125 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1777.7220458984375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1063.0968017578125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 659.4141845703125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 4671.7607421875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 944.3051147460938 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 898.6286010742188 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 524.7130126953125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 872.3123779296875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 1346.5604248046875 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 983.84765625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 730.330810546875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 519.3546752929688 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8491, loss_val: nan, pos_over_neg: 595.6885986328125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1398.6279296875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 733.2489624023438 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 975.0786743164062 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 627.0157470703125 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 747.3985595703125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:21:55<101590:09:33, 1219.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1761.6484375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 847.629150390625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 668.6856079101562 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1315.8648681640625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1841.731689453125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 990.6096801757812 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 832.4381103515625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 553.8651123046875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 686.2616577148438 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.847, loss_val: nan, pos_over_neg: 753.4320678710938 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1015.0311279296875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 661.7188110351562 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 689.445068359375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 1562.4739990234375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 2743.70068359375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 1299.06201171875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 727.5330810546875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 779.2245483398438 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 541.3114624023438 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 1032.415771484375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 1498.4886474609375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 1389.9901123046875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 528.2318115234375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 634.77392578125 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 533.5504760742188 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1337.8321533203125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1592.4945068359375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 781.1649169921875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 801.2142944335938 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8522, loss_val: nan, pos_over_neg: 795.419921875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 1898.6163330078125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 2528.939208984375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1645.7230224609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 677.5701293945312 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 1212.62890625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 719.1747436523438 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 1479.8648681640625 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 2809.97119140625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 2322.7333984375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 435.65576171875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1128.478515625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 659.3782958984375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 1189.301513671875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1180.81005859375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 3702.970947265625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 5204.8037109375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1363.19775390625 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 999.1173706054688 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 1508.5704345703125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1363.313720703125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1613.7344970703125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: 1043.122314453125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 1921.576416015625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 4187.86767578125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1195.8316650390625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 2230.710205078125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 1954.00146484375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 5050.609375 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 986.0313110351562 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1796.0316162109375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1019.7804565429688 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 915.492431640625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 914.9775390625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 957.5809326171875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 793.4308471679688 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1340.0621337890625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1565.2420654296875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 4500.3857421875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1553.133056640625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 607.7257080078125 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 590.63623046875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 964.23583984375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 5409.84130859375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 795.6231689453125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1488.961181640625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 584.9472045898438 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1095.910888671875 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8307, loss_val: nan, pos_over_neg: 5362.24658203125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 18750.20703125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1471.8140869140625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 2532.9736328125 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1219.4881591796875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 2077.650390625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 730.125 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 1006.440673828125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 1142.61962890625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 1202.3414306640625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 929.5300903320312 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 544.6387939453125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1603.943603515625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 881.1341552734375 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1663.866943359375 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1353.1572265625 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1461.322265625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1188.6748046875 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1133.82177734375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 2791.794677734375 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1354.8607177734375 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1233.66015625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 1031.0235595703125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 1027.706787109375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 3482.568115234375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 970.1575317382812 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 2635.134033203125 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 1039.673828125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1358.5662841796875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 841.2327880859375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 543.5482788085938 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1071.1011962890625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 1396.0128173828125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1227.54345703125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 6888.30419921875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1303.483154296875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 6231.142578125 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 747.044677734375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8241, loss_val: nan, pos_over_neg: 6161.5126953125 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1184.8516845703125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1518.45458984375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 889.4111938476562 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 826.8479614257812 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 923.5333251953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8399, loss_val: nan, pos_over_neg: 599.5157470703125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 782.9646606445312 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1222.658935546875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 700.052001953125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 764.6180419921875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1393.89892578125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1993.9515380859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 2435.685302734375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 2242.87890625 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1670.474853515625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 765.7630615234375 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 1003.0770263671875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 799.7337036132812 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 2151.2109375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8465, loss_val: nan, pos_over_neg: 1122.5819091796875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8539, loss_val: nan, pos_over_neg: 548.9436645507812 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 585.3453369140625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1179.9737548828125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 3364.482177734375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 857.9737548828125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 2394.34765625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 722.4658813476562 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 691.6024169921875 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 1326.10546875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1152.0628662109375 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1031.2899169921875 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 764.7399291992188 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 1325.958251953125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 3411.70849609375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 658.9818115234375 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 891.2991333007812 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1304.5552978515625 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 992.1779174804688 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1053.5877685546875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 2469.464111328125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 672.0791625976562 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 818.2217407226562 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.843, loss_val: nan, pos_over_neg: 583.7147827148438 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 3003.04638671875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1787.6475830078125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 1777.50341796875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 919.8094482421875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1464.7626953125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 961.010498046875 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 1250.36279296875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 1339.177490234375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 828.4066772460938 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 887.16796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 1744.5577392578125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1412.7784423828125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1278.6676025390625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 955.2020874023438 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 2976.02001953125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 823.510009765625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1298.605224609375 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1732.150634765625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 1291.428955078125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 1665.8131103515625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: -60707.44140625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1554.58447265625 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1891.7781982421875 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 1655.1876220703125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 809.3760375976562 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1361.6754150390625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 4797.89990234375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 198145.796875 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 1039.4716796875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.846, loss_val: nan, pos_over_neg: 1464.5758056640625 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2721.74462890625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 1189.2589111328125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1249.7275390625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1330.2666015625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: -2459762.75 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 2262.915283203125 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8421, loss_val: nan, pos_over_neg: 3761.057861328125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1405.7359619140625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1110.99755859375 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1127.271484375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8445, loss_val: nan, pos_over_neg: 926.1417236328125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1481.15771484375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 757.3014526367188 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 762.8093872070312 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1027.7452392578125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1425.1529541015625 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 4833.59716796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 642.9827270507812 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 980.7171630859375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 653.1341552734375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 950.7120361328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 1258.9237060546875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 1219.5751953125 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 630.098876953125 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 957.1484375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 1245.22900390625 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 931.77587890625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 1200.107666015625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1596.6566162109375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 1293.206787109375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 969.8030395507812 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1153.375244140625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 6356.96923828125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 821.7037353515625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 467.15911865234375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 988.1654663085938 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1172.61865234375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 1083.1451416015625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 1085.3236083984375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 445.5337219238281 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 6826.3935546875 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 2780.570556640625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1579.151611328125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 4622.39501953125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 979.2216796875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1423.6170654296875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 2039.66015625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 1468.64208984375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 859.6023559570312 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 2108.281982421875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 900.5894775390625 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8455, loss_val: nan, pos_over_neg: 605.8192138671875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 935.5167236328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 3305.79736328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1259.9639892578125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1052.01708984375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1438.9298095703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1050.8411865234375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2003.5445556640625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 8179.5419921875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1969.7506103515625 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 2230.55859375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8463, loss_val: nan, pos_over_neg: 653.1273803710938 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1955.3834228515625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 818.5706787109375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 502.7691650390625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 897.375732421875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 900.4459838867188 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 1227.467529296875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 8030.15234375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1043.615478515625 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 1981.5615234375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 2543.96142578125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: -50524.19921875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 1235.9073486328125 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1547.1094970703125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8256, loss_val: nan, pos_over_neg: 1012.4259643554688 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 1187.41259765625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 1226.47021484375 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8427, loss_val: nan, pos_over_neg: 695.9686279296875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 542.2723388671875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 995.2891845703125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 1163.4566650390625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1720.787841796875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: -8633.7861328125 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1531.134033203125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1076.798583984375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 752.9918823242188 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 1954.4488525390625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 1292.55078125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1498.4716796875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 806.935302734375 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 11439.3232421875 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1861.398193359375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 824.8850708007812 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1256.382080078125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 761.92138671875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 812.3538208007812 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 2890.362548828125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 1018.8563232421875 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 1016.6927490234375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1075.496826171875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8416, loss_val: nan, pos_over_neg: 658.88720703125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 982.5597534179688 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1075.534912109375 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 1970.741455078125 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: -1070268.0 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 1675.5521240234375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 1448.0419921875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 1622.97900390625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 6712.26708984375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1190.2586669921875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1033.8724365234375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1647.6436767578125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 2150.405517578125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1351.2802734375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: -12729.8212890625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 3316.530029296875 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1088.451904296875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 2370.33837890625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 3306.816162109375 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: -143179.9375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 1078.1593017578125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 1680.9066162109375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1059.6629638671875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 2254.34326171875 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: -5055.01123046875 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1274.1673583984375 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 966.9865112304688 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 1446.6898193359375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 7950.23486328125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1719.463134765625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 1181.2662353515625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1497.0352783203125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 1859.87109375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1431.6683349609375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1141.8099365234375 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 817.8656616210938 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1855.0096435546875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1975.159912109375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1863.726318359375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 815.93994140625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 1386.44091796875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 571.60693359375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1430.0665283203125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 1716.1942138671875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 990.9434204101562 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 6002.3330078125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 3925.250244140625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 896.5948486328125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 534.5431518554688 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.8541, loss_val: nan, pos_over_neg: 406.9793395996094 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 5967.85888671875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 2876.667236328125 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1091.6173095703125 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1000.1805419921875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1389.7225341796875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 5642.68798828125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 1812.4332275390625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1422.02392578125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1484.1109619140625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1366.99609375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1401.152587890625 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1134.2734375 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 888.4265747070312 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 793.2929077148438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 549.1451416015625 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 1432.6029052734375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 792.779541015625 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1258.3050537109375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 654.9765625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 933.96142578125 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 734.4993896484375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 4754.86376953125 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 718.0896606445312 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 926.6531372070312 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 656.1041259765625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 601.6697387695312 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1328.614501953125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1271.34326171875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 550.5231323242188 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.841, loss_val: nan, pos_over_neg: 595.7896728515625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 870.6589965820312 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1553.2757568359375 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 761.0241088867188 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1653.322265625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1313.8892822265625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 4438.86083984375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 731.3092041015625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1104.4454345703125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 875.4684448242188 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 5650.4873046875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 2097.9697265625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 2553.19189453125 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 2389.48876953125 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1061.9727783203125 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 5086.6318359375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 14613.775390625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 7143.90576171875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1188.5118408203125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 754.5592651367188 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 732.4410400390625 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 2145.82373046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 4428.29541015625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 2335.5712890625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 807.7437133789062 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1462.2142333984375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 816.8790283203125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 3157.571533203125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 10389.8642578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 1039.56640625 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 1411.9188232421875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1033.0306396484375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1429.3548583984375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1495.5675048828125 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1349.6119384765625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 968.7388305664062 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 538.9431762695312 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 1146.631591796875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 2977.4345703125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1202.6646728515625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 973.9865112304688 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2290.251220703125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 759.3386840820312 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 1264.01318359375 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1873.091552734375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1296.2037353515625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 4914.7607421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1007.9849243164062 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 730.6224365234375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 959.0465698242188 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 778.1013793945312 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 438.66058349609375 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 1097.0416259765625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1644.991943359375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 1974.185302734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 879.5113525390625 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 727.0352783203125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 960.1184692382812 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 887.798583984375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 2496.24267578125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 2544.232177734375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1305.6085205078125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 2110.57373046875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 1619.4609375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 664.0211791992188 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 1047.1630859375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 961.9776000976562 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 532.1211547851562 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 2041.6741943359375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 1721.57421875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1077.1104736328125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 987.1314086914062 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1395.2210693359375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 2772.892333984375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 2245.653076171875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 1157.046630859375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 952.1714477539062 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 996.0139770507812 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 890.0855102539062 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 911.1802368164062 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1068.9945068359375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 990.7896728515625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 1439.4349365234375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 2637.219482421875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: -3522.8828125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8265, loss_val: nan, pos_over_neg: 1122.0145263671875 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8488, loss_val: nan, pos_over_neg: 569.5034790039062 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 2251.709716796875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1146.0870361328125 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 1973.783935546875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 1173.913818359375 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1232.179931640625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 791.0881958007812 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 801.33251953125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1347.19775390625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 745.172607421875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 800.7225341796875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1167.2314453125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 694.1593627929688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1386.773193359375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 10693.5712890625 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 781.9170532226562 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 802.8883666992188 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 1007.8999633789062 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1021.9421997070312 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8357, loss_val: nan, pos_over_neg: 2008.9674072265625 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 2343.59423828125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1269.963134765625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1868.70068359375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 2880.55029296875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 654.7786865234375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1016.3472290039062 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.844, loss_val: nan, pos_over_neg: 639.3283081054688 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8393, loss_val: nan, pos_over_neg: 2412.0087890625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1073.372314453125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1233.2598876953125 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 723.38330078125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8398, loss_val: nan, pos_over_neg: 3493.52197265625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 2527.289306640625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1509.697509765625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 7643.07958984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 3636.474609375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 3924.017822265625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 2563.85546875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1039.9005126953125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1003.3331298828125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1237.3165283203125 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 1759.2255859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1317.7738037109375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 987.3857421875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1293.9097900390625 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 804.4463500976562 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 2050.990478515625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 1870.2674560546875 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 3736.455322265625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 1047.753662109375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1416.2633056640625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 3251.585205078125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 3652.88037109375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 1016.6421508789062 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1087.659423828125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1972.091552734375 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1562.8521728515625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 685.718994140625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 853.1524658203125 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8296, loss_val: nan, pos_over_neg: -26655.48046875 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 2968.875244140625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 650.6043090820312 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 1106.547119140625 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 966.990478515625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1338.7071533203125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 975.087646484375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8397, loss_val: nan, pos_over_neg: 843.1787719726562 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 867.4424438476562 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 862.5328369140625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1421.395751953125 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 2698.33935546875 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1745.8604736328125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 698.4071655273438 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1641.277587890625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 4009.8115234375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 1066.58447265625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 1252.3037109375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8286, loss_val: nan, pos_over_neg: 2468.428955078125 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 623.3859252929688 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1030.073486328125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 869.266357421875 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1131.9852294921875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1193.368896484375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1356.6236572265625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 2253.5126953125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8383, loss_val: nan, pos_over_neg: 1626.3673095703125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 2310.85009765625 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 21109.0 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 1035.0045166015625 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 1207.9488525390625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 1540.1676025390625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1113.260986328125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 1236.3948974609375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1180.516845703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 796.8729858398438 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 1040.9664306640625 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 1136.7091064453125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1145.8642578125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1493.8009033203125 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1656.228759765625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1206.315185546875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 811.8040771484375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 1414.1636962890625 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 4866.7060546875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 520.3863525390625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 872.5254516601562 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 938.2286376953125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 1623.0904541015625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1110.549560546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 922.8544311523438 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1315.188232421875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 4006.410888671875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8389, loss_val: nan, pos_over_neg: 826.2622680664062 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 456.5150451660156 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1015.4288940429688 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 600.2960205078125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 599.477783203125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1157.11669921875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 1338.863037109375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 1435.782958984375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 653.0372314453125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 2429.373291015625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: -35947.359375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.848, loss_val: nan, pos_over_neg: 795.0707397460938 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1688.2625732421875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 1558.185791015625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1472.51123046875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.835, loss_val: nan, pos_over_neg: 839.2440185546875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 964.008544921875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 582.5628662109375 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 618.0391845703125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 775.6982421875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1321.135986328125 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1573.0723876953125 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 1474.4853515625 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 5921.49169921875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1614.379638671875 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 1731.427001953125 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 2333.74560546875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 1027.3389892578125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 7773.60400390625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1359.570068359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 539.8961181640625 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 985.8133544921875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1095.2708740234375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 749.505126953125 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 951.4469604492188 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 617.656982421875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 771.454833984375 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 1185.55810546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8339, loss_val: nan, pos_over_neg: 1133.5196533203125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 500.0378723144531 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 1632.8798828125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1617.293212890625 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 1606.521728515625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 939.8297729492188 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1988.569091796875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8323, loss_val: nan, pos_over_neg: 2443.702880859375 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.8473, loss_val: nan, pos_over_neg: 645.6990356445312 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8369, loss_val: nan, pos_over_neg: 1714.7530517578125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 734.91357421875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1882.0306396484375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 1088.9022216796875 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1024.0250244140625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 1576.1715087890625 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1016.4878540039062 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1208.8641357421875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1572.9056396484375 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 953.2872314453125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8412, loss_val: nan, pos_over_neg: 898.5368041992188 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 2985.890380859375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 4512.90625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 1381.09521484375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 1069.5877685546875 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 9562.73828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 953.3612670898438 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8337, loss_val: nan, pos_over_neg: 5319.22802734375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 2270.462890625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2973.43701171875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 2319.861328125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1657.5947265625 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 5053.16162109375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.834, loss_val: nan, pos_over_neg: 1290.1834716796875 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 251213.1875 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.837, loss_val: nan, pos_over_neg: 855.7034912109375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1712.7574462890625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 2475.614501953125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 2610.869384765625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1229.1993408203125 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1527.9024658203125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 1055.508544921875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 1536.4078369140625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8241, loss_val: nan, pos_over_neg: 2183.315185546875 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 1819.6864013671875 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 1590.814208984375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1124.2032470703125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 1780.478271484375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 432.7210998535156 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 1524.92724609375 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2685.72021484375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1043.9471435546875 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 826.6189575195312 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 2040.1737060546875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1256.52783203125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1131.8607177734375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1273.167724609375 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 6384.00341796875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 829.2600708007812 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 3262.757568359375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 857.2152099609375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 957.7610473632812 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8403, loss_val: nan, pos_over_neg: 949.5421142578125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 903.1853637695312 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 2048.41162109375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8443, loss_val: nan, pos_over_neg: 1965.901611328125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 3658.853759765625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1224.573974609375 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1555.9725341796875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1207.4940185546875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8367, loss_val: nan, pos_over_neg: 1015.4581909179688 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8277, loss_val: nan, pos_over_neg: 1192.618896484375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1493.6177978515625 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 855.3773193359375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 839.56689453125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 678.2215576171875 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 912.8176879882812 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1224.812744140625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: 2728.108642578125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 1864.5848388671875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 6344.6220703125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 1086.114990234375 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 1110.6395263671875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 3885.3876953125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 2830.548828125 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.823, loss_val: nan, pos_over_neg: 1163.6470947265625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 466.2599182128906 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 769.7616577148438 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 964.4031982421875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.829, loss_val: nan, pos_over_neg: 1153.53515625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:42:20<101761:22:17, 1221.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 517.5589599609375 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8294, loss_val: nan, pos_over_neg: 579.0269165039062 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 519.8887939453125 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 815.5110473632812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1181.8153076171875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1608.9056396484375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 1737.9708251953125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.829, loss_val: nan, pos_over_neg: 759.5174560546875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8406, loss_val: nan, pos_over_neg: 452.32623291015625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 524.4247436523438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 1023.4647216796875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 568.506591796875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8345, loss_val: nan, pos_over_neg: 1587.258056640625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1000.1834716796875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 905.7734985351562 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 952.4957275390625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1488.7830810546875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 686.6911010742188 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1106.8143310546875 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 448.60504150390625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 1021.7305297851562 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1479.129150390625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1708.08544921875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1164.3150634765625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8384, loss_val: nan, pos_over_neg: 501.2030944824219 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 821.1290283203125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1424.2874755859375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 1231.8868408203125 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1271.27197265625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 3268.97900390625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1210.770751953125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1490.017822265625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 996.4209594726562 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 831.9703979492188 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1085.8243408203125 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 1710.5517578125 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 1741.9383544921875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8317, loss_val: nan, pos_over_neg: 1182.1878662109375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 1091.8980712890625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 1013.3361206054688 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1097.0220947265625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.829, loss_val: nan, pos_over_neg: 1833.5179443359375 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 970.1388549804688 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 908.3738403320312 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1527.1160888671875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 3713.226806640625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 1002.7662963867188 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1350.9642333984375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 946.3936157226562 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 2767.929443359375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 1036.352294921875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.836, loss_val: nan, pos_over_neg: 465.76824951171875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 1233.4478759765625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 642.1516723632812 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1268.071044921875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1585.4837646484375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1540.2589111328125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 841.6671752929688 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1201.715087890625 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1285.012451171875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8309, loss_val: nan, pos_over_neg: 1505.6329345703125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1093.800048828125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 13041.5556640625 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 7378.42578125 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 3719.414306640625 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.83, loss_val: nan, pos_over_neg: -10275.8359375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 710.7628173828125 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8401, loss_val: nan, pos_over_neg: 681.9032592773438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 1237.914306640625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8446, loss_val: nan, pos_over_neg: 996.6409912109375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8225, loss_val: nan, pos_over_neg: 48300.58203125 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.833, loss_val: nan, pos_over_neg: 10307.873046875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 788.09765625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 5744.2060546875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8291, loss_val: nan, pos_over_neg: 5768.9658203125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 3378.9755859375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8372, loss_val: nan, pos_over_neg: 8219.5234375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1156.929443359375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 936.0974731445312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 638.6007690429688 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 505.7452087402344 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 1935.3756103515625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 4793.634765625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 1347.7156982421875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 2764.791259765625 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8271, loss_val: nan, pos_over_neg: 4849.3212890625 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 2893.601806640625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1916.1798095703125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8255, loss_val: nan, pos_over_neg: 3909.74755859375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1683.8857421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 936.6182861328125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 952.0680541992188 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 973.3963623046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 944.804443359375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 813.4931030273438 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 595.38427734375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 2529.684326171875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1399.5233154296875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 2614.8173828125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.832, loss_val: nan, pos_over_neg: 908.80322265625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 669.80615234375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1867.825927734375 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1100.1444091796875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.8442, loss_val: nan, pos_over_neg: 1582.431640625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 1550.1439208984375 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 4426.49609375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8334, loss_val: nan, pos_over_neg: 3641.882568359375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 64570.015625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8423, loss_val: nan, pos_over_neg: 686.9468383789062 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 4581.35009765625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 1537.224853515625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 2592.467041015625 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 994.225341796875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 3288.056884765625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.8404, loss_val: nan, pos_over_neg: 743.179443359375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1101.3343505859375 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 647.8855590820312 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 6539.568359375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8266, loss_val: nan, pos_over_neg: 18801.681640625 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8338, loss_val: nan, pos_over_neg: 781.0001831054688 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8235, loss_val: nan, pos_over_neg: 7954.56494140625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 1899.86328125 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 2087.241455078125 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 2041.374755859375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8366, loss_val: nan, pos_over_neg: 2738.51953125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1313.7398681640625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 1387.9622802734375 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8297, loss_val: nan, pos_over_neg: 946.7952880859375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1443.090087890625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8346, loss_val: nan, pos_over_neg: 1265.0062255859375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8411, loss_val: nan, pos_over_neg: 703.76708984375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 2132.165283203125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 2824.19970703125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1106.7989501953125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8336, loss_val: nan, pos_over_neg: 1251.0943603515625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8307, loss_val: nan, pos_over_neg: 1233.095458984375 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=transl0'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
