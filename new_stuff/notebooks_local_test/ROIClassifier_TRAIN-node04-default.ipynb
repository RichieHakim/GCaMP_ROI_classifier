{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc #, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_SYT = data_labeled['SYTmasks']\n",
    "# labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "# non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "# labels_SYT = labels_SYT[non_nan]\n",
    "# masks_SYT = masks_SYT[non_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 512\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.6.0.conv1.weight\n",
      "base_model.6.0.bn1.weight\n",
      "base_model.6.0.bn1.bias\n",
      "base_model.6.0.conv2.weight\n",
      "base_model.6.0.bn2.weight\n",
      "base_model.6.0.bn2.bias\n",
      "base_model.6.0.downsample.0.weight\n",
      "base_model.6.0.downsample.1.weight\n",
      "base_model.6.0.downsample.1.bias\n",
      "base_model.6.1.conv1.weight\n",
      "base_model.6.1.bn1.weight\n",
      "base_model.6.1.bn1.bias\n",
      "base_model.6.1.conv2.weight\n",
      "base_model.6.1.bn2.weight\n",
      "base_model.6.1.bn2.bias\n",
      "base_model.7.0.conv1.weight\n",
      "base_model.7.0.bn1.weight\n",
      "base_model.7.0.bn1.bias\n",
      "base_model.7.0.conv2.weight\n",
      "base_model.7.0.bn2.weight\n",
      "base_model.7.0.bn2.bias\n",
      "base_model.7.0.downsample.0.weight\n",
      "base_model.7.0.downsample.1.weight\n",
      "base_model.7.0.downsample.1.bias\n",
      "base_model.7.1.conv1.weight\n",
      "base_model.7.1.bn1.weight\n",
      "base_model.7.1.bn1.bias\n",
      "base_model.7.1.conv2.weight\n",
      "base_model.7.1.bn2.weight\n",
      "base_model.7.1.bn2.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[11]) < 6:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[11]) >= 6:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "    \n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAD8CAYAAABHPleGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAA8r0lEQVR4nO29X6gs233f+fmtVVXdvfc+59xz7r36E0XYCtEMo8BANMIOJIRAJkTxPCgvGeJAyIBALw6TQB5yHT/kyeDkITAveRBEJAPBiocExg8GY5sMJjDJSGMUR39GtmzH0bXlXF2dc+/+0/+q1vrNw1pVXV1dVV3du/fZda72F/Y53dX1Z61Vv/Vbv/9LVJUHPGAIzH034AGvDh6I5QGD8UAsDxiMB2J5wGA8EMsDBuOBWB4wGHdGLCLyWRH5toh8R0TeuqvnPODlQe7CziIiFvgt4C8BbwNfAX5SVb958oc94KXhrjjLjwHfUdXfVdU18GXgc3f0rAe8JCR3dN+PAd+tfX8b+PGukzOZ6sxcbB9UZQjPE+g9b/t33foPCf+IETAWrEGtQa2EP0P1h4AKmAJMDib3iPPgPKgHERATbq4KPvwfOHejhc3ni8T+6s5v7ajfTzrHYfeYtn6s44oX76rqm22/3RWxtPVyq3ki8gXgCwBTOefPzP6n7bO9R53ffDddA1e/pjECJrwIrV6e371GDGY2RR5doOczitcvWD/NWD+y5GdCcSa4KbgM1MLkOTz6w4LZHy6wV0vkao7mOZJl6CRFvMJqjS6X6DpH12twbnsgYlvEWrAWsRZ1DpwL/4sJBCxmu+9l/+r9ELN/bFqu0+ZYRfyq+1e/33WbuyKWt4GP177/ceAP6yeo6heBLwI8Nq+rOhdmmDGBULT2csW0EsIW6r+r3wx0/Xxv2gkGwPvAESBwEQuagE/BJ1TT1OZKMvfY6xXkBaQJTDP8dILOUiR3yLUg8eUj0j6JxaBeEVGQWj9FA6F09a08r20cOgigrc87zyjh2g/D3RHLV4BPisgngD8A/jrwN3qv8IoaEO8332PnxbS8/AMGpkKTYOqzUrUiFhXBW8Glgs82xCI+LEHJTYFc3kBi0UmGP5vgzlPcWYJZe1JVZJVDngfi34O6kiHWbtrX5CD1fsCGa9bHI15T5xw7hNEcy+azOnAnxKKqhYj8beCXAQt8SVW/0XNFaKw3QT44bVt6fxcRiMsB1qJW8KkEzmKp2iO+lFcUsyrC0pMm6CzDXWQU5wluKhgrJDcJJDbIQbJniYj93m6U2fxWP6/2kksuLCUXjudoC4dV38KtYHt5ayOgBu6Ks6CqvwT80rCzpXXtrTrYti77xjK1uaj9EW3rvYnLXpaiswn+YoI7S3ATwadUS08p1NolJCsFEWQ2xZ/PKCKh+FRQEy5QIRBJeX8xuy9y0LBsX9fKYfddf0LcGbEcjCYxlEtGjeXWsaUrlbOuax2uEdZm4EtCFCRNw1JykVGcGYpJWILUgDgwDuxCSedglxo0pdkEf5ZSXKQUs12OqNYgxgSZxchGFuhaWrrGw7HDVbqgjQlU9lHjkt4ppwzEeIilDR2dqy8t6jWs8wMEva2ZLQYRCdcmFs0S3MQGOSXKKEFFBrtW7BLShccUihrBTzP8LMEncckSEAUMaGLCMpQkYXkbijZO2fZ9H+ralHrEyKbv9XsdyOnGQyxlw/cRSGM5adUcmrO3awmwFtIETSw+tfhEqpcPgYskK8WuFJsrZq2Y3AdimSX4xGwZCdQELqKpQbMESYLtRkRQMYjxqO+QR2B7uXFUL7ZzOfbbQmkgihZCiwSzpYo3l/EBXGckxKIbVuu1veEdtgJpLFOKbi81XYRSyROyK9hG21qyUibve+zKI06DDUUD5/BG8Glt+RFBhUhwgVg0jZzFmKBp1dXS+OJ2lgfdEJSYDk5TH7nShrR164YMWNcAK+3Pt45fH0ZCLA0MUIv7hMWtwY8zuv050QLrHOIVcVSmw/K7yRWzLl8alXXXW0GTaOktJ75SEdTOc5qy0la3tpfHXsRzKyJtI5TGuVtcrJyM5bgM5CowGmKR/kHqMCpVgputLVOlYFx+BoL2zvbAekWLIpyWF9F8rxgXbWTRriKFR1SrJcanJnCPKACXxCJeA7E4kJrNBq+btjVtJeVSUBNgW4XQNhtIm/GsOYbN63a4dxiXIVwFRkMsHegyFEnDEqueiiBga6bUB2JnNjoXmMA6R3KHXXtMYTBF1ITKWRyNdD41aCQUb6OMYoFohBUHplCkUHAarcItfSjdEIYtLUms3e1b59B0c6o9F24IcyCRlBgvsfQRSvWxJuV3CMi9RrlSbigcsliT3CSBc9hgZxEFPzH4CbjM4NPg8BMfiEMteCuIKpKDXXmSRYGdrzHzFbJcV9xrp3tR1thZTgcQSr3vWzJPXcXep6L76LiMhDsE4yOWvsFqGt9KKb+OksXu8yVtXeOQ5Qp7ZUltEFqLWXhWMQvE4yZBpTa5BBU6143ajCBesUuPnReY+Rq5WQRnYtMZ6rVdluiztLb1v63vLffceXbtPPWKqKDWbvxyPRgXsXR4hXdQdnyotbbrvHJWOQfrHFZrzCrFLi0+FdwkGOh8GhyKpZFOPEGNThUVoQxfEFUoPOTBHUBRbNTbjpfV276ONoc+tnAQWrSr5rU7bTDBmWllRxVvYiTE0h0+0IohxqTm/To5TjTHl2o0VFpPMRHyi6AOh6VHMYViF550XqBJipwHR6NPhGJmMasEs0oRY9A2A1itXYO4SBenbRPeGwa4LS2waX+BICNFDImYHAmxNNAWXlCii1DqTrEh637JVUofTvXsaC+x4KaQXwRB1C4hWYLNIVk47E2OmVqMC7KMT8FNDW6aYFO7sSqbaJSLE6Ka+Ud6fkMTG0uG90FYboRgbHueG/c+wm80vuj+QzrRNDRVhxuCY3lefXZXArEJ3uEk/PnU4CcGlwkuC3JK0IzAriBZeOyywCxykqUjWSp2FZyNKlHj6hEYd7hJqTp3yRrl5wPsIV22nB1h2sj2JNvDsUfCWfbMNN8yUG2GpsbxVk1hy+tc+odMsOJmCX5iKWaGYlrzERWQLJX0xpPcOMwiyDf2JiWdJcEuEw1y4tkEUtUHv27/qfWh+QKr9raFD8R+quom7oeGFbdD8N/r8b6veJaj0FxqGuboLQxk113XlYa86rkiIfY2tbiJDUJtBppEwdUFrpJee5J5jizWyCoP3OU6QXyCtxINeYo4DUKz+m2hsWuprAni9bZVtpj6uNQIpjleW1zowPEaEj4xnmWoJXRwb3gh7HKYrVv0mNbji1QXuYAGQ1ppidUou5QjZAolWbigFq/WIQrOh0DtklAgakTeo0VR3XvnxbahxYJ7UJpOm0BcN04a2XZIHoGRcJY4M8rlpkXF642frUeb9fmCmpepIs6hRYHkRYifLaJPyClQ8/s4sMsCWYYlSPM8EEpmcFOz8S15gr+pKCDPWx19TY1Ivdl9kX0zvbm81TlWef99AWGNa6rn98TgjoezlOhyqu1jpU0Bcc/s0cjSQ1R9cChK6VCsOwMFQrScYhY5UhJKDMYOvqJg+q98QpFjqfO7RF+3/dS5yYA2nwR14joQI+EsDQG3zcJZndphf+iwVjZ9KM1wALwJL9458BpeuGyWFbSUWRRZrIPxDiBJUBvv6SMx5YpZO8R5tG6Mq/ep2dYe+WJLZjuUkNpie2+JURCLAGLNdoBTHzGU35sW2jYNwMd0DN8iA4WTwnJU9xRDFfkmPoRVmtwji1W1/EiaotHWEbzTil05zNpB4cK9arEiO3aQnqAt3fpci20ZQjD1cdk3hl3XdWCcy9AhnSxxCGttsvwqT0mDzAFVuEEV0Z9rpdlIksAkQ23wCZlcsSuPWbmQAlIz8+846ZoR+of081gtsA+N2J8+jIKzKETNoaba7nv5zaj3LpYrZaDPnpgZr5VWJF6RQjDr0Di7CioxxiCTScgGmGbhe6FY74Khbr4OnuZSsHUumPybfWnz9NZ8PmKhkqxDB3fb22fV7nMVtEUivlLBTxoFTagk+X1m8Sar3mHTlWGqNFo0lqq6sataTrQyrJkCQCuDnDgN2YeATjP8JEWtYJyHHMyiQOYrWK6CXONc7MNw2SHkMJU2FLv7Yof4xLZu2BL3Uwt++sDEs+wQSpsTsIkuw1SXvaayEEeu5hwmDzG3SSZoHia4XQWOo5MUjMHPUnxmQ4pHjKYz66B+a55vCL8LXluT6Y4qf9KlDNTHoc0AGBP6DiGY8RFLmzGui00Oybth2/G2xcFKqEbbiEPWBXbpSZKNRmSX4YX4aQZJcBb6LNpWCg3VFHy4nqIIhNVnQCxnd72dXS/dtxB9mx+nLyGttiw1l+5DMkBHRyx7ZYuuFJA62ggllrUo77H1Mp3bGOfWBXbpKrWY0t8jgj9L8anBTUPaiF17rA+2GQpXWW33cpYtq/NGptHSRbAVDdgSk1OP3639trM0t6UsNZfuA9Tr0RFLKxqzqG3WthKPryXbl3JJLYdn656RYKgZ0kJgdrjOFAY8aCIU05hlKEGlrtwFRVGV12jN0xkYX7vjKxoYPbeTN7Qv+OtAjJNYegZ0K0WzRHMw6izdEYijfro1qDZmZ/nCy0oKRihmhvwssGpThMh/b8FlpdNQsItoS4kcpeIqdjtyPthZOrSW6mt82XU7yT7bCGxrUm2ujrr95hYYJ7F0YFBnW4J/xHhUZbMkWRu4TTTNV4FELhjTxHlUwMVIObWCyUEKYs2W8HLsWkKqqrTIEkdoG3sxpP817tU6Xk0XwwF4pYhlk+i9OyPjh26u5BWsIFkKadYQFl0ozCMGKRyS+7C8EFNaU1ARjAlJ8S4Ll4a86EAwane5Wxk9Xz2nr221vuykoIZONwdj9/qGLDZo+WpzR3RgPMTStK+0oZwVlt7B68ypMQaSBDmbbiesr3NUlhuHYu4wRVyOYvUnUfBI+B6JJQRyh1iYEHEnu+2qybo77erwffV5zavxabm2WUWhfs/tcWgZ3wGGufEQy6FCWM/5nZWOSp/OJAsJ8aUjcJWEJaMo0GmGTgJHUUMVKumTqFAlQW4BqrxobRnnQ+SDHQNkB4fsu2erLFfve2NMKlnqANvOeIglol5TZBduvxe2w8Eo1iJpEspgJBafJZCEypSSJUiaIKq4RxPWTzLWj0PEXOlMDKmqUVA1BB9S/E3cJiyh3o/OkMlGO7eOt2hOJTG1yhkdKSX7ZJLWOJs9GA2xDA2frLzHfZpCwyZRVoUsCUUTg6YmWGFTE176NAxF/iRl9diSn4eA7ZIgfEIVZlnlQZehCYWvwhLqXCKU2NBuy/KQ5aHZry7zf3xG62TqMCvsxADtwTiIZajScAt7gUhtppWaaWJwmYnlvULZjfVjw+qJUMyk4iLeBvlEEwWVGqGAWXtkXWzCEo7BkcFInWhy1zbnYcQhFaH2jr6IfElE3hGRr9eOPRORXxGR347/P6399tOxXv+3ReQvD2pFF7XEUAKJtWJ740jrKRX1P69slUr1tYi4WB3BT4Ti3LB6Ylg9FvJHQWUuLiA/h+JM8RON0f5aVa20KyVZhjxp8jw2a1v22FkOmjGyfXHGZf+7OENzrGrPKFM/Kqtwx5JziAo95Kx/Dny2cewt4NdU9ZPAr8XviMinCGVM/1S85p/GOv770WKgqv4Xs6ko2RZ+2LCttMo7ZS2WIgYnuZB6GlThYHxbPyoJRSnOleIs/NWLJlfR/utYPmxebIKi6v1oBEuXbavQdEnsG5t6bO5Qe5N3DdeCthJjW5vasJdYVPXXgeeNw58D/kX8/C+Av1o7/mVVXanq7wHfIdTx3/eUfjN4xSE6Bqkp2O32ISwRMXxSClfl3WgS5BGXBSOcy4Jq7CeKO/O4c4c/8/hMUauID3EuyY2SXnvMfI0uYwFl7ztjXLe4Imz1Z6cEWlv/25LRYt/Le+9oVF1oe84dqs4fVtXvAajq90TkQ/H4x4B/Xzvv7XjsaGxm4x5NqOZZbavBpjHAGuc31lshlNdIasnvUTX2mYdUkdSjXmBlUGdCfMscJldKepUjNwv8fD5MXmnjiK7xW1tmQ2Mstio21eNyqusN1Y1L7tEXF3MqmeVAtD21dRRF5Asi8lUR+Wquq24bQm1GHeTbaBsg5wJrdr4ScpHSVhL8PWoDB8GAZI5kUmAnDhKtCvdUJdkXOSxXoUa/c8Pyg+r96vveOFbPd+paznZeurQsdbfwDx1LLP9VRD4aG/FR4J14fG/N/hKq+kVV/YyqfiZl0v2kNhmlD21m8BKlD0hiAZ4YkW/XsSLlCsxaEFdaWKPcJJHDZGE5cmksE5YGdVzSZFvuKNXSvjynZv/qMknzHDpMC1W/dpe+puC6ZVc5Mpb3WGL5ReBvxc9/C/g/a8f/uohMYt3+TwL/z5HP2F3nw8HN5661vEkwddWxPtucjxpN/Fsodh2i+cUJ6iU6IBVJPGR+I9Okgk8tZGnwN9nt0hetaCMgE3Ot6y98aBjDAC5R2aSaBHwEweyVWUTk54G/ALwhIm8D/xD4OeAXROTzwH8B/hqAqn5DRH4B+CZQAD+lqnsigQhGrraZ02aN7cM+l35oZBBE8wKzKkgWBpUEMFveZRTEKDZxWOuRwuIEfJZs9iKy0S9kbRBwyzbQ4gdq7XdNMBfZrhI14BpgN+Ku/NwMV2hrVyMmdx/2Eouq/mTHT3+x4/yfBX5275O30N/Q0nS+13IL2783DVMSiERlhYhgVJHCg5sAKTYTpJBo3lds5nh0tiJLCpZ5wmKVsUyCFzEUGvSdgu0hyWHHxN5WweD1/h1g3NsZzwHXj8OCC9tcZMjxVi1BWn+vPNqqUG4W5TxSFMg6JyFac2cm1sINwm6aOl4/u2FqC66SIFctkrPKzF8WUq5edZehrO8lNK9pToYewf6YF96LPcvaeIilVPm0u9RGm3t+n7l665oyot25sMOYxpCE6QSTTyqrLgJY5WK24uPn7zGza96ev8Y8TxENS5VdeMyyCMtPXhxv6g+NHHzqloNSPZ2ZluHknefcZrOHURCLwG5dWNhZZ8vP/Z7prodsawYSa+CKmCrPGYhWXUUmjmezOX/y7B1ScdwUE/7r/BG4EO2fXuWYmyW6XMVAbbfznKoPruV4M+i6p919oZIH4ZaxuKMgFspwx1pUfB/H6CSSIYNREkXJYYoCU7iY60zlOLSp5/XpDf/t9HsA/MHqtUDUhWBXYK9XyHyJXy7RvOiVpbbki7KdjQkQDjf6e2igd9u19e/l4+uFpPdZj2sYB7GUGgp0z6RB9/GNry2WyvpLKsttlTYIKa23ymya88bkmjftJbkmGFEKH6pvm0KR3O1slNkeUqHV/13EP2hpaFioW0Mv98lGjbjgQwgFRkIsCttq445T8TDr7VZ1J2CnVHvd9tKQNdSCZsrZZM3TdM4zs+R9P8GrsMoTpAjCLbG9oR6dQR29+UI7wnfTh9OVINbkEJHL7ARS9Xmv6/1T3d1bcWD0/6nN/UeitnaXxqmGVVI6Pg9GpxAolaEuyCugiTJNCiZSkMWdURc+C8TiCJtQlfaVJIE03TaoDTWqNQyJg/tVt/oeQCitjstaOMM+jIKzdKIeLtiYTX12jDKgu5VVlzCCRFO9pgk+s1WKhxTC1XLC26unfGP9IZ67C/5o8Yj1KiVRKKZC8doZiTEhVNMuEBG8V7SScw8g6Br3aN0CZh+ahrkuZ2RDVjpUDhofsXR1smnmb8yonT0Uo5m+S/CstryLJTT8xOLS+Fsu3Cwy3p6/xtcnH+d9N+Od+SPc0pKoUExh/VpIJ7FWMESZpFTHyzb3yFDtwmzNmFi/T3nuPuJp2lkGOCsPWd7HRyxdqM+WpkzTFTxUfwFN2NKvk+GmCW5iqj2ExEGxTvj+/Jz/L/sw8yLjvfkMcgOiIfnskcU4xaxTdJmHLEfTkEMaBNPr52r0cbukekf8WJOD3DHGTSxdHuS6xRL6KwG0LQemjPYPRXn8NOyg6iZUFbXxwtViyneTpyyLhOUyRQqJmYqQnwnJPDxYCheLDdacdH0xN31tq/cRaBXO69e2Has/r5Eh0OTQOwR8DzvGH4chjsD6uSW6uEffy4icRScZbmpDlNwkJJEhoE5YLlPe0QuKwuIWSQhdMBtimWTx/mWcTF8bh+Tv1ATOJioNpoP4t9C4vtWEANvjNkB+GRexHIs25yHQWsa9hJFQgj21uMzEGBWCB9wBK4PThEVhUCdhCSKGYE6hyGOCPISY3jYfT9szm00f4ibw7cV/us5toi2HaV+SfhtGQiyy+7L7Gi9mawa21amv+4O2UDtXjYSyXHHnMp8AGgKgwOALQYuyXUCi+BinZYqwUTgQditra3fTmdnTlh2u0rxPJPwtG0kTPe6DrZTZZijnQIyEWFrQ4n0FujvYtM42nY71SLR6IFRpW4nZhuLBLgVTgC8Enys+VTQJ4ZYqIX1EnInEpSHttTQqHiJ0NuSwekJa6+4htQ0w91Xu7kJrPMtAjIdY2iyVe7BjazEybFdSAK/IOkcWOclNRjK3JFNC1qEhbEhVllfX6BBPPCBQprEKYePMPBTx6U0JPcaJ11S/68tRl4mheWzf/Yeey0iIZcvr3Kyh1sVah+a7dDn3igLmS4wI6WXK5NziExuXo7jZlAfxQiHBX4QARoOHQAAFyUO4Q70066A27iOqtna3LWWNkI6q+HTHXgc7E2ygqR9GQiyl11mqHbl0WzV0LY64Fq3ioIgz52C1QtVjZxOy8xSfBI3IpTF4jhAEJUUgGpWSSgiEUrqgdFve6Mry66xc0Bec3eAu6tpTYqqtZ4yJdOx2CabHK97WjiZGQSxar4NbNrwkmOiirzzRjaiwkkg6l4C22R5DLNW5EEa5XJNc52SpoSgMZhLWGJ+yvcekCjhBctnsXDZLMY8u0NU61MDtspp6synu0/FyOl0YbTlRNE8p1eGW/mpj7BrP3tzTblJkWjAKYgHaX3QXa9wjmDVVxZ29AmOIpaiiziPrHHOzIkkM4sLOZN4aXAHU44MV8GByweRx/p4l2MeP4OomyC1l4HZs506S3E5UQ4MbtZn2u+Ja4ufWPYr6xq727ENynUdCLN0+nAoHBgD1OvLKEEtcYNnrHFmusamN9VpMtZVdtdx4wIecIikkZAAI+MSg0yxsLbOsRfk3nyemXS6oZIaOUqTQLvh3aYd7ZI9O2aQR69KGkRDLQOpuGZitum0RWzPGbByKO5ts+qgrexfYtyvN5kGVrnYn80EzEhcT0GqiSyCocjbX2H1fu7vQIcDuvc8Rnupmiu+QYokjIZYjUZMHdr2tdiPTRC2rlHu2qlg6QtHjMgeaqA3Z2v6EGqy6kssmnqU8XspKDVmhz2DY1ReNOcpd13RtI7MjOLcRa5dGeUAoxatLLHXto745ZjM4utSy2lOuq3tVO6YCVf6zkcqeIkCo0x+WIJODKUAKrUqiDsEuIey+/J2g9C53xlB0XdOQdV6RZag2IENiN1o1jkgx1oZlp6YdVDOuLSSxfFas31Jtpmk2Vl01gdTK+nJ2JSQLSJYesyo2XmdozwZszux6JmJHyKi2yCk73Ko8Xn/RZVhGjzukVagtl/QejIRYNtiqSN2VXkFtxgF1M3j4urG57Ng02u5VBoxXQeNsiCWpyS8abC5hM/CQHy25jyXCfNUGpaa5dMgZEuvmqoOd0vB7JkpbrlCzn7tb7ESjXduy0+NTquMIG/QdoW39rEfIdc2ORj5Q9fuQ6tZlRJr3VZEfoDLOFVMozhU3jXsAOMGuYyrIklCl0koIokqSTZWG8t71/7v6OUBmaC0Q0AcjreNzW4yEWPYMRN2aWxuErWpHt4kYc3Hrl8KhUgtDOFeKC48782hSqyW3VJKlD4pUasOuZmnabUUuOUyb5tERUtD2ufW+jXtU95ZGRajqEt1wnTLLYSBGQiy0q28lcdDBKVpmTuvLGjIgVWhACD1wk2Dy15mDNLwUsxbsAtK5kt547CpwIk3MTjYCtCwnA1XU0LVGsUXZLma415dT5yz7uMur5EgssfOia+GCnZbJZuR6qUXs006MIJIEgTjLkEmGzib4bLNhUzDGBRO/XQrpNUzeV6Y/cEy/v0DWcePwVdwYvFEyo25JLmNph2gdfahrSVWmY63vnZytDUNsODWMiliAw83+zc/N3bk6rhURJMuCvDGZhN1Uz6e4iQmqctR8cIKsDMmNkF0q0xeO6Ttz7B+9CJ5rCH6t2l5DdY1kpx/Vkmq2bDNNR2lrBuM+9bmH22xV6d4ZjFfK3B/RlMrbBmeIvWGfQ1EM1cYM1oY6/tOsyh3Scg/OAszCbHGVyXs59sUN/sV7bG0/U1fRT4Cjqx10uAH2yj6vJGfZo3J2EkkzdmSfOuijgFczqKkN1Z9cGqL4zVpIPaTXwuSFMnlRYC/XsFpviKOestoS17plRyl9UobKWrw5teNlNseh5jzcKURYOhv33aNxr53PHRgPsXRFyQ2N+OoY7E6ze3zRUkiVt4wRfAyxhGBPMTdC9r4ye+6YPF9hruboOt9u8x5f0N7igUO3qmsLaaj3vctH1GVbqccyv1LE0sRQs/Ye/8ZOINHWM0KIAnn0Ot8kZJcJLhOShVS7xU8uPZMfrDDv3SA3CzRfd/thymPNiLctu0vDvVyPcTnGLlKP/Wmia2wOiRWOGFKA8OPA/w58hOCo/6Kq/m8i8gz4V8CPAv8Z+J9V9UW85qeBzxM8Nf+rqv7y4Bb1LTMD8nB2rinRMuvUxUrbi8DSTV4wWxdk709DJcq4e7xZFJirOXI9R5fLwFnaKibUCaPJITqceTt7KB6ooVTnVzsQ6y7HaXueDuMmdQzhLAXw91T1N0TkEfD/isivAP8LoX7/z4nIW4T6/X+/Ub//jwG/KiL/zaCqlY0wglZtoN7Bmt1iR7gsZ06Lnaa+uaWi6GqFrNfofI68f4m1Fms3RrTSaOdjceRyN/itNpbnlvetazNHzOJyPDpRI8zmMlJtw1tysJoJYvcRwznakGqV3wPK0utXIvItQon1zxFKnkKo3/9/AX+fWv1+4PdEpKzf/3/vbU0NnctHjaPsEEpdNY3n1PrR/bDSieY8Sr65d918Xwq0fS+9j9DrbY9R+gfbWw6J+Tmhmb/EQTKLiPwo8KeB/8Ad1e/vlTHqaA54M3a3dl4vOtj+jqZTos2r3MSQ6LVm4FbkDp0lw9qW3Vbt67BAqNZsxQ4MJhYRuQD+NfB3VfWyZ1a0/bDTGhH5AvAFgClnB6+f1Y1rOcLhxi1+olq6Q2/MaVNl97qJpm/KAG3xsPtmc1v4RU2j2oldOQTSsrdRE76RBlsL2RhCMINaJSIpgVD+par+m3j4VvX7t2r3y7Tdh7GPgLzuCoU91xwSnFy/lzZfrsYNv6PJvXWgm76i5uf6X0v7eqsxHegA7LzHgRN078hJYCH/DPiWqv6T2k+/yCnr9zdM3bUv29pF/dza8a3BrV/TNiAtTr86trhmy30qDjBksFueVRYS2npOJBJp9K0zob32/J29hloIsQ+nXIb+LPA3gf8kIl+Lx/4BJ6zfL7AJGmrpwBDTd8lKO69rkyM6XvagxHNVylp0g03zOwK42chFQwhviHbURJsm1vi+1f7b1GdR1X9HuxwCJ63f39MGX0tyr8skO5bSjqqWzYHsqipZ01bqx1qJwcTovLrqPASlYF7Xrloss2Vs7han6Wpvn8MStgmmy1I+AKOz4LaansXsEgxUoYlQCrrRF+Pctgq5NXPcVjXs1pdde77YhrVVWuJrh6Der9Iz3qMBdaWw9nqj+3CohbftFoPPvGOUrH9QYWH1265+opwxdJY3hemu8/uO1yLfBg14PRgJugXUAT6h1vOastU+WadxnsgrkmS2q1e3m8ZbUW6MWYvg39paBbbZcpMzNGGELd/NIVpHnxOwujfbL46WpawnDGOQfFR3ATQ45vYyHc0C3oRY4j0YBbEAO9ZPbQseGkJE5cvumll1Ymh6javlLfyuTeGzSQQHxpy0Zi603beBfdUZ2s7dYJO01lYtoUybHRLBN5plqMKhSwId2suxwUP1R4rsJc7BMbVN98ErCDlldNfRjRD5PnADvHvfbTkAb/DBbO+PqOqbbT+MglgAROSrqvqZ+27HUPwwtvfV5osPeKl4IJYHDMaYiOWL992AA/FD197RyCwPGD/GxFkeMHI8EMsDBuPeiUVEPisi3xaR78TA73uHiHxJRN4Rka/Xjj0TkV8Rkd+O/z+t/fbTsf3fFpG/fA/t/biI/FsR+ZaIfENE/s6dtLmsIXsffwS7++8AfwLIgP8IfOo+2xTb9eeBTwNfrx37x8Bb8fNbwD+Knz8V2z0BPhH7Y19yez8KfDp+fgT8VmzXSdt835zlx4DvqOrvquoa+DIhO+Beoaq/DjxvHP4cIYuB+P9frR3/sqquVPX3gDKb4aVBVb+nqr8RP18B9QyMk7X5vonlY8B3a98PygR4ydjKZgDq2Qyj6UNfBga3bPN9E8ugTICRYzR9aGZg9J3acmxvm++bWAZlAowEt8pmuGvcRQZGE/dNLF8BPikinxCRjJD2+ov33KYunDab4YR4aRkYI9A8foIgvf8O8DP33Z7Ypp8npOzmhFn4eeB14NeA347/P6ud/zOx/d8G/so9tPfPEZaR3wS+Fv9+4tRtfjD3P2Aw7mwZGqOx7QG3w51wFhGxhKXlLxHY+FeAn1TVb578YQ94abgrzjJKY9sDboe7iu5vM/r8eP2EehUFi/0fznh8R015wCG44sW72hGDe1fEstfoo6pfJAbkPJZn+uPmfxx253LZbIuqV20/3nWPOsq85SRF0lBMORwu91D2oLqp/FSlifS0p+95jed2XndIf/adW29Hx7m/6v+P3++6/K6IZRSGqkMhSYp94xn+9ddwjyahYnUiSKFhq5hlgb2eo1fX+MvrSDC+nxggvJgeAu297lQ4gWx6V8RSGduAPyAY2/7Gre/a1+G237pmUvm5MSMlTfCvv8b8E4+Zv7HZ49nmSnqjZFeeybsTUkAWy3ALV/6zB6d88XXU+9jGiY4h0g7cCbGoaiEifxv4ZUIYwpdU9Rt38awKxwxASSTWIkmCefyI1ZtnXH3MsnxT4uZUil2EzapMLmSJCclibcQ3NpyQUOAO01dV9ZeAXzrpTY/taN91YjBnZ8jjR/g3njD/cMbyTWH1zGPWgllDsoTpC+X8D5Yk716jVzehbn+VZD5Qtqhj3zVD5ZDmErdvuSmfO/T+NYwn1/ku0CUr1E8xArMp/tkjlh8+Y/6mYfmGhyc57jLBrgzJXJm+m5P+/vfxl1foeh32b94qT7ZfeNw5d6hwfCjB7MOR8st4iOWY2bnvfk2U9VjisiNZhkwn+A89ZfnRC24+nLB+AjrxiChSCHYhJHPFLgp0vkCXqyDYHlkw8c5wKMGU1xyA8RAL9BNMcyCGCHKN+4oJ28ZIliLn5+jjc4onM5ZvTrn5sGX5hpBfhPvoIiG9EbJLyG48pvDIJEOKApYrtKgX9BmJzDJ07LrO3UNs4yIWaGfNXZpOU6vZBzGBo0yn6ONz8g9dsHw94+ZDJsgpTz2aKiiYpQnEcqUkcx/sLGmKZCnkOdqyMfxgDOEC+/p/CIE2n3cEocD9x7PsQmTYQBwzmzXUcVMX9gnKzxMWzwzr14T8keLPPBgwK0MyDxtnitftgbxtSdHboBybW2h+R//OmDhLH7UPmWVtM6ft+jyvvrupYf1EWD9W3LmHzMPcYpdCch00IQCtKjb5iuB62zgE9Tbvu8eplrlBgnL3z+PjLPtwyMxqnKs+cJXSVF9MhOIM3JmiM4edOBAwa7DrwFW8DVbcsPNZqAknUqvAdOhSeErc9pkHXj8eztKFo5abdkFZjCDWINaGHVMFVECtYieO6WzNzdLiMoPLQArBFEoxM+SPMmRxgSl3GVHd9hE1n7+v/UNf1D6uc1st8oDrx00sdzEI5Z6IZZVLBQykWcHFdMVyluKnCX4peAeFD8uRfWQx6ympKlK4sOUMRBmmw97S1o4uItkndPbdp6uv+zTIfdc3MG5iuUuY4PfxCWjiSRLHNClIU8dy6immgkrYBg8E4wymSDFrRzJPA3cqilAscZ9r6FVcolrwQ0Us6jVsqbs24MFlUFx4ZOYQUVbOIqKQedwjwZ2BOMFNDaIGu4b0Mgkbb8aXMbTuPXC44azv/CaXuc1y9IFYhk6FasB9MNPHF+wmgntUkE0KBFgXgVjsxOGNItaDwDrLsCtLei24iSEt92RuLf3eIfCWx7sIoM9DfkhcS/05bce72jsAPxzEUoNYi0wnlYCLF7wzFM5gRMkSR3axoPCGPLcUeYI6EAd2rdjcI4ULnKV344U+5+VAjjG4U0faUF5Zc/9tpfoBL0eSFHM+Qy4uWE8TxCn22lAkYRgS63kyW/JsOmftLb///Cmr5ynT55bpc2X63FX7OrdqQYe2d4gAegrZo0ujOtBGNC47y10KgiJh78PpFH10hpsYTAHJjUEWFpeHoXg8WfLJR+/wJx99H2MUe2PIXsD0hSd7vgz7Oi+XYXePIzZ4Oggjy+kaF7E0UQpux17XuFYkeJwxBlFIFkp6FfxAYpUnsyUfnV3yo9N3+VB2RWI8kgvJQkluPPZqGfZ1XudsbZZwVziF5faEBDeeZQjuZnDqy1uMcFMRxCnpXJm8r6yfgiaOj5xf8iOzH/Cx9EU83SM+WHOThUOuF+hiEeJZbrsMDcWhAeh3aFEeD2c5JOSgC/uuiRH64j1m7bALT3qjiIPJpOAT5z/gj2fPeWQWWDzeG0wumFwxKxdiWZar7s2tDsWx1ul93wd74Q9zSo6Ls9RxTDBPFyJ3UVV0uUTSBDNJMRcpasBN4eOPr/jvz75LKgXfzV/nN28+ztXNlMkKTLmXVWLRNK3yXAbJLPuE2ENU47br236/I1lnPJylC0MHcUjsqYvc4eoaM18hLjgK3Znnk4+/z6cn32UqOb+1+AjffPER3GUWwhScBjU7S5FJBmkaou2spXdLl0Nm+ClxW39UB8ZNLKcYxKbXuSiigKq4zFCcgZ45/tjkPT4StyH63uoJ3786x8wN4kKIgpsm6MUZcn6GTKchiMp27J94anS95BMK/0Mw3mWoiWPiRlqspmItkqXo2YT1E8vymZCdr0nFcaWe5+6C99dT8nWCKPgEipmwfpIg/pw0tRgR8C7KL57eLUtPhaEOyUMDtz8QMksdXakObR3tsVaKkSpQuzjPWD0W1k89T8+XpOJ47lKeFxdc5VNcEdRrTaCYgniD2hQEstwhyxXkBUre3e6hmsnQ2ONmGsdLxqtBLF3o8oV0QUyws2QpPjX4RFCrrArL7y7e5Imd85+XrzPP03B7GziLm0gMrzQkM0syTbCTDPIcWZv98bjHLqf7fEjH4sj2vBrE0jebDlmejICNBCNgnGJXwvXljK9NP8Z7+YznqzPWRYJYxaeKzxSfCb4QdK34VPBZgkkTSJIqeu5WBrrbBF/vu9cH1ijXhyHr9d5bhGVI0wRvBfEhhJKrlD+yT7haTgDwKhhRnFV8ErhLSGUNwq5agTQJ92ru+3wqDCGIocFNJ8KrQyx1HLtuGwNpij+bUJxbipngJjFcYZ5wVcwQo4gBtzbYuSG5EZI52KVi14rJNSxJ1Q7sJzT5Hyqc3uY5RxDWq0ksx0IEnaS485T83JCfg7vwqFVkbWBp8FbBKpIbkmtDdgXpVSCUZKXYtcfkMY/IucNjaWM7en8fep+++w29zwdCGxoS9Dz09/I0a9GzCfnjlPUjobgIUf0UgiwMdikhiNuE5WnyHkxeaMhIzBUpFLtwyLoIMS3OV+khre0fGvPaF9ty15rPByJge4hQeygmE4qnM64/mrB4UyjOPGI0bIRdCHYpVRpIcqPMnnumP8ixi6juKNj5GrlZwnIF+ZrOAo5dZv0hfWhGyjVxbP/bhN8PBGeBwwZ4yO2ylNXTlPlHhOWbHp264Ofxwf8TuIkyeU+ZvnBkz5fYFzdIXoAxqDXh83KFLpfBEnwKp+JtEtT6tJ8TceQS4yYWOAkrFmuDujzJKKaG4lzRSVg6dG0wK7PhKHOYvOeY/GCJeXENL94PcbtRVa7cBXkeEtbK9NZBdp4TLit3rPm0YfzEArcbZDGhcsJsij+bogJ2Lbh5sNDiBbsEuxBMDuIJcouVTfyLKqxzJC9qBQj9pvTGKe0kddw21PTEGCextA3SkXaWUGYjRc7O8LNgmTUrSK8FcYIUYIrAVUyuiCpqBJ8YbGKrqpXkOd7FXOfbhlP2LSPNPh6yrOx71i2Jb3zEMoQQDuU0SYJOUnxiMC4sNcYJZhUi9lFAQgQ/Ggxw2BAYJCKo94GTFPlxDs1D+tDXr3vmNHtDFF765pJ162SfK73PJ1S/TqI5Pp5n8hBOmV4q0+fK7F3P5H3FLuMSVN4inq/NNhwYXdaLY0zzt4mRuWW7h8Sz/HPgs41jbwG/pqqfJGxN8lZoi3yKUMb0T8Vr/mms4383GKpWmih/AHbtSWLs7ex5wdk7aybvu8BhINpZor1laPzrXdlCThXEdCIC30sseh+bS54y6MmEpQQjiCp26cmuPZP3Hen7OfZyjfgQs7J6KsFYNxV8aiAxSLOMadtz7mJpOPaed9UejpdZtjZqFJH6Ro3/vnZe50aN9dr9U87aTtjfin0zrKqlEuUPp9ilC/Xhco+9XiGrAjVnrJ8IyzcVnwTDXHZl0NRWqSO8jIi4fdjHaV4xR2Jba1vfaLN2/2lbIYFQrA3xskmCphYVQln1G4+si6AOO49aoZhB8dghhSW7lBCKkBisNYgtU0hiKMLQl3OoJnOKaP87xLExuKPeXBLAZCnm8QXmtSf4pxcUj6e48xRUkZslMo/l1GcTfBpfkt8IuWoIpN9lIh/68m+zLAyJsDvk/FviWGK5280lhwzSnpcmWYZcnAdCeTJj/SSlOLNhObqeo4sliODPJ7g0xrasDKaQSpXevOw4TLexrdS1u2O0nlLVvkXA9aD29WDvMiQiPw/8BeANEXkb+IfAzwG/ICKfB/4L8NfC8/QbIvILwDeBAvgp1SG7IByAtpjUZjCQGMhSdDrBXUwozixuIpginKN5DmLQWcb66YT8LNTpFw+Sg81j3EqfV/ll4y65xsB77yUWVf3Jjp/+Ysf5Pwv87KCnd2Gf9tHmXGxyGWvRSYKfWNQIxilmraGil7XIbMr62RnzDyWsngrFmYbQBAfptTJ5L8derULFhLwI4Qj72ravT8fIOm332XfOoRho5Bx33lAX9gx2GcWvkxSXhTosJg+BS6IafjufsXqWsnjTsH5NcdNILDlk157kxSIsV8vlZlOHU7S7r+13abNpPqeJAbLV+Mz9t0S5HQxZiptY3Mzg7cbIlmQWkyb4LAmlTafBvG/yILfYBZi1R3IHZYCTc4eVA7utP2ekGB+xDAkb7Jh9JaHIdIJmKX5iQ0jCRPAp2BUkiwz7Igk8tbTbraPneQXpjWIK3d1TqPnsoUvlITaQ8rp9TtNj/Ecn4FjjIpa+yLgh66qEgGzSDJ2muImpCiMXM8GuIb+0ZJMMtZvSpnYVCWmuJMvwDJ8lweu8r62n4Bx9+U9NGeWYl36iLIBxEcs+dBFMw7Qv1gS/jobcIPESsguFwGmezlAr2LUy/YHiMqmSyfJzMC4BnSK5C9vbOReyD4t70opOLcweiXERyzHhhU0YqYjKOMUUIVzSRCbhJsLyzQy78iRzz8XCs3xqWbxhWD8iWG5Tg5oUk8/I5mehHCrLGOzUsAQMDRvok2OOWTb6rqm36YSENi5iGYKe5agqA5al4Twf5A9TBC6iJiSM5WcCGLL3C+y8wE2mzA1RfRYg+ofOLOkkuAuqIKi2548loq1NVmr+3sQBxPTqEQt0xGoEeUVmM/R8hjtLI4cI3mbxgjhIlkq68Jh1cBrqo5RiIlF2EZJliOzPbpTkpkAWK3S1CruJtD1/CKHcNvb20OfdEV5NYmlBZVuZTXCPpxRnCX4Sdk0FEBfkl+zak12GF5+fJ6zPDcUsyDd2BckNZFdKdulIrlbozQJfyi1ttpYhhDLkvH3XH/K8Q2J8D8AHg1hK3401kFh8YkKVBBuIRTxYH0z4Zu2RtQ9q9Zlh9ZrBTYIdxi6DRpRde9KrHJmvYLXa3jxzyMx+iZ7gVjQJpqu9B3K88RPLgJcjpubsc2E/Q+PCIKiEgOxkFZYeUfBTy/pxwvKpYfFGuLddlUuUktw47M0aWef4el7QKcMeh+AUIQtDOd+Ado+fWGCYTcOEWSKFQ1YOKTZhkqZQkmuHzT0uNRQzy+qxZfVUWD0L8bcml0gwnmReRK6y7jaKHRM/ewhedmxLRTTdp7waxDIUXsE5JHfYlSNZhN08knkkAK+4ia2S4osZ+EyDqV+DXCMFMUShFmFnBPUG8LSqp7Df6nrXeAnPHD+xNNXBlhmnXhFC8hfOI85hljmZKmoNZlVgljmaWnwi5OeG4ixUfRIficQF2UYtwVM9y5BlWgV6t7ZnSJuPxaksxCfUnsZPLLCfYNSj3qDOIS44AM18jVlEdTfGpaid4TMhPw8uAJ8ASlCrS/nVhm1i7DRB0lCRcue1Dx38l6XiviRO9moQCwzQQHxV53YrIt8a9GyKv5ixfn3G4qll9RrkF4qmGio62Vj+KwWXCjaR4DuKhr2jMRZj3Ym41KtDLAOgRQGLRdiOrnQUTqfoaxes3pgx/1DK8s1QodKdhY2nUPCJxWXgJiGN1SeCxrBK1Ze0+8d9ENUra2e57YCpouVuqOu8UqeNGDQx5I9szAtS3EXcw7mQ8CeEenG2ZsRT4m5lR4RVHmMUaxOYT62q33KMx0Msp3T1qw/+PgkVtSV30SCnJAvBXQejnSmCypzcCOkc7EJJ5570KtpZVvntd//oe0H7COKQ9JEhzsN94Q97MB5iuQtEOSao0kqyADcn1sDVuONHSJRP5zEH+tqTXq4wl/NQ598dkCd0n9jHlQ6Ja+7AB5xYwtJkVjnJTUF2HXd8d6E8qbhosFtEYrlypO+vMZcL9CbuWHYKWeU+iO0OnvXBIJaul1FqM6s1yeWSmYFknpBdxeraUT5J557ssiC5XGEuF8jVTcgryvNN7O2hMStNjIUr3ULNHjexnMJppx5dLjHvJ6S5I8kSfGbD38TiMkN6lZM+nyMvLtHlCr9ahd3KSi3oGEK5bfDRkOsOjRG+pT1m3MQCwyX4jnPUa9i9w1gkLxBjMIlF0wSdZWhqMfM18uISf3kVasblt0z96HpRp+YuXfc8llBiKGoXxkMszRnRlkhW/73r+859fbC/zOewsjHs0kCSYK5sSJpfr9FFzA9qaj+3JNadezXP7dOUhtyvz0fVfG75+9AougbGQywl9i4rhwuLmgdusbORVG0XMu0KbiqfORaZow2HjsnQeJcGxkcsQ3HIAEUiUMfRs+oo3LXPpi1N5FAZ74C+v3rpq2XoQImSFd/mfkN+L59zaNJX814vm0OdUEMbF2fZFxNy6oE+1f1OGah0zL1OERIxAOMhlqEz/Jhrx4AhgnmbwHrs/e8A4yGWOu5KoDxF6uchOLQPJYe4q/63hYIe8JzxEcsh6/9tUh5OlZrR98y7fsaQNuy73wGT4dUTcA/FbQnlkMDsuhB834RSx4nko/FwlraU0BKHqIO3kX36rhlq/3lZOEYFbrvmAA49HmIpceisv6uXNDbuUMc9JbENqd3/cRH5tyLyLRH5hoj8nXj87ur3H4JydnTZMOrHD/GZ7FPj+9pxlziVwN38PKDtQ2SWAvh7qvrfAX8G+KlYo38c9fuH4tiXeMpZfFsDYhcO5cZHjsWQ2v3fU9XfiJ+vgG8RSqx/jrus3/+ycIg/5bac4z4suCVOQKQHaUMi8qPAnwb+A436/UC9fv93a5e11u8XkS+IyFdF5Ks5q80Ph8y+PhP8kN+H3Ps+X3AbDmnPids9mFhE5AL418DfVdXLvlNbju28LVX9oqp+RlU/kzJpueLAF3xXLP5lYIimtU9l78IJCWYQsYhISiCUf6mq/yYePm39/kM7dddC55g4Sl3F7XNmntLf1IIh2pAA/wz4lqr+k9pPp6/f32YAa7O/vKocpAunJOZDCeaAsRxiZ/mzwN8E/pOIfC0e+we8zPr9VT5QjyHqZQQonfIZRwQfnRyHxMEwrHb/v6NdDoG7rN8PtzPLD8VQArhvlbeJpvB9TPs+MOb+U5jtX2ZU3BDchpt0ySlNg+Mdeq3HQyz3iUMGd8xugDt+xjiJpavDh7LcISb7Uw7uEcHkg1Hv+z1xx/GEKByi5o40kqzCoRrbIefe4zI6HmL5IOIDpuKLjqBDIvJ94AZ4977bcgDe4IPZ3h9R1TfbfhgFsQCIyFdV9TP33Y6h+GFs78My9IDBeCCWBwzGmIjli/fdgAPxQ9fe0cgsDxg/xsRZHjBy3DuxiMhnY2D3d0TkrftuD4CIfElE3hGRr9eOjSNAvb29LyeoXlXv7Q+wwO8AfwLIgP8IfOo+2xTb9eeBTwNfrx37x8Bb8fNbwD+Knz8V2z0BPhH7Y19yez8KfDp+fgT8VmzXSdt835zlx4DvqOrvquoa+DIh4Pteoaq/DjxvHB5tgLq+pKD6+yaWQcHdI8GtAtRfFk4ZVN/EfRPLoODukWM0fTh1UH0T900sxwV33w9OG6B+YryMoPr7JpavAJ8UkU+ISEbIZPzFe25TF04foH4ivLSg+hFoHj9BkN5/B/iZ+25PbNPPA98DcsIs/DzwOiFN97fj/89q5/9MbP+3gb9yD+39c4Rl5DeBr8W/nzh1mx8suA8YjPtehh7wCuGBWB4wGA/E8oDBeCCWBwzGA7E8YDAeiOUBg/FALA8YjAdiecBg/P+1eK0iMv0kQgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/695, loss_train: 7.5411, loss_val: nan, pos_over_neg: 1.0592623949050903 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 7.1111, loss_val: nan, pos_over_neg: 3.236527442932129 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 6.7996, loss_val: nan, pos_over_neg: 4.7997331619262695 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 6.6477, loss_val: nan, pos_over_neg: 27.631826400756836 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 6.5866, loss_val: nan, pos_over_neg: 21.695289611816406 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 6.5191, loss_val: nan, pos_over_neg: 27.44672203063965 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 6.4748, loss_val: nan, pos_over_neg: 24.605899810791016 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 6.4226, loss_val: nan, pos_over_neg: 25.321962356567383 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 6.3898, loss_val: nan, pos_over_neg: 38.1678352355957 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 6.367, loss_val: nan, pos_over_neg: 79.34306335449219 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 6.3407, loss_val: nan, pos_over_neg: 123.05043029785156 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 6.3282, loss_val: nan, pos_over_neg: 115.31581115722656 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 6.3047, loss_val: nan, pos_over_neg: 143.41839599609375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 6.275, loss_val: nan, pos_over_neg: 149.4844207763672 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 6.2552, loss_val: nan, pos_over_neg: 151.4322509765625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 6.2373, loss_val: nan, pos_over_neg: 192.05929565429688 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 6.2175, loss_val: nan, pos_over_neg: 232.6253204345703 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 6.2126, loss_val: nan, pos_over_neg: 248.4789276123047 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 6.1946, loss_val: nan, pos_over_neg: 277.1551818847656 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 6.1811, loss_val: nan, pos_over_neg: 311.49444580078125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 6.1752, loss_val: nan, pos_over_neg: 426.5718994140625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 6.1536, loss_val: nan, pos_over_neg: 333.12835693359375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 6.1633, loss_val: nan, pos_over_neg: 218.2388916015625 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 6.1337, loss_val: nan, pos_over_neg: 428.0062561035156 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 6.1324, loss_val: nan, pos_over_neg: 547.6406860351562 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 6.1194, loss_val: nan, pos_over_neg: 229.388916015625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 6.1222, loss_val: nan, pos_over_neg: 350.32806396484375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 6.1184, loss_val: nan, pos_over_neg: 544.017822265625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 6.1055, loss_val: nan, pos_over_neg: 474.1595458984375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 6.1104, loss_val: nan, pos_over_neg: 301.6853942871094 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 6.0887, loss_val: nan, pos_over_neg: 617.2664794921875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 6.0841, loss_val: nan, pos_over_neg: 328.838623046875 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 6.0869, loss_val: nan, pos_over_neg: 407.0340576171875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 6.0728, loss_val: nan, pos_over_neg: 391.86224365234375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 6.0741, loss_val: nan, pos_over_neg: 377.46392822265625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 6.0556, loss_val: nan, pos_over_neg: 1403.2742919921875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 6.07, loss_val: nan, pos_over_neg: 496.73272705078125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 6.0613, loss_val: nan, pos_over_neg: 252.94784545898438 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 6.0545, loss_val: nan, pos_over_neg: 394.7818603515625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 6.0477, loss_val: nan, pos_over_neg: 463.29058837890625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 6.054, loss_val: nan, pos_over_neg: 596.0033569335938 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 6.0405, loss_val: nan, pos_over_neg: 849.3046264648438 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 6.0392, loss_val: nan, pos_over_neg: 447.1424255371094 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 6.0335, loss_val: nan, pos_over_neg: 959.4078979492188 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 6.0415, loss_val: nan, pos_over_neg: 454.496826171875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 6.0281, loss_val: nan, pos_over_neg: 654.4046630859375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 6.0266, loss_val: nan, pos_over_neg: 648.5693969726562 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 6.0269, loss_val: nan, pos_over_neg: 657.3994140625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 6.0228, loss_val: nan, pos_over_neg: 580.0164794921875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 6.0119, loss_val: nan, pos_over_neg: 1128.383544921875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 6.016, loss_val: nan, pos_over_neg: 373.59765625 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 6.0241, loss_val: nan, pos_over_neg: 559.369384765625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 6.0131, loss_val: nan, pos_over_neg: 682.6962890625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 6.0186, loss_val: nan, pos_over_neg: 915.0721435546875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 6.0056, loss_val: nan, pos_over_neg: 270.90631103515625 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.9994, loss_val: nan, pos_over_neg: 381.2758483886719 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 6.0081, loss_val: nan, pos_over_neg: 264.4792175292969 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 6.0012, loss_val: nan, pos_over_neg: 496.9595642089844 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 6.0, loss_val: nan, pos_over_neg: 188.16627502441406 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.9987, loss_val: nan, pos_over_neg: 274.4844055175781 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.9978, loss_val: nan, pos_over_neg: 132.4634246826172 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.9818, loss_val: nan, pos_over_neg: 377.1601257324219 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.9843, loss_val: nan, pos_over_neg: 285.33544921875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.9943, loss_val: nan, pos_over_neg: 117.46995544433594 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 6.0019, loss_val: nan, pos_over_neg: 197.0648651123047 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.9961, loss_val: nan, pos_over_neg: 187.56951904296875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.977, loss_val: nan, pos_over_neg: 441.5752868652344 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.98, loss_val: nan, pos_over_neg: 232.3636474609375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.9742, loss_val: nan, pos_over_neg: 285.3995361328125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.9804, loss_val: nan, pos_over_neg: 282.4725036621094 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.9806, loss_val: nan, pos_over_neg: 214.15911865234375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.9726, loss_val: nan, pos_over_neg: 273.689697265625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.9764, loss_val: nan, pos_over_neg: 455.23773193359375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.973, loss_val: nan, pos_over_neg: 164.8780975341797 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.9749, loss_val: nan, pos_over_neg: 327.25384521484375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.9595, loss_val: nan, pos_over_neg: 291.9214782714844 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.9716, loss_val: nan, pos_over_neg: 186.5828094482422 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.9667, loss_val: nan, pos_over_neg: 300.794189453125 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.9614, loss_val: nan, pos_over_neg: 358.3593444824219 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.9598, loss_val: nan, pos_over_neg: 243.73641967773438 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.9477, loss_val: nan, pos_over_neg: 490.9085693359375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.9611, loss_val: nan, pos_over_neg: 274.26806640625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.9568, loss_val: nan, pos_over_neg: 282.9115295410156 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.9606, loss_val: nan, pos_over_neg: 236.66439819335938 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.946, loss_val: nan, pos_over_neg: 1601.255859375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.9521, loss_val: nan, pos_over_neg: 264.9794006347656 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.9497, loss_val: nan, pos_over_neg: 297.61474609375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.9527, loss_val: nan, pos_over_neg: 307.84088134765625 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.9489, loss_val: nan, pos_over_neg: 317.65216064453125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.9382, loss_val: nan, pos_over_neg: 588.5206909179688 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.9394, loss_val: nan, pos_over_neg: 297.5496520996094 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.9402, loss_val: nan, pos_over_neg: 406.12091064453125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.9532, loss_val: nan, pos_over_neg: 193.471435546875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.9401, loss_val: nan, pos_over_neg: 280.0110168457031 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.9352, loss_val: nan, pos_over_neg: 220.61473083496094 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 260.3316345214844 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.9494, loss_val: nan, pos_over_neg: 306.73065185546875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.9473, loss_val: nan, pos_over_neg: 232.29537963867188 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.9396, loss_val: nan, pos_over_neg: 455.9507751464844 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 388.6786804199219 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.9297, loss_val: nan, pos_over_neg: 385.90814208984375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.9433, loss_val: nan, pos_over_neg: 602.34423828125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.934, loss_val: nan, pos_over_neg: 384.77655029296875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.942, loss_val: nan, pos_over_neg: 503.0411682128906 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.9305, loss_val: nan, pos_over_neg: 551.2427368164062 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.9366, loss_val: nan, pos_over_neg: 398.56683349609375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.9284, loss_val: nan, pos_over_neg: 753.1249389648438 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.9334, loss_val: nan, pos_over_neg: 1121.3704833984375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.928, loss_val: nan, pos_over_neg: 810.7379760742188 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.9218, loss_val: nan, pos_over_neg: 923.5484619140625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.9257, loss_val: nan, pos_over_neg: 386.66351318359375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.9203, loss_val: nan, pos_over_neg: 826.7962036132812 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 348.5617980957031 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.9246, loss_val: nan, pos_over_neg: 457.6337585449219 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.9153, loss_val: nan, pos_over_neg: 696.3280639648438 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.9358, loss_val: nan, pos_over_neg: 143.2862091064453 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.9232, loss_val: nan, pos_over_neg: 537.2359619140625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.9212, loss_val: nan, pos_over_neg: 379.6175231933594 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.9204, loss_val: nan, pos_over_neg: 392.92108154296875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.9331, loss_val: nan, pos_over_neg: 216.22085571289062 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.9208, loss_val: nan, pos_over_neg: 312.5001220703125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.9292, loss_val: nan, pos_over_neg: 202.8278045654297 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.9187, loss_val: nan, pos_over_neg: 388.5715026855469 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.9179, loss_val: nan, pos_over_neg: 297.5546569824219 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.9154, loss_val: nan, pos_over_neg: 206.49893188476562 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.9202, loss_val: nan, pos_over_neg: 158.20448303222656 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.9176, loss_val: nan, pos_over_neg: 326.2987060546875 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.9196, loss_val: nan, pos_over_neg: 179.46607971191406 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 712.624267578125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.9205, loss_val: nan, pos_over_neg: 210.852783203125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.9137, loss_val: nan, pos_over_neg: 516.0079956054688 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.9131, loss_val: nan, pos_over_neg: 436.426513671875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.9199, loss_val: nan, pos_over_neg: 977.9222412109375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 384.3013610839844 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.9102, loss_val: nan, pos_over_neg: 368.4949645996094 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.9289, loss_val: nan, pos_over_neg: 305.60894775390625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 237.3574981689453 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.9151, loss_val: nan, pos_over_neg: 370.9686279296875 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.9178, loss_val: nan, pos_over_neg: 207.4466094970703 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.9159, loss_val: nan, pos_over_neg: 256.1927490234375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.9152, loss_val: nan, pos_over_neg: 306.4562683105469 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 226.64071655273438 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.9128, loss_val: nan, pos_over_neg: 216.37283325195312 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.915, loss_val: nan, pos_over_neg: 539.3475952148438 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.9075, loss_val: nan, pos_over_neg: 251.2081298828125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.9043, loss_val: nan, pos_over_neg: 343.9605407714844 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.922, loss_val: nan, pos_over_neg: 201.6256561279297 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.9147, loss_val: nan, pos_over_neg: 244.97772216796875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.9065, loss_val: nan, pos_over_neg: 258.48040771484375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.9121, loss_val: nan, pos_over_neg: 501.0278625488281 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.9145, loss_val: nan, pos_over_neg: 286.36981201171875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.9112, loss_val: nan, pos_over_neg: 355.0216369628906 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.9039, loss_val: nan, pos_over_neg: 198.9960174560547 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.907, loss_val: nan, pos_over_neg: 236.0748748779297 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.9079, loss_val: nan, pos_over_neg: 191.46946716308594 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.909, loss_val: nan, pos_over_neg: 225.55987548828125 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 458.0835876464844 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.9085, loss_val: nan, pos_over_neg: 151.4971466064453 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.9018, loss_val: nan, pos_over_neg: 220.41973876953125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.9141, loss_val: nan, pos_over_neg: 318.3958435058594 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.9107, loss_val: nan, pos_over_neg: 149.64987182617188 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 392.7933654785156 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.9139, loss_val: nan, pos_over_neg: 293.2562255859375 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.9071, loss_val: nan, pos_over_neg: 225.13719177246094 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.9009, loss_val: nan, pos_over_neg: 305.9122619628906 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.9002, loss_val: nan, pos_over_neg: 230.43447875976562 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.9088, loss_val: nan, pos_over_neg: 283.45477294921875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8981, loss_val: nan, pos_over_neg: 309.3838195800781 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.9013, loss_val: nan, pos_over_neg: 202.70986938476562 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.8941, loss_val: nan, pos_over_neg: 480.5371398925781 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.9004, loss_val: nan, pos_over_neg: 277.9996337890625 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 218.21177673339844 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8959, loss_val: nan, pos_over_neg: 359.65863037109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8999, loss_val: nan, pos_over_neg: 254.7541961669922 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8942, loss_val: nan, pos_over_neg: 332.85662841796875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.9072, loss_val: nan, pos_over_neg: 156.02476501464844 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.9051, loss_val: nan, pos_over_neg: 319.7023010253906 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 363.14794921875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.9045, loss_val: nan, pos_over_neg: 312.6104431152344 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.898, loss_val: nan, pos_over_neg: 443.8044738769531 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.9012, loss_val: nan, pos_over_neg: 325.35662841796875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8853, loss_val: nan, pos_over_neg: 379.8262939453125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 421.6338195800781 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8872, loss_val: nan, pos_over_neg: 469.672607421875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 518.434326171875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.891, loss_val: nan, pos_over_neg: 557.9445190429688 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8899, loss_val: nan, pos_over_neg: 434.3085632324219 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8917, loss_val: nan, pos_over_neg: 458.1703186035156 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8873, loss_val: nan, pos_over_neg: 508.2038879394531 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8912, loss_val: nan, pos_over_neg: 458.6784973144531 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8896, loss_val: nan, pos_over_neg: 292.3089599609375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8835, loss_val: nan, pos_over_neg: 635.7197875976562 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8878, loss_val: nan, pos_over_neg: 414.8761901855469 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8908, loss_val: nan, pos_over_neg: 319.72088623046875 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.8922, loss_val: nan, pos_over_neg: 470.2619934082031 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8901, loss_val: nan, pos_over_neg: 343.1057434082031 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8985, loss_val: nan, pos_over_neg: 411.7867126464844 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8825, loss_val: nan, pos_over_neg: 330.2955322265625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8807, loss_val: nan, pos_over_neg: 428.52435302734375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.8891, loss_val: nan, pos_over_neg: 347.2069396972656 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.881, loss_val: nan, pos_over_neg: 472.94403076171875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8958, loss_val: nan, pos_over_neg: 905.311279296875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8892, loss_val: nan, pos_over_neg: 494.8865051269531 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8934, loss_val: nan, pos_over_neg: 498.85418701171875 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8833, loss_val: nan, pos_over_neg: 390.2584533691406 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.895, loss_val: nan, pos_over_neg: 239.40716552734375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.8839, loss_val: nan, pos_over_neg: 705.3309326171875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8932, loss_val: nan, pos_over_neg: 363.6382751464844 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8802, loss_val: nan, pos_over_neg: 436.1103820800781 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 287.1240539550781 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.8924, loss_val: nan, pos_over_neg: 419.9329833984375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.879, loss_val: nan, pos_over_neg: 306.43426513671875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8847, loss_val: nan, pos_over_neg: 349.94891357421875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8815, loss_val: nan, pos_over_neg: 401.9412841796875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8898, loss_val: nan, pos_over_neg: 225.37738037109375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8915, loss_val: nan, pos_over_neg: 561.5005493164062 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8843, loss_val: nan, pos_over_neg: 573.62939453125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8783, loss_val: nan, pos_over_neg: 289.2052001953125 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.883, loss_val: nan, pos_over_neg: 275.31817626953125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.885, loss_val: nan, pos_over_neg: 415.0041809082031 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8776, loss_val: nan, pos_over_neg: 443.7305603027344 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 382.38909912109375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8841, loss_val: nan, pos_over_neg: 593.7662353515625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8826, loss_val: nan, pos_over_neg: 331.3123779296875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8813, loss_val: nan, pos_over_neg: 385.1626281738281 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8848, loss_val: nan, pos_over_neg: 421.878662109375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 266.75537109375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8887, loss_val: nan, pos_over_neg: 315.4652099609375 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8907, loss_val: nan, pos_over_neg: 281.0825500488281 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8775, loss_val: nan, pos_over_neg: 365.30206298828125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.8738, loss_val: nan, pos_over_neg: 486.8310241699219 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8849, loss_val: nan, pos_over_neg: 241.57492065429688 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 466.8612060546875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8743, loss_val: nan, pos_over_neg: 395.83544921875 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8837, loss_val: nan, pos_over_neg: 449.4658508300781 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 571.2069702148438 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8732, loss_val: nan, pos_over_neg: 422.54339599609375 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 514.652587890625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.8772, loss_val: nan, pos_over_neg: 494.34979248046875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8747, loss_val: nan, pos_over_neg: 814.4564819335938 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8801, loss_val: nan, pos_over_neg: 506.8334655761719 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 668.687255859375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8725, loss_val: nan, pos_over_neg: 502.43804931640625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8824, loss_val: nan, pos_over_neg: 342.2362365722656 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8659, loss_val: nan, pos_over_neg: 473.1329650878906 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 358.3614196777344 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8724, loss_val: nan, pos_over_neg: 373.12640380859375 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8737, loss_val: nan, pos_over_neg: 460.9366760253906 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8751, loss_val: nan, pos_over_neg: 532.1607055664062 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8754, loss_val: nan, pos_over_neg: 222.36178588867188 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8703, loss_val: nan, pos_over_neg: 989.005615234375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.8717, loss_val: nan, pos_over_neg: 232.90170288085938 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8673, loss_val: nan, pos_over_neg: 485.6452331542969 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8678, loss_val: nan, pos_over_neg: 632.5682373046875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 356.87982177734375 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 766.8944091796875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.8706, loss_val: nan, pos_over_neg: 1001.2249145507812 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8712, loss_val: nan, pos_over_neg: 333.91131591796875 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.8726, loss_val: nan, pos_over_neg: 518.7106323242188 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 291.0449523925781 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8755, loss_val: nan, pos_over_neg: 321.33868408203125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 537.3133544921875 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8683, loss_val: nan, pos_over_neg: 622.097412109375 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8697, loss_val: nan, pos_over_neg: 468.6255187988281 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 492.1857604980469 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.8613, loss_val: nan, pos_over_neg: 583.0111083984375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8676, loss_val: nan, pos_over_neg: 410.21734619140625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8719, loss_val: nan, pos_over_neg: 514.7424926757812 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 767.5147705078125 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.8624, loss_val: nan, pos_over_neg: 452.93170166015625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8668, loss_val: nan, pos_over_neg: 764.035400390625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 929.900146484375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8759, loss_val: nan, pos_over_neg: 356.7826843261719 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8773, loss_val: nan, pos_over_neg: 621.1871948242188 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8646, loss_val: nan, pos_over_neg: 455.56683349609375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8634, loss_val: nan, pos_over_neg: 477.9310607910156 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.8792, loss_val: nan, pos_over_neg: 297.9556884765625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8756, loss_val: nan, pos_over_neg: 410.4861145019531 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 567.8734741210938 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.8723, loss_val: nan, pos_over_neg: 538.5025634765625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8656, loss_val: nan, pos_over_neg: 294.5372619628906 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8787, loss_val: nan, pos_over_neg: 288.2881774902344 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 587.89111328125 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8685, loss_val: nan, pos_over_neg: 631.7052612304688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8741, loss_val: nan, pos_over_neg: 264.55657958984375 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8653, loss_val: nan, pos_over_neg: 562.7904052734375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 469.24395751953125 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.865, loss_val: nan, pos_over_neg: 391.81085205078125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.864, loss_val: nan, pos_over_neg: 1379.3255615234375 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.8715, loss_val: nan, pos_over_neg: 304.302978515625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 600.3980102539062 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8549, loss_val: nan, pos_over_neg: 2369.85400390625 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8651, loss_val: nan, pos_over_neg: 394.75787353515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8594, loss_val: nan, pos_over_neg: 315.8934631347656 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 647.4309692382812 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8693, loss_val: nan, pos_over_neg: 502.6444396972656 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8672, loss_val: nan, pos_over_neg: 840.049560546875 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1033.3291015625 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.8626, loss_val: nan, pos_over_neg: 340.087646484375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8586, loss_val: nan, pos_over_neg: 415.79754638671875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8641, loss_val: nan, pos_over_neg: 443.61846923828125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.867, loss_val: nan, pos_over_neg: 371.7149658203125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8606, loss_val: nan, pos_over_neg: 434.893310546875 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.8555, loss_val: nan, pos_over_neg: 652.0569458007812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 569.2185668945312 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8652, loss_val: nan, pos_over_neg: 320.2918395996094 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8599, loss_val: nan, pos_over_neg: 301.64312744140625 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8568, loss_val: nan, pos_over_neg: 433.0592956542969 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 326.4866638183594 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8593, loss_val: nan, pos_over_neg: 379.67926025390625 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.8589, loss_val: nan, pos_over_neg: 467.2384033203125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 512.5743408203125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8574, loss_val: nan, pos_over_neg: 383.418212890625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 381.0321350097656 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 593.867431640625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8565, loss_val: nan, pos_over_neg: 697.5585327148438 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8601, loss_val: nan, pos_over_neg: 588.2948608398438 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 405.96527099609375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8546, loss_val: nan, pos_over_neg: 534.5236206054688 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8696, loss_val: nan, pos_over_neg: 161.1255340576172 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 337.5500183105469 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 312.1282043457031 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8618, loss_val: nan, pos_over_neg: 391.6070556640625 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8558, loss_val: nan, pos_over_neg: 402.0593566894531 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.863, loss_val: nan, pos_over_neg: 348.2807312011719 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8478, loss_val: nan, pos_over_neg: 824.428955078125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8598, loss_val: nan, pos_over_neg: 373.6597595214844 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8655, loss_val: nan, pos_over_neg: 218.78521728515625 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.8569, loss_val: nan, pos_over_neg: 511.8371276855469 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.8623, loss_val: nan, pos_over_neg: 327.03216552734375 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.8636, loss_val: nan, pos_over_neg: 335.37646484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8553, loss_val: nan, pos_over_neg: 553.4093627929688 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8581, loss_val: nan, pos_over_neg: 453.43231201171875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 434.94488525390625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.8456, loss_val: nan, pos_over_neg: 513.2366333007812 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.859, loss_val: nan, pos_over_neg: 637.9635620117188 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.8513, loss_val: nan, pos_over_neg: 599.1920776367188 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8617, loss_val: nan, pos_over_neg: 506.46435546875 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1008.5286254882812 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.861, loss_val: nan, pos_over_neg: 385.7738952636719 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 317.024169921875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8611, loss_val: nan, pos_over_neg: 753.14990234375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 624.4983520507812 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8544, loss_val: nan, pos_over_neg: 524.7177734375 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 562.3231811523438 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.8537, loss_val: nan, pos_over_neg: 989.3966674804688 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8499, loss_val: nan, pos_over_neg: 384.5263977050781 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8498, loss_val: nan, pos_over_neg: 722.433837890625 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8489, loss_val: nan, pos_over_neg: 579.1134643554688 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 929.143310546875 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.8461, loss_val: nan, pos_over_neg: 805.1903076171875 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 576.2215576171875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 675.2417602539062 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.8604, loss_val: nan, pos_over_neg: 485.8847351074219 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8543, loss_val: nan, pos_over_neg: 545.9652709960938 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8535, loss_val: nan, pos_over_neg: 797.72314453125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 1132.9017333984375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 488.50628662109375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 538.30712890625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8518, loss_val: nan, pos_over_neg: 938.3367919921875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 733.3816528320312 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 852.255859375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.85, loss_val: nan, pos_over_neg: 719.9910888671875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8564, loss_val: nan, pos_over_neg: 557.5885620117188 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.8501, loss_val: nan, pos_over_neg: 916.4676513671875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8578, loss_val: nan, pos_over_neg: 486.8207702636719 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.8572, loss_val: nan, pos_over_neg: 1093.91845703125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8477, loss_val: nan, pos_over_neg: 526.3231201171875 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1104.137451171875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 625.903076171875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8562, loss_val: nan, pos_over_neg: 713.1082153320312 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.849, loss_val: nan, pos_over_neg: 1022.2177734375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 765.5811767578125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1051.768798828125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1487.0621337890625 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8591, loss_val: nan, pos_over_neg: 307.50970458984375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 676.9764404296875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.8519, loss_val: nan, pos_over_neg: 830.6240844726562 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 645.2799682617188 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8495, loss_val: nan, pos_over_neg: 2537.415283203125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 576.5502319335938 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.8622, loss_val: nan, pos_over_neg: 334.3704833984375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.8484, loss_val: nan, pos_over_neg: 642.4667358398438 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 737.310302734375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 239.9048309326172 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.8514, loss_val: nan, pos_over_neg: 360.8359375 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8487, loss_val: nan, pos_over_neg: 502.2469787597656 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.8474, loss_val: nan, pos_over_neg: 344.71197509765625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 413.959716796875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.853, loss_val: nan, pos_over_neg: 420.13330078125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8509, loss_val: nan, pos_over_neg: 330.8096618652344 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8529, loss_val: nan, pos_over_neg: 356.646728515625 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8486, loss_val: nan, pos_over_neg: 382.5062255859375 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8462, loss_val: nan, pos_over_neg: 374.5344543457031 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 462.89752197265625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 677.7240600585938 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.8439, loss_val: nan, pos_over_neg: 396.5465393066406 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.8415, loss_val: nan, pos_over_neg: 464.0140380859375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.8395, loss_val: nan, pos_over_neg: 1239.0283203125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.8435, loss_val: nan, pos_over_neg: 462.6827697753906 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 457.5995178222656 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 474.6021728515625 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.851, loss_val: nan, pos_over_neg: 441.3175048828125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8431, loss_val: nan, pos_over_neg: 340.4575500488281 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.8534, loss_val: nan, pos_over_neg: 914.5401000976562 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 799.7611694335938 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.8432, loss_val: nan, pos_over_neg: 319.72210693359375 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8454, loss_val: nan, pos_over_neg: 388.0775146484375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.845, loss_val: nan, pos_over_neg: 896.423583984375 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 466.6935119628906 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8492, loss_val: nan, pos_over_neg: 488.80023193359375 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8482, loss_val: nan, pos_over_neg: 534.0309448242188 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8521, loss_val: nan, pos_over_neg: 967.771728515625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8449, loss_val: nan, pos_over_neg: 1592.5372314453125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 588.8681030273438 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 634.07373046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8468, loss_val: nan, pos_over_neg: 471.0279846191406 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 628.9730834960938 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.8485, loss_val: nan, pos_over_neg: 287.7059020996094 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 733.87939453125 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 520.7992553710938 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8418, loss_val: nan, pos_over_neg: 408.29638671875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.8407, loss_val: nan, pos_over_neg: 928.2449951171875 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.8476, loss_val: nan, pos_over_neg: 581.7451171875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.8394, loss_val: nan, pos_over_neg: 408.7223815917969 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 1324.7889404296875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.8523, loss_val: nan, pos_over_neg: 648.5648193359375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.8349, loss_val: nan, pos_over_neg: 454.48516845703125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.838, loss_val: nan, pos_over_neg: 408.1045837402344 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8527, loss_val: nan, pos_over_neg: 532.1016235351562 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 600.2213745117188 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8496, loss_val: nan, pos_over_neg: 449.72222900390625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 568.0477294921875 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.8402, loss_val: nan, pos_over_neg: 303.34844970703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 552.1015625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8466, loss_val: nan, pos_over_neg: 856.4384155273438 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.8424, loss_val: nan, pos_over_neg: 781.7352294921875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1642.169189453125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 519.1943969726562 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 414.6566162109375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.8414, loss_val: nan, pos_over_neg: 562.1210327148438 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1248.7664794921875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 428.65142822265625 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.8422, loss_val: nan, pos_over_neg: 396.2169189453125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 614.4996337890625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.8448, loss_val: nan, pos_over_neg: 374.6924743652344 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.857, loss_val: nan, pos_over_neg: 301.31005859375 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 604.4854736328125 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 694.83544921875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 490.6768493652344 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.8434, loss_val: nan, pos_over_neg: 446.1412048339844 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1467.4649658203125 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.8419, loss_val: nan, pos_over_neg: 558.8969116210938 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8408, loss_val: nan, pos_over_neg: 649.419189453125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.8437, loss_val: nan, pos_over_neg: 456.5855407714844 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 400.8526306152344 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.8391, loss_val: nan, pos_over_neg: 604.0333862304688 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 877.4136352539062 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8505, loss_val: nan, pos_over_neg: 275.3774108886719 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 308.7056884765625 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 657.24853515625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 495.155517578125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8472, loss_val: nan, pos_over_neg: 579.3722534179688 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.8428, loss_val: nan, pos_over_neg: 621.1004638671875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1076.231689453125 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1531.187744140625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 505.3993835449219 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.8252, loss_val: nan, pos_over_neg: 1016.1669921875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 1906.1016845703125 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8433, loss_val: nan, pos_over_neg: 480.2744445800781 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 351.4363708496094 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 625.3176879882812 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.8453, loss_val: nan, pos_over_neg: 341.2700500488281 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 550.7190551757812 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.8308, loss_val: nan, pos_over_neg: 343.8113708496094 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.8301, loss_val: nan, pos_over_neg: 860.2276611328125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 266.06280517578125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 742.4344482421875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.8425, loss_val: nan, pos_over_neg: 367.4197692871094 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 406.0503234863281 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 905.5067749023438 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 502.0009460449219 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 386.6715087890625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.8481, loss_val: nan, pos_over_neg: 209.82801818847656 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.8335, loss_val: nan, pos_over_neg: 681.6788330078125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8377, loss_val: nan, pos_over_neg: 366.9575500488281 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.8413, loss_val: nan, pos_over_neg: 477.9522705078125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.8441, loss_val: nan, pos_over_neg: 991.9985961914062 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8284, loss_val: nan, pos_over_neg: 634.1044921875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 502.2566223144531 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 778.9251098632812 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.8306, loss_val: nan, pos_over_neg: 606.4273681640625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.8376, loss_val: nan, pos_over_neg: 406.390869140625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.8447, loss_val: nan, pos_over_neg: 324.155029296875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.8358, loss_val: nan, pos_over_neg: 452.3140869140625 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.8405, loss_val: nan, pos_over_neg: 578.397705078125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.84, loss_val: nan, pos_over_neg: 335.6120910644531 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.8429, loss_val: nan, pos_over_neg: 415.4976501464844 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 533.9605712890625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 220.33486938476562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.8362, loss_val: nan, pos_over_neg: 522.7935180664062 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8382, loss_val: nan, pos_over_neg: 430.2674255371094 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 266.4413757324219 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 1003.791259765625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.8381, loss_val: nan, pos_over_neg: 614.7468872070312 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8452, loss_val: nan, pos_over_neg: 248.79005432128906 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8444, loss_val: nan, pos_over_neg: 335.9574279785156 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.839, loss_val: nan, pos_over_neg: 589.9540405273438 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 555.3246459960938 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8385, loss_val: nan, pos_over_neg: 225.8197479248047 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 459.1532287597656 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8368, loss_val: nan, pos_over_neg: 473.6697082519531 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 453.5172424316406 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 304.99664306640625 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.8333, loss_val: nan, pos_over_neg: 649.6158447265625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.8387, loss_val: nan, pos_over_neg: 580.8357543945312 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.8359, loss_val: nan, pos_over_neg: 619.82080078125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.8361, loss_val: nan, pos_over_neg: 513.2044677734375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.8305, loss_val: nan, pos_over_neg: 494.3911437988281 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8248, loss_val: nan, pos_over_neg: 796.4515380859375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8307, loss_val: nan, pos_over_neg: 344.7320861816406 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 693.7025146484375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 908.9283447265625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 529.6823120117188 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 343.26275634765625 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 271.9039306640625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.8409, loss_val: nan, pos_over_neg: 432.78900146484375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 623.993896484375 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 356.4383239746094 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 481.62432861328125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.8396, loss_val: nan, pos_over_neg: 377.511474609375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 413.170166015625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.8364, loss_val: nan, pos_over_neg: 555.2539672851562 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 640.47119140625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.8356, loss_val: nan, pos_over_neg: 480.7867431640625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.8363, loss_val: nan, pos_over_neg: 398.307861328125 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 638.1804809570312 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 436.0295715332031 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 486.3904724121094 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 403.04718017578125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 569.5184936523438 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.8311, loss_val: nan, pos_over_neg: 541.5629272460938 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 590.60546875 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.8304, loss_val: nan, pos_over_neg: 483.9391174316406 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.8348, loss_val: nan, pos_over_neg: 352.1444091796875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 833.4563598632812 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.8351, loss_val: nan, pos_over_neg: 675.1600341796875 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8374, loss_val: nan, pos_over_neg: 389.90130615234375 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8354, loss_val: nan, pos_over_neg: 365.2412109375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1064.157470703125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.8314, loss_val: nan, pos_over_neg: 470.7794189453125 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 392.1070251464844 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.8318, loss_val: nan, pos_over_neg: 334.0672302246094 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8343, loss_val: nan, pos_over_neg: 288.62518310546875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.8332, loss_val: nan, pos_over_neg: 338.1103820800781 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.8378, loss_val: nan, pos_over_neg: 464.911376953125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 1308.352783203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8298, loss_val: nan, pos_over_neg: 611.2966918945312 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.8313, loss_val: nan, pos_over_neg: 418.8901672363281 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 748.3226928710938 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8312, loss_val: nan, pos_over_neg: 548.1212768554688 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 380.4026184082031 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.8279, loss_val: nan, pos_over_neg: 771.3038940429688 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8316, loss_val: nan, pos_over_neg: 856.170654296875 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 445.44561767578125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 449.34759521484375 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 595.2987670898438 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.8289, loss_val: nan, pos_over_neg: 786.5506591796875 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.8233, loss_val: nan, pos_over_neg: 409.54815673828125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 604.8433227539062 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 684.7357788085938 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.8342, loss_val: nan, pos_over_neg: 494.9058532714844 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.8258, loss_val: nan, pos_over_neg: 466.783935546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 857.8911743164062 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 643.4329223632812 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.8303, loss_val: nan, pos_over_neg: 406.2275085449219 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.8288, loss_val: nan, pos_over_neg: 632.1973266601562 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.8355, loss_val: nan, pos_over_neg: 904.4273681640625 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1060.1351318359375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.8243, loss_val: nan, pos_over_neg: 503.7689514160156 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.8344, loss_val: nan, pos_over_neg: 558.1215209960938 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.8322, loss_val: nan, pos_over_neg: 545.02392578125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.8347, loss_val: nan, pos_over_neg: 571.070068359375 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 403.8465881347656 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.8375, loss_val: nan, pos_over_neg: 313.7022705078125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 1197.669189453125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.8227, loss_val: nan, pos_over_neg: 880.1221923828125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.818, loss_val: nan, pos_over_neg: 1784.7100830078125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.8328, loss_val: nan, pos_over_neg: 375.5597839355469 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 455.51995849609375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.8276, loss_val: nan, pos_over_neg: 399.49359130859375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 399.482421875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.8365, loss_val: nan, pos_over_neg: 426.6829528808594 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.8353, loss_val: nan, pos_over_neg: 940.7045288085938 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 781.1063842773438 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.8329, loss_val: nan, pos_over_neg: 413.47967529296875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 258.06097412109375 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.8299, loss_val: nan, pos_over_neg: 675.0615844726562 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.8223, loss_val: nan, pos_over_neg: 939.785888671875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.8327, loss_val: nan, pos_over_neg: 304.0751647949219 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 329.0513916015625 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 352.1204833984375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 1131.4246826171875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 406.0364074707031 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 402.13165283203125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 456.70458984375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.8325, loss_val: nan, pos_over_neg: 559.2935180664062 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 667.9758911132812 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.8331, loss_val: nan, pos_over_neg: 398.1170959472656 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 606.24267578125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 484.7119445800781 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8265, loss_val: nan, pos_over_neg: 536.5654296875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 649.2756958007812 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.8236, loss_val: nan, pos_over_neg: 570.1985473632812 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 616.9313354492188 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8259, loss_val: nan, pos_over_neg: 592.9872436523438 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 710.9930419921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 665.0933227539062 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.8228, loss_val: nan, pos_over_neg: 786.7293090820312 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.8217, loss_val: nan, pos_over_neg: 550.3607788085938 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.8275, loss_val: nan, pos_over_neg: 370.47723388671875 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.8274, loss_val: nan, pos_over_neg: 727.4169921875 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 686.7257690429688 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.8267, loss_val: nan, pos_over_neg: 716.2828369140625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 769.8720703125 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.8315, loss_val: nan, pos_over_neg: 300.39483642578125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.826, loss_val: nan, pos_over_neg: 655.7926025390625 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 733.3910522460938 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 565.4588623046875 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.8262, loss_val: nan, pos_over_neg: 426.8345642089844 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.8326, loss_val: nan, pos_over_neg: 445.9276428222656 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 531.850830078125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.8388, loss_val: nan, pos_over_neg: 306.11016845703125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.8341, loss_val: nan, pos_over_neg: 372.9922790527344 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 487.0645751953125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.8249, loss_val: nan, pos_over_neg: 401.42645263671875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 940.7601318359375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.8324, loss_val: nan, pos_over_neg: 330.9799499511719 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.8293, loss_val: nan, pos_over_neg: 374.5946960449219 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.8319, loss_val: nan, pos_over_neg: 530.8577880859375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.83, loss_val: nan, pos_over_neg: 293.49688720703125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 411.21356201171875 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.8282, loss_val: nan, pos_over_neg: 355.646728515625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.831, loss_val: nan, pos_over_neg: 309.9566650390625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 490.20428466796875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.8253, loss_val: nan, pos_over_neg: 491.3627014160156 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 577.621826171875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.8247, loss_val: nan, pos_over_neg: 473.2652587890625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.8264, loss_val: nan, pos_over_neg: 484.10284423828125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 727.4931640625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.8238, loss_val: nan, pos_over_neg: 669.9480590820312 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 825.7979125976562 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.827, loss_val: nan, pos_over_neg: 514.7442626953125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8245, loss_val: nan, pos_over_neg: 543.551025390625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 894.072265625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1085.5855712890625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 457.0010681152344 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.8202, loss_val: nan, pos_over_neg: 449.70068359375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1038.057373046875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 1127.398193359375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.8165, loss_val: nan, pos_over_neg: 830.6585083007812 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.8184, loss_val: nan, pos_over_neg: 618.5556030273438 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 939.9082641601562 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.8201, loss_val: nan, pos_over_neg: 368.7000732421875 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.8273, loss_val: nan, pos_over_neg: 397.2945251464844 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 738.4837646484375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.8204, loss_val: nan, pos_over_neg: 554.9605102539062 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.8254, loss_val: nan, pos_over_neg: 346.14276123046875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.8257, loss_val: nan, pos_over_neg: 626.1834716796875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 569.7244262695312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.8283, loss_val: nan, pos_over_neg: 426.8154602050781 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 544.4388427734375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.8278, loss_val: nan, pos_over_neg: 395.9910583496094 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.819, loss_val: nan, pos_over_neg: 831.58740234375 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 530.4847412109375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.8263, loss_val: nan, pos_over_neg: 492.579345703125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 507.4964294433594 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 392.9183349609375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.8224, loss_val: nan, pos_over_neg: 322.3828125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.8191, loss_val: nan, pos_over_neg: 513.7591552734375 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 592.9238891601562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.8222, loss_val: nan, pos_over_neg: 395.6203918457031 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 382.91888427734375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1378.7587890625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.8244, loss_val: nan, pos_over_neg: 566.3374633789062 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.8221, loss_val: nan, pos_over_neg: 286.6322937011719 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.8231, loss_val: nan, pos_over_neg: 707.004638671875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.8189, loss_val: nan, pos_over_neg: 640.4971313476562 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 420.3765869140625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.8295, loss_val: nan, pos_over_neg: 523.762451171875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 1191.95263671875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.8261, loss_val: nan, pos_over_neg: 420.2113952636719 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 647.4256591796875 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.8239, loss_val: nan, pos_over_neg: 1233.4156494140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 440.71002197265625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [20:31<102665:57:50, 1232.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 763.1651611328125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 698.0069580078125 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.825, loss_val: nan, pos_over_neg: 476.7935791015625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.8212, loss_val: nan, pos_over_neg: 964.7258911132812 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.8211, loss_val: nan, pos_over_neg: 676.50390625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1042.5511474609375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 761.4599609375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.8219, loss_val: nan, pos_over_neg: 434.4414978027344 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1378.072509765625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 325.4195556640625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 712.9781494140625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 670.8097534179688 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 1409.117919921875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 651.9111328125 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 659.716796875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.8265, loss_val: nan, pos_over_neg: 742.426513671875 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 696.2476806640625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 410.61456298828125 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.8234, loss_val: nan, pos_over_neg: 499.8667297363281 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 513.530517578125 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.8124, loss_val: nan, pos_over_neg: 931.8460693359375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1120.0777587890625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 2476.891357421875 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.8269, loss_val: nan, pos_over_neg: 523.9747924804688 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.8125, loss_val: nan, pos_over_neg: 1081.8157958984375 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 682.0394287109375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.8191, loss_val: nan, pos_over_neg: 462.35614013671875 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1009.284912109375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 563.0723266601562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 775.4041137695312 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.8139, loss_val: nan, pos_over_neg: 957.9944458007812 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.822, loss_val: nan, pos_over_neg: 937.8580932617188 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 1211.7552490234375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1329.2210693359375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 952.8217163085938 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 738.874755859375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1174.6923828125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.8147, loss_val: nan, pos_over_neg: 1463.1436767578125 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1018.5989379882812 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.8209, loss_val: nan, pos_over_neg: 594.3646850585938 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 2539.3359375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 2180.929931640625 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 926.7683715820312 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.8173, loss_val: nan, pos_over_neg: 387.8627624511719 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 1226.68603515625 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.817, loss_val: nan, pos_over_neg: 641.4188232421875 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 6033.982421875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 2482.529052734375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.8237, loss_val: nan, pos_over_neg: 357.2684326171875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 1159.159423828125 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1493.3924560546875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.8164, loss_val: nan, pos_over_neg: 1677.6651611328125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 603.8197631835938 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.8213, loss_val: nan, pos_over_neg: 674.0306396484375 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.8218, loss_val: nan, pos_over_neg: 561.9432373046875 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.8246, loss_val: nan, pos_over_neg: 334.361083984375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 399.0252685546875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.8152, loss_val: nan, pos_over_neg: 358.8720703125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 427.7254333496094 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 412.93359375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 438.9312438964844 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.8162, loss_val: nan, pos_over_neg: 699.4108276367188 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.8166, loss_val: nan, pos_over_neg: 856.6190185546875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 385.64605712890625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 403.0106201171875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 937.644775390625 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.824, loss_val: nan, pos_over_neg: 609.4912109375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 736.4808349609375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.8281, loss_val: nan, pos_over_neg: 389.73663330078125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 497.38299560546875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.8135, loss_val: nan, pos_over_neg: 736.948974609375 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.8208, loss_val: nan, pos_over_neg: 366.1299743652344 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 468.6256408691406 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 433.89373779296875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1806.80419921875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 664.3115234375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 658.9043579101562 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.8242, loss_val: nan, pos_over_neg: 774.197998046875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1701.887939453125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.8178, loss_val: nan, pos_over_neg: 479.3162841796875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.828, loss_val: nan, pos_over_neg: 282.85308837890625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 722.1370849609375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.8207, loss_val: nan, pos_over_neg: 710.86328125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1108.25 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 919.1700439453125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 731.271240234375 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.815, loss_val: nan, pos_over_neg: 602.2167358398438 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1000.0205078125 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.8176, loss_val: nan, pos_over_neg: 808.900390625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.8163, loss_val: nan, pos_over_neg: 604.2078857421875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1126.7119140625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1520.5899658203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 723.3223876953125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 843.7172241210938 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 612.8182983398438 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 686.744384765625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.8151, loss_val: nan, pos_over_neg: 575.6328735351562 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 462.97247314453125 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 594.08203125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 402.9997253417969 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 528.3252563476562 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8129, loss_val: nan, pos_over_neg: 662.3430786132812 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 443.90985107421875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 563.4451293945312 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.8179, loss_val: nan, pos_over_neg: 320.2216796875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.8148, loss_val: nan, pos_over_neg: 677.2987060546875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.8185, loss_val: nan, pos_over_neg: 467.7087097167969 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 385.75439453125 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 684.8018798828125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 455.82525634765625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.8197, loss_val: nan, pos_over_neg: 696.2030639648438 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 751.947021484375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 642.511474609375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1029.7919921875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 1105.10009765625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.8144, loss_val: nan, pos_over_neg: 650.5270385742188 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.8131, loss_val: nan, pos_over_neg: 301.7258605957031 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.8172, loss_val: nan, pos_over_neg: 627.120849609375 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 589.7308959960938 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.8174, loss_val: nan, pos_over_neg: 596.56494140625 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.8203, loss_val: nan, pos_over_neg: 643.4275512695312 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 482.24090576171875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 2087.58837890625 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.8268, loss_val: nan, pos_over_neg: 484.9422607421875 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1424.002685546875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.8068, loss_val: nan, pos_over_neg: 729.2522583007812 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.8137, loss_val: nan, pos_over_neg: 504.4057922363281 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 2213.58740234375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 1262.3349609375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 816.3905029296875 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.8169, loss_val: nan, pos_over_neg: 2260.930908203125 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 2338.155029296875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1236.899169921875 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.8226, loss_val: nan, pos_over_neg: 759.2911987304688 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.8215, loss_val: nan, pos_over_neg: 491.49237060546875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.8098, loss_val: nan, pos_over_neg: 855.6123657226562 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 514.6741943359375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 610.9224853515625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 697.313232421875 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.8106, loss_val: nan, pos_over_neg: 809.2507934570312 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.8229, loss_val: nan, pos_over_neg: 447.2563171386719 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.8196, loss_val: nan, pos_over_neg: 585.6387329101562 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 1242.07275390625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 454.97857666015625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 850.9401245117188 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.8167, loss_val: nan, pos_over_neg: 534.25634765625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 717.6690673828125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 833.8004760742188 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 2781.5751953125 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.8177, loss_val: nan, pos_over_neg: 879.0545043945312 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 873.2664794921875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 915.5911254882812 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1217.951904296875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 923.6704711914062 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 683.2911987304688 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 651.9796752929688 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 639.4520263671875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 779.3659057617188 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 895.8731079101562 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 432.7518615722656 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.814, loss_val: nan, pos_over_neg: 605.4259033203125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 678.6248779296875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.8119, loss_val: nan, pos_over_neg: 366.51580810546875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 444.35614013671875 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.8107, loss_val: nan, pos_over_neg: 943.7920532226562 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 422.5785827636719 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 590.3883666992188 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 829.6067504882812 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 374.1181335449219 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.802, loss_val: nan, pos_over_neg: 1438.912841796875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 418.08135986328125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.821, loss_val: nan, pos_over_neg: 456.906005859375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.8045, loss_val: nan, pos_over_neg: 551.402587890625 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.8193, loss_val: nan, pos_over_neg: 545.2080688476562 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 928.3632202148438 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 530.2096557617188 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.816, loss_val: nan, pos_over_neg: 460.41168212890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 422.9637756347656 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.8175, loss_val: nan, pos_over_neg: 470.3834228515625 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.8183, loss_val: nan, pos_over_neg: 336.2019958496094 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.8168, loss_val: nan, pos_over_neg: 471.9013671875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.8153, loss_val: nan, pos_over_neg: 472.78936767578125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 431.24517822265625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.8195, loss_val: nan, pos_over_neg: 576.881591796875 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 540.2484130859375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.8112, loss_val: nan, pos_over_neg: 871.9070434570312 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.8134, loss_val: nan, pos_over_neg: 601.9852905273438 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 713.4906005859375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.8092, loss_val: nan, pos_over_neg: 580.7798461914062 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.8128, loss_val: nan, pos_over_neg: 663.7246704101562 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1201.7154541015625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 582.386962890625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.8138, loss_val: nan, pos_over_neg: 380.87353515625 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.8075, loss_val: nan, pos_over_neg: 791.3986206054688 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1702.52978515625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1787.1839599609375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 972.836181640625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 1030.8956298828125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1407.6976318359375 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1908.7767333984375 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 954.3912963867188 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.8087, loss_val: nan, pos_over_neg: 394.3869934082031 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1271.2261962890625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 579.7183837890625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.8121, loss_val: nan, pos_over_neg: 495.24981689453125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.8056, loss_val: nan, pos_over_neg: 868.209716796875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.809, loss_val: nan, pos_over_neg: 1064.20263671875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 435.7030944824219 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 530.0988159179688 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 772.604736328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 2380.43505859375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 806.7615966796875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 348.7166442871094 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.8199, loss_val: nan, pos_over_neg: 668.3764038085938 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1122.38037109375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.8132, loss_val: nan, pos_over_neg: 579.92138671875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 379.0142822265625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.8149, loss_val: nan, pos_over_neg: 499.54071044921875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1168.796875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 740.933349609375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.8104, loss_val: nan, pos_over_neg: 670.5765991210938 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 472.83056640625 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 935.11865234375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 714.30810546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 390.1725769042969 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 510.5552978515625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.8141, loss_val: nan, pos_over_neg: 382.47247314453125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 851.0830688476562 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 824.8532104492188 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 519.0655517578125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 906.2153930664062 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.8097, loss_val: nan, pos_over_neg: 766.5719604492188 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 858.6887817382812 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 485.3473815917969 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.8122, loss_val: nan, pos_over_neg: 614.8735961914062 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.8154, loss_val: nan, pos_over_neg: 533.947265625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 643.5886840820312 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.8142, loss_val: nan, pos_over_neg: 1061.277587890625 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.81, loss_val: nan, pos_over_neg: 596.6171264648438 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.8102, loss_val: nan, pos_over_neg: 795.1245727539062 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 520.03662109375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.8136, loss_val: nan, pos_over_neg: 757.1956787109375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 946.01318359375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 688.8826904296875 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 664.7505493164062 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 872.6712646484375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 697.1812133789062 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1305.7476806640625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.8038, loss_val: nan, pos_over_neg: 813.4403686523438 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 537.7213745117188 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1458.40283203125 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 743.6881103515625 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1084.9459228515625 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1138.8489990234375 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 534.4567260742188 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.8123, loss_val: nan, pos_over_neg: 485.159423828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.813, loss_val: nan, pos_over_neg: 390.5928955078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 529.0042114257812 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 2334.989990234375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 675.7506713867188 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.8103, loss_val: nan, pos_over_neg: 473.7375183105469 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 695.1329956054688 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.8055, loss_val: nan, pos_over_neg: 1723.3941650390625 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 659.7634887695312 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 893.8411254882812 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 972.8357543945312 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1048.65576171875 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.8008, loss_val: nan, pos_over_neg: 587.7943725585938 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 344.04107666015625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 984.5750732421875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 754.2061767578125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 577.0736083984375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 543.8947143554688 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 878.4201049804688 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 898.182861328125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 702.3134155273438 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 531.81005859375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 649.7445678710938 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 836.7142333984375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 392.69891357421875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 395.9484558105469 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.8126, loss_val: nan, pos_over_neg: 483.6117858886719 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 770.448974609375 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 474.16217041015625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.8072, loss_val: nan, pos_over_neg: 378.6313171386719 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.8052, loss_val: nan, pos_over_neg: 705.0174560546875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 475.9314270019531 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.8088, loss_val: nan, pos_over_neg: 551.8212890625 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 585.5553588867188 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 509.6341552734375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 826.9478149414062 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 509.6580505371094 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.8085, loss_val: nan, pos_over_neg: 311.755859375 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 710.3956298828125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 3554.033935546875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1218.551025390625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 780.0655517578125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.8158, loss_val: nan, pos_over_neg: 358.5574035644531 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1155.6334228515625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 485.8587951660156 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.8114, loss_val: nan, pos_over_neg: 329.737548828125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 514.7277221679688 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 825.9283447265625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.807, loss_val: nan, pos_over_neg: 882.2424926757812 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 657.8804931640625 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.8078, loss_val: nan, pos_over_neg: 588.118896484375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 668.9876098632812 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1577.0992431640625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 840.33740234375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.8066, loss_val: nan, pos_over_neg: 575.572998046875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 623.2982788085938 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.8096, loss_val: nan, pos_over_neg: 521.9666137695312 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.8057, loss_val: nan, pos_over_neg: 956.6228637695312 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1333.360107421875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 662.2750244140625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.8115, loss_val: nan, pos_over_neg: 477.8021240234375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 491.6752624511719 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 2012.102783203125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 459.1629333496094 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.8037, loss_val: nan, pos_over_neg: 733.5505981445312 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1050.111328125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1593.78369140625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 799.0186157226562 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 476.18896484375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.8048, loss_val: nan, pos_over_neg: 536.7991333007812 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.8046, loss_val: nan, pos_over_neg: 563.196533203125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.8049, loss_val: nan, pos_over_neg: 917.0784912109375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 842.99658203125 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 499.5499572753906 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1735.73828125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 7925.0439453125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.8105, loss_val: nan, pos_over_neg: 434.5365905761719 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.8155, loss_val: nan, pos_over_neg: 414.416015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1081.508544921875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.811, loss_val: nan, pos_over_neg: 690.9625854492188 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 681.1044311523438 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1442.1900634765625 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 536.681396484375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 628.2658081054688 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 970.8355102539062 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1339.8277587890625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.8094, loss_val: nan, pos_over_neg: 434.9658203125 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 379.25299072265625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 774.6320190429688 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 1224.65625 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 624.5450439453125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 523.8099975585938 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 652.3477172851562 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.8065, loss_val: nan, pos_over_neg: 362.9884948730469 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.8061, loss_val: nan, pos_over_neg: 343.8351745605469 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 957.7969360351562 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 411.5618896484375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 456.4035949707031 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1652.3089599609375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 422.83001708984375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 831.4210205078125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 678.5230712890625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 539.6493530273438 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 415.9213562011719 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.8082, loss_val: nan, pos_over_neg: 596.9321899414062 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1128.2242431640625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 534.04052734375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 421.006103515625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 964.0458984375 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1808.47998046875 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.8089, loss_val: nan, pos_over_neg: 508.46441650390625 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 469.20147705078125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 811.8060302734375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.8073, loss_val: nan, pos_over_neg: 640.4015502929688 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.808, loss_val: nan, pos_over_neg: 544.9495239257812 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 1012.517822265625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 609.8886108398438 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 7128.17626953125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 856.867919921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 415.0548400878906 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 488.21649169921875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 382.7571105957031 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 494.62261962890625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 477.3162536621094 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.8062, loss_val: nan, pos_over_neg: 433.7618103027344 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 643.8600463867188 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 1290.12841796875 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 768.8301391601562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 459.1131591796875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.8071, loss_val: nan, pos_over_neg: 797.3894653320312 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 1015.5499877929688 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 661.0585327148438 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 736.913818359375 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1802.028076171875 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 990.3292236328125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 546.9397583007812 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1269.60693359375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1106.9832763671875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.8067, loss_val: nan, pos_over_neg: 567.3433227539062 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1633.460205078125 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1607.07568359375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1152.3057861328125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 2759.1015625 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1218.7510986328125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 976.234130859375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 858.1039428710938 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.8051, loss_val: nan, pos_over_neg: 873.7568359375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 1252.5313720703125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 530.8359985351562 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1754.0323486328125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.8091, loss_val: nan, pos_over_neg: 730.772705078125 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 654.2706298828125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 661.907470703125 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.8041, loss_val: nan, pos_over_neg: 941.86279296875 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 522.1565551757812 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.8111, loss_val: nan, pos_over_neg: 876.4891967773438 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 426.4448547363281 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.8028, loss_val: nan, pos_over_neg: 521.7489624023438 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 586.3397216796875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 730.556640625 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 587.36474609375 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 361.3953857421875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 506.34478759765625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 643.8887329101562 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.8043, loss_val: nan, pos_over_neg: 607.2106323242188 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 980.0944213867188 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 537.9724731445312 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 3781.333984375 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 867.7156982421875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 495.3766784667969 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1159.2322998046875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 491.358642578125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1307.0699462890625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 555.8013916015625 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.8108, loss_val: nan, pos_over_neg: 425.6192626953125 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.8032, loss_val: nan, pos_over_neg: 609.585693359375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 473.06573486328125 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.8047, loss_val: nan, pos_over_neg: 572.9671020507812 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1583.2498779296875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.8001, loss_val: nan, pos_over_neg: 616.0672607421875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.8064, loss_val: nan, pos_over_neg: 356.7377014160156 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7993, loss_val: nan, pos_over_neg: 536.14794921875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 516.6758422851562 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.8026, loss_val: nan, pos_over_neg: 806.3627319335938 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 652.0236206054688 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.803, loss_val: nan, pos_over_neg: 605.2034912109375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.8017, loss_val: nan, pos_over_neg: 485.40008544921875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1021.1365356445312 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7959, loss_val: nan, pos_over_neg: 910.822021484375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.8027, loss_val: nan, pos_over_neg: 520.847900390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 589.6528930664062 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 1185.1348876953125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 949.3745727539062 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7975, loss_val: nan, pos_over_neg: 620.530029296875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 891.7156372070312 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 746.4663696289062 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.8083, loss_val: nan, pos_over_neg: 730.1854248046875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 631.7598876953125 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.8101, loss_val: nan, pos_over_neg: 668.3384399414062 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1695.3116455078125 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1083.4205322265625 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 729.3195190429688 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1547.2139892578125 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1117.614990234375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.8019, loss_val: nan, pos_over_neg: 690.2161254882812 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.8117, loss_val: nan, pos_over_neg: 606.9176635742188 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1124.1728515625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.8143, loss_val: nan, pos_over_neg: 864.0062866210938 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1159.15087890625 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 1207.186279296875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 675.849365234375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1955.80517578125 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 546.3269653320312 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 1896.380615234375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 784.8008422851562 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 526.3732299804688 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 857.0545043945312 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 706.813720703125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1225.5635986328125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7978, loss_val: nan, pos_over_neg: 724.7910766601562 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 858.4526977539062 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7971, loss_val: nan, pos_over_neg: 855.8195190429688 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 447.54498291015625 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 633.2122802734375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1023.9971313476562 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 604.3153686523438 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 638.7923583984375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 2158.329833984375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 721.8656005859375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.804, loss_val: nan, pos_over_neg: 706.6146850585938 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 423.3343811035156 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1022.8851318359375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1551.627197265625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 865.9823608398438 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 746.4150390625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 500.8408203125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 594.3768920898438 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1557.5843505859375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 671.8844604492188 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 560.09912109375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 466.7461853027344 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 941.2637329101562 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1413.630126953125 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 968.998046875 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.8012, loss_val: nan, pos_over_neg: 935.9346313476562 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 921.562255859375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.8113, loss_val: nan, pos_over_neg: 466.110107421875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1958.26025390625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.8058, loss_val: nan, pos_over_neg: 638.2957153320312 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 724.8153076171875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.8054, loss_val: nan, pos_over_neg: 588.1596069335938 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.8036, loss_val: nan, pos_over_neg: 485.6064453125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1157.274169921875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 909.4971313476562 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 479.06103515625 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.8063, loss_val: nan, pos_over_neg: 682.496337890625 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 675.5211181640625 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.8006, loss_val: nan, pos_over_neg: 667.9928588867188 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.806, loss_val: nan, pos_over_neg: 874.9114379882812 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 533.849365234375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 493.0564880371094 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 741.49951171875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 1072.4754638671875 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1018.7433471679688 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.8021, loss_val: nan, pos_over_neg: 503.07379150390625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.8031, loss_val: nan, pos_over_neg: 569.23681640625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 842.5955200195312 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 2173.865478515625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 649.0989990234375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1437.5538330078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 688.3198852539062 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1018.0075073242188 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 545.3466186523438 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 972.3363037109375 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 712.7850341796875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 546.4573974609375 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 719.1783447265625 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1704.4434814453125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 649.9447631835938 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1415.0623779296875 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 661.9866333007812 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 454.5727844238281 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.8022, loss_val: nan, pos_over_neg: 401.0436706542969 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1234.8714599609375 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 818.74951171875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 442.328369140625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 488.0174255371094 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.8076, loss_val: nan, pos_over_neg: 733.9201049804688 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 939.396484375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 961.26123046875 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.8004, loss_val: nan, pos_over_neg: 839.3538208007812 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 907.5137329101562 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.8079, loss_val: nan, pos_over_neg: 356.4577941894531 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1280.0614013671875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1403.512451171875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 914.5428466796875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.8081, loss_val: nan, pos_over_neg: 506.4419860839844 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 537.1762084960938 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.8015, loss_val: nan, pos_over_neg: 756.291748046875 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.799, loss_val: nan, pos_over_neg: 924.1920776367188 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 516.955322265625 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7977, loss_val: nan, pos_over_neg: 502.67913818359375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.8053, loss_val: nan, pos_over_neg: 623.6324462890625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1141.4669189453125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 719.0401611328125 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.8023, loss_val: nan, pos_over_neg: 649.2711791992188 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 883.7443237304688 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1148.01025390625 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.8086, loss_val: nan, pos_over_neg: 576.3505249023438 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1094.653076171875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1195.3658447265625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.8029, loss_val: nan, pos_over_neg: 578.1932373046875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 582.7999267578125 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7991, loss_val: nan, pos_over_neg: 720.0486450195312 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1490.5655517578125 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7999, loss_val: nan, pos_over_neg: 792.9171142578125 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1050.5106201171875 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 904.8412475585938 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1017.9303588867188 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1450.689208984375 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 856.600341796875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.801, loss_val: nan, pos_over_neg: 481.3893127441406 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 831.8634643554688 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7995, loss_val: nan, pos_over_neg: 725.3458251953125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 37623.7578125 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1469.778076171875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 862.1202392578125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1209.5262451171875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1232.1822509765625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1226.727783203125 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1680.1563720703125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1019.2022094726562 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.8034, loss_val: nan, pos_over_neg: 817.1326293945312 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.805, loss_val: nan, pos_over_neg: 415.6828308105469 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 676.62841796875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.797, loss_val: nan, pos_over_neg: 1105.5633544921875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 870.1123046875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 860.3096313476562 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7984, loss_val: nan, pos_over_neg: 521.5769653320312 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 662.4168701171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 2456.1962890625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 801.4794921875 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 723.2429809570312 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 565.2657470703125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1366.7054443359375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 809.7364501953125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 521.2547607421875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 483.02105712890625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 715.6849365234375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1145.3878173828125 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1093.975830078125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 720.6135864257812 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7982, loss_val: nan, pos_over_neg: 471.4652404785156 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1806.81298828125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.8044, loss_val: nan, pos_over_neg: 758.32470703125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.793, loss_val: nan, pos_over_neg: 849.4337158203125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.8, loss_val: nan, pos_over_neg: 697.8895263671875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.8074, loss_val: nan, pos_over_neg: 547.8289794921875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1383.0438232421875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 845.6901245117188 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.8018, loss_val: nan, pos_over_neg: 651.4674072265625 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 505.9544982910156 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1696.514404296875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 741.22705078125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 636.5236206054688 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1028.59033203125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1464.92626953125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 673.893310546875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1029.4140625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 663.7100219726562 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1597.818115234375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.8024, loss_val: nan, pos_over_neg: 471.7082214355469 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 303.7674865722656 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 622.6654052734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1121.0238037109375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1070.09765625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 911.9268188476562 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7983, loss_val: nan, pos_over_neg: 923.1475219726562 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1544.01513671875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1572.396484375 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 1613.3787841796875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1194.4842529296875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 619.9692993164062 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1153.6573486328125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 849.454833984375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1013.458984375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 502.37432861328125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1498.1171875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 3626.615478515625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1683.4508056640625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 4054.21337890625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1283.6279296875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7957, loss_val: nan, pos_over_neg: 700.9979248046875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 884.3660278320312 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1098.8795166015625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 877.2803955078125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1194.3807373046875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1256.425537109375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 830.0621948242188 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 2033.3248291015625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 901.6048583984375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7962, loss_val: nan, pos_over_neg: 623.8736572265625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 577.7469482421875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 854.5260620117188 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 621.9373168945312 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 550.0579223632812 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 469.7515869140625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 716.8029174804688 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1144.545166015625 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 601.7817993164062 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7966, loss_val: nan, pos_over_neg: 501.914794921875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 652.34326171875 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1190.7755126953125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 470.0259704589844 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 623.2373046875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 460.44708251953125 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 483.3597412109375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7973, loss_val: nan, pos_over_neg: 403.4478454589844 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 763.895751953125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 815.9918823242188 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 704.4000244140625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 550.1887817382812 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 649.589599609375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7934, loss_val: nan, pos_over_neg: 1364.98828125 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 914.28759765625 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2012.453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 533.1202392578125 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 792.8777465820312 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1265.0552978515625 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7968, loss_val: nan, pos_over_neg: 865.1387329101562 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1721.052734375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7948, loss_val: nan, pos_over_neg: 857.5718383789062 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1863.5194091796875 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1183.36767578125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 1867.2506103515625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.787, loss_val: nan, pos_over_neg: 1390.6331787109375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1603.9561767578125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1494.6002197265625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 482.46844482421875 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 642.0213623046875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [41:05<102736:51:58, 1232.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 724.3930053710938 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1201.9158935546875 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7998, loss_val: nan, pos_over_neg: 2244.66259765625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 430.5342102050781 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.796, loss_val: nan, pos_over_neg: 626.291748046875 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1172.35791015625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 835.0794067382812 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 764.2077026367188 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 840.1447143554688 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7994, loss_val: nan, pos_over_neg: 620.227294921875 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1177.754150390625 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1151.77001953125 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7908, loss_val: nan, pos_over_neg: 348.13604736328125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 438.56414794921875 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1496.75 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1443.302734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 908.5895385742188 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1334.438232421875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7953, loss_val: nan, pos_over_neg: 592.3383178710938 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1346.535888671875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 2892.77490234375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 887.9925537109375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 539.065673828125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 502.7187805175781 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 885.8357543945312 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 1176.6009521484375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 811.042724609375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7967, loss_val: nan, pos_over_neg: 315.38623046875 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 704.9727172851562 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 797.4718017578125 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7964, loss_val: nan, pos_over_neg: 575.2963256835938 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 582.0376586914062 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 597.0794067382812 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 4062.118408203125 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1261.271240234375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7887, loss_val: nan, pos_over_neg: 711.8606567382812 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 922.4844360351562 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7986, loss_val: nan, pos_over_neg: 465.7955017089844 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 577.5223388671875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1128.503662109375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1443.7210693359375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 812.3889770507812 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1031.4666748046875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 561.1472778320312 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 773.157470703125 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 886.3502807617188 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 819.7889404296875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7929, loss_val: nan, pos_over_neg: 550.060302734375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 826.788818359375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 763.9710693359375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 840.2200317382812 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1102.854736328125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 463.2434387207031 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 655.5379638671875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 641.0249633789062 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 583.2935180664062 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 800.5714721679688 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 466.8383483886719 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 664.3200073242188 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 329.36590576171875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 768.0330810546875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 1138.5394287109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7992, loss_val: nan, pos_over_neg: 546.9310302734375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 540.0394897460938 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 824.3797607421875 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 870.6693115234375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 420.67974853515625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7947, loss_val: nan, pos_over_neg: 622.3526000976562 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 569.7347412109375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 431.0658264160156 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1143.9029541015625 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 444.6146240234375 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1178.0699462890625 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 550.1353149414062 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 563.5213012695312 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1829.6107177734375 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 746.33251953125 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7988, loss_val: nan, pos_over_neg: 600.791748046875 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 762.3448486328125 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 679.065185546875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1221.7880859375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 857.3890380859375 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 482.0655517578125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1012.8381958007812 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7952, loss_val: nan, pos_over_neg: 575.6546630859375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 797.8084716796875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 756.198486328125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 1492.470458984375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 3184.623046875 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 583.5540161132812 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 939.5029296875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 407.4434814453125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 381.99041748046875 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1064.7142333984375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.8002, loss_val: nan, pos_over_neg: 596.9783325195312 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 533.5936889648438 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 528.2655029296875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1208.355712890625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1944.2691650390625 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 467.3396301269531 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1023.1339111328125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.8014, loss_val: nan, pos_over_neg: 396.3404846191406 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7996, loss_val: nan, pos_over_neg: 590.1669921875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7933, loss_val: nan, pos_over_neg: 780.7333374023438 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 588.9022827148438 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 661.7670288085938 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 813.1373291015625 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 654.0759887695312 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 908.41845703125 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 680.4024658203125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1087.9937744140625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 669.3040161132812 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 859.189208984375 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7976, loss_val: nan, pos_over_neg: 344.152099609375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 944.9771728515625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 755.6168823242188 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7958, loss_val: nan, pos_over_neg: 2023.11572265625 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 513.9039306640625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7941, loss_val: nan, pos_over_neg: 657.6314086914062 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 474.8557434082031 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7979, loss_val: nan, pos_over_neg: 660.021728515625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7997, loss_val: nan, pos_over_neg: 730.3541870117188 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 431.8539123535156 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7927, loss_val: nan, pos_over_neg: 701.4269409179688 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 2343.71142578125 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 744.4559326171875 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7912, loss_val: nan, pos_over_neg: 607.9195556640625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 931.28271484375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.8007, loss_val: nan, pos_over_neg: 390.47479248046875 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 633.3516845703125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 755.1139526367188 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 577.0468139648438 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 800.005859375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 640.9368286132812 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 657.35498046875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 578.797119140625 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7945, loss_val: nan, pos_over_neg: 608.5211181640625 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 656.7185668945312 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 712.8452758789062 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7981, loss_val: nan, pos_over_neg: 398.8460693359375 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7944, loss_val: nan, pos_over_neg: 658.2979125976562 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 890.0187377929688 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7965, loss_val: nan, pos_over_neg: 635.3085327148438 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 940.4547119140625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 776.4760131835938 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1474.76953125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1034.277099609375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1052.251708984375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1532.7098388671875 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 704.2489013671875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1047.4930419921875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 851.4295043945312 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1112.4444580078125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 461.0573425292969 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 602.1358642578125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 610.3892211914062 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 828.9769287109375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 622.5206909179688 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.789, loss_val: nan, pos_over_neg: 702.8347778320312 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7955, loss_val: nan, pos_over_neg: 793.015625 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 528.898193359375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7974, loss_val: nan, pos_over_neg: 578.887939453125 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1182.186279296875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7874, loss_val: nan, pos_over_neg: 955.3357543945312 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 585.0310668945312 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 594.1452026367188 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7985, loss_val: nan, pos_over_neg: 661.67333984375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 882.90478515625 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 788.6522216796875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7956, loss_val: nan, pos_over_neg: 490.0376892089844 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 978.8841552734375 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1148.52490234375 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7899, loss_val: nan, pos_over_neg: 670.4725952148438 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7919, loss_val: nan, pos_over_neg: 538.5413208007812 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 627.8648681640625 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 646.4127197265625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1118.62890625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7922, loss_val: nan, pos_over_neg: 323.2146911621094 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 487.38665771484375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 908.1773681640625 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 3004.61083984375 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7951, loss_val: nan, pos_over_neg: 639.1220703125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 416.7198486328125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7897, loss_val: nan, pos_over_neg: 684.8115234375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.8005, loss_val: nan, pos_over_neg: 989.383056640625 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 488.7938537597656 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 433.3251037597656 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 621.2537231445312 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 898.3868408203125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 656.8615112304688 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1318.3905029296875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 476.4703063964844 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 797.6028442382812 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 634.54736328125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 671.6155395507812 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 810.1262817382812 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 907.4165649414062 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 639.571044921875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7954, loss_val: nan, pos_over_neg: 845.664794921875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 643.1565551757812 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7921, loss_val: nan, pos_over_neg: 548.7400512695312 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 656.38330078125 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.795, loss_val: nan, pos_over_neg: 711.31201171875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 806.6524047851562 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 735.3155517578125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 643.3574829101562 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2137.719482421875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1368.49365234375 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1088.63916015625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1853.2109375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1844.6136474609375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 499.4566955566406 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 808.5485229492188 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 767.8034057617188 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 927.9896240234375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1587.0849609375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1125.726806640625 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1410.8935546875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 665.5909423828125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 935.3954467773438 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1752.732421875 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 559.49951171875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 836.0909423828125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 634.4410400390625 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1414.1522216796875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 954.5809326171875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1090.5281982421875 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 750.9763793945312 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 978.9118041992188 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1375.301025390625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 2135.1787109375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7926, loss_val: nan, pos_over_neg: 717.0054321289062 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7938, loss_val: nan, pos_over_neg: 435.88800048828125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.798, loss_val: nan, pos_over_neg: 429.135009765625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1130.013427734375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2799.949951171875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1752.2716064453125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7881, loss_val: nan, pos_over_neg: 3099.580078125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 766.426025390625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 1402.5120849609375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 1095.3258056640625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7946, loss_val: nan, pos_over_neg: 2711.401611328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 867.8486328125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 1506.0902099609375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 793.1217651367188 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 699.7759399414062 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7889, loss_val: nan, pos_over_neg: 582.6475219726562 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1193.697998046875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1537.9810791015625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 629.0814819335938 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 493.6322021484375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 840.8731079101562 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 569.0083618164062 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7961, loss_val: nan, pos_over_neg: 718.4776000976562 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 897.943603515625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7909, loss_val: nan, pos_over_neg: 801.84765625 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 490.5522155761719 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 753.9550170898438 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 2049.645751953125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 766.8724975585938 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 846.3890991210938 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 2136.960693359375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 676.6845092773438 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 590.8648071289062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1358.4464111328125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1009.3780517578125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1368.351318359375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1351.55712890625 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 397.3365173339844 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 598.6039428710938 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1307.621337890625 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1216.8011474609375 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 883.3825073242188 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1001.50390625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 707.7366333007812 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 823.691162109375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.794, loss_val: nan, pos_over_neg: 752.890869140625 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1553.6778564453125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1523.9036865234375 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 613.3063354492188 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1032.86328125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1487.1876220703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 724.7564086914062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 594.6554565429688 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1233.6744384765625 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 621.9412841796875 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 678.3218994140625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 441.2156066894531 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 523.593994140625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 571.0250244140625 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1386.8851318359375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 606.922607421875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1052.0865478515625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1178.161376953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 808.121826171875 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 656.1209106445312 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 736.6035766601562 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 3049.1005859375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1499.3306884765625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7942, loss_val: nan, pos_over_neg: 571.5401000976562 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 996.0636596679688 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 819.061767578125 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 789.9547119140625 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1007.6793823242188 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1157.7742919921875 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 698.3949584960938 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1655.939453125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 786.7567138671875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7925, loss_val: nan, pos_over_neg: 695.45947265625 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 3411.245361328125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7868, loss_val: nan, pos_over_neg: 551.6386108398438 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7876, loss_val: nan, pos_over_neg: 527.0109252929688 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1247.8289794921875 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1062.978515625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 515.6482543945312 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 742.7123413085938 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 443.0167236328125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 779.1709594726562 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 924.0355224609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 738.9909057617188 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1093.077392578125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 876.7951049804688 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 494.0279235839844 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 2015.8358154296875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 989.665771484375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7896, loss_val: nan, pos_over_neg: 442.6564636230469 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 378.89031982421875 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 911.3136596679688 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1370.292236328125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 971.0836181640625 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1000.1177978515625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 914.0799560546875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 657.662109375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7892, loss_val: nan, pos_over_neg: 592.4545288085938 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 988.9271240234375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 853.0933837890625 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 856.6325073242188 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7937, loss_val: nan, pos_over_neg: 882.0394287109375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 2036.11328125 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1008.9032592773438 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 582.7003173828125 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1004.9915771484375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 921.1022338867188 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7841, loss_val: nan, pos_over_neg: 741.1400756835938 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 490.09197998046875 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7865, loss_val: nan, pos_over_neg: 364.81268310546875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.792, loss_val: nan, pos_over_neg: 921.9945678710938 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 645.2422485351562 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7913, loss_val: nan, pos_over_neg: 710.5584716796875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1139.38720703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1260.56884765625 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 671.4712524414062 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7795, loss_val: nan, pos_over_neg: 986.4515991210938 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 609.3965454101562 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 423.81207275390625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 977.6632690429688 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 579.9698486328125 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 519.405517578125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 606.2074584960938 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7901, loss_val: nan, pos_over_neg: 716.2667846679688 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 901.2449951171875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 319.8618469238281 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 654.2353515625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7878, loss_val: nan, pos_over_neg: 849.6389770507812 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 517.521484375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7862, loss_val: nan, pos_over_neg: 587.4661254882812 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 480.4108581542969 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1247.8487548828125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 576.6929321289062 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.791, loss_val: nan, pos_over_neg: 611.859619140625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7911, loss_val: nan, pos_over_neg: 678.0615844726562 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 556.7138671875 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 992.8502197265625 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7866, loss_val: nan, pos_over_neg: 675.070068359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1264.8331298828125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 464.82635498046875 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 860.2899169921875 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 732.5602416992188 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1100.15283203125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1287.8175048828125 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 591.2576904296875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7914, loss_val: nan, pos_over_neg: 672.564697265625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 655.8758544921875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1031.0233154296875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1356.539306640625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 936.0636596679688 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 463.1073913574219 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 581.6195678710938 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 639.3225708007812 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1221.9287109375 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1230.3704833984375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 550.8204956054688 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 503.43365478515625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 470.60369873046875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 802.2674560546875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1258.917724609375 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 714.8732299804688 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 396.9356994628906 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 402.76251220703125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1421.5517578125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1165.0992431640625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1027.3021240234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 607.0294189453125 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 476.93170166015625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 1054.447265625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1517.1678466796875 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1238.84228515625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 677.2702026367188 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 443.0380554199219 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1117.1103515625 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1121.1470947265625 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 589.3674926757812 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1689.6185302734375 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 657.51025390625 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 3136.249755859375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1213.16796875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 944.4323120117188 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2727.631103515625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 2290.829345703125 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 650.7697143554688 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1807.444091796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1801.64892578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 676.8078002929688 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7906, loss_val: nan, pos_over_neg: 2183.1396484375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1262.4014892578125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 2830.834716796875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 751.5320434570312 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1132.5499267578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 493.8045349121094 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 602.888427734375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 629.5725708007812 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1746.0076904296875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1563.180908203125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1041.39208984375 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1439.83837890625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1333.4215087890625 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 855.043701171875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1365.3671875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7829, loss_val: nan, pos_over_neg: 788.9750366210938 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1211.936279296875 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1165.3338623046875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 742.0060424804688 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 2920.602783203125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 4117.24951171875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 926.472900390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 2907.68115234375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1372.06103515625 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 618.7511596679688 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7917, loss_val: nan, pos_over_neg: 798.328857421875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.786, loss_val: nan, pos_over_neg: 1023.0452270507812 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 632.01708984375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 993.60888671875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1088.8046875 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1235.2421875 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 1015.8865356445312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7893, loss_val: nan, pos_over_neg: 544.0628662109375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2804.890380859375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7924, loss_val: nan, pos_over_neg: 390.4808349609375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 660.9501342773438 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 972.7112426757812 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 553.4803466796875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1348.623779296875 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7918, loss_val: nan, pos_over_neg: 661.8927612304688 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 734.13037109375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 590.5958862304688 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 843.4293212890625 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 2599.08447265625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1465.4154052734375 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7903, loss_val: nan, pos_over_neg: 516.078857421875 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 489.60333251953125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 583.2010498046875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 731.4058837890625 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1525.4647216796875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1708.2442626953125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 665.37744140625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7885, loss_val: nan, pos_over_neg: 416.9092712402344 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1472.5166015625 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7886, loss_val: nan, pos_over_neg: 959.901611328125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7882, loss_val: nan, pos_over_neg: 698.2230224609375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 580.9950561523438 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7895, loss_val: nan, pos_over_neg: 853.3333129882812 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1774.515869140625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 2259.495849609375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1016.1201782226562 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 610.0234985351562 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 564.04833984375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 588.836669921875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 823.8233032226562 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7891, loss_val: nan, pos_over_neg: 814.723876953125 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 780.1739501953125 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 991.6884765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1295.5743408203125 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 878.90625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1248.9730224609375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1471.2513427734375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 798.952880859375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.79, loss_val: nan, pos_over_neg: 923.867431640625 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 761.4691162109375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 689.0166015625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1200.589111328125 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1271.6622314453125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 856.9720458984375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7904, loss_val: nan, pos_over_neg: 772.1983642578125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 3230.216064453125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1288.58154296875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 936.306396484375 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 682.095947265625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 546.7369995117188 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1075.8385009765625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 700.4226684570312 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 635.1922607421875 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 924.3461303710938 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 883.173095703125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7883, loss_val: nan, pos_over_neg: 542.2943115234375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 827.0676879882812 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7843, loss_val: nan, pos_over_neg: 608.5228881835938 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1346.0191650390625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1352.6866455078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 946.0851440429688 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1052.2005615234375 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1333.3475341796875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1126.0045166015625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1062.6514892578125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2787.380126953125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 372.33428955078125 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1182.416259765625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1827.0406494140625 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7818, loss_val: nan, pos_over_neg: 416.3896789550781 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 559.6304321289062 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7898, loss_val: nan, pos_over_neg: 453.7890625 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1031.4903564453125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 522.5239868164062 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 696.2603149414062 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 662.3294677734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1319.7340087890625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 624.1215209960938 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 483.0776672363281 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 657.8204956054688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1362.4405517578125 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1088.28466796875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 848.0132446289062 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 652.1152954101562 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 2515.96630859375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1066.080078125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 567.8597412109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 455.86383056640625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1096.3681640625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 605.3374633789062 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1062.329833984375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 547.5086669921875 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 995.5543823242188 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7894, loss_val: nan, pos_over_neg: 521.3347778320312 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7834, loss_val: nan, pos_over_neg: 611.5047607421875 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7852, loss_val: nan, pos_over_neg: 779.4934692382812 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 653.3134155273438 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7856, loss_val: nan, pos_over_neg: 440.7424011230469 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 844.18408203125 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7888, loss_val: nan, pos_over_neg: 613.546142578125 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7935, loss_val: nan, pos_over_neg: 382.34552001953125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 728.8699340820312 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 823.8004760742188 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 951.891357421875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7879, loss_val: nan, pos_over_neg: 716.432373046875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 333.4165954589844 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 645.5667114257812 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 787.3324584960938 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 431.2933654785156 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 645.8456420898438 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 834.7578125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7848, loss_val: nan, pos_over_neg: 659.0805053710938 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 913.962646484375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 808.0700073242188 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7863, loss_val: nan, pos_over_neg: 793.8768310546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 826.6168823242188 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 1035.9146728515625 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7815, loss_val: nan, pos_over_neg: 659.4055786132812 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.785, loss_val: nan, pos_over_neg: 664.2909545898438 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.784, loss_val: nan, pos_over_neg: 792.9194946289062 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1136.01220703125 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 804.8558349609375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 749.3307495117188 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1301.9207763671875 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1241.1805419921875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1518.8834228515625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1261.43359375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1146.967041015625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1175.706298828125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1125.74267578125 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 919.5171508789062 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 857.0787353515625 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 706.6017456054688 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1855.3668212890625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 756.4385986328125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 776.701171875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7805, loss_val: nan, pos_over_neg: 786.5992431640625 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1373.77001953125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1288.7666015625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 1077.3486328125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 850.8241577148438 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 893.4186401367188 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1464.3431396484375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 813.3597412109375 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 889.10888671875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 658.9889526367188 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 960.1105346679688 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1022.810546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1182.8310546875 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 829.9506225585938 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7907, loss_val: nan, pos_over_neg: 867.6143188476562 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 964.9081420898438 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 936.4375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1027.3184814453125 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1180.8636474609375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7884, loss_val: nan, pos_over_neg: 844.6327514648438 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1345.7437744140625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1868.69189453125 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 987.6175537109375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 824.3359375 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1395.8682861328125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 813.408447265625 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7832, loss_val: nan, pos_over_neg: 914.1832885742188 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 825.5884399414062 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 991.6984252929688 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1004.0665283203125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7931, loss_val: nan, pos_over_neg: 464.6661071777344 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1464.9600830078125 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1160.749267578125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1524.5284423828125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7902, loss_val: nan, pos_over_neg: 865.8901977539062 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 482.58258056640625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1196.68701171875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7877, loss_val: nan, pos_over_neg: 743.613037109375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1390.1051025390625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 830.4119873046875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 777.8389892578125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1444.5672607421875 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1883.8948974609375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 673.9883422851562 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 821.5248413085938 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 463.8523254394531 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 634.9339599609375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1162.205322265625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 599.1289672851562 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1465.610107421875 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7847, loss_val: nan, pos_over_neg: 739.9853515625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 827.1959228515625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1172.3001708984375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1881.2457275390625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1003.5679321289062 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 886.2670288085938 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 560.6842651367188 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 641.6329345703125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 2230.97265625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 763.3800659179688 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 781.1688232421875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 826.4129638671875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1102.4700927734375 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7812, loss_val: nan, pos_over_neg: 668.1226806640625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 563.7305908203125 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1408.1953125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1225.9786376953125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 891.934326171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 806.7689819335938 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 914.8496704101562 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 973.85400390625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 4376.2998046875 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 586.3743896484375 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 696.7428588867188 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1307.1448974609375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 815.0255126953125 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 5416.5244140625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 891.6075439453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 657.21875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7845, loss_val: nan, pos_over_neg: 433.0115661621094 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 691.7605590820312 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 765.9154052734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 707.3366088867188 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 633.7153930664062 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2309.933837890625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 879.4631958007812 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 966.3067016601562 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 935.30859375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1108.0421142578125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 597.591796875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7858, loss_val: nan, pos_over_neg: 619.3667602539062 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1063.52880859375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1010.3697509765625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 712.7491455078125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7837, loss_val: nan, pos_over_neg: 640.7702026367188 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1038.6888427734375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1399.1893310546875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 922.855712890625 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7853, loss_val: nan, pos_over_neg: 623.5573120117188 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2305.294677734375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1286.82666015625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:01:36<102679:51:34, 1232.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1280.7095947265625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 859.1217651367188 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1021.0913696289062 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1258.7589111328125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1103.7548828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1813.1912841796875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1506.253173828125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1016.82470703125 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1076.43798828125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7827, loss_val: nan, pos_over_neg: 841.2095336914062 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 752.9331665039062 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 781.5733032226562 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1254.24609375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 884.2095947265625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1007.05810546875 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1094.2650146484375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1170.506591796875 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1410.444091796875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 902.4143676757812 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2037.599609375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1213.54345703125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1091.3780517578125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 4096.08740234375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1304.4462890625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 822.5621948242188 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 632.7693481445312 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 867.4949340820312 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1258.366943359375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2066.941650390625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1102.9578857421875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1550.197998046875 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1369.0218505859375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 833.5358276367188 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1871.6302490234375 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1308.72802734375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1411.0374755859375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 2958.510498046875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1026.53466796875 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1964.2840576171875 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 2056.35302734375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1733.676025390625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 761.1174926757812 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 846.6138305664062 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 2066.2880859375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 700.4956665039062 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 804.121337890625 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 802.294189453125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 775.5609741210938 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1397.24755859375 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7854, loss_val: nan, pos_over_neg: 712.2144775390625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 817.6162109375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 532.390869140625 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 651.3143310546875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 772.370361328125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 882.5807495117188 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 2509.115478515625 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 881.7333374023438 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 722.6307373046875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1243.2928466796875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1376.9095458984375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1142.26171875 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1041.1988525390625 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7871, loss_val: nan, pos_over_neg: 829.473388671875 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 599.372802734375 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 583.7250366210938 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 855.2197875976562 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 596.9999389648438 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 858.1329345703125 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1159.0791015625 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 974.2547607421875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 793.9979858398438 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 991.1483764648438 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 698.4271850585938 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 866.4871215820312 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 563.8948974609375 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 465.86090087890625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 654.92041015625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 632.42724609375 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1584.2919921875 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 761.0327758789062 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1285.3795166015625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1250.614013671875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1042.1265869140625 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1119.280029296875 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 693.47607421875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 653.0020141601562 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1155.95703125 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 760.7098999023438 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 564.642333984375 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1017.8692016601562 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 690.484619140625 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 861.564697265625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7857, loss_val: nan, pos_over_neg: 425.2555847167969 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1593.5316162109375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 822.8994750976562 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 682.7667846679688 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 808.9656982421875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 762.2722778320312 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 734.7501220703125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1128.7945556640625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1523.690673828125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 501.25372314453125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 969.9266967773438 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 780.5678100585938 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1590.15087890625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1147.110107421875 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 2797.169677734375 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 810.7481079101562 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 774.5445556640625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1544.069580078125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 840.32958984375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 777.4915771484375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1201.593017578125 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 758.6939697265625 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 520.6668701171875 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1647.8680419921875 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1094.798095703125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 519.9625244140625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 928.1497802734375 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 8065.2060546875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 941.3767700195312 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1144.685546875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 2298.4794921875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1497.33984375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1157.526123046875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 738.4834594726562 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 793.9220581054688 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1394.5426025390625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 8525.470703125 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1378.6661376953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1016.4152221679688 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1134.5206298828125 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1696.4512939453125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1169.2603759765625 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1340.822265625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 936.31689453125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 783.7926635742188 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1687.6446533203125 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1987.531005859375 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1592.0594482421875 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2445.43798828125 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1214.197265625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1159.3033447265625 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 3946.6962890625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 836.5857543945312 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7826, loss_val: nan, pos_over_neg: 556.5744018554688 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 545.6654052734375 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2031.81103515625 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 749.0364990234375 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7803, loss_val: nan, pos_over_neg: 883.973876953125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 635.7218017578125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7833, loss_val: nan, pos_over_neg: 897.2379760742188 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 3718.390380859375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 2086.417724609375 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 800.1785888671875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7796, loss_val: nan, pos_over_neg: 924.9100341796875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1639.3485107421875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 2801.644287109375 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1395.670166015625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 782.4204711914062 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 829.5516357421875 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 751.508056640625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 694.5678100585938 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 2021.28759765625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1209.050048828125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 750.406494140625 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 814.4484252929688 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 814.1019287109375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 2042.0538330078125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1329.7056884765625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.78, loss_val: nan, pos_over_neg: 548.9470825195312 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 629.2607421875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7855, loss_val: nan, pos_over_neg: 597.537109375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 868.6279296875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1124.239501953125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1064.2303466796875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 907.8082275390625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 764.5259399414062 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1154.398193359375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1245.727783203125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 938.0167846679688 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 979.33984375 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1770.701416015625 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1028.00439453125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 789.7951049804688 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 959.0387573242188 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 880.3931274414062 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 623.2890014648438 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 485.2953186035156 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1186.3099365234375 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1164.6351318359375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 837.224365234375 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 788.4578247070312 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1199.20068359375 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1131.1409912109375 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1297.8232421875 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 832.6542358398438 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1709.0908203125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1346.8035888671875 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 849.2589721679688 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 852.7382202148438 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1341.379638671875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 786.2879638671875 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1818.00390625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7811, loss_val: nan, pos_over_neg: 520.4985961914062 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 666.7454833984375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7809, loss_val: nan, pos_over_neg: 691.0364990234375 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 570.6001586914062 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.788, loss_val: nan, pos_over_neg: 657.2035522460938 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1054.951171875 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 883.1854858398438 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 666.7731323242188 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 774.9232177734375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 717.0786743164062 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 649.3096923828125 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 6887.2333984375 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1029.5550537109375 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7782, loss_val: nan, pos_over_neg: 557.3704833984375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1121.866455078125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 12825.9150390625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 627.8457641601562 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7823, loss_val: nan, pos_over_neg: 530.3848876953125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 814.0894775390625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1961.7127685546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 775.1314697265625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 707.9967041015625 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 811.2948608398438 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 881.2974243164062 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 736.9241333007812 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 950.7888793945312 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 824.9921264648438 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7772, loss_val: nan, pos_over_neg: 583.178466796875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1040.476806640625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 979.6951293945312 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 924.4324340820312 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 841.6129760742188 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 1079.9112548828125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 741.3685302734375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 611.2942504882812 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 4633.1533203125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1050.7274169921875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 650.59912109375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1315.169677734375 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2159.45751953125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 738.8565673828125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7804, loss_val: nan, pos_over_neg: 764.2434692382812 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 728.6211547851562 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1012.5321044921875 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 11983.6484375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1893.52392578125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2759.165771484375 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1560.7236328125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1934.88232421875 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1393.681640625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1924.3804931640625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 5208.466796875 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 840.040771484375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7798, loss_val: nan, pos_over_neg: 786.8656005859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7768, loss_val: nan, pos_over_neg: 898.2841186523438 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1181.796630859375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1244.605224609375 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 698.2120361328125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1369.151123046875 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1479.56494140625 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2035.6309814453125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 833.3828125 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 660.3118896484375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1229.8917236328125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 552.359375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 4105.42529296875 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1125.2568359375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7873, loss_val: nan, pos_over_neg: 466.3746032714844 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1066.258544921875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1476.8482666015625 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1308.85302734375 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1392.531005859375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1011.716064453125 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1829.5625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 986.5070190429688 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 690.496826171875 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 454.4317932128906 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 767.0706787109375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 777.7661743164062 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 958.3232421875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2378.1767578125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 933.1422119140625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1325.6787109375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 947.8327026367188 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7789, loss_val: nan, pos_over_neg: 541.6334838867188 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1791.513427734375 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1564.701904296875 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7787, loss_val: nan, pos_over_neg: 492.5316162109375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 817.7928466796875 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2127.0888671875 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.782, loss_val: nan, pos_over_neg: 696.9768676757812 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 526.275634765625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1919.093505859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 982.088623046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1310.72607421875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 643.1694946289062 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1411.4947509765625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 2013.130859375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1331.0704345703125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7792, loss_val: nan, pos_over_neg: 697.79541015625 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 1121.880126953125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1147.21240234375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 3941.111083984375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 940.47021484375 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7842, loss_val: nan, pos_over_neg: 497.22943115234375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 636.3535766601562 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1258.544921875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1722.2281494140625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1269.5042724609375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 1428.9456787109375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 624.3121337890625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1741.8720703125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1509.453857421875 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 695.9024047851562 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1004.5075073242188 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 759.6696166992188 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 5925.26220703125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1119.5374755859375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 961.3175659179688 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1080.424560546875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1140.736328125 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1396.5703125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1030.9742431640625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 820.8782958984375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 713.9854736328125 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 671.7759399414062 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 745.6565551757812 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 799.5889282226562 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1005.2032470703125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7813, loss_val: nan, pos_over_neg: 449.70831298828125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 882.5437622070312 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1344.168701171875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 833.9199829101562 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7824, loss_val: nan, pos_over_neg: 734.8881225585938 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1144.156494140625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1363.227294921875 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: -205439.046875 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: -28849.07421875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1164.5269775390625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 737.7466430664062 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 719.6798095703125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7821, loss_val: nan, pos_over_neg: 515.8712158203125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1116.267822265625 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 2079.27294921875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 571.27783203125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 715.5171508789062 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.778, loss_val: nan, pos_over_neg: 789.1732788085938 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 938.773193359375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 883.2628784179688 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 591.7420654296875 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 814.9149780273438 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7859, loss_val: nan, pos_over_neg: 436.5791320800781 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1057.8629150390625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2257.223876953125 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1079.7801513671875 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7755, loss_val: nan, pos_over_neg: 565.5315551757812 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 819.4485473632812 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1134.1063232421875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 691.8439331054688 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7849, loss_val: nan, pos_over_neg: 630.9650268554688 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.783, loss_val: nan, pos_over_neg: 556.5285034179688 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1168.05126953125 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 732.8079223632812 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 666.33203125 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 755.0948486328125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1306.2574462890625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 729.6446533203125 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 568.7118530273438 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 688.2799682617188 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 640.0827026367188 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 779.5526733398438 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 749.1198120117188 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1131.338134765625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 945.826904296875 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 704.8761596679688 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 797.2066650390625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1438.2945556640625 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 929.9468994140625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 842.4412841796875 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 703.9569091796875 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1060.5810546875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 906.0789184570312 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 695.5486450195312 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 714.27587890625 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 667.6702270507812 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 764.3971557617188 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 873.1778564453125 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1055.3455810546875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1319.4168701171875 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 682.2357177734375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7756, loss_val: nan, pos_over_neg: 554.9981079101562 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 2673.2890625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 2391.297119140625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7788, loss_val: nan, pos_over_neg: 616.3489990234375 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 624.9263916015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1255.8712158203125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1571.599365234375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7807, loss_val: nan, pos_over_neg: 441.40850830078125 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 3111.265625 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2887.06005859375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 992.3614501953125 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 610.1478881835938 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1275.6383056640625 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1966.2293701171875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1134.806884765625 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 722.0926513671875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 895.3992309570312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 530.3782348632812 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1450.676513671875 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1724.8826904296875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 812.1495361328125 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1004.9214477539062 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7808, loss_val: nan, pos_over_neg: 657.72216796875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1064.84228515625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 6030.99609375 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 872.1748046875 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7828, loss_val: nan, pos_over_neg: 565.328125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1270.5504150390625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2138.617431640625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1070.880859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 987.4310913085938 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 494.8869323730469 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 869.1771850585938 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 621.8594360351562 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 663.220703125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 698.7744750976562 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 922.3487548828125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7745, loss_val: nan, pos_over_neg: 640.789306640625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 867.5147705078125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1857.795654296875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 711.4390869140625 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 570.1812133789062 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1401.3695068359375 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 769.2960815429688 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 428.7864685058594 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 531.733642578125 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 604.4739379882812 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 668.6077270507812 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 778.5337524414062 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1199.4102783203125 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1255.686767578125 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1049.613037109375 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 871.7492065429688 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 952.1896362304688 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 628.1857299804688 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 754.5784912109375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7822, loss_val: nan, pos_over_neg: 681.4129638671875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1032.605224609375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 2850.3115234375 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 1107.376708984375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1629.1204833984375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 3066.58984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 910.53662109375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.777, loss_val: nan, pos_over_neg: 812.0999755859375 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7778, loss_val: nan, pos_over_neg: 665.7659912109375 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1820.70361328125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7831, loss_val: nan, pos_over_neg: 840.5935668945312 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 956.8451538085938 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7816, loss_val: nan, pos_over_neg: 651.5776977539062 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 742.1880493164062 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1082.7193603515625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 824.6403198242188 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1303.12353515625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1046.549560546875 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 580.8323974609375 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7773, loss_val: nan, pos_over_neg: 605.0438232421875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 655.269287109375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1020.6323852539062 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1417.58251953125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7817, loss_val: nan, pos_over_neg: 434.4497375488281 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 658.6763305664062 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 643.302490234375 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1934.238037109375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1411.640380859375 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1109.5380859375 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.779, loss_val: nan, pos_over_neg: 795.7522583007812 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1064.78564453125 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 901.7568969726562 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7802, loss_val: nan, pos_over_neg: 449.9353332519531 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1838.990478515625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1030.9764404296875 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 967.341796875 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7814, loss_val: nan, pos_over_neg: 904.8040161132812 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1211.8404541015625 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1258.8865966796875 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 986.00341796875 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1643.108154296875 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 639.79248046875 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 691.7601318359375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 3580.47216796875 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 6708.52734375 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1360.9285888671875 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 3165.318115234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: -8042.234375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 2036.619384765625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 7076.326171875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1809.25537109375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1365.929443359375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1004.7141723632812 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 785.5567626953125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1021.1776733398438 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1552.2459716796875 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 2084.927490234375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 788.9459838867188 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 903.3848876953125 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1518.34423828125 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 2776.677734375 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1275.3594970703125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1263.549072265625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1706.8380126953125 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 910.1526489257812 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 971.1410522460938 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.776, loss_val: nan, pos_over_neg: 1052.90625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 993.5390014648438 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 959.3857421875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1244.332275390625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 2081.08154296875 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2551.02978515625 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 682.73291015625 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1055.671875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1468.8114013671875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 921.1692504882812 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 790.4170532226562 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 912.9087524414062 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 899.417236328125 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 3180.155517578125 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 746.7435913085938 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 751.335693359375 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 724.7034301757812 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 898.1107177734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 3101.848876953125 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 975.6795654296875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 617.6768798828125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7742, loss_val: nan, pos_over_neg: 544.5431518554688 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 550.6539916992188 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 699.1010131835938 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1735.514892578125 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1818.6160888671875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1006.9115600585938 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7786, loss_val: nan, pos_over_neg: 796.4638671875 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 572.915771484375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2408.704345703125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1262.6676025390625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 716.73681640625 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 786.82861328125 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 931.0972900390625 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1150.4306640625 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 906.0277099609375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1180.150390625 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 787.240234375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1182.772216796875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 691.7395629882812 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1271.14794921875 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1332.0465087890625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1277.429443359375 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 888.9134521484375 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 973.9036865234375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 861.8340454101562 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 927.3185424804688 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 601.9541625976562 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 745.0863037109375 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1401.2314453125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 936.3766479492188 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 538.0115356445312 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 801.617431640625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 508.7646789550781 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7797, loss_val: nan, pos_over_neg: 636.9368286132812 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1453.224609375 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1082.063720703125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 948.968505859375 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 2159.653076171875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 568.3479614257812 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1179.4638671875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 800.9559326171875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1560.2427978515625 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1354.12451171875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 841.08154296875 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7758, loss_val: nan, pos_over_neg: 823.1797485351562 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1681.869873046875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 4205.1279296875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2640.050537109375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 776.8471069335938 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 851.9050903320312 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 860.5357055664062 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2155.4091796875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 798.7064208984375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 707.4542846679688 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1576.0994873046875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 863.2930297851562 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 649.4166870117188 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 530.300048828125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 539.5927124023438 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 660.8511352539062 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 826.841064453125 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1513.513427734375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1013.2348022460938 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 841.7986450195312 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 778.99560546875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 752.5531005859375 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7799, loss_val: nan, pos_over_neg: 703.8623046875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 807.4041748046875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1005.4273071289062 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1046.6884765625 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1373.611572265625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1063.104736328125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 2204.70654296875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1878.92822265625 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.781, loss_val: nan, pos_over_neg: 1471.5787353515625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 2344.89453125 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1004.6243286132812 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 3153.95947265625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1746.487060546875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 2271.455078125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 4128.5087890625 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 3349.4345703125 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1666.073486328125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 189611.28125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1368.6234130859375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 5851.51806640625 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1204.3375244140625 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 2047.3553466796875 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 3533.177734375 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 983.6844482421875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 2585.90673828125 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1354.372802734375 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1065.702880859375 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2627.160400390625 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 2016.7705078125 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 838.1229858398438 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1822.2574462890625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 855.04150390625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 576.5160522460938 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1372.71533203125 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1463.239990234375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 835.468994140625 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 767.57958984375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1935.797607421875 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1024.275146484375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1339.8729248046875 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3706.439453125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1288.7054443359375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 630.0732421875 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 495.4972839355469 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1156.6651611328125 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1151.0814208984375 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1095.019287109375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1367.2933349609375 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1440.717529296875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1676.62353515625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 670.6381225585938 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1349.195556640625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1914.6370849609375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1008.0099487304688 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 874.15771484375 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 982.7728881835938 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 948.3711547851562 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1412.48876953125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 3225.1826171875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1042.033935546875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1243.536376953125 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 973.7167358398438 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1167.8392333984375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1081.6602783203125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1126.2001953125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1436.68798828125 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 833.737060546875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 861.4049682617188 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1056.0081787109375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1116.366455078125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1229.590576171875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 2949.443359375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 2488.75927734375 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1558.0423583984375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1156.3740234375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 4678.57666015625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 995.10888671875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7751, loss_val: nan, pos_over_neg: 818.2212524414062 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1376.52099609375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 16838.375 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 923.460693359375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 736.8858642578125 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1192.219970703125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1970.871337890625 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 2039.9517822265625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 986.5167846679688 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1392.44384765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1841.867431640625 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1480.8260498046875 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 738.5921020507812 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 469.60882568359375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1543.03564453125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:22:09<102682:38:22, 1232.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 748.7344970703125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 901.8676147460938 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 557.8255004882812 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1104.2650146484375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1375.758056640625 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 803.4733276367188 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1367.10302734375 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 6710.83349609375 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1138.9603271484375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 742.7294311523438 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 525.0872802734375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1347.7177734375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 608.4573974609375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 686.2486572265625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1307.7728271484375 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 551.8882446289062 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1176.40625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7736, loss_val: nan, pos_over_neg: 728.2660522460938 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 981.0083618164062 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1424.677978515625 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 601.727783203125 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7741, loss_val: nan, pos_over_neg: 475.9935607910156 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1448.2958984375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1054.4375 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 886.3543090820312 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 4232.9111328125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 2136.085693359375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 3612.195556640625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2905.609130859375 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1117.4832763671875 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1142.8236083984375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1119.0802001953125 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 7150.61279296875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1175.56982421875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 753.8128051757812 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 869.8958129882812 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 918.169677734375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 974.8301391601562 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1245.6226806640625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2052.520263671875 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 13282.13671875 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7785, loss_val: nan, pos_over_neg: 679.6918334960938 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1949.4444580078125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2532.193359375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1115.51171875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1672.7855224609375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 887.918701171875 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 4560.5751953125 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1347.249267578125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 2655.384521484375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1293.4580078125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1072.6151123046875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 711.7078857421875 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 760.0161743164062 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2221.84814453125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 872.4344482421875 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1043.2294921875 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 2022.5860595703125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1355.561279296875 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 14178.673828125 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 2029.6324462890625 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2392.162841796875 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 707.2373657226562 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1492.3240966796875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3604.042236328125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1403.8929443359375 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 757.2715454101562 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 929.5112915039062 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1058.9090576171875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 2992.88134765625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 713.6763305664062 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 564.904296875 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2036.0389404296875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 2123.144287109375 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1748.515625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 518.6890869140625 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1272.8160400390625 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 972.0872192382812 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 837.3298950195312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 984.5423583984375 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 906.1917114257812 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7779, loss_val: nan, pos_over_neg: 574.3055419921875 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 955.9824829101562 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 942.1854858398438 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7806, loss_val: nan, pos_over_neg: 608.5310668945312 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7793, loss_val: nan, pos_over_neg: 918.659423828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1266.6480712890625 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1083.6468505859375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 932.2440795898438 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 617.0213623046875 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1007.819091796875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 793.2403564453125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 613.6008911132812 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1445.9744873046875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 964.0697021484375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1678.6456298828125 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 1030.3233642578125 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 608.82666015625 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7776, loss_val: nan, pos_over_neg: 530.4434814453125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 966.5938720703125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1224.594970703125 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 820.7102661132812 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 673.159423828125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.775, loss_val: nan, pos_over_neg: 845.3560791015625 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 908.736328125 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1774.52099609375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1447.55908203125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1105.476806640625 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7775, loss_val: nan, pos_over_neg: 722.50537109375 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1197.1302490234375 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 4824.0537109375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1583.0606689453125 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1304.46435546875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1300.6121826171875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1310.012939453125 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 3913.562744140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1197.234375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 2587.233154296875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 669.3886108398438 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 929.2128295898438 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1085.953125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1141.2166748046875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2633.35693359375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1233.3741455078125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1038.9837646484375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1216.6627197265625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 6667.78125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1494.1832275390625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1344.3978271484375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 6039.9501953125 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1321.3450927734375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 906.2907104492188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 735.5493774414062 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 974.9950561523438 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7697, loss_val: nan, pos_over_neg: 860.7045288085938 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 990.7117309570312 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1108.966552734375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1454.051025390625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 767.5450439453125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1265.7506103515625 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1014.106201171875 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1600.049560546875 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 638.9783325195312 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 916.2531127929688 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1284.413330078125 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1227.6424560546875 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2383.044189453125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1066.5198974609375 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 922.9237670898438 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2321.1533203125 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1527.2156982421875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1305.192138671875 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1062.6622314453125 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 968.9273681640625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7738, loss_val: nan, pos_over_neg: 828.314453125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 2518.37109375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2234.029296875 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 657.9185180664062 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 779.4990234375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2467.35595703125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1470.81884765625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 12235.9951171875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7673, loss_val: nan, pos_over_neg: 721.1306762695312 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1475.411376953125 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 5590.14892578125 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 703.1287231445312 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 3272.373046875 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1057.9522705078125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 2628.750244140625 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 3084.05859375 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1215.0213623046875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1494.084716796875 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1209.80224609375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1060.4962158203125 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 992.9089965820312 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1195.9132080078125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1336.6634521484375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1713.6734619140625 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 958.6781616210938 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1550.47412109375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 594.7557373046875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 697.7555541992188 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 629.1904907226562 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 705.4239501953125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1017.9554443359375 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7753, loss_val: nan, pos_over_neg: 721.3463745117188 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 814.632568359375 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1017.498046875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2007.443359375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 923.50390625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1704.9600830078125 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 699.2918090820312 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1163.2618408203125 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 983.1373291015625 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 792.6286010742188 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1357.036376953125 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 2149.909912109375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 3012.686767578125 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1421.5142822265625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2043.7091064453125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 958.631591796875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1808.250244140625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 979.2186889648438 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1562.4208984375 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 911.14990234375 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 993.04052734375 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7769, loss_val: nan, pos_over_neg: 998.4684448242188 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2626.325439453125 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 3959.2880859375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1188.5557861328125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7725, loss_val: nan, pos_over_neg: 742.4786987304688 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1189.4931640625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1174.4290771484375 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 3052.353515625 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.771, loss_val: nan, pos_over_neg: 2118.499755859375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 9192.884765625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 841.767578125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1507.226318359375 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 4439.88671875 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 2089.89306640625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 755.1187133789062 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1561.2357177734375 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1125.3369140625 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1148.69970703125 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 2159.873046875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1493.247802734375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1193.619384765625 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1140.258056640625 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 720.290283203125 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 2429.617431640625 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1652.6077880859375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 950.5362548828125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1096.34326171875 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 2506.06298828125 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1045.258544921875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1164.5338134765625 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1344.4771728515625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1420.6708984375 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7767, loss_val: nan, pos_over_neg: 578.7144775390625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1030.2171630859375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1300.15234375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 3047.82177734375 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1696.1124267578125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1046.005615234375 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1115.609375 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 2263.604248046875 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1022.7173461914062 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1095.3087158203125 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1167.0101318359375 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2157.601318359375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 586.25390625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 662.1341552734375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1426.845458984375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1029.1690673828125 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 908.5095825195312 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 824.8759155273438 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 9047.505859375 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2042.749755859375 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 861.737548828125 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 750.3375854492188 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 734.8958129882812 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 798.13037109375 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 858.8130493164062 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1535.924560546875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 2111.138916015625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 457.83892822265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 779.3766479492188 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 972.4922485351562 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 445.4956359863281 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 817.9152221679688 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 667.665283203125 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 768.5986938476562 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 789.6190185546875 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 662.7835693359375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 447.8181457519531 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1574.4786376953125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 883.08544921875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1354.47119140625 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 958.7202758789062 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 880.8907470703125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1176.76025390625 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 738.5750122070312 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 466.2973327636719 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 627.6717529296875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1403.52392578125 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 995.8806762695312 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 846.4400634765625 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7731, loss_val: nan, pos_over_neg: 660.2354125976562 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 894.5030517578125 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 950.8783569335938 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1620.5743408203125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1237.9718017578125 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1496.6142578125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1236.145751953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 995.4682006835938 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 12963.794921875 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1366.104736328125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1689.5203857421875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 908.5692138671875 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 2152.63671875 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1100.85888671875 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1674.4779052734375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1345.1253662109375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 933.2472534179688 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1744.2064208984375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 2795.4677734375 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1696.6409912109375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1169.18798828125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 689.6832885742188 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 704.9782104492188 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1495.292236328125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1386.427978515625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 684.6250610351562 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1345.3056640625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1485.5479736328125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1188.8343505859375 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 2387.27001953125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 892.7371826171875 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 2711.609375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1015.0193481445312 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1056.6485595703125 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1286.6640625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1269.48974609375 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 1314.0931396484375 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1374.3602294921875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1006.1714477539062 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 461.29730224609375 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 2204.46240234375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 2055.519775390625 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7732, loss_val: nan, pos_over_neg: 942.7120971679688 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1597.4302978515625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 677.7492065429688 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1467.87451171875 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1708.414306640625 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 982.9505615234375 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 652.1642456054688 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 640.42333984375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1281.1800537109375 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 797.4130859375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 915.09814453125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 989.4785766601562 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 780.3759155273438 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1393.142822265625 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1279.2789306640625 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1181.0706787109375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1054.866455078125 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 758.5352783203125 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1150.1602783203125 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 740.78955078125 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1964.107666015625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 826.7611694335938 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1104.536865234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 587.5429077148438 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1331.8173828125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 745.9234619140625 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1401.5726318359375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2384.724609375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1209.4779052734375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1193.53369140625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1363.1490478515625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2264.703857421875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 3975.544677734375 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 2745.379638671875 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 697.4594116210938 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 964.5181274414062 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 636.4296264648438 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1069.3255615234375 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 960.523681640625 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1501.895263671875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7734, loss_val: nan, pos_over_neg: 852.72119140625 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1503.513916015625 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1006.8805541992188 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7727, loss_val: nan, pos_over_neg: 673.5319213867188 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 491.07568359375 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 620.6752319335938 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7771, loss_val: nan, pos_over_neg: 890.47119140625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1191.8636474609375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1082.1693115234375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7747, loss_val: nan, pos_over_neg: 865.1937255859375 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1150.0640869140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 3611.378662109375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 2614.465087890625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 727.2978515625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 834.3551635742188 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 942.4292602539062 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1267.172119140625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7801, loss_val: nan, pos_over_neg: 715.01220703125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1336.7576904296875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 885.1602172851562 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 887.4269409179688 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 661.2731323242188 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1319.73046875 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 879.5235595703125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1008.0762939453125 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1120.08349609375 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 756.9364013671875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1826.26953125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1628.60986328125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1470.623291015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 635.1318969726562 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1045.41650390625 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1887.427490234375 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2542.1279296875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1851.7760009765625 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1795.0404052734375 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 976.6823120117188 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1468.82861328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1938.3597412109375 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 3502.5361328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 934.4776000976562 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 631.5750122070312 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1853.4345703125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1035.222900390625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1194.10205078125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 2844.963623046875 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1460.108154296875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1756.5447998046875 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1442.7322998046875 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1746.42138671875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7749, loss_val: nan, pos_over_neg: 539.2720336914062 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 891.5525512695312 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 2943.6669921875 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1639.8717041015625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 780.1160888671875 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 868.0875854492188 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 950.2289428710938 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1233.8321533203125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 929.949951171875 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1083.24609375 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1577.9312744140625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 659.216552734375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 758.5516357421875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1186.956298828125 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1310.4210205078125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1262.1622314453125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1130.31201171875 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1009.4203491210938 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 4501.166015625 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 2785.3427734375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1035.648681640625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1240.92724609375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2257.418701171875 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1547.155517578125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7791, loss_val: nan, pos_over_neg: 729.0008544921875 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1152.9468994140625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1800.446044921875 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 932.8726196289062 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 2022.8480224609375 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 2048.437255859375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 838.9887084960938 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1503.0379638671875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1518.0650634765625 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 929.6032104492188 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1085.1114501953125 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1169.8414306640625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1270.3145751953125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1635.1422119140625 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1808.4197998046875 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 858.199462890625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 3012.759033203125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1433.7811279296875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 868.5186157226562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 3061.485107421875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 976.3807373046875 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1287.510498046875 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7723, loss_val: nan, pos_over_neg: 641.1043701171875 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1826.9110107421875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 5642.8330078125 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 777.8352661132812 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 4869.84326171875 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 4477.23583984375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 984.1044921875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1188.17333984375 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1975.469970703125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 1095.2144775390625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1826.8807373046875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 2167.359130859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 661.4974975585938 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1134.2657470703125 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1129.3463134765625 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1206.322021484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1028.6793212890625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7784, loss_val: nan, pos_over_neg: 679.0088500976562 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7698, loss_val: nan, pos_over_neg: 867.068603515625 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2254.47412109375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1566.5771484375 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1695.357177734375 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1660.4617919921875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1700.3060302734375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1362.0162353515625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 2650.78759765625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7739, loss_val: nan, pos_over_neg: 661.961181640625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1364.015869140625 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 2661.300048828125 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1136.03125 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 623.272705078125 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 492.38934326171875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1242.281005859375 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1584.7626953125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1684.0623779296875 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1094.9854736328125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1208.6981201171875 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1076.790283203125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 769.0874633789062 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 765.6327514648438 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 3374.776611328125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1248.212646484375 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1018.9032592773438 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 841.6666259765625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 617.8176879882812 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1665.0528564453125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1725.2569580078125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 795.7880859375 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1342.7259521484375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1191.5009765625 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1034.9278564453125 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1854.7701416015625 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7744, loss_val: nan, pos_over_neg: 731.19580078125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1422.8663330078125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1133.669921875 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1296.6712646484375 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 937.7479248046875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1373.6690673828125 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1623.1051025390625 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1225.197998046875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1419.702880859375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 849.2891235351562 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1407.3189697265625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 809.3185424804688 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1433.5057373046875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1471.2618408203125 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 729.2350463867188 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1106.223388671875 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1222.0611572265625 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1493.216552734375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 572.2772216796875 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1310.04150390625 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1702.63037109375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 538.8256225585938 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 896.8056640625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 832.2056274414062 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1322.6103515625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 785.028564453125 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 545.6634521484375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1456.997802734375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 762.082275390625 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1075.81884765625 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 847.9362182617188 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 920.1151123046875 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1167.4310302734375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 664.0087280273438 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 722.0618896484375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1050.28955078125 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1509.78662109375 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 3032.078857421875 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 721.7933959960938 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1387.164306640625 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1327.941650390625 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 807.9834594726562 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 889.473388671875 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 525.4693603515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7683, loss_val: nan, pos_over_neg: 851.1768188476562 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1561.587158203125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1252.7886962890625 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 736.8606567382812 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1113.1085205078125 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 805.9288940429688 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 791.4060668945312 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1093.053955078125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1571.13525390625 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1425.1834716796875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 777.7444458007812 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1569.6680908203125 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 670.9677124023438 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 736.3327026367188 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1002.1647338867188 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1270.4578857421875 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1888.1710205078125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1388.4945068359375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1127.2255859375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 820.5020751953125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 899.8850708007812 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 747.9330444335938 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1151.4703369140625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1872.404052734375 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 809.50830078125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1101.932861328125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 771.3276977539062 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1892.883544921875 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1136.0501708984375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1550.89013671875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1431.4761962890625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1648.434814453125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1607.191650390625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1635.57080078125 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 799.336181640625 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 554.3214111328125 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1655.5008544921875 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1104.6199951171875 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1510.287109375 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 2420.089111328125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 766.0174560546875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1307.0601806640625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1053.7425537109375 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 923.0089721679688 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1621.282470703125 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 703.5319213867188 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1098.2008056640625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7716, loss_val: nan, pos_over_neg: 984.2603149414062 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1250.2852783203125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1058.3323974609375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7729, loss_val: nan, pos_over_neg: 609.8001708984375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1333.6776123046875 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1464.69921875 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1104.4508056640625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1111.3948974609375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1134.0089111328125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1152.104248046875 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 670.9778442382812 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1196.7694091796875 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 2003.7081298828125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1840.407470703125 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1411.053955078125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 994.5663452148438 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7693, loss_val: nan, pos_over_neg: 3092.0791015625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 2483.9873046875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 783.7772827148438 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1544.3787841796875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 534.0106811523438 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 642.9960327148438 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 736.450439453125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7714, loss_val: nan, pos_over_neg: 938.7559204101562 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 521.6334838867188 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 767.4898681640625 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 975.4611206054688 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7706, loss_val: nan, pos_over_neg: 730.808349609375 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 538.6937255859375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1053.801025390625 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 936.7501220703125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 730.9293823242188 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 956.5938720703125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 563.1969604492188 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 828.0647583007812 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1270.812255859375 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1407.6107177734375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 827.4957275390625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1124.7303466796875 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7728, loss_val: nan, pos_over_neg: 952.2467651367188 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 969.361572265625 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1339.8157958984375 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 932.1070556640625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 912.4450073242188 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 940.7667236328125 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1046.0325927734375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 958.4205322265625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 899.777587890625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1088.65869140625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 962.2027587890625 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1370.8121337890625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1620.3074951171875 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 3879.912109375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 871.7940673828125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 960.3407592773438 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 941.3640747070312 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 733.1467895507812 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 589.2136840820312 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 855.3900756835938 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 654.2186279296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 643.61083984375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1733.9232177734375 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1239.1732177734375 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1190.4044189453125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 833.2061767578125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1961.302978515625 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 2113.915283203125 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 728.5987548828125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 856.1949462890625 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1357.5189208984375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1266.11572265625 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1159.1033935546875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1300.5167236328125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1417.638427734375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1722.5645751953125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1165.52685546875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7688, loss_val: nan, pos_over_neg: 829.7971801757812 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1626.15771484375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1156.9578857421875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 871.5979614257812 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1517.6612548828125 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1421.1187744140625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.772, loss_val: nan, pos_over_neg: 1856.5819091796875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1576.7364501953125 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 928.5654907226562 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 871.328369140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1030.68310546875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [1:42:47<102855:46:31, 1234.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 989.275390625 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1082.8218994140625 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 915.406494140625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 910.8564453125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1154.419677734375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1458.289794921875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7777, loss_val: nan, pos_over_neg: 655.6886596679688 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 687.8790893554688 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7748, loss_val: nan, pos_over_neg: 863.7052001953125 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2132.72265625 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 563.3482055664062 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.774, loss_val: nan, pos_over_neg: 427.2876892089844 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 972.4315185546875 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 905.4677734375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7708, loss_val: nan, pos_over_neg: 824.4879760742188 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 967.3168334960938 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1914.546630859375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 5994.169921875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1355.175537109375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.77, loss_val: nan, pos_over_neg: 985.0735473632812 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 651.566162109375 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1558.783447265625 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2118.486083984375 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 788.0914306640625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 2986.544921875 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1089.4495849609375 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 770.6963500976562 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 984.9702758789062 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 3425.778076171875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1381.0570068359375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1160.6890869140625 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 2085.54443359375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1128.40625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1736.197509765625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 851.0160522460938 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 625.8845825195312 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 2081.9970703125 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 800.9121704101562 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 751.8397827148438 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 736.9684448242188 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1003.4527587890625 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 679.5204467773438 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1093.25048828125 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 750.2413330078125 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1131.4755859375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1137.7962646484375 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1167.540283203125 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1539.9853515625 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1312.92626953125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1372.10791015625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 3262.14208984375 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1112.13232421875 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1290.0972900390625 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 976.4314575195312 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2382.65771484375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2122.48828125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2910.91552734375 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1481.6890869140625 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1085.6605224609375 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2184.43896484375 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1206.8402099609375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2150.73193359375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1909.3538818359375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 3202.410400390625 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1919.286376953125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1283.2138671875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 2781.359619140625 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 639.281982421875 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1048.094970703125 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 999.8280029296875 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1249.9471435546875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1871.8414306640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 843.6961669921875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1045.151123046875 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1051.417236328125 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 833.9119262695312 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7733, loss_val: nan, pos_over_neg: 760.8103637695312 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 880.4290161132812 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 956.1514282226562 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1303.9169921875 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 674.3851928710938 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1853.4166259765625 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 716.4655151367188 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 970.799072265625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 903.927734375 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1159.0799560546875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1051.8193359375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 2426.654296875 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1567.64453125 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1866.3101806640625 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1002.3685913085938 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 2208.65283203125 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1755.626708984375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1257.4791259765625 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1477.2725830078125 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1156.0648193359375 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1097.664794921875 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 970.1340942382812 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1185.34130859375 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1033.6378173828125 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1521.8450927734375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1182.8282470703125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 2526.8466796875 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1215.4842529296875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1334.8533935546875 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.767, loss_val: nan, pos_over_neg: 1268.174072265625 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 774.0354614257812 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1265.591552734375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 896.2992553710938 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1329.8892822265625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1091.606689453125 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 631.366943359375 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 948.071044921875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 825.5357055664062 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 4713.35009765625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 802.8873291015625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1685.9404296875 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 892.22265625 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 710.7748413085938 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1065.622314453125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 729.20361328125 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1723.1141357421875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 674.329833984375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 862.544189453125 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1024.4608154296875 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 994.0318603515625 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7717, loss_val: nan, pos_over_neg: 754.632080078125 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 930.3837890625 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2754.4208984375 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 970.2490234375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 943.2605590820312 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 931.8719482421875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1086.84130859375 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1084.223388671875 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2289.213623046875 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 739.6563720703125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1053.5733642578125 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1467.62744140625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7696, loss_val: nan, pos_over_neg: 713.079345703125 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 2624.29736328125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 532.7694702148438 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 699.66015625 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 827.8613891601562 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1597.9052734375 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 904.863525390625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1371.677978515625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 881.35009765625 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1023.9158935546875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1102.523681640625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 807.16796875 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 816.5526733398438 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 811.037109375 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 883.43505859375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1148.8377685546875 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 797.38427734375 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1141.80712890625 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 934.1026000976562 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1353.4234619140625 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1540.6768798828125 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 775.97314453125 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1197.81640625 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 786.3717651367188 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 963.1790161132812 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1956.0015869140625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 966.2113647460938 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1807.5037841796875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 935.872802734375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 765.9736328125 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 1493.7987060546875 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 844.1710205078125 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1059.8897705078125 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 879.9518432617188 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 2646.165283203125 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 1813.99169921875 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1240.300537109375 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 827.2781982421875 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7722, loss_val: nan, pos_over_neg: 926.84765625 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 3677.464111328125 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1327.816162109375 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 2247.28125 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 970.7924194335938 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 83833.5078125 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1972.2821044921875 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7686, loss_val: nan, pos_over_neg: 613.70849609375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 994.5032348632812 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 36066.03125 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 936.070556640625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1239.6058349609375 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1394.4200439453125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2658.83447265625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 734.1773681640625 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7701, loss_val: nan, pos_over_neg: 482.56011962890625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7705, loss_val: nan, pos_over_neg: 977.1055297851562 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1537.7872314453125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1748.985107421875 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1639.36572265625 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7724, loss_val: nan, pos_over_neg: 862.947021484375 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 4126.40087890625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1681.5865478515625 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1284.3795166015625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1295.7845458984375 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1459.943359375 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1286.35302734375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1199.66015625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1321.8321533203125 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1469.2301025390625 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 649.6557006835938 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1853.8453369140625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 2363.92822265625 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 854.3743896484375 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 2731.150146484375 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 3931.60107421875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 3122.27685546875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1984.4925537109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1332.550537109375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1130.79248046875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1211.24951171875 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 945.9312133789062 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 536.7728271484375 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1018.818603515625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1525.75634765625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1657.6739501953125 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1398.393798828125 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 2157.018310546875 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 818.2175903320312 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1503.621826171875 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 2150.7705078125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 723.551513671875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 970.76708984375 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1920.7314453125 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1712.206787109375 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 946.4864501953125 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1520.859619140625 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2576.09912109375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1629.073974609375 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 970.7070922851562 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1807.7281494140625 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 854.5162963867188 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7709, loss_val: nan, pos_over_neg: 855.20654296875 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 3004.082763671875 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1287.95068359375 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1154.1739501953125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1273.366455078125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1391.470703125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 12147.0126953125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1784.236083984375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1010.4039306640625 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 559.5552368164062 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1706.1314697265625 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1059.262451171875 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1489.1336669921875 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1468.648193359375 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 664.6633911132812 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1300.0904541015625 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1545.390625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 909.909423828125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 729.4066772460938 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 621.85595703125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 652.9544067382812 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1731.80615234375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2656.38330078125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 973.8614501953125 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 649.2753295898438 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 766.079833984375 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1444.7593994140625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1054.5101318359375 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 741.7423095703125 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 757.8368530273438 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 661.6507568359375 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 675.6154174804688 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 697.9866943359375 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1433.57666015625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1015.8081665039062 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1241.199951171875 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1142.7169189453125 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1021.8717651367188 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1302.303466796875 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1350.2813720703125 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 722.58251953125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1070.0943603515625 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 827.4874877929688 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1027.8516845703125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 760.7246704101562 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1596.37451171875 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1231.7908935546875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1244.345947265625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2639.35693359375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2545.016845703125 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 3338.18994140625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1597.07763671875 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1571.61865234375 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1046.416748046875 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1308.746337890625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1025.6376953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 2790.880615234375 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1322.0020751953125 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1071.8134765625 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 994.3985595703125 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 3627.640869140625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2187.31396484375 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2200.8017578125 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 5150.49609375 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 819.1068115234375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1006.7257080078125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 865.632080078125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1317.6246337890625 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1838.0264892578125 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 669.704345703125 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1189.10205078125 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1419.555908203125 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1045.314453125 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 5323.13037109375 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7684, loss_val: nan, pos_over_neg: 559.7650146484375 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 2381.796875 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 2387.84765625 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1424.4580078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.773, loss_val: nan, pos_over_neg: 1214.92822265625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 3959.54052734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1078.947021484375 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1914.281494140625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 600.3833618164062 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1829.2781982421875 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 7192.5830078125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 760.1510009765625 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2614.5185546875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 842.9005126953125 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 991.72265625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 2338.423583984375 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1398.3099365234375 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 767.22607421875 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1449.978271484375 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1749.3582763671875 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1464.4287109375 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1469.9366455078125 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2424.69873046875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1415.8800048828125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 899.9676513671875 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1043.533447265625 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1125.7294921875 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1569.80078125 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 3074.292724609375 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 2394.560546875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1487.2860107421875 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 652.9930419921875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1056.031005859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1774.4051513671875 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1056.1912841796875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1408.447021484375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1090.3582763671875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1024.8720703125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1596.325439453125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1268.8944091796875 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7651, loss_val: nan, pos_over_neg: 787.1757202148438 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 730.5264892578125 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 53305.7734375 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1128.177490234375 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1373.56787109375 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1055.42724609375 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1163.062744140625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2846.905029296875 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.769, loss_val: nan, pos_over_neg: 1949.943603515625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1182.9744873046875 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 422.2174377441406 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2030.34423828125 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 3546.630859375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1698.048583984375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 957.6619873046875 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1608.0589599609375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 807.6824951171875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1019.571533203125 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 6496.14794921875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7695, loss_val: nan, pos_over_neg: 759.3355102539062 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 598.3961791992188 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 731.6114501953125 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1399.8900146484375 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1240.7816162109375 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 3715.230712890625 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 2591.4072265625 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 924.5283203125 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 946.3133544921875 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 776.4183959960938 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 3121.04541015625 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 951.6231079101562 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 636.888916015625 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1020.4911499023438 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1188.57470703125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1341.3956298828125 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1013.3095092773438 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 702.113037109375 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 1068.768798828125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2218.3623046875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 966.9973754882812 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 477.0714111328125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 751.15625 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2707.7451171875 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 598.4708862304688 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 530.0184936523438 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 820.35546875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 733.3795166015625 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1149.282958984375 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 957.4768676757812 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 684.2018432617188 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7652, loss_val: nan, pos_over_neg: 756.4097290039062 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 905.913818359375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7657, loss_val: nan, pos_over_neg: 957.9996337890625 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 572.5060424804688 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 915.0185546875 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1049.2877197265625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2222.142578125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1100.7655029296875 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1027.5252685546875 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1067.1373291015625 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1024.74658203125 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 981.2808227539062 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 910.9939575195312 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2210.873046875 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1042.604736328125 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1335.8282470703125 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 951.2456665039062 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 745.273681640625 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 979.9985961914062 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 851.9605102539062 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1190.9136962890625 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 576.3368530273438 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1623.3553466796875 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 1026.6898193359375 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1561.7442626953125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 491.8150634765625 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1538.1824951171875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1241.818359375 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 774.9906005859375 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1227.853759765625 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 725.1881713867188 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1495.08447265625 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1472.4412841796875 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1151.544677734375 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2934.8623046875 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 735.30224609375 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 1537.8450927734375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 2152.431640625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 743.23046875 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1159.3057861328125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 1217.126953125 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 630.4808349609375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 925.85400390625 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 797.9061889648438 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 983.778564453125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 2120.54150390625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 664.943115234375 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 862.5804443359375 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1104.53369140625 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 703.5613403320312 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1852.301025390625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7699, loss_val: nan, pos_over_neg: 633.7988891601562 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1039.2720947265625 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 851.7130737304688 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1299.661865234375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 953.5660400390625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 584.0613403320312 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 848.171142578125 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1112.2869873046875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1361.2003173828125 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 778.4796752929688 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 651.7364501953125 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 826.8717651367188 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 697.911865234375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1409.3250732421875 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1018.7833862304688 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.768, loss_val: nan, pos_over_neg: 545.4328002929688 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 555.51953125 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 891.6456909179688 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1340.1400146484375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1781.2100830078125 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 942.294189453125 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 482.3736267089844 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1372.6746826171875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1241.868408203125 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 740.94091796875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1871.1884765625 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1744.6982421875 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1206.7867431640625 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 787.601806640625 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1085.0286865234375 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1127.0472412109375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1661.600830078125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 598.5569458007812 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 701.1778564453125 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 585.4924926757812 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 751.5436401367188 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1614.6849365234375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 4730.2666015625 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 841.2378540039062 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 911.6634521484375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2706.484619140625 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 931.7800903320312 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1177.9322509765625 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1123.685546875 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 913.186279296875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 995.8551025390625 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 805.1267700195312 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 844.0360107421875 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1130.5218505859375 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1652.8245849609375 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1298.420166015625 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1068.0347900390625 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1095.228515625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1980.291015625 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1054.8603515625 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1516.833984375 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 890.25927734375 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 879.100830078125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7641, loss_val: nan, pos_over_neg: 952.6039428710938 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1895.2197265625 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 2390.411865234375 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1689.868896484375 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1490.6617431640625 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1129.2276611328125 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 838.6276245117188 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1815.487548828125 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1090.679931640625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 723.6030883789062 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 721.46826171875 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 795.0353393554688 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1291.314208984375 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.764, loss_val: nan, pos_over_neg: 3568.11669921875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 5352.03759765625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 7215.1083984375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 3072.64697265625 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1802.6070556640625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1805.732177734375 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1270.1785888671875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 851.5174560546875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1396.4310302734375 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 918.8397827148438 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 2666.93359375 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 2128.271728515625 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 2421.939453125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 920.603271484375 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 598.9175415039062 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2088.701416015625 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1185.73291015625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1359.744140625 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1472.8284912109375 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1123.90380859375 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 539.5425415039062 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1161.2979736328125 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 930.9054565429688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1473.6580810546875 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 919.4141235351562 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1353.39501953125 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1292.6717529296875 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1700.2373046875 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 664.3422241210938 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1505.569580078125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 903.3683471679688 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 911.440185546875 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1016.9850463867188 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 908.513671875 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1200.7010498046875 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7654, loss_val: nan, pos_over_neg: 990.2457275390625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1942.8907470703125 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1782.9442138671875 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1416.53173828125 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 799.3329467773438 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 573.8206176757812 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1130.6619873046875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2203.62158203125 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1126.21630859375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 492.5735168457031 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 621.7310180664062 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 2625.900634765625 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 2499.98828125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1282.84326171875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 657.4811401367188 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 817.9684448242188 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1830.0499267578125 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1018.9073486328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1649.8670654296875 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1306.0159912109375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 856.865234375 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1189.1553955078125 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 918.94287109375 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 920.5428466796875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1056.2061767578125 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1345.7457275390625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 832.1021728515625 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1910.96533203125 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 899.4896240234375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1440.4703369140625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2225.625732421875 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2002.9033203125 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1979.6314697265625 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1498.3189697265625 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 780.8570556640625 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1010.4909057617188 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1239.975830078125 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1699.24462890625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1088.991943359375 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1312.3251953125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1322.6483154296875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 944.3505859375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1789.195556640625 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 558.7836303710938 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2149.2060546875 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1438.9969482421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1725.830810546875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 4486.9443359375 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 2144.77685546875 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1855.4730224609375 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1900.119873046875 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1881.0545654296875 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 5590.03662109375 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 885.84619140625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 961.53173828125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7663, loss_val: nan, pos_over_neg: 777.700439453125 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2232.450927734375 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1743.1669921875 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1151.2667236328125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 948.1024780273438 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1028.3505859375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 903.7826538085938 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 2180.90771484375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1858.8175048828125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 827.4512939453125 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1000.4143676757812 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 3028.107666015625 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1600.85302734375 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1154.8531494140625 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7664, loss_val: nan, pos_over_neg: 867.5972290039062 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 762.65283203125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1370.7069091796875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1683.2557373046875 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 15606.005859375 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2233.564208984375 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1467.5506591796875 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2731.741455078125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2184.067138671875 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1457.7657470703125 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1379.8438720703125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1984.9757080078125 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2144.763916015625 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2078.241455078125 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 935.10009765625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1133.063720703125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1594.5906982421875 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1786.2320556640625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2053.1728515625 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 964.3231201171875 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 768.2776489257812 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 834.1328125 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1264.177001953125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1527.5130615234375 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1935.3218994140625 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1007.4240112304688 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1797.9947509765625 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1362.0198974609375 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1302.7890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 992.6076049804688 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2228.16748046875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1161.985595703125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 943.6344604492188 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1805.016845703125 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 804.2469482421875 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1028.849365234375 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1817.1373291015625 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1180.2164306640625 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 2366.4970703125 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1587.1717529296875 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1880.560302734375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 2643.3134765625 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1234.0845947265625 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2386.993408203125 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 721.7671508789062 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2046.71875 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 763.3319091796875 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1545.482421875 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1072.6849365234375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1069.089599609375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 952.779296875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 869.623779296875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 908.4981689453125 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1686.755615234375 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 801.8629150390625 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1712.5098876953125 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1219.7037353515625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1366.2503662109375 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 896.3142700195312 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 559.8462524414062 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1269.6558837890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1512.1488037109375 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1596.5145263671875 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1008.8704223632812 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1191.6324462890625 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1186.525146484375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1750.94970703125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:03:27<103016:14:45, 1236.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 873.6111450195312 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1001.1782836914062 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1692.8115234375 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2364.4619140625 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1294.3135986328125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2074.734130859375 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 994.3507690429688 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 849.922119140625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1571.00927734375 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 4690.46142578125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 683.06201171875 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1296.1494140625 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1658.140625 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1739.4447021484375 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 885.9360961914062 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 919.2357177734375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7655, loss_val: nan, pos_over_neg: 484.09039306640625 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1547.0213623046875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1287.316162109375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 821.6377563476562 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2454.17919921875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1010.5321044921875 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1078.6817626953125 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1797.5504150390625 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1137.61962890625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2606.22900390625 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2764.5927734375 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 708.8939819335938 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1385.9215087890625 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2565.91015625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 828.4259033203125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 606.0690307617188 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 772.73974609375 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1317.1201171875 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1992.02734375 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1539.600341796875 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1208.476318359375 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 746.7572021484375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 806.095703125 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1387.2989501953125 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1102.8060302734375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1964.92236328125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1200.3446044921875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1679.365478515625 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1175.422607421875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1030.0850830078125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1011.0709838867188 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 760.3128051757812 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1746.3797607421875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 8990.255859375 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 825.57763671875 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 758.1635131835938 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 4282.095703125 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1410.9346923828125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 968.8624877929688 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2444.317626953125 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 5605.6845703125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 6458.61328125 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 746.2510375976562 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1466.57666015625 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2258.960693359375 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1128.066162109375 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1289.859375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1140.0203857421875 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1262.972412109375 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1241.1971435546875 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1472.2386474609375 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1635.245849609375 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1573.3524169921875 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 775.0615234375 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 974.6107788085938 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 842.7608642578125 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 975.0369873046875 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 665.1043090820312 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 841.75634765625 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 718.4864501953125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1096.070068359375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 748.2182006835938 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 397.2931823730469 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1598.576416015625 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1481.82958984375 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 938.421142578125 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 761.5499877929688 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7638, loss_val: nan, pos_over_neg: 521.1575317382812 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1587.24560546875 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 828.3221435546875 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 861.7503662109375 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 839.0028686523438 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 480.5797119140625 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 666.745849609375 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 666.3955078125 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1613.713134765625 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 994.7982177734375 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 960.208984375 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7633, loss_val: nan, pos_over_neg: 747.3460083007812 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 898.8214721679688 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 2040.1441650390625 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 978.3845825195312 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 681.4033203125 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1100.0406494140625 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 837.9951171875 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1571.676513671875 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2466.8037109375 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 5569.982421875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1593.517822265625 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7458, loss_val: nan, pos_over_neg: 2690.4111328125 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 3017.048828125 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1799.4505615234375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1551.1324462890625 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3604.86376953125 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2710.29833984375 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1626.9248046875 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1134.627685546875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2195.98583984375 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1158.8658447265625 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1308.515869140625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 9379.109375 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1208.8426513671875 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1017.4815063476562 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1577.6759033203125 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1220.164794921875 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 909.6528930664062 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1852.3489990234375 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1216.59619140625 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 710.3915405273438 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 977.4263305664062 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 628.1080322265625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1356.392578125 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1147.5914306640625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1287.9720458984375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1620.0191650390625 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 863.8366088867188 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1869.5806884765625 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1608.251708984375 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 770.2650146484375 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1054.4423828125 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 4032.7646484375 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1168.364990234375 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1460.633056640625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7678, loss_val: nan, pos_over_neg: 899.8944702148438 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 792.8060913085938 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1398.609130859375 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1196.906005859375 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1869.2935791015625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 2385.160888671875 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1897.187744140625 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1581.8282470703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1202.2391357421875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 977.7937622070312 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 1128.0335693359375 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 4518.126953125 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1842.5718994140625 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1311.918701171875 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 2042.294189453125 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 3117.846435546875 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 876.098388671875 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1539.925537109375 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1364.8526611328125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1158.753662109375 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1358.8499755859375 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1185.243896484375 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1677.607666015625 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1021.6151123046875 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1680.8677978515625 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2726.363525390625 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1539.8065185546875 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1511.882568359375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 2794.61083984375 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 618.266845703125 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 3067.37890625 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 842.6199340820312 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1138.045166015625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1173.0966796875 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1367.208984375 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1670.73388671875 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 814.493408203125 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1464.9443359375 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1965.67529296875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1107.4432373046875 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1527.16943359375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1537.0565185546875 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1864.1138916015625 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 951.468505859375 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1909.654052734375 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2283.778076171875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.765, loss_val: nan, pos_over_neg: 1258.6243896484375 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2153.580322265625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 2291.49169921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1774.552001953125 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7636, loss_val: nan, pos_over_neg: 920.326171875 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 6217.755859375 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 4408.3408203125 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1257.17724609375 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 849.9828491210938 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1770.859619140625 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1047.7083740234375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2029.4683837890625 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1259.8306884765625 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1274.946533203125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2181.97314453125 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1434.175048828125 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1729.7926025390625 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1431.5592041015625 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1842.107666015625 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1431.9217529296875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 897.9053955078125 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 822.4553833007812 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 875.9283447265625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1022.5569458007812 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1069.68798828125 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 545.7879638671875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 821.14697265625 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 961.2678833007812 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1080.707763671875 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1295.1058349609375 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 737.08056640625 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3857.805908203125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1994.4951171875 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1615.641845703125 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1443.1517333984375 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 2047.0130615234375 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1679.4041748046875 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 2693.915771484375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1372.995849609375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1586.1591796875 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 905.7203979492188 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1065.66845703125 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1088.5682373046875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1252.36181640625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1505.56396484375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1545.97900390625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2475.8466796875 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.7644, loss_val: nan, pos_over_neg: 845.0641479492188 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 15948.353515625 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 2004.3299560546875 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 871.0101318359375 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 989.626220703125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1042.9571533203125 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 910.431884765625 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1048.7833251953125 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1362.416259765625 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1788.307861328125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2060.447509765625 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1793.0738525390625 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 652.9054565429688 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1212.624755859375 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1023.0336303710938 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1669.5191650390625 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 948.5424194335938 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 988.4583740234375 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2768.170166015625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2007.378173828125 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1135.1146240234375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 876.018310546875 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1524.1715087890625 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1106.4151611328125 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 964.975830078125 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 794.171142578125 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1769.2327880859375 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 744.90283203125 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 592.5108642578125 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7646, loss_val: nan, pos_over_neg: 647.1326293945312 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 853.6797485351562 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 962.3970336914062 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 812.8355712890625 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 608.3580322265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1092.144775390625 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 719.3253784179688 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1442.9578857421875 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 646.6427001953125 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 730.0453491210938 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1713.81884765625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1534.2550048828125 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 811.1966552734375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 856.8004760742188 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 899.6749267578125 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1380.9954833984375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 715.8616943359375 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1064.0108642578125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1711.6475830078125 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1083.8125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1277.779052734375 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1166.4705810546875 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 910.3433227539062 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1065.2529296875 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 8567.3916015625 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 605.25927734375 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 868.3786010742188 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 785.4967041015625 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 993.206298828125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1564.762939453125 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 906.6118774414062 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2126.928955078125 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 864.1095581054688 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 858.0945434570312 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1330.32568359375 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1313.83203125 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 984.4632568359375 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1051.2806396484375 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1798.134033203125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1343.8355712890625 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 399.6438293457031 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1179.68505859375 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1517.43017578125 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1437.991455078125 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1605.116455078125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1027.236083984375 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1243.922119140625 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 3061.663818359375 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1141.0740966796875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1623.1629638671875 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1093.2803955078125 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1754.0933837890625 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 2903.9697265625 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1056.9581298828125 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1400.9288330078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 538.0557861328125 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1610.1339111328125 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1212.347900390625 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 979.7626953125 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1706.3309326171875 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 643.11474609375 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 739.050048828125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1768.14794921875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1228.6982421875 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1466.735107421875 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.766, loss_val: nan, pos_over_neg: 1773.441650390625 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 887.6703491210938 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 936.6661987304688 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2014.792236328125 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1609.3004150390625 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 10126.572265625 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1583.022705078125 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 852.7933349609375 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 758.3748168945312 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 14687.2763671875 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2183.261474609375 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1869.24658203125 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1131.035400390625 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 605.9243774414062 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1248.8756103515625 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1429.199951171875 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7622, loss_val: nan, pos_over_neg: 625.2415771484375 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7647, loss_val: nan, pos_over_neg: 798.9708862304688 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1769.3720703125 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1708.5198974609375 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 866.7662963867188 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 4516.8046875 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1165.6180419921875 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1491.1337890625 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1171.077392578125 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1096.7708740234375 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.7648, loss_val: nan, pos_over_neg: 744.8701782226562 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 3163.90234375 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1820.7862548828125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1449.2374267578125 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2186.59228515625 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 831.6201171875 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 3898.259521484375 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1667.922119140625 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1664.603515625 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 769.2487182617188 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 853.7476806640625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 812.611572265625 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1436.4915771484375 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1096.1649169921875 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 3132.18798828125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 3575.47802734375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1019.8006591796875 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 692.0595092773438 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1190.8734130859375 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2223.649658203125 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 940.659423828125 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 802.121826171875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 663.9239501953125 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 686.8428955078125 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 21797.6328125 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 3731.407470703125 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 929.2799072265625 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 2216.244140625 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1973.709716796875 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 2405.4990234375 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2836.7890625 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 617.0778198242188 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1606.478515625 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1198.520751953125 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 3653.814697265625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1825.1748046875 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2535.172119140625 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1062.4649658203125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1476.95849609375 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7642, loss_val: nan, pos_over_neg: 914.7738037109375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 4142.47705078125 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 3509.741943359375 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 894.1863403320312 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1217.7850341796875 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2229.087158203125 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1472.2869873046875 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1266.8858642578125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 529.5694580078125 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2051.85693359375 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 724.828857421875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1075.6571044921875 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 910.4902954101562 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1635.0350341796875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2259.82958984375 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 800.2291870117188 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1221.611572265625 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 710.8360595703125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 957.1769409179688 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 950.766845703125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 775.5917358398438 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1770.734619140625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1506.0821533203125 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1615.6663818359375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1048.4688720703125 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 4174.17822265625 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1173.4820556640625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7669, loss_val: nan, pos_over_neg: 750.2515869140625 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1360.302734375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1129.553466796875 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 2612.05712890625 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 897.5403442382812 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1595.370849609375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 1749.977783203125 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 759.5125732421875 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2318.16064453125 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1369.1436767578125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1667.12158203125 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 1787.0582275390625 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1055.1016845703125 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 769.413818359375 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 969.3609008789062 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2446.720703125 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 2143.2109375 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1925.3876953125 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 790.2194213867188 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1457.535888671875 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 873.3627319335938 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 3186.31396484375 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 4385.78564453125 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 3975.406005859375 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1181.8485107421875 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1016.8645629882812 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 800.8934326171875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 923.0261840820312 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 990.203125 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1867.3592529296875 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1388.401611328125 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1731.06689453125 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1535.8426513671875 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1793.5155029296875 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2395.6806640625 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 906.287109375 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 959.5691528320312 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 5290.61083984375 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 735.5076904296875 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 885.4547729492188 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 694.1685791015625 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 813.0731201171875 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1216.4595947265625 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 818.9180297851562 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1237.1275634765625 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 616.3597412109375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 588.5098266601562 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1637.486083984375 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 873.6725463867188 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 828.0097045898438 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 665.8207397460938 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 876.51318359375 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1242.8182373046875 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 906.035888671875 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 698.0647583007812 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7649, loss_val: nan, pos_over_neg: 625.5240478515625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1629.9512939453125 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 954.0001220703125 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 900.6688232421875 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1353.9638671875 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 962.3065795898438 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 2707.040771484375 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 818.1135864257812 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 649.3010864257812 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1046.2017822265625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 885.6163940429688 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1041.4844970703125 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 865.3157348632812 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 718.1895141601562 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 765.5223388671875 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 682.24462890625 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 695.3031005859375 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1102.454833984375 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1139.468994140625 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1401.224365234375 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 719.0543823242188 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 823.9302368164062 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 583.181396484375 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 4586.490234375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 896.1027221679688 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2822.368896484375 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1211.6715087890625 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2346.587646484375 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 744.30322265625 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1051.0953369140625 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2265.5361328125 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1480.449951171875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1147.1519775390625 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 3906.236328125 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1092.9522705078125 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1765.6656494140625 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1295.28857421875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 771.7182006835938 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1176.165283203125 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 2404.8125 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 676.1254272460938 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 882.237060546875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1413.087646484375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 983.2943725585938 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1109.134033203125 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 668.7233276367188 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1191.585205078125 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1666.5391845703125 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1243.8089599609375 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1708.92138671875 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 799.6163330078125 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1246.599365234375 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3846.418212890625 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1079.474365234375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 3862.571044921875 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1119.9754638671875 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 828.3701782226562 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1669.1402587890625 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1117.466064453125 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7643, loss_val: nan, pos_over_neg: 3127.802978515625 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1680.6107177734375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 970.304931640625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 829.3383178710938 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 698.6041870117188 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 813.2401123046875 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 746.6580810546875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1234.233154296875 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1274.0726318359375 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2149.138671875 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 844.6229858398438 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1603.953125 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7623, loss_val: nan, pos_over_neg: 2414.2578125 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1463.5135498046875 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1425.46630859375 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1189.3770751953125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2396.1708984375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 28398.59375 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 7656.9208984375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1284.394287109375 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 856.9650268554688 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 819.8449096679688 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 677.1129150390625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 954.3848876953125 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 988.6032104492188 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1203.7044677734375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 3683.0048828125 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1681.8316650390625 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 667.228515625 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 2327.066650390625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7608, loss_val: nan, pos_over_neg: 984.6201171875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 838.2562866210938 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1049.729736328125 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1058.5123291015625 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7626, loss_val: nan, pos_over_neg: 928.3781127929688 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 3021.816162109375 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 779.4986572265625 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 873.4625854492188 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2253.695068359375 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1471.9669189453125 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1018.174560546875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1960.8311767578125 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1198.4503173828125 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 10640.9375 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1911.4385986328125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 820.2052612304688 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2085.857177734375 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 1422.358154296875 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1215.6597900390625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1775.978759765625 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 964.4110107421875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1107.748779296875 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 2175.699951171875 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1584.3389892578125 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 735.0927734375 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 823.83349609375 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7665, loss_val: nan, pos_over_neg: 847.1160278320312 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 992.7816162109375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1132.1751708984375 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1930.2645263671875 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1866.251220703125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1000.958740234375 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7614, loss_val: nan, pos_over_neg: 2042.8153076171875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1633.0916748046875 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1008.7660522460938 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1411.0484619140625 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1814.204833984375 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1296.57421875 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 11914.8359375 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 913.5634155273438 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1220.13134765625 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1038.311767578125 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1823.3914794921875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 4236.05029296875 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 892.1350708007812 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1020.9651489257812 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 1233.89453125 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1818.8692626953125 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 3854.0380859375 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1640.811279296875 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1757.5533447265625 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1036.924072265625 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1072.7509765625 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1614.7254638671875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1533.4473876953125 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1891.968017578125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 886.9717407226562 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1940.1075439453125 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 16567.078125 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1006.9762573242188 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 847.7783203125 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 675.4064331054688 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 841.6568603515625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1617.6522216796875 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1266.801513671875 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1283.623046875 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 722.2842407226562 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 904.0634765625 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1096.18798828125 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1950.267333984375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 995.953369140625 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1092.0692138671875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1452.0733642578125 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 654.6988525390625 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1893.0277099609375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1285.480224609375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1377.7939453125 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1072.7728271484375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1486.99609375 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1870.21728515625 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1296.6119384765625 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1466.4501953125 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1506.8997802734375 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 969.41064453125 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 851.6752319335938 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7628, loss_val: nan, pos_over_neg: 933.4378662109375 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 966.8927612304688 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1574.35009765625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1864.1466064453125 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1605.4022216796875 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 913.7054443359375 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1326.595458984375 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.759, loss_val: nan, pos_over_neg: 1173.5770263671875 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7677, loss_val: nan, pos_over_neg: 497.4517822265625 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 887.4149780273438 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 895.7008666992188 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 625.6744384765625 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 812.0703125 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 960.6787109375 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 510.5201416015625 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1989.03173828125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 763.5259399414062 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 502.74896240234375 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 523.2550659179688 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1306.33056640625 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1487.38330078125 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1182.1204833984375 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 991.0300903320312 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 714.7177124023438 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 2810.636474609375 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1168.82373046875 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 763.1903686523438 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1045.8958740234375 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1591.4844970703125 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1145.7044677734375 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 898.8038940429688 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1212.4794921875 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1448.1563720703125 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1149.09765625 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1245.2841796875 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 822.3527221679688 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1138.006591796875 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1008.1268310546875 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 780.8212280273438 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1171.83544921875 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1343.59765625 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1240.6011962890625 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1048.3885498046875 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 991.8350219726562 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 740.7175903320312 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2464.396484375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 2204.53271484375 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1606.9864501953125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [2:24:06<103100:06:15, 1237.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 2822.497314453125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1018.8573608398438 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 791.387451171875 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1578.17578125 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1226.47802734375 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1232.1617431640625 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1520.988525390625 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 863.732421875 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 863.0729370117188 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1285.159423828125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1175.7591552734375 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1916.1668701171875 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1054.366943359375 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1920.406494140625 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1257.7867431640625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 12816.306640625 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 2043.0635986328125 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1668.475341796875 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 4865.109375 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1164.279296875 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1822.5164794921875 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1486.491943359375 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 557.4087524414062 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.762, loss_val: nan, pos_over_neg: 719.6631469726562 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 948.5435791015625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 885.4020385742188 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1364.224853515625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1055.0201416015625 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 519.1239013671875 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 771.0208740234375 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7621, loss_val: nan, pos_over_neg: 962.415771484375 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1337.2552490234375 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 2169.371337890625 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1075.1832275390625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 752.596435546875 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 791.1732788085938 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1299.6678466796875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1144.4427490234375 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 852.0728759765625 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 817.39208984375 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 831.8389892578125 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1976.6534423828125 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 2421.1640625 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1891.443359375 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 991.28466796875 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1021.4020385742188 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 869.5847778320312 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7666, loss_val: nan, pos_over_neg: 848.5510864257812 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1063.5098876953125 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1821.0166015625 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1942.4180908203125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1258.7930908203125 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 861.6943359375 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1153.959716796875 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1039.538818359375 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1875.9466552734375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1079.6710205078125 lr: 0.00031623\n",
      "Iter: 57/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1202.54296875 lr: 0.00031623\n",
      "Iter: 58/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 833.2764892578125 lr: 0.00031623\n",
      "Iter: 59/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3411.69921875 lr: 0.00031623\n",
      "Iter: 60/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1121.0137939453125 lr: 0.00031623\n",
      "Iter: 61/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1101.2052001953125 lr: 0.00031623\n",
      "Iter: 62/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 2285.208984375 lr: 0.00031623\n",
      "Iter: 63/695, loss_train: 5.7589, loss_val: nan, pos_over_neg: 878.3258666992188 lr: 0.00031623\n",
      "Iter: 64/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 1190.135986328125 lr: 0.00031623\n",
      "Iter: 65/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1477.6085205078125 lr: 0.00031623\n",
      "Iter: 66/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1118.66748046875 lr: 0.00031623\n",
      "Iter: 67/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 762.2597045898438 lr: 0.00031623\n",
      "Iter: 68/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1749.353271484375 lr: 0.00031623\n",
      "Iter: 69/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 3202.76025390625 lr: 0.00031623\n",
      "Iter: 70/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 2446.0966796875 lr: 0.00031623\n",
      "Iter: 71/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2101.64306640625 lr: 0.00031623\n",
      "Iter: 72/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1644.5804443359375 lr: 0.00031623\n",
      "Iter: 73/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1413.3028564453125 lr: 0.00031623\n",
      "Iter: 74/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 2116.9873046875 lr: 0.00031623\n",
      "Iter: 75/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1458.0830078125 lr: 0.00031623\n",
      "Iter: 76/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1050.3695068359375 lr: 0.00031623\n",
      "Iter: 77/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1524.1578369140625 lr: 0.00031623\n",
      "Iter: 78/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 739.6716918945312 lr: 0.00031623\n",
      "Iter: 79/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2640.0283203125 lr: 0.00031623\n",
      "Iter: 80/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1048.150390625 lr: 0.00031623\n",
      "Iter: 81/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 708.2095336914062 lr: 0.00031623\n",
      "Iter: 82/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 683.55126953125 lr: 0.00031623\n",
      "Iter: 83/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 2690.408447265625 lr: 0.00031623\n",
      "Iter: 84/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 2758.39111328125 lr: 0.00031623\n",
      "Iter: 85/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1536.862548828125 lr: 0.00031623\n",
      "Iter: 86/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 2764.110107421875 lr: 0.00031623\n",
      "Iter: 87/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: -5116.130859375 lr: 0.00031623\n",
      "Iter: 88/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 981.7300415039062 lr: 0.00031623\n",
      "Iter: 89/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 961.7498168945312 lr: 0.00031623\n",
      "Iter: 90/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1672.817138671875 lr: 0.00031623\n",
      "Iter: 91/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1180.4197998046875 lr: 0.00031623\n",
      "Iter: 92/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 2209.53173828125 lr: 0.00031623\n",
      "Iter: 93/695, loss_train: 5.7465, loss_val: nan, pos_over_neg: 2951.74560546875 lr: 0.00031623\n",
      "Iter: 94/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 2358.945068359375 lr: 0.00031623\n",
      "Iter: 95/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1339.0478515625 lr: 0.00031623\n",
      "Iter: 96/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 867.5938110351562 lr: 0.00031623\n",
      "Iter: 97/695, loss_train: 5.7477, loss_val: nan, pos_over_neg: 2337.869873046875 lr: 0.00031623\n",
      "Iter: 98/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1766.5301513671875 lr: 0.00031623\n",
      "Iter: 99/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2974.879638671875 lr: 0.00031623\n",
      "Iter: 100/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1386.3929443359375 lr: 0.00031623\n",
      "Iter: 101/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1245.4434814453125 lr: 0.00031623\n",
      "Iter: 102/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 838.28125 lr: 0.00031623\n",
      "Iter: 103/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 711.8582763671875 lr: 0.00031623\n",
      "Iter: 104/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 779.6487426757812 lr: 0.00031623\n",
      "Iter: 105/695, loss_train: 5.7604, loss_val: nan, pos_over_neg: 704.9818115234375 lr: 0.00031623\n",
      "Iter: 106/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1023.4987182617188 lr: 0.00031623\n",
      "Iter: 107/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1573.8226318359375 lr: 0.00031623\n",
      "Iter: 108/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1114.5185546875 lr: 0.00031623\n",
      "Iter: 109/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2841.529541015625 lr: 0.00031623\n",
      "Iter: 110/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 999.9931640625 lr: 0.00031623\n",
      "Iter: 111/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 791.25 lr: 0.00031623\n",
      "Iter: 112/695, loss_train: 5.7443, loss_val: nan, pos_over_neg: 1953.1971435546875 lr: 0.00031623\n",
      "Iter: 113/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1968.531982421875 lr: 0.00031623\n",
      "Iter: 114/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1213.53662109375 lr: 0.00031623\n",
      "Iter: 115/695, loss_train: 5.7488, loss_val: nan, pos_over_neg: 1417.4244384765625 lr: 0.00031623\n",
      "Iter: 116/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1258.1436767578125 lr: 0.00031623\n",
      "Iter: 117/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 987.7013549804688 lr: 0.00031623\n",
      "Iter: 118/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1499.1748046875 lr: 0.00031623\n",
      "Iter: 119/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1666.030029296875 lr: 0.00031623\n",
      "Iter: 120/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1641.939697265625 lr: 0.00031623\n",
      "Iter: 121/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1052.48046875 lr: 0.00031623\n",
      "Iter: 122/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1050.42919921875 lr: 0.00031623\n",
      "Iter: 123/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1607.8558349609375 lr: 0.00031623\n",
      "Iter: 124/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1851.718505859375 lr: 0.00031623\n",
      "Iter: 125/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1678.68115234375 lr: 0.00031623\n",
      "Iter: 126/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 2739.8759765625 lr: 0.00031623\n",
      "Iter: 127/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1196.8079833984375 lr: 0.00031623\n",
      "Iter: 128/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1443.271728515625 lr: 0.00031623\n",
      "Iter: 129/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2269.851318359375 lr: 0.00031623\n",
      "Iter: 130/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2035.1029052734375 lr: 0.00031623\n",
      "Iter: 131/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 711.4559326171875 lr: 0.00031623\n",
      "Iter: 132/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1461.37939453125 lr: 0.00031623\n",
      "Iter: 133/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1480.862548828125 lr: 0.00031623\n",
      "Iter: 134/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1071.33837890625 lr: 0.00031623\n",
      "Iter: 135/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2439.426513671875 lr: 0.00031623\n",
      "Iter: 136/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1535.0389404296875 lr: 0.00031623\n",
      "Iter: 137/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1408.734619140625 lr: 0.00031623\n",
      "Iter: 138/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 3147.654541015625 lr: 0.00031623\n",
      "Iter: 139/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 703.81689453125 lr: 0.00031623\n",
      "Iter: 140/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 789.6726684570312 lr: 0.00031623\n",
      "Iter: 141/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1121.57861328125 lr: 0.00031623\n",
      "Iter: 142/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2047.6209716796875 lr: 0.00031623\n",
      "Iter: 143/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1261.642822265625 lr: 0.00031623\n",
      "Iter: 144/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1229.990478515625 lr: 0.00031623\n",
      "Iter: 145/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1486.5303955078125 lr: 0.00031623\n",
      "Iter: 146/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1257.5782470703125 lr: 0.00031623\n",
      "Iter: 147/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1009.752685546875 lr: 0.00031623\n",
      "Iter: 148/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 905.6324462890625 lr: 0.00031623\n",
      "Iter: 149/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 863.0866088867188 lr: 0.00031623\n",
      "Iter: 150/695, loss_train: 5.7662, loss_val: nan, pos_over_neg: 2338.469482421875 lr: 0.00031623\n",
      "Iter: 151/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1288.09814453125 lr: 0.00031623\n",
      "Iter: 152/695, loss_train: 5.7671, loss_val: nan, pos_over_neg: 2175.249755859375 lr: 0.00031623\n",
      "Iter: 153/695, loss_train: 5.7631, loss_val: nan, pos_over_neg: 935.2291259765625 lr: 0.00031623\n",
      "Iter: 154/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 3762.4111328125 lr: 0.00031623\n",
      "Iter: 155/695, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1217.0970458984375 lr: 0.00031623\n",
      "Iter: 156/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1556.1441650390625 lr: 0.00031623\n",
      "Iter: 157/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1394.9503173828125 lr: 0.00031623\n",
      "Iter: 158/695, loss_train: 5.7687, loss_val: nan, pos_over_neg: 831.002197265625 lr: 0.00031623\n",
      "Iter: 159/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2541.975341796875 lr: 0.00031623\n",
      "Iter: 160/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 2611.49658203125 lr: 0.00031623\n",
      "Iter: 161/695, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1333.014404296875 lr: 0.00031623\n",
      "Iter: 162/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1040.3922119140625 lr: 0.00031623\n",
      "Iter: 163/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1234.980224609375 lr: 0.00031623\n",
      "Iter: 164/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1433.82568359375 lr: 0.00031623\n",
      "Iter: 165/695, loss_train: 5.7637, loss_val: nan, pos_over_neg: 706.9884643554688 lr: 0.00031623\n",
      "Iter: 166/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 745.917724609375 lr: 0.00031623\n",
      "Iter: 167/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1250.0233154296875 lr: 0.00031623\n",
      "Iter: 168/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2252.538818359375 lr: 0.00031623\n",
      "Iter: 169/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2031.981201171875 lr: 0.00031623\n",
      "Iter: 170/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1426.6483154296875 lr: 0.00031623\n",
      "Iter: 171/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1159.9146728515625 lr: 0.00031623\n",
      "Iter: 172/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2671.152099609375 lr: 0.00031623\n",
      "Iter: 173/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1587.7496337890625 lr: 0.00031623\n",
      "Iter: 174/695, loss_train: 5.7667, loss_val: nan, pos_over_neg: 854.385986328125 lr: 0.00031623\n",
      "Iter: 175/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 2685.582275390625 lr: 0.00031623\n",
      "Iter: 176/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1322.22998046875 lr: 0.00031623\n",
      "Iter: 177/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1400.51513671875 lr: 0.00031623\n",
      "Iter: 178/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 850.2250366210938 lr: 0.00031623\n",
      "Iter: 179/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 3755.064208984375 lr: 0.00031623\n",
      "Iter: 180/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1293.6689453125 lr: 0.00031623\n",
      "Iter: 181/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 819.9945678710938 lr: 0.00031623\n",
      "Iter: 182/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1532.6973876953125 lr: 0.00031623\n",
      "Iter: 183/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1763.895751953125 lr: 0.00031623\n",
      "Iter: 184/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1808.591796875 lr: 0.00031623\n",
      "Iter: 185/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1357.6285400390625 lr: 0.00031623\n",
      "Iter: 186/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1109.7828369140625 lr: 0.00031623\n",
      "Iter: 187/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1361.5477294921875 lr: 0.00031623\n",
      "Iter: 188/695, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1018.2122802734375 lr: 0.00031623\n",
      "Iter: 189/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1246.50244140625 lr: 0.00031623\n",
      "Iter: 190/695, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1038.7772216796875 lr: 0.00031623\n",
      "Iter: 191/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1096.2877197265625 lr: 0.00031623\n",
      "Iter: 192/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 983.0774536132812 lr: 0.00031623\n",
      "Iter: 193/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 1635.2080078125 lr: 0.00031623\n",
      "Iter: 194/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 857.0209350585938 lr: 0.00031623\n",
      "Iter: 195/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1882.7532958984375 lr: 0.00031623\n",
      "Iter: 196/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 3729.611328125 lr: 0.00031623\n",
      "Iter: 197/695, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1473.3465576171875 lr: 0.00031623\n",
      "Iter: 198/695, loss_train: 5.7586, loss_val: nan, pos_over_neg: 980.538330078125 lr: 0.00031623\n",
      "Iter: 199/695, loss_train: 5.7618, loss_val: nan, pos_over_neg: 901.6539306640625 lr: 0.00031623\n",
      "Iter: 200/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1505.2518310546875 lr: 0.00031623\n",
      "Iter: 201/695, loss_train: 5.7619, loss_val: nan, pos_over_neg: 615.66748046875 lr: 0.00031623\n",
      "Iter: 202/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1918.6087646484375 lr: 0.00031623\n",
      "Iter: 203/695, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1385.8173828125 lr: 0.00031623\n",
      "Iter: 204/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1317.0428466796875 lr: 0.00031623\n",
      "Iter: 205/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1961.3221435546875 lr: 0.00031623\n",
      "Iter: 206/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 949.8934326171875 lr: 0.00031623\n",
      "Iter: 207/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1128.029541015625 lr: 0.00031623\n",
      "Iter: 208/695, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1041.02490234375 lr: 0.00031623\n",
      "Iter: 209/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1394.7515869140625 lr: 0.00031623\n",
      "Iter: 210/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1457.1336669921875 lr: 0.00031623\n",
      "Iter: 211/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1275.8651123046875 lr: 0.00031623\n",
      "Iter: 212/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 6067.3857421875 lr: 0.00031623\n",
      "Iter: 213/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1325.4847412109375 lr: 0.00031623\n",
      "Iter: 214/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 780.6007690429688 lr: 0.00031623\n",
      "Iter: 215/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 3024.754638671875 lr: 0.00031623\n",
      "Iter: 216/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 21161.73828125 lr: 0.00031623\n",
      "Iter: 217/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1314.978759765625 lr: 0.00031623\n",
      "Iter: 218/695, loss_train: 5.7587, loss_val: nan, pos_over_neg: 664.8958129882812 lr: 0.00031623\n",
      "Iter: 219/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2111.601806640625 lr: 0.00031623\n",
      "Iter: 220/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1333.972900390625 lr: 0.00031623\n",
      "Iter: 221/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 3322.25 lr: 0.00031623\n",
      "Iter: 222/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1959.1976318359375 lr: 0.00031623\n",
      "Iter: 223/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 2314.984130859375 lr: 0.00031623\n",
      "Iter: 224/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 1504.78369140625 lr: 0.00031623\n",
      "Iter: 225/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 3435.27099609375 lr: 0.00031623\n",
      "Iter: 226/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 2042.4114990234375 lr: 0.00031623\n",
      "Iter: 227/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 10062.5576171875 lr: 0.00031623\n",
      "Iter: 228/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1787.9332275390625 lr: 0.00031623\n",
      "Iter: 229/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 21456.162109375 lr: 0.00031623\n",
      "Iter: 230/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 4968.18994140625 lr: 0.00031623\n",
      "Iter: 231/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2432.152587890625 lr: 0.00031623\n",
      "Iter: 232/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1105.5703125 lr: 0.00031623\n",
      "Iter: 233/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 825.21630859375 lr: 0.00031623\n",
      "Iter: 234/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1292.935791015625 lr: 0.00031623\n",
      "Iter: 235/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1571.5985107421875 lr: 0.00031623\n",
      "Iter: 236/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1120.7598876953125 lr: 0.00031623\n",
      "Iter: 237/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 2864.928466796875 lr: 0.00031623\n",
      "Iter: 238/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 2095.212646484375 lr: 0.00031623\n",
      "Iter: 239/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 2783.614990234375 lr: 0.00031623\n",
      "Iter: 240/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 4424.24169921875 lr: 0.00031623\n",
      "Iter: 241/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1630.5079345703125 lr: 0.00031623\n",
      "Iter: 242/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1685.5177001953125 lr: 0.00031623\n",
      "Iter: 243/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2249.975830078125 lr: 0.00031623\n",
      "Iter: 244/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1411.0792236328125 lr: 0.00031623\n",
      "Iter: 245/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1000.7188720703125 lr: 0.00031623\n",
      "Iter: 246/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1099.6170654296875 lr: 0.00031623\n",
      "Iter: 247/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 779.3130493164062 lr: 0.00031623\n",
      "Iter: 248/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1210.676513671875 lr: 0.00031623\n",
      "Iter: 249/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 12659.61328125 lr: 0.00031623\n",
      "Iter: 250/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1386.7613525390625 lr: 0.00031623\n",
      "Iter: 251/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 925.685546875 lr: 0.00031623\n",
      "Iter: 252/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1183.818359375 lr: 0.00031623\n",
      "Iter: 253/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 788.6682739257812 lr: 0.00031623\n",
      "Iter: 254/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1132.875732421875 lr: 0.00031623\n",
      "Iter: 255/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 750.5881958007812 lr: 0.00031623\n",
      "Iter: 256/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 918.1486206054688 lr: 0.00031623\n",
      "Iter: 257/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 822.3283081054688 lr: 0.00031623\n",
      "Iter: 258/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1776.8172607421875 lr: 0.00031623\n",
      "Iter: 259/695, loss_train: 5.76, loss_val: nan, pos_over_neg: 753.017333984375 lr: 0.00031623\n",
      "Iter: 260/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1495.7271728515625 lr: 0.00031623\n",
      "Iter: 261/695, loss_train: 5.7595, loss_val: nan, pos_over_neg: 731.9619750976562 lr: 0.00031623\n",
      "Iter: 262/695, loss_train: 5.763, loss_val: nan, pos_over_neg: 557.2421264648438 lr: 0.00031623\n",
      "Iter: 263/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 562.57373046875 lr: 0.00031623\n",
      "Iter: 264/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 513.3575439453125 lr: 0.00031623\n",
      "Iter: 265/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 1604.8892822265625 lr: 0.00031623\n",
      "Iter: 266/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1423.591552734375 lr: 0.00031623\n",
      "Iter: 267/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1157.122314453125 lr: 0.00031623\n",
      "Iter: 268/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 992.3707275390625 lr: 0.00031623\n",
      "Iter: 269/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1112.8316650390625 lr: 0.00031623\n",
      "Iter: 270/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 827.6278686523438 lr: 0.00031623\n",
      "Iter: 271/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 784.04931640625 lr: 0.00031623\n",
      "Iter: 272/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 834.1065063476562 lr: 0.00031623\n",
      "Iter: 273/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1033.255859375 lr: 0.00031623\n",
      "Iter: 274/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 788.1724853515625 lr: 0.00031623\n",
      "Iter: 275/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1459.505859375 lr: 0.00031623\n",
      "Iter: 276/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 2586.599609375 lr: 0.00031623\n",
      "Iter: 277/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 995.9962768554688 lr: 0.00031623\n",
      "Iter: 278/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 863.1993408203125 lr: 0.00031623\n",
      "Iter: 279/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 928.1392211914062 lr: 0.00031623\n",
      "Iter: 280/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 685.516845703125 lr: 0.00031623\n",
      "Iter: 281/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 758.39892578125 lr: 0.00031623\n",
      "Iter: 282/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1440.13916015625 lr: 0.00031623\n",
      "Iter: 283/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 2478.232666015625 lr: 0.00031623\n",
      "Iter: 284/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 857.7329711914062 lr: 0.00031623\n",
      "Iter: 285/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1263.849365234375 lr: 0.00031623\n",
      "Iter: 286/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 898.9472045898438 lr: 0.00031623\n",
      "Iter: 287/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 4282.185546875 lr: 0.00031623\n",
      "Iter: 288/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 949.4324340820312 lr: 0.00031623\n",
      "Iter: 289/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2019.8165283203125 lr: 0.00031623\n",
      "Iter: 290/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1355.982666015625 lr: 0.00031623\n",
      "Iter: 291/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1776.0423583984375 lr: 0.00031623\n",
      "Iter: 292/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1779.24072265625 lr: 0.00031623\n",
      "Iter: 293/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1342.442626953125 lr: 0.00031623\n",
      "Iter: 294/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1103.44140625 lr: 0.00031623\n",
      "Iter: 295/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1128.5478515625 lr: 0.00031623\n",
      "Iter: 296/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1396.7952880859375 lr: 0.00031623\n",
      "Iter: 297/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1762.0213623046875 lr: 0.00031623\n",
      "Iter: 298/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 587.1494140625 lr: 0.00031623\n",
      "Iter: 299/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1152.5889892578125 lr: 0.00031623\n",
      "Iter: 300/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2638.947021484375 lr: 0.00031623\n",
      "Iter: 301/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 4938.08837890625 lr: 0.00031623\n",
      "Iter: 302/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1443.09423828125 lr: 0.00031623\n",
      "Iter: 303/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 2265.652099609375 lr: 0.00031623\n",
      "Iter: 304/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 3838.30859375 lr: 0.00031623\n",
      "Iter: 305/695, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1864.0655517578125 lr: 0.00031623\n",
      "Iter: 306/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 954.8611450195312 lr: 0.00031623\n",
      "Iter: 307/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1331.9774169921875 lr: 0.00031623\n",
      "Iter: 308/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 805.9767456054688 lr: 0.00031623\n",
      "Iter: 309/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1191.2996826171875 lr: 0.00031623\n",
      "Iter: 310/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1837.0452880859375 lr: 0.00031623\n",
      "Iter: 311/695, loss_train: 5.7415, loss_val: nan, pos_over_neg: 8670.728515625 lr: 0.00031623\n",
      "Iter: 312/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2026.419189453125 lr: 0.00031623\n",
      "Iter: 313/695, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1087.9388427734375 lr: 0.00031623\n",
      "Iter: 314/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 5217.8974609375 lr: 0.00031623\n",
      "Iter: 315/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1624.007080078125 lr: 0.00031623\n",
      "Iter: 316/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3351.0791015625 lr: 0.00031623\n",
      "Iter: 317/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1607.5325927734375 lr: 0.00031623\n",
      "Iter: 318/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1202.711669921875 lr: 0.00031623\n",
      "Iter: 319/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 3163.23291015625 lr: 0.00031623\n",
      "Iter: 320/695, loss_train: 5.7609, loss_val: nan, pos_over_neg: 2924.004150390625 lr: 0.00031623\n",
      "Iter: 321/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1289.4771728515625 lr: 0.00031623\n",
      "Iter: 322/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1899.3682861328125 lr: 0.00031623\n",
      "Iter: 323/695, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1114.2244873046875 lr: 0.00031623\n",
      "Iter: 324/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 5179.20068359375 lr: 0.00031623\n",
      "Iter: 325/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 934.9794311523438 lr: 0.00031623\n",
      "Iter: 326/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 960.0226440429688 lr: 0.00031623\n",
      "Iter: 327/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 2311.61669921875 lr: 0.00031623\n",
      "Iter: 328/695, loss_train: 5.7645, loss_val: nan, pos_over_neg: 726.7384643554688 lr: 0.00031623\n",
      "Iter: 329/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 885.2197875976562 lr: 0.00031623\n",
      "Iter: 330/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 2874.302001953125 lr: 0.00031623\n",
      "Iter: 331/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 945.5060424804688 lr: 0.00031623\n",
      "Iter: 332/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 3430.357666015625 lr: 0.00031623\n",
      "Iter: 333/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 1150.1561279296875 lr: 0.00031623\n",
      "Iter: 334/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2417.8466796875 lr: 0.00031623\n",
      "Iter: 335/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2180.3125 lr: 0.00031623\n",
      "Iter: 336/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1076.905517578125 lr: 0.00031623\n",
      "Iter: 337/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 948.0155639648438 lr: 0.00031623\n",
      "Iter: 338/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 804.0589599609375 lr: 0.00031623\n",
      "Iter: 339/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 966.8508911132812 lr: 0.00031623\n",
      "Iter: 340/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 935.8580932617188 lr: 0.00031623\n",
      "Iter: 341/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 2252.523193359375 lr: 0.00031623\n",
      "Iter: 342/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 806.1492309570312 lr: 0.00031623\n",
      "Iter: 343/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 3196.85107421875 lr: 0.00031623\n",
      "Iter: 344/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 875.515380859375 lr: 0.00031623\n",
      "Iter: 345/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 869.8931274414062 lr: 0.00031623\n",
      "Iter: 346/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1828.2733154296875 lr: 0.00031623\n",
      "Iter: 347/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 2288.675537109375 lr: 0.00031623\n",
      "Iter: 348/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 967.950927734375 lr: 0.00031623\n",
      "Iter: 349/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 818.380126953125 lr: 0.00031623\n",
      "Iter: 350/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1281.775146484375 lr: 0.00031623\n",
      "Iter: 351/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 778.4246215820312 lr: 0.00031623\n",
      "Iter: 352/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 868.9420776367188 lr: 0.00031623\n",
      "Iter: 353/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 755.8746948242188 lr: 0.00031623\n",
      "Iter: 354/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 865.6455078125 lr: 0.00031623\n",
      "Iter: 355/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 844.8742065429688 lr: 0.00031623\n",
      "Iter: 356/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 864.3820190429688 lr: 0.00031623\n",
      "Iter: 357/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 915.9976196289062 lr: 0.00031623\n",
      "Iter: 358/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1289.6959228515625 lr: 0.00031623\n",
      "Iter: 359/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1533.7620849609375 lr: 0.00031623\n",
      "Iter: 360/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1759.0194091796875 lr: 0.00031623\n",
      "Iter: 361/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1034.846923828125 lr: 0.00031623\n",
      "Iter: 362/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 2050.031494140625 lr: 0.00031623\n",
      "Iter: 363/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 866.7622680664062 lr: 0.00031623\n",
      "Iter: 364/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1106.8760986328125 lr: 0.00031623\n",
      "Iter: 365/695, loss_train: 5.7585, loss_val: nan, pos_over_neg: 646.5068359375 lr: 0.00031623\n",
      "Iter: 366/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1178.1671142578125 lr: 0.00031623\n",
      "Iter: 367/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1174.693115234375 lr: 0.00031623\n",
      "Iter: 368/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 975.7794189453125 lr: 0.00031623\n",
      "Iter: 369/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 641.4266357421875 lr: 0.00031623\n",
      "Iter: 370/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1026.946044921875 lr: 0.00031623\n",
      "Iter: 371/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 1319.09130859375 lr: 0.00031623\n",
      "Iter: 372/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1170.596435546875 lr: 0.00031623\n",
      "Iter: 373/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 2599.44921875 lr: 0.00031623\n",
      "Iter: 374/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 727.4198608398438 lr: 0.00031623\n",
      "Iter: 375/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 657.24853515625 lr: 0.00031623\n",
      "Iter: 376/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 826.980224609375 lr: 0.00031623\n",
      "Iter: 377/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1100.2093505859375 lr: 0.00031623\n",
      "Iter: 378/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1068.89404296875 lr: 0.00031623\n",
      "Iter: 379/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 2922.82177734375 lr: 0.00031623\n",
      "Iter: 380/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1238.316162109375 lr: 0.00031623\n",
      "Iter: 381/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 1003.9073486328125 lr: 0.00031623\n",
      "Iter: 382/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 650.9255981445312 lr: 0.00031623\n",
      "Iter: 383/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1764.71240234375 lr: 0.00031623\n",
      "Iter: 384/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 2009.49951171875 lr: 0.00031623\n",
      "Iter: 385/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1168.728759765625 lr: 0.00031623\n",
      "Iter: 386/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1249.6119384765625 lr: 0.00031623\n",
      "Iter: 387/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1468.9893798828125 lr: 0.00031623\n",
      "Iter: 388/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1217.62939453125 lr: 0.00031623\n",
      "Iter: 389/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1636.6309814453125 lr: 0.00031623\n",
      "Iter: 390/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1189.2586669921875 lr: 0.00031623\n",
      "Iter: 391/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1071.64599609375 lr: 0.00031623\n",
      "Iter: 392/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1794.8463134765625 lr: 0.00031623\n",
      "Iter: 393/695, loss_train: 5.7569, loss_val: nan, pos_over_neg: 2114.629638671875 lr: 0.00031623\n",
      "Iter: 394/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1470.4185791015625 lr: 0.00031623\n",
      "Iter: 395/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2032.6962890625 lr: 0.00031623\n",
      "Iter: 396/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 790.8013916015625 lr: 0.00031623\n",
      "Iter: 397/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 2259.919189453125 lr: 0.00031623\n",
      "Iter: 398/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1814.6885986328125 lr: 0.00031623\n",
      "Iter: 399/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 5054.90966796875 lr: 0.00031623\n",
      "Iter: 400/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 1034.9237060546875 lr: 0.00031623\n",
      "Iter: 401/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1065.9080810546875 lr: 0.00031623\n",
      "Iter: 402/695, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1192.86865234375 lr: 0.00031623\n",
      "Iter: 403/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1550.7425537109375 lr: 0.00031623\n",
      "Iter: 404/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 2048.821044921875 lr: 0.00031623\n",
      "Iter: 405/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 1038.951416015625 lr: 0.00031623\n",
      "Iter: 406/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 2599.323486328125 lr: 0.00031623\n",
      "Iter: 407/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1876.4617919921875 lr: 0.00031623\n",
      "Iter: 408/695, loss_train: 5.7607, loss_val: nan, pos_over_neg: 26597.486328125 lr: 0.00031623\n",
      "Iter: 409/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 5858.6298828125 lr: 0.00031623\n",
      "Iter: 410/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1800.9073486328125 lr: 0.00031623\n",
      "Iter: 411/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1522.453125 lr: 0.00031623\n",
      "Iter: 412/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 2196.83056640625 lr: 0.00031623\n",
      "Iter: 413/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2534.2060546875 lr: 0.00031623\n",
      "Iter: 414/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 5666.234375 lr: 0.00031623\n",
      "Iter: 415/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1349.552978515625 lr: 0.00031623\n",
      "Iter: 416/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 840.8849487304688 lr: 0.00031623\n",
      "Iter: 417/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1306.1070556640625 lr: 0.00031623\n",
      "Iter: 418/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 8771.6171875 lr: 0.00031623\n",
      "Iter: 419/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 4132.1083984375 lr: 0.00031623\n",
      "Iter: 420/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1346.0986328125 lr: 0.00031623\n",
      "Iter: 421/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 3236.405517578125 lr: 0.00031623\n",
      "Iter: 422/695, loss_train: 5.745, loss_val: nan, pos_over_neg: 1872.2491455078125 lr: 0.00031623\n",
      "Iter: 423/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 2193.5859375 lr: 0.00031623\n",
      "Iter: 424/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1287.44677734375 lr: 0.00031623\n",
      "Iter: 425/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 942.7487182617188 lr: 0.00031623\n",
      "Iter: 426/695, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1446.5628662109375 lr: 0.00031623\n",
      "Iter: 427/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1454.6220703125 lr: 0.00031623\n",
      "Iter: 428/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1239.75732421875 lr: 0.00031623\n",
      "Iter: 429/695, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1203.82421875 lr: 0.00031623\n",
      "Iter: 430/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 875.0294799804688 lr: 0.00031623\n",
      "Iter: 431/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 1065.1668701171875 lr: 0.00031623\n",
      "Iter: 432/695, loss_train: 5.7556, loss_val: nan, pos_over_neg: 1266.4993896484375 lr: 0.00031623\n",
      "Iter: 433/695, loss_train: 5.7452, loss_val: nan, pos_over_neg: 3194.203857421875 lr: 0.00031623\n",
      "Iter: 434/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2398.155517578125 lr: 0.00031623\n",
      "Iter: 435/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 588.3468627929688 lr: 0.00031623\n",
      "Iter: 436/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 851.3088989257812 lr: 0.00031623\n",
      "Iter: 437/695, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1310.731689453125 lr: 0.00031623\n",
      "Iter: 438/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1866.2025146484375 lr: 0.00031623\n",
      "Iter: 439/695, loss_train: 5.7467, loss_val: nan, pos_over_neg: 2435.482666015625 lr: 0.00031623\n",
      "Iter: 440/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1187.60302734375 lr: 0.00031623\n",
      "Iter: 441/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 885.1602783203125 lr: 0.00031623\n",
      "Iter: 442/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1460.6165771484375 lr: 0.00031623\n",
      "Iter: 443/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1362.8499755859375 lr: 0.00031623\n",
      "Iter: 444/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 5170.19873046875 lr: 0.00031623\n",
      "Iter: 445/695, loss_train: 5.7459, loss_val: nan, pos_over_neg: 3368.3876953125 lr: 0.00031623\n",
      "Iter: 446/695, loss_train: 5.7462, loss_val: nan, pos_over_neg: 700.966796875 lr: 0.00031623\n",
      "Iter: 447/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1593.9127197265625 lr: 0.00031623\n",
      "Iter: 448/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 3538.596435546875 lr: 0.00031623\n",
      "Iter: 449/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1087.8759765625 lr: 0.00031623\n",
      "Iter: 450/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1118.6458740234375 lr: 0.00031623\n",
      "Iter: 451/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1549.2601318359375 lr: 0.00031623\n",
      "Iter: 452/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 648.4404907226562 lr: 0.00031623\n",
      "Iter: 453/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 3227.592041015625 lr: 0.00031623\n",
      "Iter: 454/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 2452.784912109375 lr: 0.00031623\n",
      "Iter: 455/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1012.7608642578125 lr: 0.00031623\n",
      "Iter: 456/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1157.14208984375 lr: 0.00031623\n",
      "Iter: 457/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1450.0894775390625 lr: 0.00031623\n",
      "Iter: 458/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1145.8875732421875 lr: 0.00031623\n",
      "Iter: 459/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 819.6788940429688 lr: 0.00031623\n",
      "Iter: 460/695, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1772.901123046875 lr: 0.00031623\n",
      "Iter: 461/695, loss_train: 5.7578, loss_val: nan, pos_over_neg: 2248.462890625 lr: 0.00031623\n",
      "Iter: 462/695, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1111.8680419921875 lr: 0.00031623\n",
      "Iter: 463/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1298.7490234375 lr: 0.00031623\n",
      "Iter: 464/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1061.386962890625 lr: 0.00031623\n",
      "Iter: 465/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1153.4718017578125 lr: 0.00031623\n",
      "Iter: 466/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2640.47119140625 lr: 0.00031623\n",
      "Iter: 467/695, loss_train: 5.7606, loss_val: nan, pos_over_neg: 778.9343872070312 lr: 0.00031623\n",
      "Iter: 468/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1484.5045166015625 lr: 0.00031623\n",
      "Iter: 469/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1518.6744384765625 lr: 0.00031623\n",
      "Iter: 470/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 868.1240234375 lr: 0.00031623\n",
      "Iter: 471/695, loss_train: 5.7576, loss_val: nan, pos_over_neg: 701.4417724609375 lr: 0.00031623\n",
      "Iter: 472/695, loss_train: 5.7584, loss_val: nan, pos_over_neg: 1189.7216796875 lr: 0.00031623\n",
      "Iter: 473/695, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1188.70556640625 lr: 0.00031623\n",
      "Iter: 474/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1335.4849853515625 lr: 0.00031623\n",
      "Iter: 475/695, loss_train: 5.7634, loss_val: nan, pos_over_neg: 719.908935546875 lr: 0.00031623\n",
      "Iter: 476/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1056.7880859375 lr: 0.00031623\n",
      "Iter: 477/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 630.7911376953125 lr: 0.00031623\n",
      "Iter: 478/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 839.2180786132812 lr: 0.00031623\n",
      "Iter: 479/695, loss_train: 5.756, loss_val: nan, pos_over_neg: 4383.1298828125 lr: 0.00031623\n",
      "Iter: 480/695, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1037.884521484375 lr: 0.00031623\n",
      "Iter: 481/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1171.327880859375 lr: 0.00031623\n",
      "Iter: 482/695, loss_train: 5.7529, loss_val: nan, pos_over_neg: 3319.644775390625 lr: 0.00031623\n",
      "Iter: 483/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 833.6986083984375 lr: 0.00031623\n",
      "Iter: 484/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 660.44677734375 lr: 0.00031623\n",
      "Iter: 485/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 2181.09765625 lr: 0.00031623\n",
      "Iter: 486/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 3678.975341796875 lr: 0.00031623\n",
      "Iter: 487/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 2322.401611328125 lr: 0.00031623\n",
      "Iter: 488/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 784.6488037109375 lr: 0.00031623\n",
      "Iter: 489/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 773.7437744140625 lr: 0.00031623\n",
      "Iter: 490/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1583.9610595703125 lr: 0.00031623\n",
      "Iter: 491/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1498.6790771484375 lr: 0.00031623\n",
      "Iter: 492/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1797.38623046875 lr: 0.00031623\n",
      "Iter: 493/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1465.478271484375 lr: 0.00031623\n",
      "Iter: 494/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 909.1106567382812 lr: 0.00031623\n",
      "Iter: 495/695, loss_train: 5.7565, loss_val: nan, pos_over_neg: 532.1959838867188 lr: 0.00031623\n",
      "Iter: 496/695, loss_train: 5.7473, loss_val: nan, pos_over_neg: 4003.017333984375 lr: 0.00031623\n",
      "Iter: 497/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1637.6053466796875 lr: 0.00031623\n",
      "Iter: 498/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 770.59033203125 lr: 0.00031623\n",
      "Iter: 499/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1257.197021484375 lr: 0.00031623\n",
      "Iter: 500/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1655.1944580078125 lr: 0.00031623\n",
      "Iter: 501/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 1406.5264892578125 lr: 0.00031623\n",
      "Iter: 502/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1082.07470703125 lr: 0.00031623\n",
      "Iter: 503/695, loss_train: 5.7616, loss_val: nan, pos_over_neg: 610.2026977539062 lr: 0.00031623\n",
      "Iter: 504/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1605.1151123046875 lr: 0.00031623\n",
      "Iter: 505/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1843.02001953125 lr: 0.00031623\n",
      "Iter: 506/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1245.9178466796875 lr: 0.00031623\n",
      "Iter: 507/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 909.841796875 lr: 0.00031623\n",
      "Iter: 508/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1502.893310546875 lr: 0.00031623\n",
      "Iter: 509/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2841.371826171875 lr: 0.00031623\n",
      "Iter: 510/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1648.942626953125 lr: 0.00031623\n",
      "Iter: 511/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1004.7644653320312 lr: 0.00031623\n",
      "Iter: 512/695, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1181.7763671875 lr: 0.00031623\n",
      "Iter: 513/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1112.5155029296875 lr: 0.00031623\n",
      "Iter: 514/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1351.0692138671875 lr: 0.00031623\n",
      "Iter: 515/695, loss_train: 5.7577, loss_val: nan, pos_over_neg: 959.9163818359375 lr: 0.00031623\n",
      "Iter: 516/695, loss_train: 5.7474, loss_val: nan, pos_over_neg: 2720.377685546875 lr: 0.00031623\n",
      "Iter: 517/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1002.2255859375 lr: 0.00031623\n",
      "Iter: 518/695, loss_train: 5.755, loss_val: nan, pos_over_neg: 1495.6982421875 lr: 0.00031623\n",
      "Iter: 519/695, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1343.33447265625 lr: 0.00031623\n",
      "Iter: 520/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2561.40869140625 lr: 0.00031623\n",
      "Iter: 521/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 4496.44970703125 lr: 0.00031623\n",
      "Iter: 522/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 849.5579223632812 lr: 0.00031623\n",
      "Iter: 523/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1686.7117919921875 lr: 0.00031623\n",
      "Iter: 524/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1533.0506591796875 lr: 0.00031623\n",
      "Iter: 525/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1963.1630859375 lr: 0.00031623\n",
      "Iter: 526/695, loss_train: 5.757, loss_val: nan, pos_over_neg: 1253.1771240234375 lr: 0.00031623\n",
      "Iter: 527/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 1132.5128173828125 lr: 0.00031623\n",
      "Iter: 528/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1476.31103515625 lr: 0.00031623\n",
      "Iter: 529/695, loss_train: 5.7472, loss_val: nan, pos_over_neg: 2270.529296875 lr: 0.00031623\n",
      "Iter: 530/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1854.98046875 lr: 0.00031623\n",
      "Iter: 531/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1397.1727294921875 lr: 0.00031623\n",
      "Iter: 532/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 3497.214111328125 lr: 0.00031623\n",
      "Iter: 533/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1744.8197021484375 lr: 0.00031623\n",
      "Iter: 534/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1831.5064697265625 lr: 0.00031623\n",
      "Iter: 535/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 982.2254028320312 lr: 0.00031623\n",
      "Iter: 536/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2862.57861328125 lr: 0.00031623\n",
      "Iter: 537/695, loss_train: 5.7583, loss_val: nan, pos_over_neg: 765.0878295898438 lr: 0.00031623\n",
      "Iter: 538/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 3070.502685546875 lr: 0.00031623\n",
      "Iter: 539/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1400.5714111328125 lr: 0.00031623\n",
      "Iter: 540/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 938.1031494140625 lr: 0.00031623\n",
      "Iter: 541/695, loss_train: 5.7495, loss_val: nan, pos_over_neg: 1169.5733642578125 lr: 0.00031623\n",
      "Iter: 542/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 891.6062622070312 lr: 0.00031623\n",
      "Iter: 543/695, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1540.6187744140625 lr: 0.00031623\n",
      "Iter: 544/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1499.9534912109375 lr: 0.00031623\n",
      "Iter: 545/695, loss_train: 5.7531, loss_val: nan, pos_over_neg: 894.4306030273438 lr: 0.00031623\n",
      "Iter: 546/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 945.8829956054688 lr: 0.00031623\n",
      "Iter: 547/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1797.678955078125 lr: 0.00031623\n",
      "Iter: 548/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1461.3271484375 lr: 0.00031623\n",
      "Iter: 549/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 975.5642700195312 lr: 0.00031623\n",
      "Iter: 550/695, loss_train: 5.7527, loss_val: nan, pos_over_neg: 1243.04443359375 lr: 0.00031623\n",
      "Iter: 551/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 632.9653930664062 lr: 0.00031623\n",
      "Iter: 552/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 775.8213500976562 lr: 0.00031623\n",
      "Iter: 553/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 2688.403564453125 lr: 0.00031623\n",
      "Iter: 554/695, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1032.343994140625 lr: 0.00031623\n",
      "Iter: 555/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 827.1808471679688 lr: 0.00031623\n",
      "Iter: 556/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 1276.3583984375 lr: 0.00031623\n",
      "Iter: 557/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1064.3704833984375 lr: 0.00031623\n",
      "Iter: 558/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 968.9437866210938 lr: 0.00031623\n",
      "Iter: 559/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 974.7411499023438 lr: 0.00031623\n",
      "Iter: 560/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1576.13037109375 lr: 0.00031623\n",
      "Iter: 561/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1155.58056640625 lr: 0.00031623\n",
      "Iter: 562/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1520.442138671875 lr: 0.00031623\n",
      "Iter: 563/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1835.2952880859375 lr: 0.00031623\n",
      "Iter: 564/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 894.6346435546875 lr: 0.00031623\n",
      "Iter: 565/695, loss_train: 5.7563, loss_val: nan, pos_over_neg: 696.8250732421875 lr: 0.00031623\n",
      "Iter: 566/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1321.881591796875 lr: 0.00031623\n",
      "Iter: 567/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1079.5084228515625 lr: 0.00031623\n",
      "Iter: 568/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 982.4962158203125 lr: 0.00031623\n",
      "Iter: 569/695, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1868.616943359375 lr: 0.00031623\n",
      "Iter: 570/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1770.375732421875 lr: 0.00031623\n",
      "Iter: 571/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 3050.93603515625 lr: 0.00031623\n",
      "Iter: 572/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1883.5155029296875 lr: 0.00031623\n",
      "Iter: 573/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: -13598.29296875 lr: 0.00031623\n",
      "Iter: 574/695, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1410.8494873046875 lr: 0.00031623\n",
      "Iter: 575/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 637.141357421875 lr: 0.00031623\n",
      "Iter: 576/695, loss_train: 5.7492, loss_val: nan, pos_over_neg: 1802.4033203125 lr: 0.00031623\n",
      "Iter: 577/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 3363.003173828125 lr: 0.00031623\n",
      "Iter: 578/695, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1753.4530029296875 lr: 0.00031623\n",
      "Iter: 579/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1740.06640625 lr: 0.00031623\n",
      "Iter: 580/695, loss_train: 5.7456, loss_val: nan, pos_over_neg: 848.117431640625 lr: 0.00031623\n",
      "Iter: 581/695, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1572.485595703125 lr: 0.00031623\n",
      "Iter: 582/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 1863.0438232421875 lr: 0.00031623\n",
      "Iter: 583/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 1489.9722900390625 lr: 0.00031623\n",
      "Iter: 584/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 2212.4697265625 lr: 0.00031623\n",
      "Iter: 585/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1699.58935546875 lr: 0.00031623\n",
      "Iter: 586/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1251.16357421875 lr: 0.00031623\n",
      "Iter: 587/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1733.0574951171875 lr: 0.00031623\n",
      "Iter: 588/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 13111.8837890625 lr: 0.00031623\n",
      "Iter: 589/695, loss_train: 5.7483, loss_val: nan, pos_over_neg: 1686.7630615234375 lr: 0.00031623\n",
      "Iter: 590/695, loss_train: 5.7501, loss_val: nan, pos_over_neg: 1094.3739013671875 lr: 0.00031623\n",
      "Iter: 591/695, loss_train: 5.7504, loss_val: nan, pos_over_neg: 1036.5501708984375 lr: 0.00031623\n",
      "Iter: 592/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1615.5880126953125 lr: 0.00031623\n",
      "Iter: 593/695, loss_train: 5.7453, loss_val: nan, pos_over_neg: 3637.68701171875 lr: 0.00031623\n",
      "Iter: 594/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1170.0233154296875 lr: 0.00031623\n",
      "Iter: 595/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 951.2595825195312 lr: 0.00031623\n",
      "Iter: 596/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1098.476806640625 lr: 0.00031623\n",
      "Iter: 597/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 988.2737426757812 lr: 0.00031623\n",
      "Iter: 598/695, loss_train: 5.7533, loss_val: nan, pos_over_neg: 1988.977783203125 lr: 0.00031623\n",
      "Iter: 599/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 621.5180053710938 lr: 0.00031623\n",
      "Iter: 600/695, loss_train: 5.7475, loss_val: nan, pos_over_neg: 8883.6455078125 lr: 0.00031623\n",
      "Iter: 601/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 5374.31201171875 lr: 0.00031623\n",
      "Iter: 602/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1169.2947998046875 lr: 0.00031623\n",
      "Iter: 603/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 913.0442504882812 lr: 0.00031623\n",
      "Iter: 604/695, loss_train: 5.7478, loss_val: nan, pos_over_neg: 2212.00732421875 lr: 0.00031623\n",
      "Iter: 605/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 13077.5615234375 lr: 0.00031623\n",
      "Iter: 606/695, loss_train: 5.7515, loss_val: nan, pos_over_neg: 2092.553955078125 lr: 0.00031623\n",
      "Iter: 607/695, loss_train: 5.7448, loss_val: nan, pos_over_neg: 4346.6181640625 lr: 0.00031623\n",
      "Iter: 608/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1361.7498779296875 lr: 0.00031623\n",
      "Iter: 609/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1888.9866943359375 lr: 0.00031623\n",
      "Iter: 610/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1630.830078125 lr: 0.00031623\n",
      "Iter: 611/695, loss_train: 5.7484, loss_val: nan, pos_over_neg: 1233.2242431640625 lr: 0.00031623\n",
      "Iter: 612/695, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1195.464599609375 lr: 0.00031623\n",
      "Iter: 613/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 1196.86328125 lr: 0.00031623\n",
      "Iter: 614/695, loss_train: 5.7523, loss_val: nan, pos_over_neg: 961.8142700195312 lr: 0.00031623\n",
      "Iter: 615/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1233.76904296875 lr: 0.00031623\n",
      "Iter: 616/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 762.5804443359375 lr: 0.00031623\n",
      "Iter: 617/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1601.8594970703125 lr: 0.00031623\n",
      "Iter: 618/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1357.9466552734375 lr: 0.00031623\n",
      "Iter: 619/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1213.669677734375 lr: 0.00031623\n",
      "Iter: 620/695, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1042.2650146484375 lr: 0.00031623\n",
      "Iter: 621/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1705.19677734375 lr: 0.00031623\n",
      "Iter: 622/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1645.1815185546875 lr: 0.00031623\n",
      "Iter: 623/695, loss_train: 5.758, loss_val: nan, pos_over_neg: 719.6492919921875 lr: 0.00031623\n",
      "Iter: 624/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 1995.9420166015625 lr: 0.00031623\n",
      "Iter: 625/695, loss_train: 5.7548, loss_val: nan, pos_over_neg: 916.6362915039062 lr: 0.00031623\n",
      "Iter: 626/695, loss_train: 5.7444, loss_val: nan, pos_over_neg: 956.2618408203125 lr: 0.00031623\n",
      "Iter: 627/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 624.5798950195312 lr: 0.00031623\n",
      "Iter: 628/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1523.28759765625 lr: 0.00031623\n",
      "Iter: 629/695, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1183.02783203125 lr: 0.00031623\n",
      "Iter: 630/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1304.05810546875 lr: 0.00031623\n",
      "Iter: 631/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 1588.6029052734375 lr: 0.00031623\n",
      "Iter: 632/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 880.47314453125 lr: 0.00031623\n",
      "Iter: 633/695, loss_train: 5.7486, loss_val: nan, pos_over_neg: 1213.6610107421875 lr: 0.00031623\n",
      "Iter: 634/695, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1075.15478515625 lr: 0.00031623\n",
      "Iter: 635/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1316.9932861328125 lr: 0.00031623\n",
      "Iter: 636/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1406.472412109375 lr: 0.00031623\n",
      "Iter: 637/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2416.54443359375 lr: 0.00031623\n",
      "Iter: 638/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1439.35302734375 lr: 0.00031623\n",
      "Iter: 639/695, loss_train: 5.761, loss_val: nan, pos_over_neg: 1241.4271240234375 lr: 0.00031623\n",
      "Iter: 640/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 3856.87939453125 lr: 0.00031623\n",
      "Iter: 641/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1792.1048583984375 lr: 0.00031623\n",
      "Iter: 642/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 2487.831298828125 lr: 0.00031623\n",
      "Iter: 643/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2800.168212890625 lr: 0.00031623\n",
      "Iter: 644/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: -32039.4140625 lr: 0.00031623\n",
      "Iter: 645/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 3148.0087890625 lr: 0.00031623\n",
      "Iter: 646/695, loss_train: 5.7514, loss_val: nan, pos_over_neg: 1898.515380859375 lr: 0.00031623\n",
      "Iter: 647/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1835.24853515625 lr: 0.00031623\n",
      "Iter: 648/695, loss_train: 5.7522, loss_val: nan, pos_over_neg: 1883.5126953125 lr: 0.00031623\n",
      "Iter: 649/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1214.5650634765625 lr: 0.00031623\n",
      "Iter: 650/695, loss_train: 5.7487, loss_val: nan, pos_over_neg: 3338.306884765625 lr: 0.00031623\n",
      "Iter: 651/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1319.960205078125 lr: 0.00031623\n",
      "Iter: 652/695, loss_train: 5.753, loss_val: nan, pos_over_neg: 704.3590698242188 lr: 0.00031623\n",
      "Iter: 653/695, loss_train: 5.7517, loss_val: nan, pos_over_neg: 1543.472900390625 lr: 0.00031623\n",
      "Iter: 654/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1417.403564453125 lr: 0.00031623\n",
      "Iter: 655/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1182.1678466796875 lr: 0.00031623\n",
      "Iter: 656/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1035.933837890625 lr: 0.00031623\n",
      "Iter: 657/695, loss_train: 5.7561, loss_val: nan, pos_over_neg: 1027.83935546875 lr: 0.00031623\n",
      "Iter: 658/695, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1083.707763671875 lr: 0.00031623\n",
      "Iter: 659/695, loss_train: 5.7506, loss_val: nan, pos_over_neg: 1726.4290771484375 lr: 0.00031623\n",
      "Iter: 660/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 2226.53125 lr: 0.00031623\n",
      "Iter: 661/695, loss_train: 5.7592, loss_val: nan, pos_over_neg: 611.30810546875 lr: 0.00031623\n",
      "Iter: 662/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1682.3975830078125 lr: 0.00031623\n",
      "Iter: 663/695, loss_train: 5.7553, loss_val: nan, pos_over_neg: 984.5790405273438 lr: 0.00031623\n",
      "Iter: 664/695, loss_train: 5.7499, loss_val: nan, pos_over_neg: 970.3162231445312 lr: 0.00031623\n",
      "Iter: 665/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 1680.2015380859375 lr: 0.00031623\n",
      "Iter: 666/695, loss_train: 5.7535, loss_val: nan, pos_over_neg: 929.3466186523438 lr: 0.00031623\n",
      "Iter: 667/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 725.2301635742188 lr: 0.00031623\n",
      "Iter: 668/695, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1164.19873046875 lr: 0.00031623\n",
      "Iter: 669/695, loss_train: 5.748, loss_val: nan, pos_over_neg: 1481.24169921875 lr: 0.00031623\n",
      "Iter: 670/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 2203.151123046875 lr: 0.00031623\n",
      "Iter: 671/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1538.0181884765625 lr: 0.00031623\n",
      "Iter: 672/695, loss_train: 5.7567, loss_val: nan, pos_over_neg: 2035.403564453125 lr: 0.00031623\n",
      "Iter: 673/695, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1320.11376953125 lr: 0.00031623\n",
      "Iter: 674/695, loss_train: 5.7471, loss_val: nan, pos_over_neg: 3584.7509765625 lr: 0.00031623\n",
      "Iter: 675/695, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1978.629150390625 lr: 0.00031623\n",
      "Iter: 676/695, loss_train: 5.7658, loss_val: nan, pos_over_neg: 749.1919555664062 lr: 0.00031623\n",
      "Iter: 677/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1745.0911865234375 lr: 0.00031623\n",
      "Iter: 678/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1355.3021240234375 lr: 0.00031623\n",
      "Iter: 679/695, loss_train: 5.7463, loss_val: nan, pos_over_neg: 1726.0721435546875 lr: 0.00031623\n",
      "Iter: 680/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1270.3939208984375 lr: 0.00031623\n",
      "Iter: 681/695, loss_train: 5.7546, loss_val: nan, pos_over_neg: 889.1303100585938 lr: 0.00031623\n",
      "Iter: 682/695, loss_train: 5.7519, loss_val: nan, pos_over_neg: 9902.845703125 lr: 0.00031623\n",
      "Iter: 683/695, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1401.3167724609375 lr: 0.00031623\n",
      "Iter: 684/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 2289.320556640625 lr: 0.00031623\n",
      "Iter: 685/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 5306.439453125 lr: 0.00031623\n",
      "Iter: 686/695, loss_train: 5.7541, loss_val: nan, pos_over_neg: 1249.141845703125 lr: 0.00031623\n",
      "Iter: 687/695, loss_train: 5.7454, loss_val: nan, pos_over_neg: 1322.341064453125 lr: 0.00031623\n",
      "Iter: 688/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 3077.9052734375 lr: 0.00031623\n",
      "Iter: 689/695, loss_train: 5.7558, loss_val: nan, pos_over_neg: 3707.06884765625 lr: 0.00031623\n",
      "Iter: 690/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 2148.80859375 lr: 0.00031623\n",
      "Iter: 691/695, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1239.0 lr: 0.00031623\n",
      "Iter: 692/695, loss_train: 5.7572, loss_val: nan, pos_over_neg: 1437.939208984375 lr: 0.00031623\n",
      "Iter: 693/695, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1609.8609619140625 lr: 0.00031623\n",
      "Iter: 694/695, loss_train: 5.7588, loss_val: nan, pos_over_neg: 827.080078125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [2:44:43<103107:07:34, 1237.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1102.3458251953125 lr: 0.00031623\n",
      "Iter: 1/695, loss_train: 5.7503, loss_val: nan, pos_over_neg: 1920.077880859375 lr: 0.00031623\n",
      "Iter: 2/695, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1267.6754150390625 lr: 0.00031623\n",
      "Iter: 3/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1291.6595458984375 lr: 0.00031623\n",
      "Iter: 4/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1378.7940673828125 lr: 0.00031623\n",
      "Iter: 5/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1004.037841796875 lr: 0.00031623\n",
      "Iter: 6/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 1548.5928955078125 lr: 0.00031623\n",
      "Iter: 7/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1827.9136962890625 lr: 0.00031623\n",
      "Iter: 8/695, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1620.94384765625 lr: 0.00031623\n",
      "Iter: 9/695, loss_train: 5.7468, loss_val: nan, pos_over_neg: 1264.5333251953125 lr: 0.00031623\n",
      "Iter: 10/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 745.9796142578125 lr: 0.00031623\n",
      "Iter: 11/695, loss_train: 5.752, loss_val: nan, pos_over_neg: 1740.30859375 lr: 0.00031623\n",
      "Iter: 12/695, loss_train: 5.749, loss_val: nan, pos_over_neg: 5883.60205078125 lr: 0.00031623\n",
      "Iter: 13/695, loss_train: 5.7554, loss_val: nan, pos_over_neg: 811.2192993164062 lr: 0.00031623\n",
      "Iter: 14/695, loss_train: 5.7539, loss_val: nan, pos_over_neg: 1148.5386962890625 lr: 0.00031623\n",
      "Iter: 15/695, loss_train: 5.75, loss_val: nan, pos_over_neg: 2106.351318359375 lr: 0.00031623\n",
      "Iter: 16/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 1195.687255859375 lr: 0.00031623\n",
      "Iter: 17/695, loss_train: 5.7479, loss_val: nan, pos_over_neg: 2007.29150390625 lr: 0.00031623\n",
      "Iter: 18/695, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1315.8951416015625 lr: 0.00031623\n",
      "Iter: 19/695, loss_train: 5.7568, loss_val: nan, pos_over_neg: 1223.83349609375 lr: 0.00031623\n",
      "Iter: 20/695, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1916.549072265625 lr: 0.00031623\n",
      "Iter: 21/695, loss_train: 5.7507, loss_val: nan, pos_over_neg: 1145.0479736328125 lr: 0.00031623\n",
      "Iter: 22/695, loss_train: 5.7557, loss_val: nan, pos_over_neg: 673.9437866210938 lr: 0.00031623\n",
      "Iter: 23/695, loss_train: 5.7593, loss_val: nan, pos_over_neg: 819.7024536132812 lr: 0.00031623\n",
      "Iter: 24/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 2365.67822265625 lr: 0.00031623\n",
      "Iter: 25/695, loss_train: 5.754, loss_val: nan, pos_over_neg: 2472.453125 lr: 0.00031623\n",
      "Iter: 26/695, loss_train: 5.747, loss_val: nan, pos_over_neg: 2691.23681640625 lr: 0.00031623\n",
      "Iter: 27/695, loss_train: 5.7485, loss_val: nan, pos_over_neg: 1521.820068359375 lr: 0.00031623\n",
      "Iter: 28/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1288.503173828125 lr: 0.00031623\n",
      "Iter: 29/695, loss_train: 5.7562, loss_val: nan, pos_over_neg: 859.1744384765625 lr: 0.00031623\n",
      "Iter: 30/695, loss_train: 5.7498, loss_val: nan, pos_over_neg: 2196.79345703125 lr: 0.00031623\n",
      "Iter: 31/695, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1209.842041015625 lr: 0.00031623\n",
      "Iter: 32/695, loss_train: 5.7516, loss_val: nan, pos_over_neg: 1529.66357421875 lr: 0.00031623\n",
      "Iter: 33/695, loss_train: 5.7526, loss_val: nan, pos_over_neg: 798.4937744140625 lr: 0.00031623\n",
      "Iter: 34/695, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1052.6422119140625 lr: 0.00031623\n",
      "Iter: 35/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1402.4193115234375 lr: 0.00031623\n",
      "Iter: 36/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1457.2537841796875 lr: 0.00031623\n",
      "Iter: 37/695, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1714.5260009765625 lr: 0.00031623\n",
      "Iter: 38/695, loss_train: 5.7476, loss_val: nan, pos_over_neg: 1026.0374755859375 lr: 0.00031623\n",
      "Iter: 39/695, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1065.5318603515625 lr: 0.00031623\n",
      "Iter: 40/695, loss_train: 5.7547, loss_val: nan, pos_over_neg: 4976.0771484375 lr: 0.00031623\n",
      "Iter: 41/695, loss_train: 5.7481, loss_val: nan, pos_over_neg: 1658.621826171875 lr: 0.00031623\n",
      "Iter: 42/695, loss_train: 5.7512, loss_val: nan, pos_over_neg: 2399.112060546875 lr: 0.00031623\n",
      "Iter: 43/695, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1021.8055419921875 lr: 0.00031623\n",
      "Iter: 44/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 2085.267333984375 lr: 0.00031623\n",
      "Iter: 45/695, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1512.3917236328125 lr: 0.00031623\n",
      "Iter: 46/695, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1107.2064208984375 lr: 0.00031623\n",
      "Iter: 47/695, loss_train: 5.7491, loss_val: nan, pos_over_neg: 1080.8607177734375 lr: 0.00031623\n",
      "Iter: 48/695, loss_train: 5.751, loss_val: nan, pos_over_neg: 3072.737060546875 lr: 0.00031623\n",
      "Iter: 49/695, loss_train: 5.7509, loss_val: nan, pos_over_neg: 4737.11279296875 lr: 0.00031623\n",
      "Iter: 50/695, loss_train: 5.7513, loss_val: nan, pos_over_neg: 2019.60986328125 lr: 0.00031623\n",
      "Iter: 51/695, loss_train: 5.7449, loss_val: nan, pos_over_neg: 982.2710571289062 lr: 0.00031623\n",
      "Iter: 52/695, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1009.1521606445312 lr: 0.00031623\n",
      "Iter: 53/695, loss_train: 5.7482, loss_val: nan, pos_over_neg: 1603.1231689453125 lr: 0.00031623\n",
      "Iter: 54/695, loss_train: 5.7518, loss_val: nan, pos_over_neg: 1549.69189453125 lr: 0.00031623\n",
      "Iter: 55/695, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1339.3626708984375 lr: 0.00031623\n",
      "Iter: 56/695, loss_train: 5.7457, loss_val: nan, pos_over_neg: 2068.6416015625 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=default'\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "\n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss  Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix  Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-TransferL2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE  RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=32,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(-1)).fit(features_train, y_labeled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
