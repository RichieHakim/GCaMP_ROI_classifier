{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joz608/.conda/envs/jupyter_launcher/bin/python3.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "outputs": [],
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(f'{base_dir}/github_repos')\n",
    "# sys.path.append(f'/media/rich/Home_Linux_partition/github_repos')\n",
    "dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition\n",
    "from GCaMP_ROI_classifier.new_stuff import util, models, training_simCLR, augmentation, training_classHead, training_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unlabeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_balanced.h5')\n",
    "data_unlabeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_balanced.h5')\n",
    "\n",
    "masks_cat_raw = torch.as_tensor(np.concatenate((data_unlabeled['SYTmasks'], data_unlabeled['NPmasks'], data_unlabeled['RHmasks']), axis=0), dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_labeled = h5_handling.simple_load(path=r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "data_labeled = h5_handling.simple_load(path=f'{base_dir}/label_data/masks_20211202_unbalanced.h5')\n",
    "\n",
    "masks_SYT = data_labeled['SYTmasks']\n",
    "labels_SYT = classification.squeeze_integers(data_labeled['SYTlabels'])\n",
    "\n",
    "nan_lst = np.concatenate(np.where(np.isnan(masks_SYT).sum(axis=-1).sum(axis=-1)))\n",
    "non_nan = [_ for _ in range(masks_SYT.shape[0]) if _ not in nan_lst]\n",
    "labels_SYT = labels_SYT[non_nan]\n",
    "masks_SYT = masks_SYT[non_nan]\n",
    "\n",
    "X_labeled_train_SYT, X_labeled_val_SYT, y_labeled_train_SYT, y_labeled_val_SYT = train_test_split(masks_SYT, labels_SYT, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([711808, 36, 36])\n",
      "Number of masks: torch.Size([711807, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {masks_cat_raw.shape}')\n",
    "\n",
    "ROIs_without_NaNs = torch.where(~torch.any(torch.any(torch.isnan(masks_cat_raw), dim=1), dim=1))[0]\n",
    "masks_cat = masks_cat_raw[ROIs_without_NaNs]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "S8AO_lypDB0o",
    "outputId": "4edfd739-a0b7-4789-aea1-4a9f6c2665c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated images shape: (9715, 32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjIUlEQVR4nO2deZhlVXXof6vuvTV0VVfP3TTdDSggEU1A0gGcefLwIcaocUSfYoKiCRrxiYoYA04RDWLQ+NRWUVRwHuMUSavh8VS0IS1gUKY0dNMT1WN1jXdY+eOceu92edaqW7eq7i056/d99dW9e5+99zr7nHWGve5aS1SVIAge/nS0W4AgCFpDKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTsiNsovIx0Tk7Q1u+xMReaVRJyLyaRHZJyK/mF0pp4+IvFREfthuOeYrIqIiclybxj5fRP66HWNn8bBUdhF5hYjcVF+mqq9R1XfNQvdPAs4C1qrqqTPpyLuoNIqqXqeqT29wvN+ZlzyTzr+KyEmTyr+Zlp+Rfr9cRD5fV68iMiQih0TkQRG5SkQKk/p4E/AO4I0i8jvnnYgsFZFvpP3cLyIvceQ8T0RuEZGDIrJNRN4vIsXp7u/DUtnnmKOBLao61G5Bmjngc818lGkK7gJePvFFRJYBpwMPTdHuJFXtA84EXgK8qq6P84C/Ap4CPBl4vohcOKn9R4BxYBXwUuCjIvIYY6wFwEXAcuC0dMyLG9i3w1HVtv4BbwEeBAaB3wJnpuWXA18FvpTW3ZpO8ES7S4B707r/AJ6blj8aGAWqwCFgf1r+GeDd6eclwHdIDui+9PPaur5/ArwyQ9bzJ/X9jrT8VcA9wF7g28CRdW2eAPwSOJD+f0Ja/p60n9G0r39KyxX4G+A+YAD4B6AjrXsF8H+BD6ZjvTstu6luPAVeA9yd7ttHAHHm5Zx0/gbT43Cxc6xeBdxZN+enpOVb0uN4GzAGFIE/A34N7E/n89F1/WwB3pr2sQ/4NNCd1p0BbAMuTfd/C/DSurZdwJXAA8Au4GNAT139m4AdwHbgL9P5OM7Yn58Af5eOV0jLXgt8NC07o+5c/PykOT6u7vtX6o7fM9P9WldXvxL4d+AF6fdeEkV/VN02nwOuaFBn/hfwz9PWtTYr+gnAVlLlAI4Bjq2b4DLwfKBEciX7T6CU1r8AOJLk6eRFwBCwuk4pbpo01mf4/8q+DHgeyRVzYXqwvjmVsmf1DTwtPSlPSU/EDwM3pnVL05P5ZSQKcG76fZk1Tnoi/ThtexTJneeVdWNXgNel/fVkyKMkF6/FafuHgLOdedkBPLnuIniKsd8vILkY/AnJxeM44Og65d0MrEtlelR6PM5Kj92bSS6GnXXb35Fuv5TkAjZxbM5I9/GqdD6fmvZ1Qlr/jyQX1KXpsftn4L1p3dkkF4DHkijU9Uyt7K8Efgg8Iy37BfB4GlR24ERgJ3D+NM77xwEjk8oupkEFBr5JgxeG+r92P8ZXSQ7oiSJSUtUtqnpvXf0tqvpVVS2THPxukkcsVPUrqrpdVWuq+iWSO1lD79CqukdVv6aqw6o6SHKXfWqT+/BS4BpVvVVVx0juWI8XkWNIrvJ3q+rnVLWiql8AfgM8a4o+36eqe1X1AZKT+9y6uu2q+uG0vxGj/RWquj9t/2PgZGesMsn896vqPlW91djulcD7VfWXmnCPqt5fV/8hVd2ayvQi4LuqekN67K4kuQg8oW77f0q330sy//X7CPB2VR1T1X8Dvgu8UESE5OniDen8DAJ/D7w4bfNC4NOqeocmr1mXO/tdz2eBl4vICcBiVf1ZA21uFZF9JBebT5I8nTRKH8mTXj0HSC5eLiLyF8B6kjmdFm1VdlW9h+Rd5HJgt4h8UUSOrNtka922NZKr7ZEAIvJyEdksIvtFZD/J1Xx5I+OKyAIR+Xi6MHIQuBFYPHmRpUGOBP7fSa+qh4A9wJrJdSn3p3UeW+s+35/2k1VnsbPu8zDJyWXxPJJH+ftF5N9E5PHGdutIXpss6uWaPCe1tH6Nsf3kfdynh6+JTNSvIHkau6XuuP8gLZ8Yd3K/jfB1kie015E8TjfCKaq6RFWPVdW/TfexUQ4B/ZPK+klej0xE5DnAFSRPIQPTGA+YBwt0qnq9qj6JZOFLgffVVa+b+CAiHcBaYLuIHA18guT9apmqLiZ5LJSJbqcY9o0krxCnqWo/yUIKde2nw/ZU9gk5e0leEx6cXJdyVFrnybmu7vNRaT8TzMRN8XfapnfqZ5O8V34T+LLRditwbIN9T54TIdmnB+u28fZxSTqPk+sHgBHgMaq6OP1bpMlCGSSvJJP7nRJVHQa+T7Ko1qiyz4S7gKKIHF9XdhLJGkcmInI2yTn/LFW9vZlB26rsInKCiDxNRLpIFo9GSB7tJ/hjEfnzdIX3IpLFn5+TvI8p6Ypp+mjz2Lp2u4C1ItJpDL0wHWu/iCwFLpvBblwP/IWInJzux98DN6vqFuB7wKNE5CUiUhSRF5G8432nTs5HZvT5JhFZIiLrgNeTLFLOBofNi4h0pnb6Renj9kEOn/96PglcLCJ/nP7W4Lj0opvFl4FnisiZIlIiubiOAT+t2+ZCEVmbzv+l/O4+viOV78nAnwJfSe+enwA+KCIr031YIyL/o27cV4jIiSKygOkd10uBp6bHbU5Jn1q+DrxTRHpF5InAszEuNCLyNOA64Hmq2vRvO9p9Z+8ieSwZIHn0XEky6RN8i+T9b2KR689Vtayq/wF8APgZyQn8hySLPBP8iOQquVNEsh53/pHkHXKA5OLxg2Z3QFU3Am8HvkZyZzmW9B1SVfeQnKhvJHm0fzPwp3WPYFeTmGX2iciHJu33LSSLXt8FPtWsfJPImpeXAVvS15nXAP8zq6GqfoXk3fp6ksfNb5IskmVt+9u0nw+TzPGzSO5I43WbXU+yMHZf+vfuurqdJMd8O8lJ/hpV/U1a9xaSxb6fpzL/K8lTGqr6fZJj+6N0mx9NMR/1Mm9X1Vb+BuGvSc7B3cAXgL9S1V8DiMhRqQ1/4snk7cAi4Htp+SER+f50B5R0dW/eISKXk6x4Zp58D1dERIHj0/WMhyUisoXEwvCvGXVnkKx8r22xWA972n1nD4KgRYSyB0FOmLeP8UEQzC5xZw+CnNBSp4VO6dJueqfecJ5SWTF92YsP2f4y5SOc/rwHLufXAGrUde6w5aistOVQ53Ygzs9IataZ1YTsAOLMh1ScOktGTw5nn70675h58pd2zp5P1ShDjOtY5t7NSNlTQ//VQAH4pKpe4W3fTS+nyZkzGbKt7H7hEzLLpWYfyRUfs395ue0vs/sDKIybVVStXw9gn4zr3vPT7Apg94tsOcadH3B2Or/3GlmZPSfq/EaxVrLrvAtLzy5bc4vD2XLUSnabsvN7w3Kvfaw7KnafhVG7zzXvs4/NdLlZN5p1TT/Gpz8t/QjwDJIfipwrIic2218QBHPLTN7ZTwXuUdX70h9LfJHkV0BBEMxDZqLsazjc6WAbGQ4eInKBiGwSkU1lxmYwXBAEM2Emyp71gpLlaLFBVder6voSXTMYLgiCmTATZd/G4R5GaznccykIgnnETFbjfwkcLyKPIHFdfDFJLK6HLSv/d/aq6c7X26vZuy+061yaNL1V+rIbbnurLUdHuUGZJjHqRA8YPzr7la1jwDMl2FWFUWfFfchuWF6Y3a7qPGRWnBV3z5pg+gvOE5pWdlWtiMhrgX8hMb1dM+G1EwTB/GNGdnZV/R6Jz3YQBPOc+LlsEOSEUPYgyAmh7EGQE0LZgyAn/L6l6pmXHHG17cgw8GorMrPvOKHOkfFMZYURo0/Py8sbyzEnjSy3KwulbM+VapfjSJLtrAX43mbj/Xa7iuHQ5/Xnmfk6HAclz9nFO0daRdzZgyAnhLIHQU4IZQ+CnBDKHgQ5IZQ9CHJCS1fjK8t72fPc7NXpZZ9sJHHm7x/LP27v167X2c4p1e7mxrPCN5kx4YBap+P44bTTLjtWVK2avaJdGLLvL1p0PGGcqvFF0w8VdfRl9uq45zTkzeOaebDi7hF39iDICaHsQZATQtmDICeEsgdBTghlD4KcEMoeBDmhpYkdu9eu03UXviGzrnjIdj5Y+97WmTQOvPR0s85yCln4xZ/Puhxb326bf4rDdjvLNDS81nZa8RxQOsYdJ5N+J01Lv+GtM2infSkM2veeDifFU80Ja1cazJa/dMhuU3HMns2eiw9cbh9PK+2VZx60uFk3clD3ZvYYd/YgyAmh7EGQE0LZgyAnhLIHQU4IZQ+CnBDKHgQ5Yd6Y3nCsOMXhbNtEccRuI07stFUfbs58su+8bI+9mm1NojRsz+++P7DNWiXHFFlZYI9X6cker9rnTLBDhxOPzTIZAdT6s21lHYdstzFvrFp3cx5xlknXOqcACk6yYc8EOLLSEcSZq2PeNn2Pz6HnnZZZftvGqzm0d2vmaDNycRWRLcAgSZariqqun0l/QRDMHbPhz/7fVHVgFvoJgmAOiXf2IMgJM1V2BX4oIreIyAVZG4jIBSKySUQ2VYeGZjhcEATNMtPH+Ceq6nYRWQncICK/UdUb6zdQ1Q3ABkgW6GY4XhAETTKjO7uqbk//7wa+AZw6G0IFQTD7NH1nF5FeoENVB9PPTwfe6bXpfHDINDNsfZsT5K8ru7zsSO+ZT5plybXZsu97hZ3iaWyRbXMRJ/3TyCrbVOZ5qdWc9EoWaqRqAqgWHPnHHC+1weyDYwXEBNCCU+elr+q2O60Y97OilSYLPzXUyConOKcj4yMutc1r+1+eff5UjfMeME3VNWcOZ/IYvwr4hohM9HO9qv5gBv0FQTCHNK3sqnofcNIsyhIEwRwSprcgyAmh7EGQE0LZgyAnhLIHQU5oqddbvyzV0+TMabfb8q5s04RnqukoOyYjx/xz1DtmN7jlnvNts1y5z5Zx+Ej7uLi52YzLty603bU6SraLoFad3Gyj9gGwcrrVSk5eNie4pee1J86xtjzienbYso8tc8YyctgBLLvd3rdKlyOjUVUctftb9PnsIKcRcDIIglD2IMgLoexBkBNC2YMgJ4SyB0FOmI1INXOOFS/McpABf9W35lzi7v2Anf7JcnTw4sV5q7ejq52AZg7u6rOxb90Lbc+gasWZkKK9Ml12VuOrfcYKf81x4nGsKx7qHGtrNX7kSCdIodNd6YAt/8BJdt2xb5p+nLnZJu7sQZATQtmDICeEsgdBTghlD4KcEMoeBDkhlD0IcsLvhenNolZ0HEKajGfmmYZ0QbYZamxJ2enQwTFdUXAcRpx2R/zhrszyU5ZvNdts3rPWrNu+Z5FZJ92OA40VX6/Tia3XZffX2WmbKUXsuSqPZ5/ilb3ddn9OjL/yIlv+4nBz987tF9vxFy2OvHL6DltxZw+CnBDKHgQ5IZQ9CHJCKHsQ5IRQ9iDICaHsQZATWmp6K6/qZft52WaGaqfdTkvZ5ZY3HECl1zbHVHuai7tXOJR9bdRhW3h1TGiet5Z2OF57K8bNuh0D2aayLT3DZps9hxaYdSUnPl1Hl21yVMO+aZUDrOg/ZNYNDPaadeNjxgkCVEaMU9wx1+Gci6UD9v2xc7+9b156MzVE9OIhjp3zJ9l93WR71015ZxeRa0Rkt4jcUVe2VERuEJG70/9LpuonCIL20shj/GeAsyeVXQJsVNXjgY3p9yAI5jFTKnuab33vpOJnA9emn68FnjO7YgVBMNs0u0C3SlV3AKT/V1obisgFIrJJRDZVR4aaHC4Igpky56vxqrpBVder6vpCj73IEgTB3NKssu8SkdUA6f/dsydSEARzQbOmt28D5wFXpP+/1VArgZoxomVeA6h0Z5tJio4ZR7x4gl2et5xj8rLGctIWWal9ALTHEdLzeivZnlfrVk5eXkkYqdgT3L9g1JbDoepE7hwrZx/oMcMLDWDbTtuo4+1zoeDk82oCL6Cnl6Kq0mP3WXSm+Mj3ZJvYxp6ZbV4D0A7PdTObRkxvXwB+BpwgIttE5HwSJT9LRO4Gzkq/B0Ewj5nyzq6q5xpV08/QGARB24ifywZBTghlD4KcEMoeBDkhlD0IcoKoNucB1gz9slRPk+x1vQcus72Cap3ZMlYdE5qXY80zr7HazolmBTasDDl2Q0eOxasPmnULu205+rtsO84j+wYyywfG+sw2HtsOLTbreoq215s1V7sGF5ptSkXbFFkq2HXjFXudec9A9njqBfv0POKc26OM2pU92+3xltyVvW+ji+3+yv3Z59U9113FyM6tmZVxZw+CnBDKHgQ5IZQ9CHJCKHsQ5IRQ9iDICaHsQZATWhpwcuyoBdz11lMz65bcZps7xpZmmxk6nJxclQXN5YGrDdtT0tmfbQ7r6LHzkNUcE09Hh+2tVXM8+vaM2AEii7I0s7zbMZP90cIHzbqugr1vO4b7zbq+UvZcaZ+9X2NVe+4Xd42YdQ8cWGzWieUR5wQdVStIJX4eOC9P4PgSe7wdT85uJxXH89E4nFbwSog7exDkhlD2IMgJoexBkBNC2YMgJ4SyB0FOaOlqPCp0jGdfXwYfYTeTWvaqZGHEXv0seKumXviuQXv1vDyeHWRMi/aqescCezV7ZMzOM1Sp2nKs7rcdaJZ0Zad5Wt11wGxTcybkqJ7smHYAK7sGzbrVnfszy3eMLzbbbN671qwbGLEjE1tONwBi3M5qxnkIuPH/vLRcVJwTa8Q+noXR7HY1x2rknsMGcWcPgpwQyh4EOSGUPQhyQih7EOSEUPYgyAmh7EGQE1pqepMKdD2UfX2pOfHkakaIt/Ii2+Tl+JG48encdE2Giae0yI4X19lpx05b3mdnte3rtPtcUBw36x6/6N7M8uGabeY7q/dOs+5Ixww1pPb8X3fgcZnlY1b+L6DgOAZ1FW0TphefbnQ8++QZHnFyNTkmNPFMdg6VfltGsRxovKxWRooq77xvJP3TNSKyW0TuqCu7XEQeFJHN6d85U/UTBEF7aeQy9Rng7IzyD6rqyenf92ZXrCAIZpsplV1VbwTsn1EFQfB7wUwW6F4rIrelj/lmrl0RuUBENonIpuqQ/Y4aBMHc0qyyfxQ4FjgZ2AF8wNpQVTeo6npVXV/otX/fHATB3NKUsqvqLlWtqmoN+ASQHWsqCIJ5Q1OmNxFZrao70q/PBe7wtp9ACzC2LNue4MWFM9PxOHYGK0YXgBYdM1+PY+8wPJ7Kg11mk9LSbC80gErNvtZ2F+wd8NptGV2eWf6yJT832xxdtE+DBR22yW60csise2zP1szygbKdhuqYPntpaL/hcQh+LLzHrNqZWb5vsR3H74EB862U8WE71Zd4HnFV+5ipkTbK8hCFKczHBlMqu4h8ATgDWC4i24DLgDNE5GRAgS3Aq6c9chAELWVKZVfVczOKPzUHsgRBMIfEz2WDICeEsgdBTghlD4KcEMoeBDmhtV5vNSgdslLdOJ5GluXNuVSJY0EbX+y5Ezk04Z00Omybrsa7be+1ihNt8Khe20R133C26e23fSvNNsNq93dCadRpZ1axZXxFZvmiop3G6WCl26yrOAf7mIW2/Hftz5aj5HjYLeqzZdxTto9LbchRp5IzWUZdzfHm04oxH45OxJ09CHJCKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTmip6a04AstuzzZ5DK7z3N6y6bBjEDK21K7TlXYwx5KTt62yxzAN9dsear0LbdOVFyjxP/fZO/CQk/dsRU92gJCfHPwDs01fwQlu2WGbBxc47e4aPiKzfPvwIrPNUMU2Uw6XbW+zmuP9OFbOPsX3GYEoAcZG7DptMqKjDNnnd4fh9Vawp54OI+CkODoRd/YgyAmh7EGQE0LZgyAnhLIHQU4IZQ+CnNDS1fhqJxw8OntVcnyR7ShQXWA4CnjOBc6qqQzYMeMqTrojM3bdIXsaD6kdO21szF71XbV40KzzUkONGw40407apc2D2c4iAH0le6wO7LkqdmRbGjoL9nLx/fvt2G8Lux0LiuPUUqlmz0exaFtCRsq2Q05xwIlB56yEezEWa51GbEMnJVoz48SdPQhyQih7EOSEUPYgyAmh7EGQE0LZgyAnhLIHQU5oJCPMOuCzwBEk0dY2qOrVIrIU+BJwDElWmBeq6j6vr1oJRlZnm0mqTtolKz5dYdi+Vlmx7gC6HClHbSsUle5sU0h1gS27lm0ZuxbZDjRdRduO88A+20S1vC/bEebO/avMNh1WkD/8NFSHyrYJc3A8u84zky3osj0/yk7Kqy7HoahgjDc2ZpvXPKqrbBOgjjrOLsN2XXEo+1wtDtn7XDMsgDN1hKkAb1TVRwOnAxeKyInAJcBGVT0e2Jh+D4JgnjKlsqvqDlW9Nf08CNwJrAGeDVybbnYt8Jw5kjEIgllgWu/sInIM8DjgZmDVRCbX9L8dqzgIgrbTsLKLSB/wNeAiVT04jXYXiMgmEdlUHcp+nwyCYO5pSNlFpESi6Nep6tfT4l0isjqtXw3szmqrqhtUdb2qri/02hFWgiCYW6ZUdhERkhTNd6rqVXVV3wbOSz+fB3xr9sULgmC2aMTr7YnAy4DbRWRzWnYpcAXwZRE5H3gAeMGUPYmdsqnrIVuUoy/7aQNiHs6e8x9v1lW7nDhiTiqnmmF6w+mOcft6OrhjoV23067r6LPNYQNG+ap+24tu/4hthhocs81rB4fsdr092Saq/U7styW9dtqlUafdzt1OwEHDi7HQZZvrHGc+GLTl6Bh3TgTvHDHqyv22IKWDRgw6R/YplV1Vb7LF4cyp2gdBMD+IX9AFQU4IZQ+CnBDKHgQ5IZQ9CHJCKHsQ5ISWBpzs2jrEcW/4+az1d/Alp5t144udlEBLvKCS9nimR5ET3FKdAJauOcbD6bJSyb5+7x7sM9sMH7LNa109jmdep+1iNTKWncqpULBtmwccE6DlzQewYJ3tLbd7b39meXWPvc9WaiWYwrzm4LWy+qz02Ae60ptdZ5m2Ie7sQZAbQtmDICeEsgdBTghlD4KcEMoeBDkhlD0IckJLTW8eQ88/zawb78u+JnWUbdNEuUnX+Z6dtpFkfFF23fgiJ1imE3CSDseG5jSrHbI9r6qGqa88Zh9qdUyH404+uu5O2yw3OGTkuHNsUF5wzqEHbS9A7XE82GrGgL12m9IOe58LI/YOFGwLoGvuHVuWLYsXUFUc70yLuLMHQU4IZQ+CnBDKHgQ5IZQ9CHJCKHsQ5ISWrsZXlvey5znZseG81cWln/7ZtMda5NTtvOgJZp27ytmED4R2Tj+tFUDXLjtd0NgqeyW5MpbdruDIURt1Vn2dNFRVJyVTsSu7XbVs75djFKDgpELS0SZivx1hp3EqDGc78QCovVCPOqmXOg/YMj7irTdnlm/9W/s8rVpOMs5UxJ09CHJCKHsQ5IRQ9iDICaHsQZATQtmDICeEsgdBTpjS9CYi64DPAkeQJEfaoKpXi8jlwKuAh9JNL1XV77l9KXQY5okl107fvObhmde8GG7iZQUyzBq1Hsde55nrLCcNQG0LFVpyzHmGc03NMfNRteu8WG2D446QhhwlwyQH4IlYWWJXyph9z7JMdoV77Xh3pWFbDs/Byov/VrKzb7Hrddnn6rp3Tz/t2S61Y/U1YmevAG9U1VtFZCFwi4jckNZ9UFWvnLZEQRC0nEZyve0AdqSfB0XkTmDNXAsWBMHsMq13dhE5BngcMPGTn9eKyG0ico2ILJlt4YIgmD0aVnYR6QO+BlykqgeBjwLHAieT3Pk/YLS7QEQ2icimyqj9PhEEwdzSkLKLSIlE0a9T1a8DqOouVa2qag34BHBqVltV3aCq61V1fbG7yfAxQRDMmCmVXUQE+BRwp6peVVe+um6z5wJ3zL54QRDMFqLq2KEAEXkS8H+A20lMbwCXAueSPMIrsAV4dbqYZ9IvS/U0OXNmEs8xA6/O9soDGD7CSNNjpOIBP/2Tl0qoa69dN7zaNr11rst+VRo9YJvQXHczxyzX4cRI004jPVG3Y9scd7zoBm0zX8F2YDM9C71j1uHEklvgxCisGGH3ADrscH0ccfX0TWwWN+tGDmr2ydPIavxNZFuLXZt6EATzi/gFXRDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOmDfpn+YLyz9ue99te2u2d1LBcf4aX2ybeArDjlnLMf8Uxux2xWK2aaur37ZPjQ04NiMnUKV3rygOGnVWOVBzzsZayTGVOe5y/fdmtzvwKHuscr891qDtLEfPbluOmh3DsmXEnT0IckIoexDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOCNPbNFj73mzvpHv/wfaU8yg6pjdxnBG7H3JMPAcWZ5aXVzpBKr3glnaVe6swzYOeJa/bMa85HnaVPrvTA8dly9HjeK8VFjuebY633Ngyu87zYmwVcWcPgpwQyh4EOSGUPQhyQih7EOSEUPYgyAmh7EGQE8L0Ng22vi3b663WZQdR7HDykHXtt001tVJzZjm7URNtgI5R537gVFWNgJOlQVuQ8aX2jhX32O1KB2xBqj3ZfY4vMpvQPWDLseJdtlfkg2+x8wvWHPNmq4g7exDkhFD2IMgJoexBkBNC2YMgJ4SyB0FOmHI1XkS6gRuBrnT7r6rqZSKyFPgScAxJ+qcXquq+uRO1/YytyHa4UCc+WnGfvYrsZV1yPVAcZ5LCqNPOQJyMTF68u2qXl/Yqu7yywInJN9qcA0rnfieWXzm7zlqlBygvbM50seZ9s5fGaS5o5M4+BjxNVU8iye12toicDlwCbFTV44GN6fcgCOYpUyq7JhxKv5bSPwWeDVybll8LPGcuBAyCYHZoND97QUQ2A7uBG1T1ZmDVRNbW9P/KOZMyCIIZ05Cyq2pVVU8G1gKnishjGx1ARC4QkU0isqmMk1s3CII5ZVqr8aq6H/gJcDawS0RWA6T/dxttNqjqelVdX8LJER4EwZwypbKLyAoRWZx+7gH+O/Ab4NvAeelm5wHfmiMZgyCYBRpxhFkNXCsiBZKLw5dV9Tsi8jPgyyJyPvAA8II5lLNl3P9OO55crbeSXWGYdwCKI/ZYoo59reY4wjimt5UfMeLkXXm63cjBMw8Wxp1YeJY50knV5Jneqk58uuKwWcWqD2XPx/Y3204r1ZLd366/sdtZY80XplR2Vb0NeFxG+R7gzLkQKgiC2Sd+QRcEOSGUPQhyQih7EOSEUPYgyAmh7EGQE0Q9889sDybyEHB/+nU5MNCywW1CjsMJOQ7n902Oo1V1RVZFS5X9sIFFNqnq+rYMHnKEHDmUIx7jgyAnhLIHQU5op7JvaOPY9YQchxNyHM7DRo62vbMHQdBa4jE+CHJCKHsQ5IS2KLuInC0ivxWRe0SkbYEqRWSLiNwuIptFZFMLx71GRHaLyB11ZUtF5AYRuTv9v6RNclwuIg+mc7JZRM5pgRzrROTHInKniPxaRF6flrd0Thw5WjonItItIr8QkV+lcrwjLZ/ZfKhqS/+AAnAv8EigE/gVcGKr5Uhl2QIsb8O4TwFOAe6oK3s/cEn6+RLgfW2S43Lg4hbPx2rglPTzQuAu4MRWz4kjR0vnhCQNZ1/6uQTcDJw+0/lox539VOAeVb1PVceBL5JEqs0NqnojsHdSccuj9RpytBxV3aGqt6afB4E7gTW0eE4cOVqKJsx6ROd2KPsaYGvd9220YUJTFPihiNwiIhe0SYYJ5lO03teKyG3pY/6cv07UIyLHkARLaWsE40lyQIvnZC4iOrdD2bNiD7XL/vdEVT0FeAZwoYg8pU1yzCc+ChxLkhBkB/CBVg0sIn3A14CLVPVgq8ZtQI6Wz4nOIKKzRTuUfRuwru77WmB7G+RAVben/3cD3yB5xWgXDUXrnWtUdVd6otWAT9CiORGREomCXaeqX0+LWz4nWXK0a07SsfczzYjOFu1Q9l8Cx4vII0SkE3gxSaTaliIivSKycOIz8HTgDr/VnDIvovVOnEwpz6UFcyIiAnwKuFNVr6qraumcWHK0ek7mLKJzq1YYJ602nkOy0nkv8LY2yfBIEkvAr4Bft1IO4Askj4Nlkied84FlJDnz7k7/L22THJ8DbgduS0+u1S2Q40kkr3K3AZvTv3NaPSeOHC2dE+CPgH9Px7sD+Lu0fEbzET+XDYKcEL+gC4KcEMoeBDkhlD0IckIoexDkhFD2IMgJoexBkBNC2YMgJ/wX9bp5bGZDJEAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV1klEQVR4nO3df6zd9X3f8ecrhBCU4AbEBTm2F7PM6QZIcYrlUaFtWciKS7KadGMybQCpTM4Y0cjWroJIU5NJ3lKt+THUweY0CLNmYZaSFC8/mlKSLItG4lyYAxiHxgsOOPbwza/GbBKtnff+OB+Uo8ux77n2vecm9/N8SEfne97fz+d8P19Zft2vPud7zidVhSSpDy9Z6gFIkibH0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihr2UvyYEkbx6zbSX5a6d4nFPuK02KoS8tsSS/luTbSf5vkj9Kct5Sj0nLl6EvLaEklwD/CbgeuBD4f8CdSzooLWuGvrqSZGOSh5L8MMnhJL+f5GWzml2d5FtJvpvk3yV5yVD/30iyL8kPknwuyWtOcJyrkzyR5GiS7yT5rRMM6deB/1ZVX6qq54B/BfxqknMW5ISlWQx99eY48M+B84FfBK4E/umsNm8DNgC/AGwGfgMgyTXAu4FfBaaA/wF87ATH+Qjwjqo6B7gU+PwJ2l0CfP2FF1X1v4G/AF43v9OSxmPoqytV9XBVfaWqjlXVAQZTK39nVrPfrarvV9XTwIeA61r9HcC/rap9VXUM+DfA+hNc7f8lcHGSFVX1g6p65ARDeiXw57Nqfw54pa9FYeirK0lel+RTSf5Pkh8xCO7zZzV7Zmj728Cr2/ZrgH/fpoZ+CHwfCLBqxKH+AXA18O0k/z3JL55gSM8BK2bVVgBHxz0naT4MffXmLuAbwLqqWsFguiaz2qwZ2v4rwKG2/QyDKZtXDT3Orqr/OfsgVfW1qtoMXAD8EbDzBOPZC7z+hRdJ/ipwFvBn8z4zaQyGvnpzDvAj4Lkkfx24eUSbf5nk3CRrgFuB/9rq/xG4vd1xQ5KfS3Lt7M5JXpbk15P8XFX9ZTve8ROM56PA30/yt5K8AvjXwCeqyit9LQpDX735LeDXGEyffJifBPqw+4GHgT3Apxl8KEtVfRL4XeC+NjX0OPDLJzjO9cCB1u6fAG8f1aiq9rb9HwWOMPijNPuDZWnBxEVUJKkfXulLUkcMfUnqiKEvSR0x9CWpIy9d6gHM5fzzz6+1a9cu9TAk6WfKww8//N2qmppd/6kP/bVr1zI9Pb3Uw5CknylJvj2q7vSOJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZOxv5CY5A5gGvlNVb01yHoMFKNYCB4B/VFU/aG1vB25isFrQP6uqz7X6ZcA9wNnAZ4Bbyx/0l7RMrb3t06fc98D73rKAI/mJ+Vzp3wrsG3p9G/BgVa0DHmyvSXIxsAW4BNgE3Nn+YMBgfdKtwLr22HRao5ckzctYoZ9kNfAW4A+GypuBHW17B3DNUP2+qnq+qp4C9gMbk6wEVlTVQ+3q/t6hPpKkCRj3Sv9DwG8DPx6qXVhVhwHa8wWtvgp4ZqjdwVZb1bZn118kydYk00mmZ2ZmxhyiJGkuc4Z+krcCR6rq4THfMyNqdZL6i4tV26tqQ1VtmJp60S+DSpJO0Tgf5F4B/EqSq4GXAyuS/CHwbJKVVXW4Td0cae0PAmuG+q8GDrX66hF1SdKEzHmlX1W3V9XqqlrL4APaz1fV24FdwI2t2Y3A/W17F7AlyVlJLmLwge3uNgV0NMnlSQLcMNRHkjQBp7OIyvuAnUluAp4GrgWoqr1JdgJPAMeAW6rqeOtzMz+5ZfOz7SFJmpB5hX5VfRH4Ytv+HnDlCdptA7aNqE8Dl853kJKkheE3ciWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjc4Z+kpcn2Z3k60n2Jnlvq78nyXeS7GmPq4f63J5kf5Ink1w1VL8syWNt3x1trVxJ0oSMs1zi88Cbquq5JGcCX07ywtq2H6yq3xtunORiBguoXwK8GvjTJK9r6+TeBWwFvgJ8BtiE6+RK0sTMeaVfA8+1l2e2R52ky2bgvqp6vqqeAvYDG5OsBFZU1UNVVcC9wDWnNXpJ0ryMNaef5Iwke4AjwANV9dW2651JHk1yd5JzW20V8MxQ94Ottqptz66POt7WJNNJpmdmZsY/G0nSSY0V+lV1vKrWA6sZXLVfymCq5rXAeuAw8P7WfNQ8fZ2kPup426tqQ1VtmJqaGmeIkqQxzOvunar6IfBFYFNVPdv+GPwY+DCwsTU7CKwZ6rYaONTqq0fUJUkTMs7dO1NJXtW2zwbeDHyjzdG/4G3A4217F7AlyVlJLgLWAbur6jBwNMnl7a6dG4D7F+5UJElzGefunZXAjiRnMPgjsbOqPpXkPydZz2CK5gDwDoCq2ptkJ/AEcAy4pd25A3AzcA9wNoO7drxzR5ImaM7Qr6pHgTeMqF9/kj7bgG0j6tPApfMcoyRpgfiNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR8ZZLvHlSXYn+XqSvUne2+rnJXkgyTfb87lDfW5Psj/Jk0muGqpfluSxtu+OtmyiJGlCxrnSfx54U1W9HlgPbEpyOXAb8GBVrQMebK9JcjGwBbgE2ATc2ZZaBLgL2Mpg3dx1bb8kaULmDP0aeK69PLM9CtgM7Gj1HcA1bXszcF9VPV9VTwH7gY1tIfUVVfVQVRVw71AfSdIEjDWnn+SMJHuAI8ADVfVV4MKqOgzQni9ozVcBzwx1P9hqq9r27Pqo421NMp1kemZmZh6nI0k6mbFCv6qOV9V6YDWDq/aTLW4+ap6+TlIfdbztVbWhqjZMTU2NM0RJ0hjmdfdOVf0Q+CKDufhn25QN7flIa3YQWDPUbTVwqNVXj6hLkiZknLt3ppK8qm2fDbwZ+AawC7ixNbsRuL9t7wK2JDkryUUMPrDd3aaAjia5vN21c8NQH0nSBLx0jDYrgR3tDpyXADur6lNJHgJ2JrkJeBq4FqCq9ibZCTwBHANuqarj7b1uBu4BzgY+2x6SpAmZM/Sr6lHgDSPq3wOuPEGfbcC2EfVp4GSfB0iSFpHfyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjLNG7pokX0iyL8neJLe2+nuSfCfJnva4eqjP7Un2J3kyyVVD9cuSPNb23dHWypUkTcg4a+QeA36zqh5Jcg7wcJIH2r4PVtXvDTdOcjGwBbgEeDXwp0le19bJvQvYCnwF+AywCdfJlaSJmfNKv6oOV9UjbfsosA9YdZIum4H7qur5qnoK2A9sTLISWFFVD1VVAfcC15zuCUiSxjevOf0kaxkskv7VVnpnkkeT3J3k3FZbBTwz1O1gq61q27Pro46zNcl0kumZmZn5DFGSdBJjh36SVwIfB95VVT9iMFXzWmA9cBh4/wtNR3Svk9RfXKzaXlUbqmrD1NTUuEOUJM1hrNBPciaDwP9oVX0CoKqerarjVfVj4MPAxtb8ILBmqPtq4FCrrx5RlyRNyDh37wT4CLCvqj4wVF851OxtwONtexewJclZSS4C1gG7q+owcDTJ5e09bwDuX6DzkCSNYZy7d64ArgceS7Kn1d4NXJdkPYMpmgPAOwCqam+SncATDO78uaXduQNwM3APcDaDu3a8c0eSJmjO0K+qLzN6Pv4zJ+mzDdg2oj4NXDqfAUqSFo7fyJWkjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWSc5RLXJPlCkn1J9ia5tdXPS/JAkm+253OH+tyeZH+SJ5NcNVS/LMljbd8dbdlESdKEjHOlfwz4zar6G8DlwC1JLgZuAx6sqnXAg+01bd8W4BJgE3BnkjPae90FbGWwbu66tl+SNCFzhn5VHa6qR9r2UWAfsArYDOxozXYA17TtzcB9VfV8VT0F7Ac2toXUV1TVQ1VVwL1DfSRJEzCvOf0ka4E3AF8FLqyqwzD4wwBc0JqtAp4Z6naw1Va17dn1UcfZmmQ6yfTMzMx8hihJOomxQz/JK4GPA++qqh+drOmIWp2k/uJi1faq2lBVG6ampsYdoiRpDmOFfpIzGQT+R6vqE638bJuyoT0fafWDwJqh7quBQ62+ekRdkjQh49y9E+AjwL6q+sDQrl3AjW37RuD+ofqWJGcluYjBB7a72xTQ0SSXt/e8YaiPJGkCXjpGmyuA64HHkuxptXcD7wN2JrkJeBq4FqCq9ibZCTzB4M6fW6rqeOt3M3APcDbw2faQJE3InKFfVV9m9Hw8wJUn6LMN2DaiPg1cOp8BSpIWjt/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZJzlEu9OciTJ40O19yT5TpI97XH10L7bk+xP8mSSq4bqlyV5rO27oy2ZKEmaoHGu9O8BNo2of7Cq1rfHZwCSXAxsAS5pfe5MckZrfxewlcGauetO8J6SpEU0Z+hX1ZeA74/5fpuB+6rq+ap6CtgPbEyyElhRVQ9VVQH3Atec4pglSafodOb035nk0Tb9c26rrQKeGWpzsNVWte3ZdUnSBJ1q6N8FvBZYDxwG3t/qo+bp6yT1kZJsTTKdZHpmZuYUhyhJmu2UQr+qnq2q41X1Y+DDwMa26yCwZqjpauBQq68eUT/R+2+vqg1VtWFqaupUhihJGuGUQr/N0b/gbcALd/bsArYkOSvJRQw+sN1dVYeBo0kub3ft3ADcfxrjliSdgpfO1SDJx4A3AucnOQj8DvDGJOsZTNEcAN4BUFV7k+wEngCOAbdU1fH2VjczuBPobOCz7SFJmqA5Q7+qrhtR/shJ2m8Dto2oTwOXzmt0kqQF5TdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNzhn6Su5McSfL4UO28JA8k+WZ7Pndo3+1J9id5MslVQ/XLkjzW9t3R1sqVJE3QOFf69wCbZtVuAx6sqnXAg+01SS4GtgCXtD53Jjmj9bkL2MpgsfR1I95TkrTI5gz9qvoS8P1Z5c3Ajra9A7hmqH5fVT1fVU8B+4GNSVYCK6rqoaoq4N6hPpKkCTnVOf0Lq+owQHu+oNVXAc8MtTvYaqva9uz6SEm2JplOMj0zM3OKQ5QkzbbQH+SOmqevk9RHqqrtVbWhqjZMTU0t2OAkqXenGvrPtikb2vORVj8IrBlqtxo41OqrR9QlSRN0qqG/C7ixbd8I3D9U35LkrCQXMfjAdnebAjqa5PJ2184NQ30kSRPy0rkaJPkY8Ebg/CQHgd8B3gfsTHIT8DRwLUBV7U2yE3gCOAbcUlXH21vdzOBOoLOBz7aHJGmC5gz9qrruBLuuPEH7bcC2EfVp4NJ5jU6StKD8Rq4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOnFfpJDiR5LMmeJNOtdl6SB5J8sz2fO9T+9iT7kzyZ5KrTHbwkaX4W4kr/71bV+qra0F7fBjxYVeuAB9trklwMbAEuATYBdyY5YwGOL0ka02JM72wGdrTtHcA1Q/X7qur5qnoK2A9sXITjS5JO4HRDv4A/SfJwkq2tdmFVHQZozxe0+irgmaG+B1vtRZJsTTKdZHpmZuY0hyhJesGcC6PP4YqqOpTkAuCBJN84SduMqNWohlW1HdgOsGHDhpFtJEnzd1pX+lV1qD0fAT7JYLrm2SQrAdrzkdb8ILBmqPtq4NDpHF+SND+nHPpJXpHknBe2gV8CHgd2ATe2ZjcC97ftXcCWJGcluQhYB+w+1eNLkubvdKZ3LgQ+meSF9/kvVfXHSb4G7ExyE/A0cC1AVe1NshN4AjgG3FJVx09r9JKkeTnl0K+qbwGvH1H/HnDlCfpsA7ad6jElSafHb+RKUkcMfUnqiKEvSR0x9CWpI4a+JHXkdL+Rq58ya2/79Cn3PfC+tyzgSCT9NPJKX5I6sqyv9L3qlZaH0/m/DP5/HuaVviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTioZ9kU5Ink+xPctukjy9JPZto6Cc5A/gPwC8DFwPXJbl4kmOQpJ5N+kp/I7C/qr5VVX8B3AdsnvAYJKlbqarJHSz5h8CmqvrH7fX1wN+sqnfOarcV2Npe/jzw5Cke8nzgu6fY92eV59yH3s65t/OF0z/n11TV1OzipH9lMyNqL/qrU1Xbge2nfbBkuqo2nO77/CzxnPvQ2zn3dr6weOc86emdg8CaodergUMTHoMkdWvSof81YF2Si5K8DNgC7JrwGCSpWxOd3qmqY0neCXwOOAO4u6r2LuIhT3uK6GeQ59yH3s65t/OFRTrniX6QK0laWn4jV5I6YuhLUkeWZej3+FMPSe5OciTJ40s9lklIsibJF5LsS7I3ya1LPabFluTlSXYn+Xo75/cu9ZgmJckZSf5Xkk8t9VgmIcmBJI8l2ZNkekHfe7nN6befevgz4O8xuEX0a8B1VfXEkg5skSX528BzwL1VdelSj2exJVkJrKyqR5KcAzwMXLOc/52TBHhFVT2X5Ezgy8CtVfWVJR7aokvyL4ANwIqqeutSj2exJTkAbKiqBf9C2nK80u/ypx6q6kvA95d6HJNSVYer6pG2fRTYB6xa2lEtrhp4rr08sz2W11XbCElWA28B/mCpx7IcLMfQXwU8M/T6IMs8DHqXZC3wBuCrSzyURdemOfYAR4AHqmrZnzPwIeC3gR8v8TgmqYA/SfJw+1maBbMcQ3+sn3rQ8pDklcDHgXdV1Y+WejyLraqOV9V6Bt9m35hkWU/lJXkrcKSqHl7qsUzYFVX1Cwx+kfiWNn27IJZj6PtTD51o89ofBz5aVZ9Y6vFMUlX9EPgisGlpR7LorgB+pc1x3we8KckfLu2QFl9VHWrPR4BPMpi2XhDLMfT9qYcOtA81PwLsq6oPLPV4JiHJVJJXte2zgTcD31jSQS2yqrq9qlZX1VoG/5c/X1VvX+JhLaokr2g3J5DkFcAvAQt2V96yC/2qOga88FMP+4Cdi/xTDz8VknwMeAj4+SQHk9y01GNaZFcA1zO48tvTHlcv9aAW2UrgC0keZXBx80BVdXELY2cuBL6c5OvAbuDTVfXHC/Xmy+6WTUnSiS27K31J0okZ+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj/x+Xg4qYnQnAzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUEklEQVR4nO3df6xf9X3f8ecrhhArgYaIC3JsJ0aZ080g1RTLo0LasiQrLqlm0g7JbAWkMpkxkMjWboL80+QPb6nWJB3aYCMDYbYsrqUkw0pCWpcmy9AI5po5GGNYrOBgxx52ShFGmlht3vvjflC/u/na9/r++N5wP8+HdPQ9530+n3M+R5Zf9+jzPd/vN1WFJKkP71joAUiSRsfQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKGvRS/JwSQfn2bbSvLXZnieGfeVRsXQlxZQkmVJdiQ50v5orFroMWlxM/SlhfUm8G3gNxd6IOqDoa+uJFmf5IkkryY5muTfJnnnpGbXJvlRkp8m+ddJ3jHQ/7eT7E/yF0n+OMkHT3Oea5M8l+REkp8k+d1h7arq5aq6F3hq7q5SOj1DX705BfxT4CLgV4CPAf9kUptPAuuAXwY2Ar8NkOQ64NPAbwBjwH8HvnKa8zwA3FpV5wOXA382lxchzZShr65U1e6q+n5Vnayqg8B/AP72pGa/X1WvVNVLwB8CN7T6rcC/qqr9VXUS+JfA2tPc7f8lsCbJBVX1F1X19LxckHSWDH11JcmHk3wjyf9O8hoTwX3RpGaHBtZ/DLy/rX8Q+DdtauhV4BUgwPIhp/pN4Frgx0n+W5JfmcvrkGbK0Fdv7gOeB1ZX1QVMTNdkUpuVA+sfAI609UNMTNm8d2BZWlX/Y/JJquqpqtoIXAz8V2D7HF+HNCOGvnpzPvAa8HqSvw7cNqTNP09yYZKVwJ3AH7X6vwfuTnIZQJJfSHL95M5J3pnkHyb5har6y3a+U6cbUJJ3Aee1zfPatjQvDH315neBfwCcAL7EXwX6oEeA3cAe4JtMvClLVX0d+H1gW5saehb4tdOc50bgYGv3j4HfOsOY/g/welt/vm1L8yL+iIok9cM7fUnqiKEvSR0x9CWpI4a+JHXknIUewFQuuuiiWrVq1UIPQ5LeVnbv3v3TqhqbXP+5D/1Vq1YxPj6+0MOQpLeVJD8eVnd6R5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOvJz/4lcSXq7WnXXN2fc9+DnPjGHI/kr3ulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNThn6SdyXZleQHSfYl+WyrfybJT5Lsacu1A33uTnIgyQtJrhmoX5lkb9t3T5LMz2VJkoaZzrdsvgF8tKpeT3Iu8HiSR9u+L1bVHww2TrIG2ARcBrwf+NMkH66qU8B9wGbg+8C3gA3Ao0iSRmLKO/2a8HrbPLctdYYuG4FtVfVGVb0IHADWJ1kGXFBVT1RVAQ8D181q9JKkszKtOf0kS5LsAY4BO6vqybbrjiTPJHkwyYWtthw4NND9cKstb+uT68POtznJeJLx48ePT/9qJElnNK3Qr6pTVbUWWMHEXfvlTEzVfAhYCxwFPt+aD5unrzPUh53v/qpaV1XrxsbGpjNESdI0nNXTO1X1KvBdYENVvdz+GLwJfAlY35odBlYOdFsBHGn1FUPqkqQRmc7TO2NJ3tvWlwIfB55vc/Rv+STwbFvfAWxKcl6SS4HVwK6qOgqcSHJVe2rnJuCRubsUSdJUpvP0zjJga5IlTPyR2F5V30jyn5KsZWKK5iBwK0BV7UuyHXgOOAnc3p7cAbgNeAhYysRTOz65I0kjNGXoV9UzwBVD6jeeoc8WYMuQ+jhw+VmOUZI0R/xEriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjkznh9HflWRXkh8k2Zfks63+viQ7k/ywvV440OfuJAeSvJDkmoH6lUn2tn33tB9IlySNyHTu9N8APlpVvwSsBTYkuQq4C3isqlYDj7VtkqwBNgGXARuAe9uPqgPcB2wGVrdlw9xdiiRpKlOGfk14vW2e25YCNgJbW30rcF1b3whsq6o3qupF4ACwPsky4IKqeqKqCnh4oI8kaQSmNaefZEmSPcAxYGdVPQlcUlVHAdrrxa35cuDQQPfDrba8rU+uS5JGZFqhX1WnqmotsIKJu/bLz9B82Dx9naH+swdINicZTzJ+/Pjx6QxRkjQNZ/X0TlW9CnyXibn4l9uUDe31WGt2GFg50G0FcKTVVwypDzvP/VW1rqrWjY2Nnc0QJUlnMJ2nd8aSvLetLwU+DjwP7ABubs1uBh5p6zuATUnOS3IpE2/Y7mpTQCeSXNWe2rlpoI8kaQTOmUabZcDW9gTOO4DtVfWNJE8A25PcArwEXA9QVfuSbAeeA04Ct1fVqXas24CHgKXAo22RJI3IlKFfVc8AVwyp/znwsdP02QJsGVIfB870foAkaR75iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI1OGfpKVSb6TZH+SfUnubPXPJPlJkj1tuXagz91JDiR5Ick1A/Urk+xt++5Jkvm5LEnSMFP+MDpwEvidqno6yfnA7iQ7274vVtUfDDZOsgbYBFwGvB/40yQfrqpTwH3AZuD7wLeADcCjc3MpkqSpTHmnX1VHq+rptn4C2A8sP0OXjcC2qnqjql4EDgDrkywDLqiqJ6qqgIeB62Z7AZKk6TurOf0kq4ArgCdb6Y4kzyR5MMmFrbYcODTQ7XCrLW/rk+vDzrM5yXiS8ePHj5/NECVJZzDt0E/yHuCrwKeq6jUmpmo+BKwFjgKff6vpkO51hvrPFqvur6p1VbVubGxsukOUJE1hWqGf5FwmAv/LVfU1gKp6uapOVdWbwJeA9a35YWDlQPcVwJFWXzGkLkkakek8vRPgAWB/VX1hoL5soNkngWfb+g5gU5LzklwKrAZ2VdVR4ESSq9oxbwIemaPrkCRNw3Se3rkauBHYm2RPq30auCHJWiamaA4CtwJU1b4k24HnmHjy5/b25A7AbcBDwFImntrxyR1JGqEpQ7+qHmf4fPy3ztBnC7BlSH0cuPxsBihJmjt+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkem88PoK5N8J8n+JPuS3Nnq70uyM8kP2+uFA33uTnIgyQtJrhmoX5lkb9t3T/uBdEnSiEznTv8k8DtV9TeAq4Dbk6wB7gIeq6rVwGNtm7ZvE3AZsAG4N8mSdqz7gM3A6rZsmMNrkSRNYcrQr6qjVfV0Wz8B7AeWAxuBra3ZVuC6tr4R2FZVb1TVi8ABYH2SZcAFVfVEVRXw8EAfSdIInNWcfpJVwBXAk8AlVXUUJv4wABe3ZsuBQwPdDrfa8rY+uT7sPJuTjCcZP378+NkMUZJ0BtMO/STvAb4KfKqqXjtT0yG1OkP9Z4tV91fVuqpaNzY2Nt0hSpKmMK3QT3IuE4H/5ar6Wiu/3KZsaK/HWv0wsHKg+wrgSKuvGFKXJI3IdJ7eCfAAsL+qvjCwawdwc1u/GXhkoL4pyXlJLmXiDdtdbQroRJKr2jFvGugjSRqBc6bR5mrgRmBvkj2t9mngc8D2JLcALwHXA1TVviTbgeeYePLn9qo61frdBjwELAUebYskaUSmDP2qepzh8/EAHztNny3AliH1ceDysxmgJGnu+IlcSeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdmc4Poz+Y5FiSZwdqn0nykyR72nLtwL67kxxI8kKSawbqVybZ2/bd034cXZI0QtO5038I2DCk/sWqWtuWbwEkWQNsAi5rfe5NsqS1vw/YDKxuy7BjSpLm0ZShX1XfA16Z5vE2Atuq6o2qehE4AKxPsgy4oKqeqKoCHgaum+GYJUkzNJs5/TuSPNOmfy5steXAoYE2h1tteVufXB8qyeYk40nGjx8/PoshSpIGzTT07wM+BKwFjgKfb/Vh8/R1hvpQVXV/Va2rqnVjY2MzHKIkabIZhX5VvVxVp6rqTeBLwPq26zCwcqDpCuBIq68YUpckjdCMQr/N0b/lk8BbT/bsADYlOS/JpUy8Yburqo4CJ5Jc1Z7auQl4ZBbjliTNwDlTNUjyFeAjwEVJDgO/B3wkyVompmgOArcCVNW+JNuB54CTwO1Vdaod6jYmngRaCjzaFknSCE0Z+lV1w5DyA2dovwXYMqQ+Dlx+VqOTJM0pP5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjU4Z+kgeTHEvy7EDtfUl2Jvlhe71wYN/dSQ4keSHJNQP1K5PsbfvuaT+QLkkaoenc6T8EbJhUuwt4rKpWA4+1bZKsATYBl7U+9yZZ0vrcB2wGVrdl8jElSfNsytCvqu8Br0wqbwS2tvWtwHUD9W1V9UZVvQgcANYnWQZcUFVPVFUBDw/0kSSNyEzn9C+pqqMA7fXiVl8OHBpod7jVlrf1yfWhkmxOMp5k/Pjx4zMcoiRpsrl+I3fYPH2doT5UVd1fVeuqat3Y2NicDU6SejfT0H+5TdnQXo+1+mFg5UC7FcCRVl8xpC5JGqGZhv4O4Oa2fjPwyEB9U5LzklzKxBu2u9oU0IkkV7Wndm4a6CNJGpFzpmqQ5CvAR4CLkhwGfg/4HLA9yS3AS8D1AFW1L8l24DngJHB7VZ1qh7qNiSeBlgKPtkWSNEJThn5V3XCaXR87TfstwJYh9XHg8rManSRpTvmJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHZlV6Cc5mGRvkj1JxlvtfUl2Jvlhe71woP3dSQ4keSHJNbMdvCTp7MzFnf7fqaq1VbWubd8FPFZVq4HH2jZJ1gCbgMuADcC9SZbMwfklSdM0H9M7G4GtbX0rcN1AfVtVvVFVLwIHgPXzcH5J0mnMNvQL+JMku5NsbrVLquooQHu9uNWXA4cG+h5uNUnSiJwzy/5XV9WRJBcDO5M8f4a2GVKroQ0n/oBsBvjABz4wyyFKkt4yqzv9qjrSXo8BX2diuublJMsA2uux1vwwsHKg+wrgyGmOe39VrauqdWNjY7MZoiRpwIxDP8m7k5z/1jrwq8CzwA7g5tbsZuCRtr4D2JTkvCSXAquBXTM9vyTp7M1meucS4OtJ3jrOf6mqbyd5Ctie5BbgJeB6gKral2Q78BxwEri9qk7NavSSpLMy49Cvqh8BvzSk/ufAx07TZwuwZabnlNSnVXd9c1b9D37uE3M0krc/P5ErSR0x9CWpI4a+JHXE0Jekjsz2w1n6OTObN7x8s0ta/LzTl6SOLOo7fe96Jen/552+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjoy8tBPsiHJC0kOJLlr1OeXpJ6NNPSTLAH+HfBrwBrghiRrRjkGSerZqO/01wMHqupHVfV/gW3AxhGPQZK6laoa3cmSvw9sqKp/1LZvBP5mVd0xqd1mYHPb/EXghRme8iLgpzPs+3blNfeht2vu7Xph9tf8waoam1wc9Y+oZEjtZ/7qVNX9wP2zPlkyXlXrZnuctxOvuQ+9XXNv1wvzd82jnt45DKwc2F4BHBnxGCSpW6MO/aeA1UkuTfJOYBOwY8RjkKRujXR6p6pOJrkD+GNgCfBgVe2bx1POeorobchr7kNv19zb9cI8XfNI38iVJC0sP5ErSR0x9CWpI4sy9Hv8qockDyY5luTZhR7LKCRZmeQ7SfYn2ZfkzoUe03xL8q4ku5L8oF3zZxd6TKOSZEmS/5nkGws9llFIcjDJ3iR7kozP6bEX25x++6qH/wX8XSYeEX0KuKGqnlvQgc2zJH8LeB14uKouX+jxzLcky4BlVfV0kvOB3cB1i/nfOUmAd1fV60nOBR4H7qyq7y/w0OZdkn8GrAMuqKpfX+jxzLckB4F1VTXnH0hbjHf6XX7VQ1V9D3hloccxKlV1tKqebusngP3A8oUd1fyqCa+3zXPbsrju2oZIsgL4BPAfF3osi8FiDP3lwKGB7cMs8jDoXZJVwBXAkws8lHnXpjn2AMeAnVW16K8Z+EPgXwBvLvA4RqmAP0myu30tzZxZjKE/ra960OKQ5D3AV4FPVdVrCz2e+VZVp6pqLROfZl+fZFFP5SX5deBYVe1e6LGM2NVV9ctMfCPx7W36dk4sxtD3qx460ea1vwp8uaq+ttDjGaWqehX4LrBhYUcy764G/l6b494GfDTJf17YIc2/qjrSXo8BX2di2npOLMbQ96seOtDe1HwA2F9VX1jo8YxCkrEk723rS4GPA88v6KDmWVXdXVUrqmoVE/+X/6yqfmuBhzWvkry7PZxAkncDvwrM2VN5iy70q+ok8NZXPewHts/zVz38XEjyFeAJ4BeTHE5yy0KPaZ5dDdzIxJ3fnrZcu9CDmmfLgO8keYaJm5udVdXFI4yduQR4PMkPgF3AN6vq23N18EX3yKYk6fQW3Z2+JOn0DH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8HaHzVE381SwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_folders = f'{base_dir}/label_data'\n",
    "# dir_folders = r'/media/rich/bigSSD/for_Josh/GCAMP_ROI_Classifier_data/label_data'\n",
    "# dir_folders = r'/users/Josh/Documents/Harvard/label_data'\n",
    "folders = [r'mouse 6_28 _ day 20200903/',\n",
    "             r'mouse6_28 _ day20200815/']\n",
    "fileNames_statFiles = [r'stat.npy']*len(folders)\n",
    "paths_statFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_statFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "sf_all = util.import_multiple_stat_files(   paths_statFiles=paths_statFiles,\n",
    "                                            fileNames_statFiles=fileNames_statFiles,\n",
    "                                            out_height_width=[32,32],\n",
    "                                            max_footprint_width=241,\n",
    "                                            plot_pref=True)\n",
    "images_labeled_raw = np.concatenate(sf_all, axis=0)\n",
    "images_labeled_raw = (images_labeled_raw / np.max(images_labeled_raw, axis=(1,2), keepdims=True)) * 1\n",
    "print(f'concatenated images shape: {images_labeled_raw.shape}')\n",
    "\n",
    "fileNames_labelFiles = ['labels_posthoc_filledIn_allCells.npy',\n",
    "             'labels_posthoc_all.npy']\n",
    "paths_labelFiles = [pathlib.Path(dir_folders) / folders[ii] / fileNames_labelFiles[ii] for ii in range(len(folders))]\n",
    "\n",
    "labels_all = util.import_multiple_label_files(paths_labelFiles=paths_labelFiles,\n",
    "                                       plot_pref=True)\n",
    "labels_raw = np.concatenate(labels_all)\n",
    "\n",
    "assert np.alltrue([sf_all[ii].shape[0] == labels_all[ii].shape[0] for ii in range(len(sf_all))]) , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAADnCAYAAACjZ7WjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABZvElEQVR4nO29aZAc533f/+me7rnvY+97sbu470PgARK8SZGOJeosWYot20qcSpVjv8gbVexKOalU5UVS+SuJK6mKneiILEuMSJEUKR7gARA3QAALYLEL7H3Mzu7M7NzHzvV/AU+blHDtcqYXAp5PFaooEZz5TvfT3+fs31eqVCoIBAKB4LMhr7UAgUAguBcQZioQCAQ1QJipQCAQ1ABhpgKBQFADhJkKBAJBDVBu9S8lSarZVn+lUpFW+98KHUKH0PHbqeV+0iFGpgKBQFADhJkKBAJBDRBmKhAIBDVAmKlAIBDUgHvGTCVJQpZlJOkzrdt/Zg2KoqAoyprquFtQFAWTyYQsr30zk2UZg8GwphpUVcXtdmOxWO6K9iFJ0prrcDgc2O32NdXwST5LW73lbv4df4iiUCqV0Ps9f1mWsdlseDwe7HY7RqORTCbDxMQEy8vLumoxGo2YzWYURaFcLlMoFMhms5TLZV113C34fD56e3vxer1Eo1HGxsYIh8O661BVlUAgQH9/P3a7naNHjxKNRnXX4XQ6OXjwIM3NzaTTaQYHB7ly5Qq5XE53LbIsY7VacbvdVCoVYrEY6XRadx0DAwM8++yzmEwmXnrpJcbHxymVSrrrgOvXxO124/f7SafThMNh8vn8ij5j1WZqNBpxOBy43W4CgQCZTIbh4eEVC/gsmM1m9u3bR29vL4lEglwuh8lkoqmpibNnz5JKpXTRYTKZaG9vp7u7m+XlZebn50kkEqiqSiqVolgs6qIDwOv10tTUhKqqjI6O6nYNPonFYmHPnj18/vOfZ/v27cRiMX75y1/ywx/+kGQyqZsOm83GAw88wIYNG9i6dSsulwuPx8OhQ4eYm5vTtfPfs2cPe/fuxWg0srCwQEdHBx6Phw8++EDXDldVVVwuF01NTbS0tGCz2ZienmZoaEg3QzUajWzcuJEXX3yRz33uc/T09PDggw/yox/9iPfee4+FhQVddBgMBjZs2IDdbqdSqaAoivbPIyMjTE5OrqiNrMpM/X4/27Ztw+/343K5NBP56U9/yuXLl3VrHG63m23bthEIBLh8+TKxWIxSqcTGjRsplUqcPHmSQqFQVw2SJBEIBHj22Wf51re+xbFjx3jttdeYmJigUqlQKBR0HbUrikJ3dzdOpxOLxcLMzAzBYFDXHt9ut7NhwwZaWlpQFIUtW7bQ1NTE4cOHuXjxom46PB4Pe/fuJZPJcOXKFZqamujv76dYLPLuu+/q9tACtLe3I8sylUoFo9GI1WrFarXq9v1w3cQ8Hg8ej4dAIEBnZyddXV00NjaSyWRYWFggk8mQyWTqqkOWZfx+P4lEgmPHjjEyMsLAwADf+ta3CAQCvPLKK0xPT9dVA1yfPe3cuZPl5WVCoRCFQoF8Po/b7WbDhg3k83kWFhbu+Nm5rZmqqoosy5TLZWRZxmw2s337dnp7ezGbzfj9fvr7+2lubiYcDjM1NUUikfjMP/R2SJKEx+Nhw4YNNDQ0EI1GiUQiJBIJLBYLgUAAm81GLBaru5ZSqUQoFOL8+fMEg0HMZjMWi4VUKqX7mpTT6eTBBx/kkUce4cSJE5w9e5bBwUEGBwd16+RMJhMmk4mRkREuX75MV1cX69atw+/3I0mSLh2LJEl0dXXR0tLCzMwMAJVKhUAgwMaNG/n44491NVNJknA6nRSLRaxWK9FolHw+r2v7UFUVRVEwGAw4HA5aWlo4ePAgXq8XSZJ4++23626kAIVCgWg0yvj4OKqq4nQ6icVi2Gw2/H4/69evJxwOk81m66rDaDTicrmoVCrk83ntt9tsNjo7O8nlciwvLxOJRO7o825rpr8+RS2VSszMzODxeHC73RQKBYxGI21tbXR0dOByuXQx06oWWZbZu3cvhUKBZDLJ1NQU2WyW+fl53abX2WyW8+fPE4lEMBgM2kNSqVR0X0tubW3FbreTyWTw+Xx4vV7cbrfWIepBPp/XRsTZbJaFhQWmpqZ0X3JYXFxkeXmZffv2kUgkcDgceDweksmkrssNAAsLC2zatIlCocD7779PPB7XfcaQy+Uol8uUSiWy2SzRaJRUKsXy8jKzs7PMz8/rYqalUon5+Xk6OjoolUoYDAbtXsViMQwGA36/n9nZ2bq22cXFRebn53niiSfYtm2bdk2KxaI2Kl3J+vptzfTXjaBYLDI9Pc26des09w6FQly6dIkTJ04Qj8dX/qtWQaVSYW5ujldeeQVVVens7OTAgQMcP36cI0eOcOnSJV0W9yuVCrlcjkQiweTkJIpy/ZJWpwx6rpcCnD17litXruByubDb7czPz5NKpXR/aKemprDZbBgMBhYWFkilUszPz+vWsVQqFYaHh/nZz37Gv/yX/5J9+/ZhNpu5fPkyH3zwAaFQSBcdVYaGhojH47S3t2M2m4lEIszNzemqoVQqEYvFMBqNpNNp5ufn+fnPf87c3BzHjx/XtbOLxWJMT0/T3t6OxWIhn8+TTqfJZrNkMhnNZOtppvl8nuPHj9PU1MSLL75IT08PV65cYXR0lKNHjxIOh1fUXqVb/eVbvc+6fv16uru7KRaLeL1eisUi77///k2HxPV4r1aSJMxmMy6Xix07dtDX18eZM2c4derUTXfz66GjeqrAbDZrF79UKpHL5cjn8zdsEHf7e8afRYeiKPT39+P3+7HZbNjtdmKxGB988IGu9wX+cZPhW9/6Fk1NTfziF7/g9ddfv+kUsl46VFXld3/3d3n++eeZmZnhBz/4AVeuXLnpZ9Xz3XyTyaSdgKnO6OLx+E073Hrem+oOevXIWCqVYm5ujlAo9Bt66qXDZDKxZcsWHn74YbLZLENDQ5w5c+amncvNdKzaTD95prN6Xq1Sqdx0JFZv8zAYDJjN5tseR6qXDlmWUVWVSqWiTe/XQsdKqZcOp9OJ2WzGYDAgSRL5fP6Wa0/1vh7V866FQuGWs4V66ujt7eWBBx4gGAxy5MiRW86c6l3opHomWpIkCoXCLUdgerTVT/pJuVy+oZ5661BVFUmSKBaLq3p2V22mK+VeNw+hQ+i4nQ5FUTCbzRSLxdsuQYmqUb99OmpyaF8gENyeYrG4Jud+Bfqw9u/5CQQCwT2AMFOBQCCoAcJMBQKBoAbccgNKIBAIBHeGGJkKBAJBDRBmKhAIBDVApJMKHULHPabjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBPc1kiRhMpk+8+fcU2Z6NwSECT6NuB+Cu51KpbL2ZlrNf1rr1Ee4XjWqubkZt9u9ZhokScJgMNwVaZx3Aw6HA5fLJa7HJ6hW0QL9OxqDwaDVDxWd3Kepxeh01YVOPB4PTzzxBP39/bz99ttcuHBhTZIWPxnB0NnZydzcHMlkUreizJIkaUWQm5ubaW9vJx6PMzMzw+LiYt0zqG6kR1VVAK0k4lq8mOHxeNi9ezd2u52xsTGuXr2qSxX3GyHLspY0sFZpsR6Ph4aGBjo7O4lGoywuLlIsFgkGg7ppMhgMPProo7S2tnL27FmmpqaYm5vTLRnj1zGZTNjtdkwmE+VymaWlJV0DOavY7XY6OztRVZWxsTHi8fiq7smqzLSaA/X1r3+dxx57jI6ODv76r/+a4eHhuue2fBKLxUJfXx8+nw+3243D4aBYLDI3N8fi4qJuOvx+P08//TQ7duygsbERk8nE3Nwcr7/+Om+++aZu16TaOKt1RCuVCplMhnQ6rWtH5/f7+dznPqfVl92yZQtdXV28++67ulZNqna0FotFCzfMZrNafIdeqKrKgQMH6OnpQVEU8vk8lUqFM2fOMD8/r5sOSZLw+Xz84R/+Id/5zncYHx9ncnKSN998k3feeUfXCGxJkuju7ub5559n//79zMzM8Ld/+7dcunRJ9wGIx+OhubmZ5uZment7uXjxIiMjIysekK3KTB0OB+vXryebzXLu3Dnsdjs7duwgkUgwMTGhS0O12Wxs27aN3bt3azn1JpMJg8FAIpEglUrpYmKSJNHS0sL69evJ5/OMjIzQ29vLI488QmNjI5FIhFOnTpHL5eo+QjQajbjdbnw+Hw6Hg3K5TDQaJR6PE41GyWazdW+oVquVzZs343a7tcgUVVXZuHEj4+PjDA4O6jJSlmUZj8dDT08PTU1NzM7OagF6BoOBbDar2+zF4/HQ3t5OLpcjEomQzWbp6OigsbERWZZ1i5QpFouMj48TiURQFIVUKkVPTw//7t/9O3p7e/lP/+k/6TYyNJlMrFu3jl27dtHT00NHRwcXL15kenr6jgPsaoHRaMRkMpHP50mlUjQ2NvLUU0/hcDj4+OOPb5oMcSNWbKaSJOH1eqlUKhw7dgy4fpOWl5dpbm4mEonokgNVLBaRZZne3l78fj+RSIR8Pk8sFiMajXLt2jVdzNRisWC32zl//rw2xVYUhc2bN3PgwAGi0SjlclnLpLpZjMlnpZryuHHjRjZv3kyxWGR+fp5yuUyxWMRgMFAqlZiYmKj5d1cxGAw0NDTQ1tbG8vIyJpOJUqmk5fu0t7dz+fJlXUxMlmXsdjsvvPACzzzzDN/73vdIJpMsLy9TLBZRVVW3sEOPx8PGjRuJRCKEw2EKhQK5XA6r1arr2mWpVOLq1avMzMyQzWYJBoMYDAbS6TRbtmzB5XLplthaXUcPh8McO3YMu92Oy+XC7XYTjUZ1W5qqxg0Vi0VKpRJGo5HHHnuMnTt38t/+23/j0qVLdzybWrGZVqeO8/Pz2gNaNQebzYbH49ElwC2fz3P16lUWFhZ4+OGH8Xg8ZDIZLZtdj00xSZJobGzEYrEQj8dxuVwYjUaMRiOFQoGPP/6YN998kwsXLpBKpTAYDKiqWpfe32q10tnZyde//nUefvhhfvaznzEyMkIsFtM6uHqPOkqlEslkElVVaW9vp1AoEIvFKJVKWtvw+XxEIpG6G1mxWCQej3Pt2jWGhoa0EYYsyxgMBm0jSK/Y6Q0bNpBIJJiZmdFihZeWlnRfww0Gg5w7dw63262Z6szMjJZiq5eZlstlwuEws7Oz2Gw2xsbGmJmZ0TX4Ef4xfbm6LFYdfOTz+RU/M6ua5kciES3uuRodqygKsViMRCKhWwMJh8McOXKE9evX89BDD2mL2LOzs8Risbp/v9Fo1DLHP5lbYzKZGBoa4qWXXuK1114jnU4D1PW6FItFMpkM77//PleuXGFkZITJyUmi0aiW+qgHS0tLjI6O8tRTT7F582ZCoRAXL15kcnKSy5cvk0wmkSQJo9Go6a6XocXjcQ4dOsTw8LA2eyqVShSLRQqFgm7ttBotvX37dqamprBarSQSCYaGhnQ306WlJc6fP89XvvIV5ufntVGgw+HAarXqpiOfzzM9Pc2RI0dwu91UKhVCoRCRSETXDdPq9a+a59zcHK+99hpjY2NMTEysaFlsVWaayWQYHh4mEAhoD0UulyMYDOq6G1cqlThz5gySJHHx4kVUVeX48eNcunRpRWsdq8FgMGC1WgmHw2SzWWw2G5lMBpvNRjKZJBaLceTIEc1I6006nWZkZISpqSnsdjuKomgmquemYLlc5uOPP+bll1+mXC5jsVi4fPkyb731FlNTU58yD1mWMRqNdZtFlMtlgsEgiUQCk8mkGWomk9HVxCKRCIcPH6a1tZWBgQEcDgeHDx9mbm5uTU4XnDhxgq6uLpxOJy0tLRiNRqLRqK6bg5VKhWAwSC6Xw263k8vliMfjup8IMhqNLC0tkUqlsNlsLC4ukkwmCYfDK9bymQL1qlOmtU44NJvN+Hw+fD4fY2Njt2wUtdJhNpu10U31ZYFPnqdUFOWWm071vB6fTIv9h++66d+tlw6j0aidMV1aWrpl5/YPI/u6FrGork3ebtRTr+uxbt06Hn/8cVRV5dq1a1y4cIFgMFiX9nE7LdW1fbfbTUNDAwAzMzM3nVXWu8DInS631FqHJElYLBbtdMXt0lFvp+OeSietruGutY47Qei4v3TIsozX6yWTyZDNZutq6rfTslLu9XtTKx33VDqp3ovXAsGdUt1wEdy7iPf8BAKBoAYIMxUIBIIaIMxUIBAIaoBIJxUIBIIaIEamAoFAUAOEmQoEAkENEGYqEAgENUBEPQsdQsc9puNu0nI/6RAjU4FAIKgBwkwFAoGgBggzFdQdkRp796Io99Qb5WvKZzJTo9FIS0sLbW1tWCyWWmlaFdXCy2uZhOn3++ns7BSJnP9ANa21mtG11lSrJd0N9+Zu6VzW+r6oqorH48Hr9a6pjk9WwFstq+qWDAYDXq+Xnp4e1q1bh9vtZm5ujnfeeYdkMrlqMavV4nQ6aW1tJRAIMDs7y8jIiK4a4HqjrNaGdLlcpFIp5ubmdK/PqCgKfr8fs9kMXC9MHI/Hdcs7gusPSDWCohrwFw6HWVpa0k1DlUAggMvlwmaz4fV6aWho4Ny5c4yMjOie2irLMi6XS4tz0TP08UZYLJaa5MWvltbWVvbu3Yvb7SYYDHLmzJk1uSZ2ux2/34/dbtcKRK+maNKqzNRmszEwMEBLS4vW2z/00EO4XC5+8Ytf6JZy6Ha78fv92oPS3t6Ow+EgGAzqauqKotDQ0ICqqhQKBYxGI93d3TQ3N/Pxxx/rFnGsKArbt29n//79eDwe1q1bhyRJHDp0iEOHDjE1NaWLgVitVnbt2oXL5SKZTJLJZHC5XCiKopupy7LM1q1b+b3f+z18Ph92u522tjYSiQQ//OEPmZyc1K2jMxqNtLe309raqkXaxONx3UIffx2TyURzczOtra3k83ld46artLa28swzz9DW1sbCwgIOh4Pe3l5isZiu6aRGo5Gmpia6urro6ekhnU7z8ccf65NOKkkSLpeL1tZWLBYL2WyWdDqN3W5n165dvP/++7qYqdlspq2tjZ6eHpxOJ4qikM1mtTjbVCqlayiXw+FAVVVt2tLV1UVjYyPxeJyLFy/qoqOjo4M9e/ZQLBaZnZ3Fbrfz/PPPs3XrVjKZDKFQSBcDURRFS9+cmZlhaWkJi8WCx+PRbeQhyzJdXV1IkkQwGKS9vR2Px0NLSwsdHR26TvWbm5v5yle+QiKR4OLFi2SzWex2O16vl9nZWd10VPF6vXR2dvLUU09RKBQIh8N1DVr8daxWKzt27GD9+vXE43EWFxeJRCI0NTXR09Oj26zBYrHQ3NxMQ0MDW7Zs4Stf+QrhcBiLxUImk2FycnJFOlZlpjabTZsiSJKE0+mkv79f13qNkiQRCAR4/PHHGRgYYGxsjFOnTpHNZmlqaiIUCunW6yuKoq3FWSwWmpqa2LlzJzt37mR4eJiRkZG6x6ioqsrmzZsxmUzMz8+TzWbJZDIUi0W2bNnC8vKybqMPo9HIxo0b2bhxI8eOHaNcLrO8vKxrxlC1ino8HiebzdLQ0IDD4cBsNiNJkq7LHna7nSeffFLLwqq2S7/fvyajQpvNRk9PDwcOHCAcDjMwMMD8/LxuI/VisUggEMDn85HL5TCbzVQqFfL5vK5Bh6qqYrVacTqd2O12VFXVOlu/37/ia7KqdNLqD60ahNvtprW1lYmJCV1inqvfXSgU2LZtG83NzVy9ehVVVbWHqDpq1gNJkjRDtVgs2pSloaEBs9msS1KqyWTCbDZrM4VisUi5XGZ4eJjp6WlOnjxZd0OvYrFY6O3txWq14vP5tJwsv9/P9PS0buZRKBRIJpOUSiXi8TjpdJp0Os3MzIyuBlZNsVVVlaGhIWKxmJaeuhaFhhYXF5mfn+fVV18lmUxSLpexWq26mWmlUqFYLOL1egkEAvT29vLWW29pSbp63ZtUKkU8Hsfv9zM8PMyPfvQjzGazltq60ud2VWa6tLREqVTC6/ViMpno6OhAURRGRkZ0C5ArlUpMTk7y0UcfsX//fkwmE1arVYtS1vNhyWazWg56VUcoFOLs2bMcO3ZMl0aayWQYGxvTRqd2ux2fz0c+n+e9994jGAzWXUOVUqmE1WrF5XJpi/upVIpAIIAsy7rdm2oWerlcJp1OMz4+zsLCAseOHdNtZFodBR8+fJhNmzbR3t6upYImEok1MdNkMsng4CDBYBBFUTQtelEqlTh16hSNjY3s27cPl8tFX18f4+Pjuj631fQDk8lELpdjYWEBRVHIZDKk0+kV35tVZUApisLOnTt59NFH6e3tpa2tjdnZWb7//e9z7NixG+6E1eNVMFVV6e3tZceOHbjdbmKxGJOTk8zPzzMzM3PDkVi9Xklrb29n06ZNtLa24na7iUQiHDlyhLGxMd1CyqxWKx0dHRiNRoxGI1arlampqVuuh9VDh8lk4tFHH2X37t1kMhnC4TBTU1OMjIwwPz9/w0Zaax2yLNPf38+BAwcIBAI0NTXR1tbG97//fX7+85/f9LPqEdpmtVqx2Wxs27YNr9fLxMQEExMTRCKRm5q6Xq+T2mw2crncLXev6/XMtLS08OCDD7J//35UVeX111/n0KFDN51B1UuHxWKhoaFBO95ZKBSIRqMkk8kb3p+aB+oZjUY6OjrYsmULZrOZCxcuMDY2dtOpdT2Dymw2G1arlXK5TCKRuGUmer10VHf0A4EA6XSaSCRCLBbTPZ30k+fk/iH185afVc/7oiiKtv5VXXbQU0d1M/KZZ55hw4YNXL16lZdeeumWJz3q+f539SxjuVyua/DjnWhZCfW8JiaTifb2dtra2hgbG2NqampNdMiy/KkE29W01c+cTmo2m5Fl+baJi3d7kYJa6biThFQ9dNwp94OO6siwUqncdpP0Xrged5OWO9VhNBqpVCq3PBZ1t9+beyrqWegQOoSOu0vL/aRj7d+rEwgEgnsAYaYCgUBQA4SZCgQCQQ0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAaIdFKho+4vMaiqetv6BPfL9dBDx92k5W7VUX3jaTVFoMU5U8GaITY5BXcbgUAAp9NZ088UZiqoK3dD3pJA8OtIkkRHR0dNs7hENOE9jJ7l7m6GwWAQhiq46wiFQgBavd1a8JlauSzLmM1mHA6HLgWQ73aq4X5+vx+v17um16SaiLDWfLIaz63+Tq359epZwtB/E0VRMBqN9+W1qVQqhEIhyuUyHo+nJp+5qpGp0WjUco/sdjuyLLOwsMDCwkJNRP02YrFY6O7uZtu2bfh8PsxmM8ePH79pfdd6YrVa6evrQ1VVzp8/r2tA2a9jNptvW4i51p2OLMu43W7tujc2NmK1WllcXCQajeoaYmexWPD7/TQ0NGCz2bSCxOPj4+Tzed10fBJZlnE4HBiNRi2xIp/P695OAZxOJw0NDeTzeUKhkG5pEPCPhe537drF4uLiZw6cXJWZWq1Went7tdFXNSAsEAiwuLhIIpHQLQLB6/WyYcMG4vE4Y2NjdU0CNZlMN6yV6nK52Lt3L21tbaxbtw6r1UpraytWq5WLFy8Si8XqpumTOJ1OHnzwQXbt2sXWrVtZXFzk7//+7zl+/PiaPLjVgsjpdJrBwcEbmlg98uPL5TK5XI5169axadMm+vv7KRaLRCIRLQFBr6RWSZL4xje+wWOPPcbg4KBW2f3q1aucOXOGkZERXZdiFEXB7Xbjdru10pmpVApJkshms7pq2bx5M//iX/wLent7OX78OKdOneLUqVPaFFwPKpUKmzZtYseOHRw7dozx8XHGx8dXFfy4YjNVVZWGhgb6+vqw2+2kUingutFURwN6xiwfPHiQ7373u4yPj/M3f/M3vP/++3WNTqnmLMH1G6EoChs2bGD//v1YLBacTidWq5VKpUJzczO9vb18/PHHdW+ksiyzYcMG/vAP/xCn06kVIn7hhRdQFIX3339f15FHQ0MDn//85/niF79IJBLhJz/5CYcPH9baiyRJWnZWPSJE8vk8VquVL3zhC1gsFj788EPS6TRNTU0cOHCA119/XZcU3eXlZYLBIJcuXeLEiRMkk0kt+8jlctX9+6tUR+ttbW00NjYiyzLhcJhyuUyhUNDW1/UYtauqyt69e/nLv/xLWltbOXfunBaEuXHjRpaWlnQbodrtdq1AtdPp5MqVK+zatYuPPvqIiYkJbDYbJpOJ2dlZLQLnZqwqndRqteL1evH7/cRiMS1dMJvNUiwWdR2qJ5NJzTxbWlro6+tjaGioLiOxfD6PoiiYTCZtatra2srTTz/Nhg0baG9vx2q1Eo/HtZH59u3bGR8fr/uDK0kSnZ2dbNq0iVgsxpUrV0gmk3g8Hj73uc9x+vRp3cIOHQ4HX/3qV/mn//Sfap3sH//xH+N0Ojly5Ig2ui+VShSLxbqNEC0WCxs2bGBhYYFUKkU0GiUcDmO1WmlubiYej9e9gykWi5w5c4aWlhZMJhOZTEb7oyj67f/KskxPTw/f/e53KZVKHD58WGsT1Y5Nr/y0xsZGfv/3f5/du3dz6dIlpqamtAFYa2srLS0tukVPOxwO8vk8V65cwefzYTQaeeqpp/ja177Gz3/+c06fPs3k5CQWi4VisXjLme+K72a5XKZcLtPS0sLevXtZXFzk2rVrJBIJJiYmSKVSuk4VPv74Y37605/i8Xi0B8Vut9dtWlssFikWi1oKavX7/H4/TU1N+Hw+5ufntaTWRCKh20ZUuVwmFoshSRLRaJRQKITNZiMajerWwRkMBvbu3csXvvAFGhsbKZVKTE1NYTAYePTRR5menmZkZIRMJkMul6NSqdRtqj87O8uvfvUrXC4XpVJJi1CRZRm73X7HqQiflVgspi2FVbO5mpqamJ2drctvvxHVtdp3330Xi8WixRgvLy+zvLystWs9lj7C4TCXL19mx44dhEIhJEnSlh0qlQoXL16su4Yqy8vLxGIxpqenSaVSGI1G4vE4hUKBkydPcuLEiTtuIys201KpRCQSIZVKsW7dOnp7eymVSly+fJmZmZlb5h7Vg0gkwi9/+Ut6eno009DjAalUKmQyGYaGhvjJT36CyWTC6XSiKAqFQoHFxUUOHz7Mhx9+SCQS0UXP9PQ07733HpIksbCwQCwWY2lpiWvXrulippIk4XQ6eeqpp1i3bh3BYJBEIkEoFCKbzTI2Nsa1a9d+4yhKPdpLpVJhYWGBoaEh1q1bh8FgwGazkU6nSaVShEIh3TbmQqEQk5OTPPbYY8iyjN/vJ5FIrGpdbrVUO5ef//znNDc3YzKZyOfzmokWCgUKhYIuz24ul+Pv/u7vANiyZQuNjY10dHRgMBi06bVeZLNZEokETqeTUqmE0+nk0qVLzMzMMDg4uCIvWVXU8/z8PG+99RYmk4nW1lYmJiYYHBxckx3KcrnM6Oio1sMVCgVdd2vz+TynT5+mUqlQKpXYuHEj586d45VXXuHChQvaGmG9KZfLjI+P8/HHH+P3+8lkMsTjcVKpFCMjI3XtYKpHj1RVxWazYbFYKJVKpFIpIpEIyWSSaDTK4cOHa3am706IxWLaOmU1X2h+fp7R0VFdO/1CocDg4CBf+9rX2L9/P+l0mv/wH/4D586d021kWtWxuLhIpVLR9jdKpRL5fF63DeMqwWCQv/7rv2bTpk089thjbNy4kUwmw6FDh3RbjoLrEemhUAiHw0G5XMblcmEymRgeHl7xs7vqDCiHw4HT6USSJPL5POl0+pahevfi+72fpLom9eSTTzI+Ps6HH354y/WVeqVxWiwWvF4viqKwvLzM0tKSNp3WQ4fFYuGb3/wmf/qnf0oikWBqaorJyUmOHDnCu+++e9PNwXrdF1VVcTgc2v9OpVK3HKXXS4fVauXJJ5/k2WefZXh4mB/84Ae37Fjq+W6+2WzG4/FgMBhIJBK3XZqr97NrMpkwGo3aBpjeycJms5mmpib8fj/Nzc34fD7eeecdZmZmVqRDBOrVUIeiKOzbtw+z2cypU6dIJBJromMl1COvfv/+/fz5n/85Xq+X48eP89Zbb3HmzJn78np8EpPJRFtbG5FIhEQiUTcDuxMt1Rc6MpnMbUfo98O9gesdryzLWK1WAJaWllakQ5hpjXU4nU56enoIBoO3PC93r14PRVHw+Xxs2LCBQqHAyMgI0Wi0rjnxd/P1WAsdd6Kl+tbTnWwW3wvXRA8d4t38GpNIJJicnNQ2o+pxhvJuplwus7CwQC6XI5PJrOnbV4Kbs9Y1G+5FhJnWgVgsRqlUwm63k06n7ytDqT6k1eM2AsH9gjDTOlCpVEgkEiiKcl/W8qye37wff7vg/kWkkwoEAkENuP9qbwkEAkEdEGYqEAgENUCYqUAgENQAkU4qdAgd95iOu0nL/aRDjEwFAoGgBggzFQgEghogzFQguI8wGo26FqW+n/hMV7Va9LhSqazZ62mSJGE0GqlUKhSLxbvmNblqIeL7EVmWsdlsWjzG5OTkmoXHwfUCI3a7nUKhcMtiK/XEaDTi9XqRJIlwOLxmb8UZDAa8Xi+xWEy7J/drO601qzJTSZIwmUzYbDYURSGXy5HNZnWNK6lSLTlntVq10LRkMrkmpmq323G73ZjNZhRFYWJiQrc6kZIkYbfbtSiQtbgXcN1I+/r66OjowGQy0djYSF9fH+fPn79pSbN6oqoqfr8fj8eD2WxmampK9xRdo9HI/v37+frXv47f7+fVV1/ljTfeWLM037a2Nvbv38/CwgKLi4sEg0Fdc9s+iaqquFwu8vn8mmmoRtZXnxu73U6lUllxltyqzNRsNtPR0cG+ffsIBAKcPXuWS5cuEY1GdX0f22Aw4HK58Hq9tLa20tbWRiaT4aOPPmJ6elq3ADmTyURvby979+6lqakJj8dDJpPhRz/6EaOjo7r0/Iqi8KUvfYmNGzfy4YcfMj09rT0seo6CrFYrPT09WkZYT08PfX199Pf38+677zI8PKzbKFWWZbxeLw6HA6vVis/nw+Vycfz48bqGLv46DoeDL37xi3z5y19mfn4egKamJl5//XUuX76sezrp5s2b+bM/+zPK5TJHjhzh//2//3fbEom1pjogCwQCPPvss7S0tPCzn/2Ma9eu6V6o2mAw4PF4sNlseL1ezGYzwWCQycnJFWlZsZlW83Meeughvv3tb9PU1MQvfvELCoUC58+f17V3MRqNuFwunE4nZrOZlpYWdu/ejd/v5wc/+IEu6ZMGg4F169bxwgsvEAgECIVCzM/PYzKZ2L9/P5lMhvn5+bo/MMVikWAwyJe+9CXS6TR2u53Ozk7Gx8cZGhrSzVC9Xi92ux2Hw8GWLVvo6elhamqKfD5Pb28v8XicyclJXbRU48eNRqO2Vuj1evH5fLqaabFYRJZlRkdHeffdd5mfn8dsNtPX18fIyIiuswij0aglMZjNZnp6evjc5z7H0tISw8PDuqVU2Gw2bDYbTqeT9vZ2HnzwQS5duqTFcevZwRiNRgKBAOvWrcPr9ZJMJonFYivObluVmZrNZnw+H4VCgZmZGdxuNwMDA0xOTpLJZHQZEUqShMPhwO1243A4MJvNWhql2Wzm7bffJplM1tVEqnk+/f392vqg1WrVYnRNJhNbt25leXm57nEdlUqF48ePazlMkUgEt9vN5z//efr6+jh27BihUKju90ZVVaxWK4FAgO3btxMIBJAkSetk9Zy52O12mpubkWVZM5GmpiZaWlqYmZnR7YEtFovMzs5y+PBhLl26RDqdxu1209HRQVNTE1NTU7rogOvGoaoq58+f13KfDAYDra2tLCws6JJOCmjPZbFY5MMPP+TixYsEg0GMRiM2m41UKlX3GV11qbKvr4+DBw+yZcsWwuEwFy9eZHl5ecXXYVWBesVikcXFRYaHh5FlmVwuR3NzMy6Xi4WFBV2n1xaLhUqlokV0zM7OEo/HUVVVC7erB2azGVmWMZlMOBwOGhoa6OrqYnJykunpaeD6lLcavKcH8XicCxcuaOuDZrOZxsZGnn76aR5++GFef/11Pvzww7pOszOZDMViUcuGd7vdbNq0iWQyic1m0y2pVZZlGhoa2L59Ow0NDSQSCYrFIoqi6PKgfpJCocDc3BzxeJzZ2VkKhQKqqtLQ0IDFYtFNh6qqOJ1OYrEYsixrI+ZsNkupVEJVVYxGoy7T7GpmWqFQYGFhgUgkQi6Xo1QqYTAY6r6B+8nIeqfTSWdnJw899BCnT5/m2LFjJBKJFT8nqwrUSyQSDA8P43a78fv92tRO7119g8GAoigYDAYMBoO2wXDu3DktrrVeVBvc7Owsx48fp6enhz179miRtZlMhsnJSUZHR3WbOlUqFaLRKLIsa2Xwzp8/z/Hjx7lw4QKhUKju65Xz8/OcOHECh8PBlStX6OjoIJ1Ok0wmCQQC2lS/3qFp5XKZVCpFQ0MDX/ziF7WI6aGhIS1UTi+Wl5eZnp6mv7+fcrlMsVgkl8sRi8V0C1yE69dkamqKt99+m/b2dtxutxaLvrCwQCqV0m29sto+qzNZSZIol8vkcjldlqQqlQrJZFL73r//+79naGiIRCLB1atXicVi9R+ZwvXRx/DwMOVyme7ublwuF6FQSPcNqHQ6TSgU0gysuj44PDysm5ZSqcTIyAj/9b/+V4aGhnjqqafI5/OcO3eO06dPs7CwoOuDm81mWVxc/FScsR5R01UqlQrXrl1jaWmJs2fPsmvXLgwGgxYj3N/fD8Dx48fr3slUp9ZtbW34fD4tk/1m2T71ZHx8nK1bt2oxzwaDgVAopGvcc6lUIpvNMj4+TiwWw+12Y7VateNaeu535PN5bWRaKBSQJEnrZPQy9GKxSDQaJZVKEQwGOX36NAaDAVmWV7WOveoMqOpunMlk0s543upITj3eq1UUBUmSUFVVM05ZlrVp/41+Wz3f71VVlZaWFmw2G/Pz8ywtLd23aa1VquuUbrdbmz0sLy9z4cKFT5l8vXRUj2dVj89dunTplqPieiZgPv/882zbto10Ok04HObIkSMMDw/XvJ3eTss//HvtGa7+s97JsZ/U8onvuunf0+uZud0SgwjUEzrWTIfBYMBkMlX/G62z++TMQQ8dNpsNVVWJxWI3/Tv11OF2u+nt7aVQKDA/P08kErnp/oIodPLbp0OYqdBxX+m4XcjhvXA97iYt95MO8W6+4L5ChPwJ6oUwU4FAIKgBwkwFAoGgBoh0UoFAIKgBYmQqEAgENUCYqUAgENQAYaYCgUBQA+6pdNLW1lYWFxdv+SrY3X5WTei4d3SoqookSat6NVGcM/3t03HPjExVVUWW75mfI7gHKJVKIhLkPuKecR9VVTEYDKLxCm6Jw+HQ7bsqlcqn3jsX3NvcM2ZaraYuuHuolgK8W6gG6+lJtTyk4N7nnsh8lSQJl8uFqqprOjJVVRWz2UyhUNA9xwauXwdFUbTSasFgcM2C9VRVxefzoSgKiURCKxq9VsiyTFtbGwaDgWAwqMt3VioVrRB0uVy+a2ZN1WpRsixTKpVuWmFNb03V2qJrhaqqlEqlVWv4zGaqqirAmkXXAjidTnp6eshms2uyblqNWvD5fFitVnK5HKFQiEQioVsjlSSJnp4eDhw4wLZt23C5XHz00Ue8/vrruplHFZvNRm9vL+3t7VpibDgcJhqN6h61bDAYaGhoYOvWrezfv59Tp04xMjKi2/cXi0UMBgMWi4VsNnvT9vBZH+SV0N/fz/PPP4/D4WB2dpZjx45x8eLFun/vzagmVZjNZiYnJ3UtmA3Xi990dnayfft2ZmdnGRkZWVV+3KrN1OfzsWfPHtrb27l27Rrnzp0jFovp2sPJsozb7Wbz5s10dXWRSCS4du1a3fOWPvn9TqcTn8+H2+3Wenu3200gEGBycpJgMKhLjIvT6eSZZ56ho6NDM/EvfOELbNy4kf/yX/6LbiF2Ho+HJ554gp07dxKJRJiamkKWZWw2G7IsUygUdA1t2717N/v372fLli3s2bOH3t5ezp07x+zsrC4a4HohZIvFgs1mI51OU6lUUBQFWZYpl8tIkqSbkTY0NPDcc88xMDBAKpXS0jKqkT96Y7PZ2LhxIx0dHSiKQkdHB4ODg7rGgvf29vLCCy+wbds2JiYmaG1t5ejRo8zPz6/Iz1Zlpk6nk29/+9t84xvfIJfL8d577wHoGhcryzLt7e309fXhdru1XJuOjg4ikYgupl4102r8gyzLKIqiJaW2tLRw6NAhXSrdBwIBnE4nExMTZLNZHA4HFouFAwcOEAqF+M//+T/Xfcqvqirbt2/nn/2zf4bRaOSDDz741Hqhw+HQpvt6zGRaW1t5/PHHMZvNJJNJ4vE4u3fvZv369czNzena8edyOS3gLxAI0NLSQrlc1jocPSruGwwGtm3bRnd3t1bIPZfLaWm2elJ9Vjdt2kRvby8+n49cLsfo6KhuU31Jkmhvb+fgwYPs2rWLYrGI2WymubmZBx54gBMnTqzI1FdspkajkZ6eHp577jna29v58MMPSSQSGI1GLBaLbmbqcrnYvHkz7e3t5HI5lpaWKBaLeDweLBaLbiF25XKZUqlEqVRCURQtpKs6Wg4Ggxw7dqzuo9NEIsHS0pJWfDmXy7G4uEipVOKRRx7hBz/4AXNzc3XVoCgKXV1dDAwMaNOkaiHoUqn0qZyweDxe1zwqWZYJBALE43GCwSA+nw9Zltm9ezeBQEBbL9QLSZIwm8088sgjfPWrX8XhcDA3N8fIyAjvvfcemUym7tNbRVFYt24dLS0tOBwOJiYmGB0dJZlM0t3dzdjYmC6dnCzLbN++nccff5zW1lYaGhqwWq1MTU1x9uxZFhYW6q4Brt8Tj8dDS0sLxWKRdDqNqqr4/X5UVWV8fLy+ZlooFJicnOSXv/wlMzMzXL58mYmJCVKplK6N0+l00tvbS3Nzs/ZwptNppqentXXcelMqlYjFYpjNZu1YlizLuFwuHA4He/bsYefOnVy6dKnuSyDxeJxIJMLAwAANDQ2YTCZcLpdm8OvWrSMajdZ1Y6xUKjE3N8fp06exWCzkcjkqlYp2eL1QKGjhevUeJVcqFVKpFDMzMyQSCaanp0mlUgQCAYxGo+5Hlqr3fs+ePTz88MNMTk5qkeDVyPLqEkA9MZvNtLe34/P5gOthf9Uk30AgUPcOF67PYB544AH27duH0WjE5/MhSRKjo6OMjo7qtlFZHXRU26WiKNqzUygU8Pv9mEymO+70V5VOurS0xP/+3/+bzZs343a7SSQSBINBXYPKJEnC5/OxadMmPB4PANPT07zxxhtaREa9qT6wc3NzFItFnE4npVIJq9XK1atXmZ+f58iRI7oF+y0tLWG1Wlm/fj1ms5lAIIDf7+e9994jnU5rm2P11HDhwgX+v//v/2PXrl1ks1kURcHlcpFKpZifnycYDOqyZlqpVAgGg3R2dlIul0mn01po2uLiou4bldUHd3p6mqmpKdLpNNFolLm5OdLptC7Hp0qlEkNDQ5w7d45t27Zp+WnVuHa9YqeXl5c5ffo0TzzxBFu3bsXpdHLs2DHeeust3db24fo9icfjjI+PUywW6erqYu/evQwODjI1NaWFQN4pq96AWlxc5IMPPsDr9VIqlUgkErqOTKs9SkdHB729vaiqSj6fp1Ao6KoDrqe1hkIhAC13/Ny5cwwNDTE+Pq7L2lzVyBYWFti3bx8PPvgguVyOv/mbv+Hll18mEonUfTRYKpUIBoMsLi4SDAbp7u7G4XCQz+eZnZ1lampKt+UXgGg0SjKZpLOzk2QySTgc5ty5c7pNI29EdY20ml8/MzNDLperewQ3oEV/V6eulUqFfD5PLpcjm82yvLysbYrVk0qlwvnz53nttddwu92cP3+ev/3bv+XSpUu6nwpaWlriyJEjdHV1MTExQTgcJpFIMDQ0xKVLl1b0zPzWZkBZrVYef/xxXnjhBfr7+0kmk7z11lu8/fbbjI6O3vSm1PP9XoPBgNVqxel0ArCwsLAmOlRVxel0YrFYCIfDtxyN1kuHqqp4vV4sFgv5fJ5EInHT9Mt66mhoaKC5uVlbArqdadW7fWzbto0XX3wRm81GNBrV4smvXLnyqTXTer2bX13eWEkHX69r4vf72blzJ2fOnLmjTdp63htFUbBYLJhMJm3JcqXP7m+tmUqSRGdnJ21tbdpwvRqvfKuRqR7FEu7kAPLdXrRB6KiPDrvdTmdnJw6HA7vdTi6XY3x8nGAw+Kn2cj8UOjGbzZhMpltGb+uhY6Xcc2YK13t6VVVZXl6+46nJ3X5DhI57W0d186u6MVcul8nn87/Rfu8HM73XdPxWv05aPZIkEPy2YDAYKBQK2jlPwb3DPVPoRCC426nWTljL988F9UOYqUCgI8Vicc2Ligjqg0gnFQgEghogRqYCgUBQA4SZCgQCQQ0QZioQCAQ14J5KJxU6hA6h4+7Scj/pECNTgUAgqAE1MVOHw7HmKYzVSu4CgUCwFtTEfYxG45omMFajQu4WM13rjkUgEOjPZ3afalEPRVm7N1MtFgs7d+7E7/evmQa4burV4rttbW33vanKsozVasXn8+FyudbselQTW6vRMoJPc79fE6PRyMDAAO3t7Z8pLv4zOaDNZqO7u5vNmzczPT3Nxx9/rGu9SrheheeBBx7gS1/6Eh0dHbzyyiu6hnFVMZlMDAwM4PV6sdls5PN5MpnMqlIOV0vVzCuVCpVKhWKxuGbxylarlZaWFrq7u/F6vcRiMU6dOqXr9VAUBY/Hw8DAALt27UKSJE6ePMnp06fX7L34ahX3u+WVUovFQmtrK9euXVtrKWuCzWbjxRdf5Hd+53fI5/McOnSIX/3qV6vykFWbaWdnJy+88AIPPPAA/f39GAwGXn75ZV5++WUtKqLehV6tViv9/f1s2bIFWZbp7+/nySef1OJ89Xxg2traeO6557RYY4PBQFNTE7FYTLcHx2g0apEpsixrSQCxWIxsNqubDqfTSX9/v5ba6vF4cLvdlEoljh49qkuHW01i2LZtG9/85jd5+umnURSFt956i7/6q7/i8uXLur3WKcsyDQ0N7N+/n4cffphQKMTx48e5cuUKS0tLK67oXgscDge9vb1s2bKFqampNTHTzs5Otm3bRqlU4uLFiywtLekaBa6qKo8//jgbNmzg2rVrxONx2tra+NrXvsabb77J1atXV1S0e0VmWp3St7S08MUvfpGenh4ymQzLy8t0dnbyyCOPMDU1xfj4uC4Vs6uRGEajkcXFRSKRCKqq0tnZqVUx1wOz2czjjz/Orl27OHr0qJa15HQ6sdvtujSQamCbzWbD4/FoyY/5fJ433niDq1ev3rI4c61QFIWNGzeyefNmKpUKpVKJZDJJPp/H4XDQ3NzM6Oho3XVUtRgMBsLhMMPDwzgcDrxeL3v27GFiYkKX66GqKvv27eORRx5hz5499PT0YDAY2LFjB6+88grHjh1jZmZG1xlEU1MTf/Inf8KePXtIJBL88pe/xGAw6FqBLRAI8NRTT9HS0oLRaKS/v5+5uTmOHj3K4uIi+Xy+7h3MwMAABw8e1KKHqoF6bW1tvPjii/zqV7/izJkzd+xltzRTp9Op/SBZlpEkCYfDwfPPP09XVxfhcBij0Ui5XGZiYgKr1YrRaNRtRFjVls1mSaVSmM1mvF4vi4uLNDU13bbKfK3w+/0cPHiQlpYWzGYziqJQLBYxmUy6JrZWp/eVSgWn08lDDz3Epk2bmJ+fZ3x8XBcNfr+fAwcO8MgjjzA5Ocn09LSWmrq0tITH49G1jSSTSU6ePMnU1BQejwer1UqhUMBisehipoFAgIcffphAIKDNWPx+P1arVQs81HvK/+yzz/JHf/RHDA0NMTw8jNPppKGhgWAwqJsGh8NBU1MTfr8fh8PB1q1bSSaTtLS0MDU1xfnz5+s+Wrbb7VgsFmRZZnl5GbfbjcFgoKGhAZ/Px+zsLJOTk3d8XW5ppp9sbFXjqoaC9fb2Isuylu9TjZF1u92YTCZdTKxYLBKPx4nH4/h8PjZv3ozD4aBYLBIMBnWbOhmNRjweD62trTQ3N5NIJEilUrrlw8M/RipX/4RCIV5//XWOHj3K5cuXdckYgusjZJvNpkUqG41GFhYWtA4lEonoshFVTV/IZrMEg0Gi0SgNDQ04nU5CoZBuI8FyuaxFYmSzWdLpNCaTiVAoRCQSIZfL6W6mlUqFyclJLl26xPj4ONlsVvdNqKph/tmf/RldXV3Mz8+Tz+eJxWIEg0Fd7s/c3ByFQoHm5mZtryMQCBAIBBgfH0dVVaxW6x1/3i3N9EbD/lwux4kTJ+jq6qKzs5NwOEwmk0FRFC5cuMDJkyd1M5BcLkc0GiUWi1EsFlEUhZ6eHqxWq3Zz9CCTyXDkyBFtnc7v95PJZLQUSr1YXl4mk8lgMpkIBoOagU1OTup2LRYXF3nppZfYtm0bW7Zswe12E4/HsVqtmEwmlpaWdBuV5vN54vE4ZrOZQqGA0WikWCySSqV0M7BYLMbZs2cxm82USiWy2SxGo5FwOMzs7OwdZR/VmlOnTvHcc89po+JYLPap/Ck9KBaL/OpXv2Lfvn3Mzc3xve99j+npaa2D0YNQKMTly5e1QZDb7cbtdmO1WhkaGmJ+fn5FG6arji3p6OjgmWeewel0ksvlMBgMnDlzhmPHjt3QhOv1KpjVaqW9vZ2Ojg7a2trw+/189NFHnDlz5oYGUg8dJpOJxsZGtm3bhtvtZnl5mUgkwtDQEHNzczccIdfrelQ3oaxWq1bVfWFh4aZmWi8dO3fu5Ctf+QpNTU3Mz89z9epVTpw4wejo6A2jnuulw+l00tHRgdPp1EYZi4uLjI2NkUwmddPR3NxMb28vmzZtorW1lcuXL/P+++8TCoVq3j5up8VkMvHFL36Rvr4+Tp06xccff8z8/PxNP6uer3E+8MADmEwmjhw5cttBWD1y5BobG9m0aRO9vb0EAgFt8/bq1au89957jI+P/4af1TwDymg00t3drYlSFIXx8fGbjsTqeUNkWcZoNGpnxNLp9E0X0+udcFilXC6vWaCeqqooiqIlLN7qHtdLhyRJWCwWWlpacLvdhEIhZmdnb3pN6nk9PB4PnZ2dGI1GUqkU0WiUhYWFG2qpdztVFAVVVbHb7UQikZtOZ+v9bn5Vx50sRdU7sbVSqdzRTKFeOmRZxmAwaD5is9kACIfDN7w/NTVTSZIwmUyUy2Vtyna7vO27vUiB0HFv6zCbzVr44q3Oeep1PQwGA+Vy+aYdnSh08tunY1XnTKtrUJ907bvlELJAcCNyuZxua3F3ggiCvPdYlZnmcjmRYyMQCASfYFXnIYSRCgQCwae5vyscCAQCQY0Q6aQCgUBQA8TIVCAQCGqAMFOBQCCoAcJMBQKBoAbUJJ20+ibFWrxps1KEDqHjXtdxN2m5n3TUZGQqDuzfmvs9vkQguB+oiZlKknTf58jcCrPZvNYSBILfwGKxrGl2271GTRzQYDBgsVhq8VH3HEajEbPZLDobwV1HY2Mj69atW2sZ9ww1ecKrVfgFn0aWZWw2m671M+9G7oaORFXVu6KNVisU3S3oHYB5L/OZxviyLGOxWHC73bqmTt5Ih9lsJpvNrumrrrIsax2LJEl4PB4ymYxuxbI/iclk0uJT1ioNU1EU/H4/drudxcVF4vG47hpkWaaxsZHGxkYKhQKTk5O6F0KuYjQa2bJlCwAXLlxYk3bxSS1+v59wOLxmGu5WJElalY+s2EyrwVsul4t169bR29tLuVzm6NGja1IAxWAw0NLSgsfjIRQKEQ6Hda/Io6oqra2tdHV10dTUhN1uJ5fLMTs7y8mTJ3XVoigK69ev54tf/CIdHR1cuHCBl19+mampKV11+P1+du7cSV9fH6VSidHRUY4dO6a7ke3du5cDBw4QiURIJBKsW7eO06dPMzs7q6sOr9fLk08+yXe+8x0A/vIv/5IjR47oqqGKwWBgy5YtHDx4UIt00TMV9G7DbrfT39+PLMuYTCYkSeLcuXMrbqsrNlOz2cyWLVt47rnn2L17Nw0NDQwNDWlFd/Usc2az2diyZQvr1q3D5XIBMDY2xoULF3SLN5Ykie7ubh5//HGampro6Ohg/fr1qKrKO++8w8jIiG7RJbIss3v3bv71v/7X7Nu3j1gsBsD777+vm5lKksTAwABf+tKX8Pv9DA0NsbCwgMFgwOFw6Gqm7e3t/NEf/REAv/jFL4hGo2zYsIE///M/59/8m3+j2xTXaDTy7LPP8tRTTzE9PY3P5+M73/kOiUSCwcFB3QcgLpeL/v5+vF4vdrudxx9/nLNnzxIKhe6qMoV6sXv3bv7gD/6Acrmsxcq88cYbvPLKKyvKolqVmb744os888wzFAoFAoEA2WyWjo4OrFZr3W9GNXrCYrHwzDPP0NfXx9jYGNFoFK/Xy9NPP8327dv56U9/ytjYWF21wPUHZf369ZTLZa5evUoymcRqtfLII4/w6KOPcujQIRYXF3XJPWpsbOTLX/4y7e3tfPTRR0xOThKJRLSYDD1CytxuN1/4whd48cUXOXnyJAsLCywuLlIoFPD5fCwsLOgyczAYDGzduhWr1cqPf/xjPvjgA5qbm9m1axf9/f34/X7dOpiGhga6u7s5ffo04XAYt9tNT08PX/jCF0in04yNjelqqAaDgWQySTgcpqWlhYGBAdrb2xkeHubixYt1H7Xb7XaamppwuVzYbDbcbjdNTU0sLS1x4cIFxsfHdcsJ8/l8vPDCC6xfv56TJ08yOztLa2srjz76KEePHl1RYuuKzdTn82EymRgfHyeVSnHp0iVyuRwWiwWn08nS0lJdG0a5XMZisWiZ1wDxeJxkMonRaKRUKqGqqi6bPpIk4ff7aWxsZG5ujlQqpaWStrS00N/fz8MPP8zo6Cizs7N17WgURWHz5s0Ui0Veeukl4vE45XIZq9WK2+3WbfPFbrfT09NDe3s7Z86coVwuUywWyefzGAwGLU6l3lQqFUKhEC+99BJHjhwhmUzywAMP8NWvflULbtOLSqVCOp0mGAySTCYpFAosLS3R3t7Oxo0bCYfDuq4nV3PbvF4vn//853nzzTeJRqM0NTVx9uzZun9/uVzG7XbT3NyMz+fj0Ucf5YEHHuCdd965YeZSvZAkiaamJhoaGrTU5ap/7dy5k/Xr16+o81+RmSqKgtPpJBqNoigK6XSaQqGAqqpaBni9qVZMv3TpEoODg3zzm99k27ZtnD59mmg0SrlcJhqN6jKFq1QqpFIpwuGwZhILCwtUKhVGRka0aIpqxEs9UVWVbDbLuXPnCAaDmM1mDAYDHR0dxGIx3TagqtHfExMTvxEhbDAYsNvtuiSllstlZmZmsNvt9PX1USwWaW9vJxKJcOjQIV03fxKJhDbaq1QqqKpKLpfj2rVrDA4O3jDYr560tbXR2NjIlStXeOONN5AkiUKhwPj4uLY0VE+y2SwLCwsEAgFMJhOVSoWZmRmGh4dZXFzUdbNUkiSmpqaYm5tjenqaxcVFwuEwkiSRTqfx+Xwkk8kbhkD+OnfsfgaDAZvNRi6XY2hoiGg0qo12FEVheXkZRVGQZVmXniUej/Pyyy+zdetWnnjiCdra2sjn8+TzeRYXF+/ox9eCZDJJKBRi3759VCoVotEopVKJq1evcvHiRQ4fPszo6Gjdpy3ZbJahoSH8fj8Wi4VkMonFYkFVVYLBoG69/dLSEu+88w52ux1VVfH7/eRyObLZLJlMBlVVb5sXViuqJ0waGhqIxWJcuHCBa9euMTk5qauZplIpLl++TGdnJwaDQQt+HBoaYmpqSlfzUFWVtrY2WltbmZ6e5uTJkzQ2NpLL5bhy5You16VSqTA7O4ssyySTSYLBICaTiatXrzI/P6/bkkelUiGTyRAIBEin08iyTKFQYHZ2llOnTjE7O7siLXdspqVSSVvfiUQidHZ20t3djaqqRCIRrl69Sjgc1vVc5eTkJP/9v/93VFVl69atABw6dIgTJ07osj4I10dA165dY8eOHaxfv55gMMj8/DyvvfaadrpAr8YRiUQIBoPs3buXVCqF1WrF4/HoMtqoUigUOHnyJOFwmA0bNuB0OjGbzSwtLTE6Oko0GtXteiwvLzM/P09fXx+Li4vMzs7i9/uJxWK6rlFWKhXGxsbw+/34/X6KxSLT09MMDw/r8pxIkoTNZsNkMmGz2RgeHv7U7CAYDHL06FEWFhbqrqVKqVRiamqKUCikJZRWl4P0ZHZ2lh//+MesX7+eRCJBLBZjamqKpaWlFX/WqtNJrVYrjY2N5PN5YrEYmUyGSqWiRcguLy9/Kn2xnjGtPT09PPbYYxgMBt5++21GR0frkvp4Mx2KorBu3Tq6urqYmZlhdHT0tsfE6nU9zGYzmzdvpr29HbfbTSaT4e23377pOeB6Rj1X43MBbe30ZtRLx/bt23G5XMzMzNDR0UFTUxMLCwtMTk4yMTHxG5rqWUyjvb2dQCBANBplfn7+lmvotSp04nA4cLlc5HI5bb22+v+3trbidDoJh8O6PzOroV46qmv55XKZUql02w6uplHPVYxG4w1z2auH1iuVSt3NtIrH48FoNN52zaVeOlRVRVXVO16r1aOBGo1GTCYThULhpg/uvf6gVDPQ0+m0tq5fLBYpl8uk0+nfaCv1zoiHO0smrZWZ3uo7qyayvLwsKr7VQMdnMtNaCLgTfht0VB/StdaxUoSOe0/H3aTlftKx9i9NCwQCwT2AMNMaodeGl0AguDsRZioQCAQ1QEQ9CwQCQQ0QI1OBQCCoAcJMBQKBoAbUJJ30TrjbjzUIHULHvaLjbtJyP+kQI1OBQCCoAcJMBQKBoAYIMxUIBIIaIMxUINCRalTI3ZCUKqgtn8lMq2mca4EkSbhcLlRVXZPvv1upPqSSJIkI7puwVtdEVVVaWlrYs2cPXq9X3Bvuvujrz8KqS+N7vV56enpwu90Eg0GmpqZ0rxju8/l44IEHiMfjZDIZpqam1ixyWpZlHA4HxWJRK0eoB9XKP5VKBY/Hg8vl0qJbDAYD6XSaWCxGPp/XNT3W5XLh8Xi0guHFYpHl5WUtYkYv7HY7LpcLs9mMxWLB6/USi8W4ePGirkWZDQYDra2tuN1uAoEAzc3NlEolLeZGT2RZxuVyaRX2M5mM7om+cD0vbPPmzbS0tGhlCa9du/ZbG+q3KjO12Wxs27aNhx56iIceegi3282rr77KD3/4QyYnJ3V5YCuVCl6vl29961vYbDYaGxsJBoP8j//xP3jnnXd0zyT3eDz8wR/8AT09Pbz33nu89957umSSl8tlZFmmubmZzZs343K5iMfj5PN5lpeXkWUZRVEIBoNcuXJFl6Ayi8XC888/T3t7O6FQiOXlZcxmM5Ikcfz4cS5evFh3DXA9YPBP//RP2bx5MzMzMwAMDAxw+fJlvvvd7+oab+zxeGhoaMDj8dDT00M8HieVSmGxWEin07pq8fv99PX10dDQgNvtJhqNcuTIEV1zsRobG3nyySd55JFHtHSISqXC//yf/5NXX31V1w7GYDDQ29vLpk2bcDgcTE1NMTg4uOJC5iueoxsMBrq7u2lqaiIejxMKhXC73XzjG99g69atuk37ZVnWilOPjo4yODjIwMAA//7f/3sOHDig+/JDX18fTzzxBA888ADf+MY3+NrXvkZDQ0Pdv7dSqZDNZolEIlQqFS0jS5IkTCYTRqORQCBAIBDQ5ZpUo54DgQDFYhGbzYbdbkdRFCwWi67tY8OGDezbtw+bzYYkSeTzeUKhEM3NzezYsUOXzLIqdrud9vZ2bTbX0tKC1+vFZrPpOipUFAW3243VasVqtbJjxw7+7b/9t3z5y1/GZDLposHj8bB37166u7sJh8Ncu3aN6elpnE4njzzyCG63Wxcd8I/t5MUXX+Rf/at/xV/8xV/wve99j6985Ssrbh8rbtlOp5O+vj6ampqQZZlgMMilS5fI5/N4PB7d1oEkScJoNDI3N8fc3BxXrlzh0KFDpNNpfvd3fxefz6eLjioej4epqSneeustrl27xqZNm3j88cd1W9NNJpOMj49rvakkSZjNZtxuNw6HA0mSdJnWKopCe3s70WiUhYUFLQeq2l5SqVTdNXxSx+nTpzl//jzLy8vk83mCwSDlcpkNGzbovt7e0dHBM888Q3d3t5ZqazabdR2FKYqC0WgkHo8zPz/P9PQ0hUKBPXv20NjYqIsGo9GI2WwmGAwyPj5OOBwmm81SLBZZv349AwMDuq2jejwetm/fTm9vL8VikUgkooVkrvR5WXHXXP2R1UiKWCympRrqOVWB62XvGhoaUBSFfD6vZes4nU66u7tZXFzUTUsymcRgMJDP57XY6eofPZYcyuUy4XCYQCCgrRO2trbS0tJCOp3mwoULuo2AyuUyBoMBs9lMa2srHR0deDweTp06pVsbKZVKTE9P09vbi6IoWK1WZFnG7XZrJlbvjr+6XlwNsdu5cyfbtm2jVCphMplIpVKMj4/raqb5fJ5KpYLZbKZQKHD16lV+9rOf3TZGpZbkcjlSqRSFQgG73U53dzcDAwN0dXWxtLSEz+fTJZhTkiQURSGVShEMBkkkEuTzea5du8aHH3644u9fsZnG43GuXr2Ky+XCYrGQzWZJJBIYjUYmJyd1W9QvlUoMDg7y0EMPsX37dm0TyuFwaDlUenL58mUuXrxIf38/mUyGyclJhoaGdF1Ml2WZpqYmenp66OnpYcOGDTgcDl599VXdInSLxSKzs7Ps3r0bn89HX18f27ZtI5VK8eqrr+pqpidOnKC1tZW9e/dit9vp7+/Hbrdz9uxZjh49WvPwNoPBgMViwWazaZ17Pp+nUCgwOTnJD3/4Q86cOcPnPvc5ZFkmEokwPT2t6zS/UqlQKpXw+XzaaPDChQta4rAepNNp0um0NmtyuVw0NTVhNps5fPgw165d06WDqVQqhEIhTp48iaqqdHV1sbi4yDvvvEMoFFrx563YTAuFAleuXCGTyTAwMEBDQwOSJDExMcHo6KiuO6QTExP8r//1v/jjP/5jNm/ejNfrZWJigp/97GcMDg7qpgOuxwr/+Mc/5sCBAySTScbGxnR/UEqlEplMBlmW8fl8BAIBZmZmOHr0KHNzc7ptDF69epW5uTl6e3tJp9McP36cS5cu8f777+u6MZjNZvnwww/xer24XC5mZ2c5fPgwP/nJT+piHDabDYfDQSqVIh6Pf+rfTU9PMzMzw5EjR3jrrbfo7e3lwoULpNPpmuu4HdVBj9frZXl5mdnZWcLhsG5ttVQqMTMzQ09PD42NjaiqysjICN///vf5v//3/+oa9wwQCoV45513tIj01Z42WXUGVHXNsqGhAYvFQiwWu2VUbD1TMK1WK01NTSiKQigUIpFI3NTU610sobpoXSqVdA8pqx7P6u7upru7m0AgwPj4OKdOnSIej99QT72uR39/Pw6Hg2w2y/LyMgsLC7eMAK/nfbFarQQCATKZDLFY7JaG/ll0WCyWislk+g0j/XUkScLpdFIul2/64Na70En1HDLcPuCvHvfG6XTS1dWF0WikXC6TSqWYmZm5ZSDl3V7oRATq3YM6qsscRqORXC53y+lsPdNa7yQ2t946Vspn0WE2myvlcvmORt/VLPt0Ol3zTu4fPv+uuCa36vglSVrRaPhubyPCTIUOoeMe03E3abmfdIh38wUCgaAGCDMVCASCGiDMVCAQCGqASCcVCASCGiBGpgKBQFADhJkKBAJBDRDppEKH0HGP6bibtNxPOsTIVCAQCGqAMFOBQCCoAcJMBQKBoAYIMxUIBIIasKrcBkVRKJfLWoGGtTqrWq0YpSgKiqIQi8XWJBjsRkiStGbXpZoaq3dQ292IwWDAaDRqRTWWl5fX7L5U9ciyrBWHXiuqz8xva3hdragWUs/n8yQSCQqFwqrbx4rNVJIk7HY7TqcTQKssv7y8rPvDa7Va8fv9OBwO7HY74XCYYDC4JjUiP4nFYqG5uZlkMqlrtX8Ak8nEnj17aGlp4cSJE8zMzKxJByPLMmazmXK5rFV31xur1cqDDz7IwYMHkWWZ6elpTp06xeDgINlsVlctqqrS3t7OunXrcDgcxGIxPvroozUxs/7+fg4cOEAgEODdd9/lwoULa2qqkiTh8XgAWFpa0q2tyLJMa2srXV1dmM1mlpeXmZyc5OrVq6uqu7viqlGqqhIIBGhvb9eiIKrV1aenp29a7q3WxxqqiZzVeGO/348kSczPz3PlyhVisVjNddhstkqlUqFSqWgpn5IkaXHLdrsdr9fLvn37eO655/i7v/s73njjjRs21Hoc8zAajbz44os8/vjjWgLC4cOHefvtt29qqPU6btLV1cXTTz+N0+lkenqa48ePMzExcdPPqrUOVVV59tlnefDBB3E6nSwuLlIsFvF6vZw5c4Zf/vKXN0zjrFftzoMHDzIwMIDD4cBmsxGNRnnttde4cOHCDcsU1utoVGNjI3/1V39FU1MTs7OzTE1NceXKFU6fPs3MzIyuNW/b29vZvXs3PT09tLa2Ui6XOX36NG+//bYu98Zms7F3714CgQDZbJZKpYLD4SAcDvPhhx+u2MtWPDKt5tqUSiWMRiPt7e3s2bOHcDjM//k//4dr167pVm3fYrHgdDpxOp1YrVbMZjMej4disciVK1dqPo369YtrMBhwuVxahfVq/MLAwIBWFNlkMunW62/fvp0/+ZM/YW5ujvHxcWw2G//kn/wTZmZmdItXrrJhwwaeeuopUqkUPT09bNq0iZ/85CdcunRJl5GH3W6npaWFK1eukM1mMZvNmM1mXC4XBw8eZHBwULdo45aWFp5//nnMZjPJZBKHw0FzczOjo6NcunRJt+fFZrPx7W9/m4aGBsbHx5mYmCAWi9HQ0MDTTz/NO++8o1tUe2dnJy+88ALbt2+nv7+fzs5OJEkiFovxzjvv1P37VVWlu7sbr9eLwWCgWCySy+XweDw88cQTZDIZTpw4saLZ9oo3oMrlMsVikWw2q0UKd3d388wzz7Bp0yZt+l9vKpUKxWIRl8tFS0sLjY2N+Hw+fD4fdrsdi8VS88C0Uqmk/SmXy5RKJYrFojYlqFYNHxoa4ic/+QnDw8O6TbFVVeXBBx+ku7tbCzdMJpP4/X6ee+45jEajLjo+qadQKGipoF1dXfze7/2ebjG+1eTcxcVF8vk8iqJgs9moVCr4fD76+/t1ScCUJImGhgba2tpobW3F5/PR29vLY489RktLi25pvgCbNm3ihRdeoFAoEIvFSKfT5HI5SqUSvb29DAwM6JKd1tDQwMGDB+no6MBoNBIKhTh27BivvPIKs7OzuiwXGo1Gmpqa8Hg8BAIB/H4/LpcLt9vNxo0beeKJJ1bcVlc8Ml1eXiabzZJOp7FYLITDYU6cOIGiKGQyGcxmM7Is1723rVQqJBIJXC4Xu3btorW1lXw+z9WrVykWixgMBq3HqRflcplsNoskSaRSKRRF0QLj0uk0yWRS1/XKTCZDsVjEZDJpozCHw8GOHTvw+/3Mzc3ppmVqaop0Oo3T6cThcGjTOL0ifCuVCna7XYs2bmtro6+vj/7+flRV1c3EZFnG4/GQSqWIRqNEo1HC4TDvvvsu7733nm77DA6HA4fDQaVSYf369YyMjGgbuK2traiqyvT0tLZ8VS+MRiN79+5l165d2O12FhcXsVgsDAwMEIvFyGQyt4wuqRXFYpFYLIbdbmfDhg2kUilCoRCBQEAL+lupjhWbaaVS+VRujcFgYHl5mXQ6zfz8vK7mEY/HCYVCtLS08OCDD3L+/Hk+/PBDksmkbg9LtfGpqqqdciiXy+RyOS06Vg8KhQJvvPEGzz77LOvWrSORSNDT00NXVxeZTAaLxaKLjipjY2OcPn2a3//936ejo4OFhQVee+01lpaWdPl+m81Gd3c3nZ2dmM1mWlpaaG1tpbm5mStXrhAMBnVpq5VKhdnZWc6fP4/NZiMQCBCJRBgcHNQ1zTeZTHLs2DH+4i/+gi9/+cs4HA7a2towmUzYbDbefPNNxsbG6q6jWCwSjUYpFou43W6mpqaYnJwkl8tpz44e5PN5BgcHyWQy2O129uzZw/79+3G73UQiEY4dO7bijuUzBeqpqorNZsNkMmkGksvlbiiiXovYjY2NPP/88/T29jI4OMjg4CDhcJhMJkMymfyN9Z966bDZbDidTkwmE4VCgXQ6TSqVuunIo146du/eze/8zu/g8XgwGo2kUik++OADjh49Sjgc1k0HgM/n44UXXmDnzp3aNO5mvX0tdRgMBtra2mhvb9eyhvx+Pw0NDRgMBpaWlvjggw+YmZmpq44qFouFjRs30tXVRX9/PyaTiTfeeIMTJ07c9LPq+W6+1+ulubmZtrY2uru7mZyc5IMPPtDl3sD10XpHRwcbNmzQ9hai0SiFQoFsNkswGNR1I8zv9/PQQw/xz//5P6enp4df/OIX/Mf/+B9vGhBatwwoWZYxGAy3DU+r14VQFAW3261N6QuFwk0NvZ46ZFnGZrNhs9mA61PudDqt+y56tZNTVVXLSC8Wi586F6yHjirV84yFQuGWI8F6PLCA1iaraZzVGcvN0mPr2T4MBgNOpxOPx3PbI3x6FDqpHl8rlUprErr4ySWfO5kl1LOtqqrKQw89xKOPPqp1dDfzRhGop4OOauM0m80Ui0VtDVNvHStB6Lj3dNxNWn6bdHg8Hvbt28fJkyeJRqMr1rGqN6AEN6ZcLmsGWh2tCwSC3w5SqRTz8/Or3oQTZloH6r0jKhAIak+hUCCVSq3q7ScQhU4EAoEAuH5sq3rudjUIMxUIBAKuzygTicSqz/6KdFKBQCCoAWJkKhAIBDVAmKlAIBDUAGGmAoFAUAOEmQoEAkENEGYqEAgENUCYqUAgENSA/x8Tz7hF7X6kLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plotting_helpers.plot_image_grid(images=images_labeled_raw,\n",
    "                                labels=None,\n",
    "                                grid_shape=(10,10), cmap=plt.get_cmap('gray'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ip1UMYy6DB0p"
   },
   "outputs": [],
   "source": [
    "labels = classification.squeeze_integers(labels_raw)\n",
    "images_labeled = images_labeled_raw[labels != 2]\n",
    "labels = labels[labels != 2]\n",
    "labels = classification.squeeze_integers(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYh_wBqCDB0q"
   },
   "source": [
    "## Balance classes of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn4Pu2cKDB0q",
    "outputId": "3afafb73-54a7-4e2d-8030-c0af6272cbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9714, 32, 32)\n",
      "(9714,)\n",
      "532\n",
      "(2128, 32, 32)\n",
      "(2128,)\n",
      "532\n",
      "532\n",
      "532\n",
      "0\n",
      "0\n",
      "(2128, 32, 32)\n",
      "(2128,)\n"
     ]
    }
   ],
   "source": [
    "duplicates = 1\n",
    "balanced = True\n",
    "\n",
    "images_dup = np.tile(images_labeled , (duplicates , 1 , 1))\n",
    "labels_dup = np.tile(labels , (duplicates))\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "if balanced:\n",
    "    numToGetTo = np.sum(labels_dup==0)\n",
    "    print(numToGetTo)\n",
    "    for ii in np.array([1,2,3]):\n",
    "  #     idxToDelete = np.cumsum(labels_dup==ii) <= (np.sum(labels_dup==ii) - numToGetTo)\n",
    "        if ii==3:\n",
    "            numToGetTo = np.sum(labels_dup==0)/1\n",
    "        else:\n",
    "            numToGetTo = np.sum(labels_dup==0)\n",
    "\n",
    "        idxToDelete = (np.cumsum(labels_dup==ii) * (labels_dup==ii)) > numToGetTo\n",
    "        images_dup = images_dup[idxToDelete==0,:,:]\n",
    "        labels_dup = labels_dup[idxToDelete==0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)\n",
    "\n",
    "numToGetTo = np.sum(labels_dup==0)\n",
    "print(numToGetTo)\n",
    "\n",
    "print(np.sum(labels_dup==0))\n",
    "print(np.sum(labels_dup==1))\n",
    "print(np.sum(labels_dup==4))\n",
    "print(np.sum(labels_dup==5))\n",
    "print(images_dup.shape)\n",
    "print(labels_dup.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "NpMB08CYDB0q"
   },
   "source": [
    "# create validation set\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images[:], labels[:], test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(images_dup[:], labels_dup[:], test_size = 0.15)\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVA_Aa6rDB0q",
    "outputId": "15b9e55a-4881-40be-f06b-dec658fa55a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((1702, 32, 32), (1702,)), ((426, 32, 32), (426,)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create validation set\n",
    "\n",
    "###### REMOVE WITH ENOUGH RAM\n",
    "images = images_dup\n",
    "labels = labels_dup\n",
    "\n",
    "# X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.3)\n",
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val = train_test_split(images_dup, labels_dup, test_size = 0.2)\n",
    "# X_train, y_train = X_labeled_train, y_labeled_train\n",
    "\n",
    "# X_labeled_val, X_test, y_labeled_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_labeled_val, y_labeled_val, test_size = 0.5)\n",
    "\n",
    "(X_labeled_train.shape, y_labeled_train.shape), (X_labeled_val.shape, y_labeled_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "mdJafJMMDB0r",
    "outputId": "2e8d00f6-32bf-4aeb-d02d-e10dcaaccfe3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(labels_dup, 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2128, 32, 32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNxaCTbcDB0r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tz9Q8wYuDB0s"
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "def classification_step(X_train, y_train, X_test, model, model_device, temperature):\n",
    "    logreg = LogisticRegression()\n",
    "    features_train = model(torch.as_tensor(X_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    logreg.fit(features_train, y_train)\n",
    "    # logreg.fit(features, y_train[y_train != 3])\n",
    "    \n",
    "    features_test = model(torch.as_tensor(X_test, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()    \n",
    "    y_hat = logreg.predict_proba(features_test)\n",
    "    y_hat = torch.as_tensor(y_hat, dtype=torch.float32, device='cpu')\n",
    "    \n",
    "#     print(y_hat)\n",
    "    print(f'accuracy: {logreg.score(features, y):.5}')\n",
    "\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train), y_hat, normalize='true')\n",
    "#     # cm = sklearn.metrics.confusion_matrix(idx_to_oneHot(y_train[y_train != 3]), y_hat, normalize='true')\n",
    "#     cm = rh_cm(y_hat, y)\n",
    "#     # cm = rh_cm(y_hat, y_train[y_train != 3])\n",
    "    \n",
    "    unc = util.loss_uncertainty(y_hat, temperature=temperature)\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     return torch.tensor(unc, dtype=torch.float32, device=model_device)\n",
    "    return unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtkRZSMqDB0v",
    "outputId": "230c559f-f22c-4182-b3ab-024ba2080a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)\n",
    "# DEVICE = torch_helpers.set_device(use_GPU=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(model_chopped, base_model_frozen, pre_head_fc_sizes=[1024, 512], post_head_fc_sizes=[64], classifier_fc_sizes=[len(np.unique(y_labeled_train))])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(model_chopped.to(DEVICE), base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "# model = torch.nn.Sequential([model_chopped.to(DEVICE), torch.nn.Linear], pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "\n",
    "\n",
    "# model = ModelTackOn(base_model_frozen, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "# model = ModelTackOn(base_model, pre_head_fc_sizes=[len(np.unique(y_train))], post_head_fc_sizes=[])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.0.7.0.block.0.0.weight\n",
      "base_model.0.7.0.block.0.1.weight\n",
      "base_model.0.7.0.block.0.1.bias\n",
      "base_model.0.7.0.block.1.0.weight\n",
      "base_model.0.7.0.block.1.1.weight\n",
      "base_model.0.7.0.block.1.1.bias\n",
      "base_model.0.7.0.block.2.fc1.weight\n",
      "base_model.0.7.0.block.2.fc1.bias\n",
      "base_model.0.7.0.block.2.fc2.weight\n",
      "base_model.0.7.0.block.2.fc2.bias\n",
      "base_model.0.7.0.block.3.0.weight\n",
      "base_model.0.7.0.block.3.1.weight\n",
      "base_model.0.7.0.block.3.1.bias\n",
      "base_model.0.8.0.weight\n",
      "base_model.0.8.1.weight\n",
      "base_model.0.8.1.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n",
      "Classifier_0.weight\n",
      "Classifier_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in ResNet model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    # print(name)\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[13]) < 7:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[13]) >= 7:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.15, 0.15), #0, .3, .45 (DEFAULT)\n",
    "                                        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "                                        shear=(-15, 15, -15, 15),\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   #scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "                                    scaler_bounds=(10**(4), 10**(6)),\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.00015,\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    torchvision.transforms.Resize(size=(224,224), \n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "    \n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "tq77tWZeDB0s"
   },
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    \n",
    "#     torchvision.transforms.RandomAdjustSharpness(torch.rand(1)*5, p=0.5),\n",
    "#         torchvision.transforms.RandomPerspective(distortion_scale=0.7, \n",
    "#                                              p=0.5, \n",
    "#                                              interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                              fill=0),\n",
    "#     torchvision.transforms.GaussianBlur(kernel_size=5,\n",
    "#                                         sigma=(0.0001, 0.1)),\n",
    "        \n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "        \n",
    "    torchvision.transforms.RandomAffine(\n",
    "                                        degrees=(-180,180),\n",
    "                                        translate=(0.2, 0.2), #0.15/.15\n",
    "                                        scale=(0.4, 1.3),  #.6, 1.2\n",
    "                                        shear=(-25, 25, -25, 25), # -15/+15 across board\n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "                                        fill=0, \n",
    "                                        fillcolor=None, \n",
    "                                        resample=None),\n",
    "    augmentation.AddPoissonNoise(   scaler_bounds=(10**(4.0), 10**(6.)), # 4.5, 6\n",
    "                                    prob=1,\n",
    "                                    base=1000,\n",
    "                                    scaling='log'),\n",
    "    augmentation.AddGaussianNoise(  mean=0, \n",
    "                                    std=0.0002, # 0.00015\n",
    "                                    prob=1),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)), # Do vs. don't do -- bounds between 0/1. Either do this OR do this followed by torchvision.transforms.Normalize\n",
    "    \n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # Do vs. don't do\n",
    "    \n",
    "    # augmentation.AddPoissonNoise(   scaler_bounds=(10**(1.5), 10**(4.0)),\n",
    "    #                                 prob=1,\n",
    "    #                                 base=1000,\n",
    "    #                                 scaling='log'),\n",
    "    # augmentation.AddGaussianNoise(  mean=0, \n",
    "    #                                 std=0.1,\n",
    "    #                                 prob=1),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "    \n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    "#     torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "\n",
    "#     torchvision.transforms.RandomAffine(\n",
    "#                                         degrees=(-180,180),\n",
    "#                                         translate=(0.0, 0.0),\n",
    "#                                         interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "#                                         fill=0, \n",
    "#                                         # fillcolor=None, \n",
    "#                                         resample=None),\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "# scripted_transforms = transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [],
   "source": [
    "dataset_train = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=2,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_train = torch.utils.data.DataLoader( dataset_train,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=800,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "for ii in idx_rand:\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(dataset_train[ii][0][0][0])\n",
    "    axs[1].imshow(dataset_train[ii][0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nstWf2PhVwfV",
    "outputId": "968f73ab-75d7-4735-ea1e-49e7fb3821cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_helpers.delete_all_cuda_tensors(globals())\n",
    "\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "model.prep_contrast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# model = models.LeNet1(dropout_prob=0.3, momentum_val=0, n_output_features=64)\n",
    "\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-3.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_dl(data, transforms, expand_dim=False):\n",
    "    setup_ds = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(data, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(data.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    transform=transforms,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "\n",
    "                                    temp_uncertainty=16,\n",
    "                                    expand_dim=expand_dim\n",
    "                                    )\n",
    "    setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                            batch_size=1024,\n",
    "                                            shuffle=False,\n",
    "                                            drop_last=False,\n",
    "                                            # pin_memory=True,\n",
    "                                            # num_workers=16,\n",
    "                                            # persistent_workers=True,\n",
    "                                            )\n",
    "\n",
    "    return setup_ds, setup_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1702, 32, 32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_labeled_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "no_transforms = torch.nn.Sequential(\n",
    ")\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR)\n",
    ")\n",
    "tile_transforms = torch.nn.Sequential(\n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train, setup_transform, expand_dim=True)\n",
    "input_xtr = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=True)\n",
    "input_xtr_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val, setup_transform, expand_dim=True)\n",
    "input_xval = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "setup_ds, setup_dl = get_ds_dl(X_labeled_val_SYT, setup_transform, expand_dim=True)\n",
    "input_xval_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "# print(\"Starting!\")\n",
    "# tik = time.time()\n",
    "# ds_run = util.dataset_simCLR(\n",
    "#                             torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "#                             torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                             n_transforms=1,\n",
    "#                             class_weights=np.array([1]),\n",
    "#                             transform=run_transform,\n",
    "#                             DEVICE='cpu',\n",
    "#                             dtype_X=torch.float32,\n",
    "#                             dtype_y=torch.int64,\n",
    "\n",
    "#                             temp_uncertainty=16,\n",
    "#                             expand_dim=False\n",
    "#                             )\n",
    "# dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "# features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# tok = time.time()\n",
    "\n",
    "# print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1702, 1, 224, 224]),\n",
       " torch.Size([54704, 1, 224, 224]),\n",
       " torch.Size([426, 1, 224, 224]),\n",
       " torch.Size([13677, 1, 224, 224]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_xtr.shape, input_xtr_SYT.shape, input_xval.shape, input_xval_SYT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale_rsz_transforms = torch.nn.Sequential(\n",
    "#     augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "#     torchvision.transforms.Resize(size=(224,224),\n",
    "#                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "# tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "# scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "# scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n",
    "\n",
    "# setup_transform = scripted_scale_rsz_transform\n",
    "# run_transform = scripted_tile_transform\n",
    "\n",
    "\n",
    "\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "\n",
    "# setup_ds = util.dataset_simCLR(\n",
    "#                                 torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "#                                 torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "#                                 n_transforms=1,\n",
    "#                                 transform=setup_transform,\n",
    "#                                 class_weights=np.array([1]),\n",
    "#                                 DEVICE='cpu',\n",
    "#                                 dtype_X=torch.float32,\n",
    "#                                 dtype_y=torch.int64,\n",
    "\n",
    "#                                 temp_uncertainty=16,\n",
    "#                                 expand_dim=False\n",
    "#                                 )\n",
    "# setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "#                                         batch_size=1024,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         # pin_memory=True,\n",
    "#                                         # num_workers=16,\n",
    "#                                         # persistent_workers=True,\n",
    "#                                         )\n",
    "\n",
    "\n",
    "# input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "# # setup_ds, setup_dl = get_ds_dl(X_labeled_train_SYT, setup_transform, expand_dim=False)\n",
    "# # input_x_SYT = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train_lst, acc_val_lst = [], []\n",
    "# acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "# print('A')\n",
    "# ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "# features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "# features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "# print('B')\n",
    "# ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "# features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "# ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "# features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "# acc_train, acc_val = {}, {}\n",
    "# acc_train_SYT, acc_val_SYT = {}, {}\n",
    "# C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "# for C in C_toUse:\n",
    "#     print('C1')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     logreg.fit(features_train, y_labeled_train)\n",
    "#     acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "#     acc_train[C] = acc_train_tmp\n",
    "#     acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "#     acc_val[C] = acc_val_tmp\n",
    "    \n",
    "#     print('C2')\n",
    "#     logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "#     acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "#     acc_train_SYT[C] = acc_train_tmp\n",
    "#     acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "#     acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "# acc_train_lst.append(acc_train)\n",
    "# acc_val_lst.append(acc_val)\n",
    "# acc_train_SYT_lst.append(acc_train_SYT)\n",
    "# acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "\n",
    "# losses_train_npy = np.array(losses_train)\n",
    "# losses_val_npy = np.array(losses_val)\n",
    "# val_accs_npy = np.array(val_accs)\n",
    "# acc_npy = np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "# display(acc_train_df)\n",
    "# display(acc_val_df)\n",
    "# display(acc_train_SYT_df)\n",
    "# display(acc_val_SYT_df)\n",
    "\n",
    "# acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "# acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "# acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "# acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient2-only7unfrozen'\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/889, loss_train: 7.349, loss_val: nan, pos_over_neg: 1.0245325565338135 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 7.1765, loss_val: nan, pos_over_neg: 1.2428301572799683 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 6.9988, loss_val: nan, pos_over_neg: 3.1196725368499756 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 6.9009, loss_val: nan, pos_over_neg: 3.7103147506713867 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 6.7802, loss_val: nan, pos_over_neg: 4.077817916870117 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 6.7269, loss_val: nan, pos_over_neg: 5.723012447357178 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 6.6701, loss_val: nan, pos_over_neg: 9.695016860961914 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 6.6192, loss_val: nan, pos_over_neg: 13.35693073272705 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 6.5942, loss_val: nan, pos_over_neg: 12.405990600585938 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 6.5699, loss_val: nan, pos_over_neg: 13.105032920837402 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 6.5927, loss_val: nan, pos_over_neg: 11.125567436218262 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 6.5821, loss_val: nan, pos_over_neg: 12.746593475341797 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 6.5284, loss_val: nan, pos_over_neg: 14.635932922363281 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 6.5061, loss_val: nan, pos_over_neg: 17.280933380126953 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 6.4861, loss_val: nan, pos_over_neg: 24.638734817504883 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 6.4894, loss_val: nan, pos_over_neg: 35.625423431396484 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 6.4357, loss_val: nan, pos_over_neg: 36.76000213623047 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 6.4505, loss_val: nan, pos_over_neg: 29.897552490234375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 6.4243, loss_val: nan, pos_over_neg: 40.164772033691406 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 6.4171, loss_val: nan, pos_over_neg: 31.975082397460938 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 6.4169, loss_val: nan, pos_over_neg: 38.8293571472168 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 6.3966, loss_val: nan, pos_over_neg: 43.021766662597656 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 6.42, loss_val: nan, pos_over_neg: 49.94255447387695 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 6.4189, loss_val: nan, pos_over_neg: 69.38328552246094 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 6.3791, loss_val: nan, pos_over_neg: 68.40487670898438 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 6.3844, loss_val: nan, pos_over_neg: 69.47258758544922 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 6.3886, loss_val: nan, pos_over_neg: 61.37703323364258 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 6.3624, loss_val: nan, pos_over_neg: 58.32057189941406 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 6.3712, loss_val: nan, pos_over_neg: 60.382022857666016 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 6.3645, loss_val: nan, pos_over_neg: 60.4225959777832 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 6.3575, loss_val: nan, pos_over_neg: 47.780967712402344 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 6.3613, loss_val: nan, pos_over_neg: 56.664268493652344 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 6.3461, loss_val: nan, pos_over_neg: 52.18281555175781 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 6.332, loss_val: nan, pos_over_neg: 59.72954559326172 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 6.3467, loss_val: nan, pos_over_neg: 50.57052230834961 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 6.3539, loss_val: nan, pos_over_neg: 77.18146514892578 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 6.3158, loss_val: nan, pos_over_neg: 98.26416778564453 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 6.3426, loss_val: nan, pos_over_neg: 130.63702392578125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 6.2716, loss_val: nan, pos_over_neg: 572.305908203125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 6.3388, loss_val: nan, pos_over_neg: 134.6428680419922 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 6.2854, loss_val: nan, pos_over_neg: 126.25874328613281 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 6.3007, loss_val: nan, pos_over_neg: 60.00991439819336 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 6.301, loss_val: nan, pos_over_neg: 108.29029846191406 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 6.2765, loss_val: nan, pos_over_neg: 226.2075653076172 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 6.2715, loss_val: nan, pos_over_neg: 169.67042541503906 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 6.2784, loss_val: nan, pos_over_neg: 300.9647216796875 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 6.262, loss_val: nan, pos_over_neg: 326.22735595703125 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 6.2809, loss_val: nan, pos_over_neg: 455.69189453125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 6.2647, loss_val: nan, pos_over_neg: 136.2460479736328 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 6.2584, loss_val: nan, pos_over_neg: 135.01808166503906 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 6.2663, loss_val: nan, pos_over_neg: 89.57090759277344 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 6.2566, loss_val: nan, pos_over_neg: 87.62063598632812 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 6.2692, loss_val: nan, pos_over_neg: 73.72130584716797 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 6.2493, loss_val: nan, pos_over_neg: 130.52716064453125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 6.231, loss_val: nan, pos_over_neg: 432.233642578125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 6.2642, loss_val: nan, pos_over_neg: 284.3470458984375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 6.2761, loss_val: nan, pos_over_neg: 168.20960998535156 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 6.2332, loss_val: nan, pos_over_neg: 126.30585479736328 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 6.2279, loss_val: nan, pos_over_neg: 145.0230712890625 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 6.2489, loss_val: nan, pos_over_neg: 197.8621826171875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 6.2318, loss_val: nan, pos_over_neg: 208.46356201171875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 6.239, loss_val: nan, pos_over_neg: 204.97372436523438 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 6.2176, loss_val: nan, pos_over_neg: 263.3623962402344 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 6.2207, loss_val: nan, pos_over_neg: 150.89712524414062 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 6.2188, loss_val: nan, pos_over_neg: 172.01687622070312 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 6.2326, loss_val: nan, pos_over_neg: 220.76437377929688 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 6.2134, loss_val: nan, pos_over_neg: 548.8804931640625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 6.195, loss_val: nan, pos_over_neg: 245.45242309570312 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 6.2012, loss_val: nan, pos_over_neg: 97.57734680175781 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 6.2155, loss_val: nan, pos_over_neg: 174.06394958496094 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 6.2034, loss_val: nan, pos_over_neg: 276.4989929199219 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 6.2136, loss_val: nan, pos_over_neg: 376.9305419921875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 6.2277, loss_val: nan, pos_over_neg: 247.68959045410156 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 6.1982, loss_val: nan, pos_over_neg: 306.3572082519531 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 6.1963, loss_val: nan, pos_over_neg: 206.88906860351562 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 6.1918, loss_val: nan, pos_over_neg: 213.86781311035156 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 6.1749, loss_val: nan, pos_over_neg: 174.4286651611328 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 6.1885, loss_val: nan, pos_over_neg: 121.41209411621094 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 6.1978, loss_val: nan, pos_over_neg: 119.81388854980469 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 6.1968, loss_val: nan, pos_over_neg: 235.82254028320312 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 6.1872, loss_val: nan, pos_over_neg: 469.0918884277344 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 6.1922, loss_val: nan, pos_over_neg: 300.2934875488281 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 6.1986, loss_val: nan, pos_over_neg: 159.82041931152344 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 6.1731, loss_val: nan, pos_over_neg: 166.102783203125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 6.1749, loss_val: nan, pos_over_neg: 179.78102111816406 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 6.1772, loss_val: nan, pos_over_neg: 161.08253479003906 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 6.1752, loss_val: nan, pos_over_neg: 420.5327453613281 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 6.1623, loss_val: nan, pos_over_neg: -7677.44384765625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 6.1704, loss_val: nan, pos_over_neg: 612.3184814453125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 6.1709, loss_val: nan, pos_over_neg: 214.66351318359375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 6.1523, loss_val: nan, pos_over_neg: 233.5143585205078 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 6.1648, loss_val: nan, pos_over_neg: 188.58128356933594 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 6.1787, loss_val: nan, pos_over_neg: 303.2115783691406 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 6.1423, loss_val: nan, pos_over_neg: 270.2520751953125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 6.1651, loss_val: nan, pos_over_neg: 355.49554443359375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 6.1658, loss_val: nan, pos_over_neg: 332.09222412109375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 6.1508, loss_val: nan, pos_over_neg: 126.70993041992188 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 6.1531, loss_val: nan, pos_over_neg: 138.2277069091797 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 6.1428, loss_val: nan, pos_over_neg: 307.4726257324219 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 6.1719, loss_val: nan, pos_over_neg: 447.552734375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 6.1825, loss_val: nan, pos_over_neg: -24877.818359375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 6.1671, loss_val: nan, pos_over_neg: 589.2442626953125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 6.1436, loss_val: nan, pos_over_neg: 138.76390075683594 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 6.1445, loss_val: nan, pos_over_neg: 113.7369613647461 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 6.1394, loss_val: nan, pos_over_neg: 136.47360229492188 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 6.1615, loss_val: nan, pos_over_neg: 154.4779510498047 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 6.1373, loss_val: nan, pos_over_neg: 596.7782592773438 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 6.151, loss_val: nan, pos_over_neg: 2300.799072265625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 6.1587, loss_val: nan, pos_over_neg: 381.8980407714844 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 6.1608, loss_val: nan, pos_over_neg: 292.99908447265625 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 6.1378, loss_val: nan, pos_over_neg: 401.57568359375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 6.1414, loss_val: nan, pos_over_neg: 371.1463623046875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 6.1598, loss_val: nan, pos_over_neg: 252.57374572753906 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 6.1391, loss_val: nan, pos_over_neg: 330.0692443847656 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 6.1662, loss_val: nan, pos_over_neg: 169.17465209960938 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 6.1482, loss_val: nan, pos_over_neg: 105.92678833007812 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 6.1207, loss_val: nan, pos_over_neg: 131.674072265625 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 6.1526, loss_val: nan, pos_over_neg: 186.0092010498047 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 6.1384, loss_val: nan, pos_over_neg: 377.05792236328125 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 6.1486, loss_val: nan, pos_over_neg: 811.4246826171875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 6.1249, loss_val: nan, pos_over_neg: 685.2899780273438 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 6.1409, loss_val: nan, pos_over_neg: 136.18072509765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 6.1477, loss_val: nan, pos_over_neg: 135.57937622070312 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 6.1245, loss_val: nan, pos_over_neg: 258.084228515625 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 6.1355, loss_val: nan, pos_over_neg: 563.0515747070312 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 6.1332, loss_val: nan, pos_over_neg: 508.1483154296875 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 6.1177, loss_val: nan, pos_over_neg: 997.213134765625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 6.1093, loss_val: nan, pos_over_neg: 577.0850830078125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 6.1365, loss_val: nan, pos_over_neg: 116.50235748291016 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 6.0957, loss_val: nan, pos_over_neg: 176.00088500976562 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 6.1115, loss_val: nan, pos_over_neg: 124.72626495361328 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 6.1123, loss_val: nan, pos_over_neg: 198.61288452148438 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 6.1336, loss_val: nan, pos_over_neg: 169.73446655273438 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 6.1458, loss_val: nan, pos_over_neg: 961.0975341796875 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 6.1302, loss_val: nan, pos_over_neg: 442.2905578613281 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 6.1224, loss_val: nan, pos_over_neg: 111.29237365722656 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 6.1224, loss_val: nan, pos_over_neg: 112.9844970703125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 6.1318, loss_val: nan, pos_over_neg: 228.81993103027344 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 6.1124, loss_val: nan, pos_over_neg: 233.8466796875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 6.1161, loss_val: nan, pos_over_neg: 176.25479125976562 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 6.1206, loss_val: nan, pos_over_neg: 138.77049255371094 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 6.093, loss_val: nan, pos_over_neg: 170.2907257080078 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 6.1344, loss_val: nan, pos_over_neg: 127.20215606689453 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 6.1163, loss_val: nan, pos_over_neg: 240.2181854248047 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 6.1155, loss_val: nan, pos_over_neg: 140.7607421875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 6.1237, loss_val: nan, pos_over_neg: 541.1458740234375 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 6.0979, loss_val: nan, pos_over_neg: 360.40264892578125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 6.1178, loss_val: nan, pos_over_neg: 784.7081298828125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 6.1056, loss_val: nan, pos_over_neg: 259.0851135253906 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 6.0839, loss_val: nan, pos_over_neg: 188.12155151367188 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 6.1094, loss_val: nan, pos_over_neg: 117.59463500976562 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 6.0784, loss_val: nan, pos_over_neg: 167.9437713623047 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 6.1269, loss_val: nan, pos_over_neg: 212.37008666992188 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 6.0913, loss_val: nan, pos_over_neg: 306.4881896972656 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 6.1034, loss_val: nan, pos_over_neg: 3306.95361328125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 6.1167, loss_val: nan, pos_over_neg: 1292.0618896484375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 6.1092, loss_val: nan, pos_over_neg: 547.3026733398438 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 6.0777, loss_val: nan, pos_over_neg: 234.54412841796875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 6.1067, loss_val: nan, pos_over_neg: 341.1305847167969 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 6.1058, loss_val: nan, pos_over_neg: 326.1900329589844 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 6.1171, loss_val: nan, pos_over_neg: 478.8206481933594 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 6.0999, loss_val: nan, pos_over_neg: 931.221435546875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 6.0887, loss_val: nan, pos_over_neg: 560.9856567382812 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 6.099, loss_val: nan, pos_over_neg: 2972.880859375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 6.0833, loss_val: nan, pos_over_neg: 1021.8251342773438 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 6.0966, loss_val: nan, pos_over_neg: 181.4689483642578 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 6.0894, loss_val: nan, pos_over_neg: 184.38783264160156 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 6.087, loss_val: nan, pos_over_neg: 191.08847045898438 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 6.1122, loss_val: nan, pos_over_neg: 251.79798889160156 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 6.0964, loss_val: nan, pos_over_neg: 287.64434814453125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 6.0989, loss_val: nan, pos_over_neg: 278.6950988769531 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 6.0924, loss_val: nan, pos_over_neg: 323.1756591796875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 6.0872, loss_val: nan, pos_over_neg: 425.6821594238281 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 6.1037, loss_val: nan, pos_over_neg: 112.01836395263672 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 6.1048, loss_val: nan, pos_over_neg: 179.1822967529297 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 6.1053, loss_val: nan, pos_over_neg: 158.2784423828125 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 6.0868, loss_val: nan, pos_over_neg: 194.80972290039062 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 6.0734, loss_val: nan, pos_over_neg: 774.9964599609375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 6.1018, loss_val: nan, pos_over_neg: 685.9425659179688 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 6.0799, loss_val: nan, pos_over_neg: 379.6163635253906 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 6.0596, loss_val: nan, pos_over_neg: 477.7045593261719 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 6.1063, loss_val: nan, pos_over_neg: 603.1556396484375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 6.0711, loss_val: nan, pos_over_neg: 242.29188537597656 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 6.0956, loss_val: nan, pos_over_neg: 407.2132873535156 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 6.0911, loss_val: nan, pos_over_neg: 309.9689025878906 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 6.0869, loss_val: nan, pos_over_neg: 526.7890625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 6.0658, loss_val: nan, pos_over_neg: 235.78839111328125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 6.0771, loss_val: nan, pos_over_neg: 179.64590454101562 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 6.0818, loss_val: nan, pos_over_neg: 222.45541381835938 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 6.0803, loss_val: nan, pos_over_neg: 339.2373962402344 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 6.0691, loss_val: nan, pos_over_neg: 765.577392578125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 6.1007, loss_val: nan, pos_over_neg: 504.1806335449219 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 6.0949, loss_val: nan, pos_over_neg: 383.8205871582031 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 6.08, loss_val: nan, pos_over_neg: 177.63140869140625 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 6.089, loss_val: nan, pos_over_neg: 277.6163635253906 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 6.1073, loss_val: nan, pos_over_neg: 569.2347412109375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 6.0773, loss_val: nan, pos_over_neg: 415.7864074707031 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 6.06, loss_val: nan, pos_over_neg: 485.172119140625 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 6.1141, loss_val: nan, pos_over_neg: 391.3511047363281 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 6.0639, loss_val: nan, pos_over_neg: 635.400146484375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 6.0695, loss_val: nan, pos_over_neg: 484.0053405761719 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 6.0605, loss_val: nan, pos_over_neg: 343.4300537109375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 6.0831, loss_val: nan, pos_over_neg: 175.85365295410156 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 6.0685, loss_val: nan, pos_over_neg: 437.9359130859375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 6.068, loss_val: nan, pos_over_neg: 551.6668090820312 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 6.078, loss_val: nan, pos_over_neg: 290.13336181640625 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 6.0858, loss_val: nan, pos_over_neg: 219.14889526367188 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 6.07, loss_val: nan, pos_over_neg: 354.9859619140625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 6.0863, loss_val: nan, pos_over_neg: 434.8337707519531 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 6.0774, loss_val: nan, pos_over_neg: 928.058349609375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 6.0528, loss_val: nan, pos_over_neg: 440.9391784667969 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 6.0455, loss_val: nan, pos_over_neg: 481.46856689453125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 6.0805, loss_val: nan, pos_over_neg: 228.8953094482422 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 6.0829, loss_val: nan, pos_over_neg: 142.84400939941406 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 6.0563, loss_val: nan, pos_over_neg: 275.2860107421875 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 6.0842, loss_val: nan, pos_over_neg: 1596.584228515625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 6.0653, loss_val: nan, pos_over_neg: 919.218017578125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 6.0694, loss_val: nan, pos_over_neg: 193.86285400390625 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 6.0773, loss_val: nan, pos_over_neg: 196.5309600830078 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 6.0561, loss_val: nan, pos_over_neg: 257.108154296875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 6.0743, loss_val: nan, pos_over_neg: 122.29447174072266 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 6.0569, loss_val: nan, pos_over_neg: 264.14404296875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 6.0709, loss_val: nan, pos_over_neg: 477.197265625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 6.0836, loss_val: nan, pos_over_neg: 265.72210693359375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 6.0769, loss_val: nan, pos_over_neg: 196.02281188964844 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 6.0784, loss_val: nan, pos_over_neg: 189.0275421142578 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 6.0748, loss_val: nan, pos_over_neg: 185.75411987304688 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 6.0757, loss_val: nan, pos_over_neg: 191.92628479003906 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 6.0383, loss_val: nan, pos_over_neg: 165.88064575195312 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 6.0737, loss_val: nan, pos_over_neg: 130.7822723388672 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 6.0447, loss_val: nan, pos_over_neg: 562.9055786132812 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 6.0731, loss_val: nan, pos_over_neg: 366.1249694824219 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 6.0734, loss_val: nan, pos_over_neg: 352.9368896484375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 6.0704, loss_val: nan, pos_over_neg: 267.0198974609375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 6.0673, loss_val: nan, pos_over_neg: 231.6077117919922 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 6.069, loss_val: nan, pos_over_neg: 109.19654846191406 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 6.0591, loss_val: nan, pos_over_neg: 82.6478042602539 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 6.0481, loss_val: nan, pos_over_neg: 289.3094787597656 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 6.0495, loss_val: nan, pos_over_neg: 783.5059204101562 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 6.0331, loss_val: nan, pos_over_neg: 1499.990234375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 6.0445, loss_val: nan, pos_over_neg: 890.4927368164062 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 6.0674, loss_val: nan, pos_over_neg: 308.8248596191406 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 6.0691, loss_val: nan, pos_over_neg: 183.21322631835938 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 6.0502, loss_val: nan, pos_over_neg: 148.223876953125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 6.0517, loss_val: nan, pos_over_neg: 132.1959686279297 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 6.034, loss_val: nan, pos_over_neg: 265.7971496582031 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 6.0604, loss_val: nan, pos_over_neg: 402.277099609375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 6.0478, loss_val: nan, pos_over_neg: 1323.299560546875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 6.0471, loss_val: nan, pos_over_neg: 403.595703125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 6.0511, loss_val: nan, pos_over_neg: 417.18902587890625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 6.0374, loss_val: nan, pos_over_neg: 149.13075256347656 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 6.0624, loss_val: nan, pos_over_neg: 154.74053955078125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 6.0735, loss_val: nan, pos_over_neg: 159.3502655029297 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 6.058, loss_val: nan, pos_over_neg: 1003.171630859375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 6.0532, loss_val: nan, pos_over_neg: 3285.30126953125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 6.0612, loss_val: nan, pos_over_neg: 3656.6171875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 6.0401, loss_val: nan, pos_over_neg: 992.57470703125 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 6.0486, loss_val: nan, pos_over_neg: 144.74978637695312 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 6.0663, loss_val: nan, pos_over_neg: 148.45652770996094 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 6.0391, loss_val: nan, pos_over_neg: 220.4055938720703 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 6.0531, loss_val: nan, pos_over_neg: 278.8292541503906 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 6.065, loss_val: nan, pos_over_neg: 1070.564208984375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 6.0587, loss_val: nan, pos_over_neg: 325.5976867675781 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 6.054, loss_val: nan, pos_over_neg: 1095.5150146484375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 6.0402, loss_val: nan, pos_over_neg: 251.8214569091797 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 6.0398, loss_val: nan, pos_over_neg: 239.9314422607422 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 6.0434, loss_val: nan, pos_over_neg: 627.7245483398438 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 6.0573, loss_val: nan, pos_over_neg: 340.116943359375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 6.0205, loss_val: nan, pos_over_neg: 417.4338684082031 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 6.0402, loss_val: nan, pos_over_neg: 404.34503173828125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 6.0459, loss_val: nan, pos_over_neg: 276.26336669921875 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 6.045, loss_val: nan, pos_over_neg: 294.5837707519531 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 6.0522, loss_val: nan, pos_over_neg: 454.353271484375 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 6.0437, loss_val: nan, pos_over_neg: 517.5831298828125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 6.0454, loss_val: nan, pos_over_neg: 668.3501586914062 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 6.0286, loss_val: nan, pos_over_neg: 10620.67578125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 6.0426, loss_val: nan, pos_over_neg: 406.9490661621094 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 6.0364, loss_val: nan, pos_over_neg: 286.02972412109375 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 6.0378, loss_val: nan, pos_over_neg: 212.52296447753906 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 6.047, loss_val: nan, pos_over_neg: 218.9250946044922 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 6.0371, loss_val: nan, pos_over_neg: 3385.96923828125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 6.0197, loss_val: nan, pos_over_neg: 2239.483642578125 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 6.0517, loss_val: nan, pos_over_neg: 2624.38525390625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 6.0369, loss_val: nan, pos_over_neg: 572.6892700195312 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 6.0159, loss_val: nan, pos_over_neg: 343.184814453125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 6.0411, loss_val: nan, pos_over_neg: 137.8662567138672 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 6.0511, loss_val: nan, pos_over_neg: 208.94766235351562 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 6.0384, loss_val: nan, pos_over_neg: 362.5743713378906 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 6.0473, loss_val: nan, pos_over_neg: 460.682861328125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 6.0332, loss_val: nan, pos_over_neg: -5406.7041015625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 6.0382, loss_val: nan, pos_over_neg: 1661.519775390625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 6.0477, loss_val: nan, pos_over_neg: 808.9295043945312 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 6.0359, loss_val: nan, pos_over_neg: 251.299072265625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 6.0427, loss_val: nan, pos_over_neg: 314.2055358886719 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 6.0469, loss_val: nan, pos_over_neg: 238.5451202392578 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 6.0356, loss_val: nan, pos_over_neg: 298.510009765625 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 6.0318, loss_val: nan, pos_over_neg: 332.7234191894531 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 6.0412, loss_val: nan, pos_over_neg: 2256.974853515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 6.0306, loss_val: nan, pos_over_neg: 421.6792907714844 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 6.0462, loss_val: nan, pos_over_neg: 562.04736328125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 6.0493, loss_val: nan, pos_over_neg: 200.7292022705078 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 6.0159, loss_val: nan, pos_over_neg: 183.5385284423828 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 6.0512, loss_val: nan, pos_over_neg: 139.75416564941406 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 6.0267, loss_val: nan, pos_over_neg: 249.395263671875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 6.0164, loss_val: nan, pos_over_neg: 750.8545532226562 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 6.0421, loss_val: nan, pos_over_neg: 348.6832275390625 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 6.0253, loss_val: nan, pos_over_neg: 1713.6400146484375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 6.0403, loss_val: nan, pos_over_neg: 425.55230712890625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 6.0484, loss_val: nan, pos_over_neg: 239.84854125976562 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 6.0451, loss_val: nan, pos_over_neg: 202.922119140625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 6.0185, loss_val: nan, pos_over_neg: 1088.005859375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 6.0367, loss_val: nan, pos_over_neg: 717.2081909179688 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 6.02, loss_val: nan, pos_over_neg: 2610.604248046875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.9974, loss_val: nan, pos_over_neg: 423.4955139160156 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 6.0349, loss_val: nan, pos_over_neg: 406.9923095703125 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 6.0422, loss_val: nan, pos_over_neg: 330.0997314453125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 6.026, loss_val: nan, pos_over_neg: 255.5793914794922 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 6.0155, loss_val: nan, pos_over_neg: 266.0823669433594 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 6.013, loss_val: nan, pos_over_neg: 536.6521606445312 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 6.0251, loss_val: nan, pos_over_neg: -17510.72265625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 6.0414, loss_val: nan, pos_over_neg: 1538.0009765625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 6.0099, loss_val: nan, pos_over_neg: 2605.526611328125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 6.0343, loss_val: nan, pos_over_neg: 948.7951049804688 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 6.0277, loss_val: nan, pos_over_neg: 304.4674072265625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 6.0295, loss_val: nan, pos_over_neg: 282.7124938964844 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 6.0125, loss_val: nan, pos_over_neg: 225.94476318359375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 6.0041, loss_val: nan, pos_over_neg: 3702.162841796875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 6.0303, loss_val: nan, pos_over_neg: 325.5401306152344 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 6.0305, loss_val: nan, pos_over_neg: 310.80108642578125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 6.0444, loss_val: nan, pos_over_neg: 476.4184265136719 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 6.0144, loss_val: nan, pos_over_neg: 307.77667236328125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 6.004, loss_val: nan, pos_over_neg: 263.8039855957031 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 6.0403, loss_val: nan, pos_over_neg: 262.9934387207031 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 6.0029, loss_val: nan, pos_over_neg: 463.28497314453125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 6.0302, loss_val: nan, pos_over_neg: 401.8743591308594 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 6.0353, loss_val: nan, pos_over_neg: 234.64244079589844 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 6.028, loss_val: nan, pos_over_neg: 264.1387023925781 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 6.0232, loss_val: nan, pos_over_neg: 660.7567749023438 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 6.0325, loss_val: nan, pos_over_neg: 364.371337890625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 6.0016, loss_val: nan, pos_over_neg: 824.596435546875 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 6.0407, loss_val: nan, pos_over_neg: 175.90689086914062 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 6.034, loss_val: nan, pos_over_neg: 734.5546264648438 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 6.0315, loss_val: nan, pos_over_neg: 629.5333251953125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 850.3240966796875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 6.0045, loss_val: nan, pos_over_neg: 1383.4215087890625 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 6.0152, loss_val: nan, pos_over_neg: 543.6287841796875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 6.0244, loss_val: nan, pos_over_neg: 292.333984375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 6.0074, loss_val: nan, pos_over_neg: 197.77818298339844 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 6.0271, loss_val: nan, pos_over_neg: 490.3291931152344 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 6.0062, loss_val: nan, pos_over_neg: 2648.751953125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 6.0025, loss_val: nan, pos_over_neg: 550.0111083984375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 6.0233, loss_val: nan, pos_over_neg: 668.4303588867188 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 6.0178, loss_val: nan, pos_over_neg: 1420.9293212890625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 6.0189, loss_val: nan, pos_over_neg: 1780.7869873046875 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 6.0141, loss_val: nan, pos_over_neg: 576.0408935546875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.9984, loss_val: nan, pos_over_neg: 333.4681701660156 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.989, loss_val: nan, pos_over_neg: 1367.8792724609375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 6.0256, loss_val: nan, pos_over_neg: 287.0678405761719 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 6.0024, loss_val: nan, pos_over_neg: 1098.151611328125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 6.0079, loss_val: nan, pos_over_neg: 527.3541870117188 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 6.0261, loss_val: nan, pos_over_neg: 2296.5322265625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 6.0209, loss_val: nan, pos_over_neg: 569.3574829101562 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 6.021, loss_val: nan, pos_over_neg: 1283.038330078125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 6.0161, loss_val: nan, pos_over_neg: 230.01858520507812 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 6.0049, loss_val: nan, pos_over_neg: 417.79486083984375 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 6.0383, loss_val: nan, pos_over_neg: 602.71630859375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 6.0006, loss_val: nan, pos_over_neg: 1266.50537109375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 677.104248046875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 6.0062, loss_val: nan, pos_over_neg: 631.6318969726562 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 6.0053, loss_val: nan, pos_over_neg: 194.23731994628906 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 6.0257, loss_val: nan, pos_over_neg: 126.38853454589844 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 6.0124, loss_val: nan, pos_over_neg: 299.2698059082031 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 6.0339, loss_val: nan, pos_over_neg: -2675.741943359375 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 6.0095, loss_val: nan, pos_over_neg: 2942.415771484375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 6.0028, loss_val: nan, pos_over_neg: 743.8992919921875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 6.0237, loss_val: nan, pos_over_neg: 717.0974731445312 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 6.0047, loss_val: nan, pos_over_neg: 254.90487670898438 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 6.0131, loss_val: nan, pos_over_neg: 171.81838989257812 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 6.0287, loss_val: nan, pos_over_neg: 210.20828247070312 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 6.0045, loss_val: nan, pos_over_neg: 326.94256591796875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 352.2470703125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.9939, loss_val: nan, pos_over_neg: 1449.8052978515625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 6.0144, loss_val: nan, pos_over_neg: 353.131103515625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 6.0276, loss_val: nan, pos_over_neg: 901.6201782226562 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 6.009, loss_val: nan, pos_over_neg: 505.0825500488281 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 6.0269, loss_val: nan, pos_over_neg: 119.44227600097656 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 6.0052, loss_val: nan, pos_over_neg: 154.17788696289062 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 6.0264, loss_val: nan, pos_over_neg: 249.44752502441406 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 6.0186, loss_val: nan, pos_over_neg: 809.7978515625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 6.0214, loss_val: nan, pos_over_neg: 2051.627685546875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 6.0385, loss_val: nan, pos_over_neg: 555.631591796875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 6.0129, loss_val: nan, pos_over_neg: 1447.7130126953125 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 6.0075, loss_val: nan, pos_over_neg: 468.21697998046875 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 6.011, loss_val: nan, pos_over_neg: 212.7982940673828 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 6.0308, loss_val: nan, pos_over_neg: 110.22721862792969 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 6.0076, loss_val: nan, pos_over_neg: 162.865478515625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 6.0134, loss_val: nan, pos_over_neg: 348.755615234375 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 6.0316, loss_val: nan, pos_over_neg: 866.540771484375 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 6.003, loss_val: nan, pos_over_neg: 492.404296875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 6.0032, loss_val: nan, pos_over_neg: 541.9801635742188 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 6.0177, loss_val: nan, pos_over_neg: 150.16270446777344 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.9985, loss_val: nan, pos_over_neg: 304.7993469238281 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 6.008, loss_val: nan, pos_over_neg: 202.5272674560547 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 6.0173, loss_val: nan, pos_over_neg: 288.7264404296875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 6.0087, loss_val: nan, pos_over_neg: 783.4584350585938 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 6.026, loss_val: nan, pos_over_neg: 452.4351806640625 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 6.0208, loss_val: nan, pos_over_neg: 788.1705932617188 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.9959, loss_val: nan, pos_over_neg: 535.8552856445312 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.9873, loss_val: nan, pos_over_neg: 348.8756408691406 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.9991, loss_val: nan, pos_over_neg: 457.10784912109375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 6.0097, loss_val: nan, pos_over_neg: 140.25148010253906 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 6.0238, loss_val: nan, pos_over_neg: 300.07666015625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.9834, loss_val: nan, pos_over_neg: 687.7653198242188 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 6.0084, loss_val: nan, pos_over_neg: 742.4940185546875 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.9698, loss_val: nan, pos_over_neg: 582.76513671875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.995, loss_val: nan, pos_over_neg: 924.9945068359375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 6.0168, loss_val: nan, pos_over_neg: 635.1290283203125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 6.0131, loss_val: nan, pos_over_neg: 259.8522644042969 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 6.0179, loss_val: nan, pos_over_neg: 303.0720520019531 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 6.0119, loss_val: nan, pos_over_neg: 241.08299255371094 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.9918, loss_val: nan, pos_over_neg: 2609.267822265625 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.9979, loss_val: nan, pos_over_neg: 2458.138671875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 1404.26318359375 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 484.1462097167969 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.9817, loss_val: nan, pos_over_neg: 639.3779907226562 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 399.2692565917969 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.9951, loss_val: nan, pos_over_neg: 366.7940368652344 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.9964, loss_val: nan, pos_over_neg: 325.4020690917969 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.9768, loss_val: nan, pos_over_neg: 4981.40185546875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.9969, loss_val: nan, pos_over_neg: 2033.0452880859375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.9987, loss_val: nan, pos_over_neg: 6055.62646484375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 416.8900146484375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.9996, loss_val: nan, pos_over_neg: 754.4828491210938 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 387.2734069824219 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.9909, loss_val: nan, pos_over_neg: 456.023681640625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.9952, loss_val: nan, pos_over_neg: 223.94259643554688 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 6.0089, loss_val: nan, pos_over_neg: 259.28082275390625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.9988, loss_val: nan, pos_over_neg: 3554.64306640625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.9857, loss_val: nan, pos_over_neg: 930.5309448242188 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.997, loss_val: nan, pos_over_neg: 1062.7816162109375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.9846, loss_val: nan, pos_over_neg: 1371.844482421875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 6.0202, loss_val: nan, pos_over_neg: 337.51214599609375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 222.5097198486328 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 210.78636169433594 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 6.0119, loss_val: nan, pos_over_neg: 342.0547790527344 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.9928, loss_val: nan, pos_over_neg: 330.0772399902344 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.99, loss_val: nan, pos_over_neg: 1904.2625732421875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.9894, loss_val: nan, pos_over_neg: 726.1651611328125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.9863, loss_val: nan, pos_over_neg: 337.3924865722656 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 1045.682861328125 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 6.0032, loss_val: nan, pos_over_neg: 581.6473388671875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.9964, loss_val: nan, pos_over_neg: 216.3961181640625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 309.2660217285156 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.9708, loss_val: nan, pos_over_neg: 446.2805480957031 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.9785, loss_val: nan, pos_over_neg: 441.1813049316406 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 6.0125, loss_val: nan, pos_over_neg: 210.8646240234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.9825, loss_val: nan, pos_over_neg: 776.7752685546875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 6.0143, loss_val: nan, pos_over_neg: 258.15533447265625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 6.0081, loss_val: nan, pos_over_neg: 411.97845458984375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.992, loss_val: nan, pos_over_neg: 205.8431854248047 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.998, loss_val: nan, pos_over_neg: 364.6636047363281 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.9931, loss_val: nan, pos_over_neg: 622.0582885742188 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.9845, loss_val: nan, pos_over_neg: 228.09947204589844 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.9941, loss_val: nan, pos_over_neg: 955.7363891601562 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.9978, loss_val: nan, pos_over_neg: 4910.35791015625 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 6.0018, loss_val: nan, pos_over_neg: 853.6832885742188 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.9787, loss_val: nan, pos_over_neg: 474.74884033203125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 428.73687744140625 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.9752, loss_val: nan, pos_over_neg: 1508.2191162109375 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 6.012, loss_val: nan, pos_over_neg: 501.4278259277344 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 6.0074, loss_val: nan, pos_over_neg: 702.370361328125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.9905, loss_val: nan, pos_over_neg: 566.3084716796875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 512.3134155273438 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.9884, loss_val: nan, pos_over_neg: 475.42694091796875 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.9791, loss_val: nan, pos_over_neg: 458.7783508300781 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.9946, loss_val: nan, pos_over_neg: 366.9732971191406 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 6.0085, loss_val: nan, pos_over_neg: 544.922607421875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.9837, loss_val: nan, pos_over_neg: 329.3006896972656 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 343.7438049316406 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.9853, loss_val: nan, pos_over_neg: 261.52691650390625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.9924, loss_val: nan, pos_over_neg: 332.1285705566406 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 399.348388671875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.9908, loss_val: nan, pos_over_neg: 669.4721069335938 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 6.0115, loss_val: nan, pos_over_neg: 450.01190185546875 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 6.0094, loss_val: nan, pos_over_neg: 264.3487854003906 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 273.2353820800781 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.9933, loss_val: nan, pos_over_neg: 441.5589904785156 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.9988, loss_val: nan, pos_over_neg: 533.7532958984375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.9694, loss_val: nan, pos_over_neg: 303.0691223144531 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.9884, loss_val: nan, pos_over_neg: 197.36729431152344 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.9849, loss_val: nan, pos_over_neg: 289.8023986816406 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.9751, loss_val: nan, pos_over_neg: 1893.3836669921875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.9972, loss_val: nan, pos_over_neg: 537.3340454101562 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.9831, loss_val: nan, pos_over_neg: 292.003662109375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.9972, loss_val: nan, pos_over_neg: 555.7028198242188 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 6.013, loss_val: nan, pos_over_neg: 328.61346435546875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 6.0029, loss_val: nan, pos_over_neg: 185.38438415527344 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 364.0512390136719 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.9638, loss_val: nan, pos_over_neg: 1631.6043701171875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.9828, loss_val: nan, pos_over_neg: 911.6468505859375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.9933, loss_val: nan, pos_over_neg: 27287.67578125 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.9894, loss_val: nan, pos_over_neg: 2510.309326171875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.9841, loss_val: nan, pos_over_neg: -3624.9453125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.9822, loss_val: nan, pos_over_neg: 335.00103759765625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.9994, loss_val: nan, pos_over_neg: 180.3646697998047 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.9739, loss_val: nan, pos_over_neg: 610.9769287109375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.9711, loss_val: nan, pos_over_neg: 433.873291015625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 6.0011, loss_val: nan, pos_over_neg: 296.5339660644531 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 1824.940673828125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.9983, loss_val: nan, pos_over_neg: 673.2130126953125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.9781, loss_val: nan, pos_over_neg: 1658.387451171875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.9857, loss_val: nan, pos_over_neg: 928.4471435546875 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.9637, loss_val: nan, pos_over_neg: 303.27349853515625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.9658, loss_val: nan, pos_over_neg: 567.9112548828125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.9844, loss_val: nan, pos_over_neg: 196.99053955078125 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.9571, loss_val: nan, pos_over_neg: 747.0789794921875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.9812, loss_val: nan, pos_over_neg: 346.44366455078125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 6.0017, loss_val: nan, pos_over_neg: 2954.9150390625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.9738, loss_val: nan, pos_over_neg: 1188.111083984375 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.9839, loss_val: nan, pos_over_neg: 1311.12890625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.9921, loss_val: nan, pos_over_neg: 355.8170166015625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.9832, loss_val: nan, pos_over_neg: 141.0496826171875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.9853, loss_val: nan, pos_over_neg: 243.82443237304688 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.9922, loss_val: nan, pos_over_neg: 588.7001342773438 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.9766, loss_val: nan, pos_over_neg: 868.017822265625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.9736, loss_val: nan, pos_over_neg: 2634.92138671875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.9885, loss_val: nan, pos_over_neg: 324.9642639160156 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.9558, loss_val: nan, pos_over_neg: 785.1195678710938 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.9851, loss_val: nan, pos_over_neg: 209.2281951904297 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 228.50389099121094 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.9768, loss_val: nan, pos_over_neg: 372.5640869140625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.9939, loss_val: nan, pos_over_neg: 513.28515625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.9736, loss_val: nan, pos_over_neg: 848.488037109375 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.977, loss_val: nan, pos_over_neg: -9711.42578125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.9718, loss_val: nan, pos_over_neg: 1426.5048828125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.9881, loss_val: nan, pos_over_neg: 746.3258666992188 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.9592, loss_val: nan, pos_over_neg: 489.2165222167969 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.9866, loss_val: nan, pos_over_neg: 178.40525817871094 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.9723, loss_val: nan, pos_over_neg: 268.8999328613281 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.9815, loss_val: nan, pos_over_neg: 505.87457275390625 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.972, loss_val: nan, pos_over_neg: 1147.2213134765625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 826.0988159179688 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.9686, loss_val: nan, pos_over_neg: 1448.1021728515625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.9628, loss_val: nan, pos_over_neg: 1361.262939453125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.9758, loss_val: nan, pos_over_neg: 1960.646728515625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.9554, loss_val: nan, pos_over_neg: 509.6680908203125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 473.6911926269531 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.9738, loss_val: nan, pos_over_neg: 211.9881591796875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.9526, loss_val: nan, pos_over_neg: 672.2615966796875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.9772, loss_val: nan, pos_over_neg: -14315.3359375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 6.0019, loss_val: nan, pos_over_neg: 398.5762939453125 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.981, loss_val: nan, pos_over_neg: 370.3598937988281 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.9819, loss_val: nan, pos_over_neg: 306.5924072265625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.9803, loss_val: nan, pos_over_neg: 420.14599609375 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.9742, loss_val: nan, pos_over_neg: 4016.10888671875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.976, loss_val: nan, pos_over_neg: 305.87335205078125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.9725, loss_val: nan, pos_over_neg: 334.7818298339844 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.9788, loss_val: nan, pos_over_neg: 698.751953125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 257.2479553222656 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.9779, loss_val: nan, pos_over_neg: 245.78921508789062 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.973, loss_val: nan, pos_over_neg: 378.21002197265625 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.9984, loss_val: nan, pos_over_neg: 286.1874084472656 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.9756, loss_val: nan, pos_over_neg: 536.7828369140625 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.9758, loss_val: nan, pos_over_neg: 294.1189880371094 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.9633, loss_val: nan, pos_over_neg: 3187.660400390625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.956, loss_val: nan, pos_over_neg: 958.70849609375 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.9843, loss_val: nan, pos_over_neg: 725.524169921875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.9847, loss_val: nan, pos_over_neg: 233.95433044433594 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.9769, loss_val: nan, pos_over_neg: 196.53610229492188 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.9869, loss_val: nan, pos_over_neg: 395.5359802246094 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.9772, loss_val: nan, pos_over_neg: 648.5795288085938 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.9899, loss_val: nan, pos_over_neg: 1624.5760498046875 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.969, loss_val: nan, pos_over_neg: 875.0302124023438 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.9799, loss_val: nan, pos_over_neg: -32054.265625 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 1028.510498046875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.9717, loss_val: nan, pos_over_neg: 222.69606018066406 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.9839, loss_val: nan, pos_over_neg: 191.39210510253906 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.9843, loss_val: nan, pos_over_neg: 413.8588562011719 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.9561, loss_val: nan, pos_over_neg: 1313.13134765625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.9716, loss_val: nan, pos_over_neg: 3854.513671875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.9577, loss_val: nan, pos_over_neg: 3896.53076171875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.9877, loss_val: nan, pos_over_neg: 342.9325256347656 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.9803, loss_val: nan, pos_over_neg: 631.974853515625 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.9504, loss_val: nan, pos_over_neg: 543.4007568359375 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.9816, loss_val: nan, pos_over_neg: 248.52639770507812 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.9637, loss_val: nan, pos_over_neg: 408.4450988769531 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.9954, loss_val: nan, pos_over_neg: 449.8481750488281 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.9754, loss_val: nan, pos_over_neg: 1422.9212646484375 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.9878, loss_val: nan, pos_over_neg: 794.1521606445312 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 606.2186889648438 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.9358, loss_val: nan, pos_over_neg: 354.6705017089844 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 229.8301544189453 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.9695, loss_val: nan, pos_over_neg: 156.2989044189453 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 449.3573913574219 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.944, loss_val: nan, pos_over_neg: 3996.75439453125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.9355, loss_val: nan, pos_over_neg: 678.0845336914062 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.9544, loss_val: nan, pos_over_neg: 1417.15576171875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.9677, loss_val: nan, pos_over_neg: 222.07052612304688 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.9597, loss_val: nan, pos_over_neg: 603.0778198242188 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.9616, loss_val: nan, pos_over_neg: 265.3634948730469 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.9824, loss_val: nan, pos_over_neg: 347.11431884765625 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.984, loss_val: nan, pos_over_neg: 370.1916809082031 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.9763, loss_val: nan, pos_over_neg: 576.8291015625 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.9403, loss_val: nan, pos_over_neg: -3547.289306640625 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.9751, loss_val: nan, pos_over_neg: 485.4084167480469 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.9615, loss_val: nan, pos_over_neg: 320.4238586425781 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.9596, loss_val: nan, pos_over_neg: 634.4950561523438 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.9431, loss_val: nan, pos_over_neg: 519.5990600585938 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.9455, loss_val: nan, pos_over_neg: 465.63885498046875 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.9646, loss_val: nan, pos_over_neg: 833.5431518554688 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.9486, loss_val: nan, pos_over_neg: 985.4978637695312 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.9747, loss_val: nan, pos_over_neg: 898.3540649414062 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.9773, loss_val: nan, pos_over_neg: 404.51165771484375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.9398, loss_val: nan, pos_over_neg: 441.4505920410156 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.9564, loss_val: nan, pos_over_neg: 577.0260620117188 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.987, loss_val: nan, pos_over_neg: 254.43324279785156 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.9922, loss_val: nan, pos_over_neg: 182.5548553466797 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.9605, loss_val: nan, pos_over_neg: 320.8453674316406 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.9583, loss_val: nan, pos_over_neg: 630.9618530273438 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.9659, loss_val: nan, pos_over_neg: 316.8423767089844 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.9913, loss_val: nan, pos_over_neg: 2148.609130859375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.947, loss_val: nan, pos_over_neg: 971.89453125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.9733, loss_val: nan, pos_over_neg: 424.2788391113281 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.9661, loss_val: nan, pos_over_neg: 708.4546508789062 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.9583, loss_val: nan, pos_over_neg: 615.819091796875 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.9684, loss_val: nan, pos_over_neg: 362.97283935546875 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.9563, loss_val: nan, pos_over_neg: 620.5018310546875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.9725, loss_val: nan, pos_over_neg: 399.40789794921875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.9577, loss_val: nan, pos_over_neg: 2310.072998046875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.974, loss_val: nan, pos_over_neg: 3406.220947265625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.9452, loss_val: nan, pos_over_neg: 1450.5042724609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.9649, loss_val: nan, pos_over_neg: 1022.3977661132812 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.9622, loss_val: nan, pos_over_neg: 313.35498046875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.9919, loss_val: nan, pos_over_neg: 201.4144287109375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.9743, loss_val: nan, pos_over_neg: 665.330810546875 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.9551, loss_val: nan, pos_over_neg: 593.214599609375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.9476, loss_val: nan, pos_over_neg: 637.4708862304688 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.9647, loss_val: nan, pos_over_neg: 1710.826416015625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.967, loss_val: nan, pos_over_neg: 2650.199462890625 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.9584, loss_val: nan, pos_over_neg: 777.09375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.9345, loss_val: nan, pos_over_neg: 1241.6331787109375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.9698, loss_val: nan, pos_over_neg: 760.531005859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.9749, loss_val: nan, pos_over_neg: 306.3925476074219 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.971, loss_val: nan, pos_over_neg: 320.2483825683594 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.9354, loss_val: nan, pos_over_neg: 329.2418518066406 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.9413, loss_val: nan, pos_over_neg: 364.5213928222656 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.9581, loss_val: nan, pos_over_neg: 397.1905212402344 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.9494, loss_val: nan, pos_over_neg: 1041.8492431640625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.9729, loss_val: nan, pos_over_neg: 503.7575378417969 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.9666, loss_val: nan, pos_over_neg: 172.5717315673828 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.9496, loss_val: nan, pos_over_neg: 767.2417602539062 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.9365, loss_val: nan, pos_over_neg: 1145.9832763671875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.9699, loss_val: nan, pos_over_neg: 350.16802978515625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.9604, loss_val: nan, pos_over_neg: 466.8750305175781 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.958, loss_val: nan, pos_over_neg: 287.72735595703125 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.9587, loss_val: nan, pos_over_neg: 860.3345947265625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.9588, loss_val: nan, pos_over_neg: 2023.935546875 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.975, loss_val: nan, pos_over_neg: 472.71099853515625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.9501, loss_val: nan, pos_over_neg: 457.0325012207031 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.9362, loss_val: nan, pos_over_neg: 508.1440124511719 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.9634, loss_val: nan, pos_over_neg: 255.64297485351562 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.968, loss_val: nan, pos_over_neg: 329.9924011230469 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.9542, loss_val: nan, pos_over_neg: 266.3347473144531 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.9636, loss_val: nan, pos_over_neg: 251.2538299560547 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.9784, loss_val: nan, pos_over_neg: 427.4071044921875 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.9859, loss_val: nan, pos_over_neg: 801.3709716796875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.9619, loss_val: nan, pos_over_neg: 509.20819091796875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.9865, loss_val: nan, pos_over_neg: 332.13201904296875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.9719, loss_val: nan, pos_over_neg: 357.98358154296875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.9526, loss_val: nan, pos_over_neg: 263.75421142578125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.965, loss_val: nan, pos_over_neg: 196.659912109375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.9585, loss_val: nan, pos_over_neg: 571.0667724609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.9468, loss_val: nan, pos_over_neg: 722.080322265625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.9482, loss_val: nan, pos_over_neg: 412.7255554199219 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: -73472.7265625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.9608, loss_val: nan, pos_over_neg: 1812.963623046875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.9417, loss_val: nan, pos_over_neg: 707.7296752929688 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 515.2927856445312 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.9543, loss_val: nan, pos_over_neg: 464.07476806640625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 344.91754150390625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.9594, loss_val: nan, pos_over_neg: 363.43267822265625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.9707, loss_val: nan, pos_over_neg: 403.4909973144531 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.9435, loss_val: nan, pos_over_neg: 424.0624084472656 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.9624, loss_val: nan, pos_over_neg: 40080.60546875 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.9556, loss_val: nan, pos_over_neg: 2189.838623046875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.9517, loss_val: nan, pos_over_neg: 1710.2567138671875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.9503, loss_val: nan, pos_over_neg: 625.9295043945312 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.9515, loss_val: nan, pos_over_neg: 564.5918579101562 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 477.0509338378906 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.9501, loss_val: nan, pos_over_neg: 257.14959716796875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.9669, loss_val: nan, pos_over_neg: 528.7546997070312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.9806, loss_val: nan, pos_over_neg: 574.2048950195312 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.9459, loss_val: nan, pos_over_neg: 1559.72705078125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.9449, loss_val: nan, pos_over_neg: 507.4721984863281 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.9592, loss_val: nan, pos_over_neg: 417.0815734863281 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 394.46917724609375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 2654.621337890625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.9477, loss_val: nan, pos_over_neg: 2310.497802734375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.9361, loss_val: nan, pos_over_neg: 12143.54296875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.953, loss_val: nan, pos_over_neg: 1002.6043090820312 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.9486, loss_val: nan, pos_over_neg: 4368.833984375 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.9617, loss_val: nan, pos_over_neg: 290.7593994140625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.9617, loss_val: nan, pos_over_neg: 495.2429504394531 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.9354, loss_val: nan, pos_over_neg: 561.4925537109375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.9686, loss_val: nan, pos_over_neg: 543.8638305664062 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.9581, loss_val: nan, pos_over_neg: 580.509521484375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.9644, loss_val: nan, pos_over_neg: 563.949462890625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.9521, loss_val: nan, pos_over_neg: 3723.449951171875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.948, loss_val: nan, pos_over_neg: 1515.534912109375 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.9606, loss_val: nan, pos_over_neg: 364.584716796875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.9597, loss_val: nan, pos_over_neg: 487.6386413574219 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.9733, loss_val: nan, pos_over_neg: 249.94363403320312 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.9314, loss_val: nan, pos_over_neg: 1337.097412109375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 1964.9063720703125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.9474, loss_val: nan, pos_over_neg: 1135.0655517578125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 1501.4339599609375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.9655, loss_val: nan, pos_over_neg: 880.9076538085938 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.9472, loss_val: nan, pos_over_neg: 339.7194519042969 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.9809, loss_val: nan, pos_over_neg: 318.3359375 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 620.979736328125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 1476.41748046875 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.9455, loss_val: nan, pos_over_neg: 8080.62353515625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.9441, loss_val: nan, pos_over_neg: 1189.607421875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.9503, loss_val: nan, pos_over_neg: -96353.1796875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.9626, loss_val: nan, pos_over_neg: 5356.9345703125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.9851, loss_val: nan, pos_over_neg: 207.588623046875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.9666, loss_val: nan, pos_over_neg: 185.9300994873047 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.9357, loss_val: nan, pos_over_neg: 548.4971923828125 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.9397, loss_val: nan, pos_over_neg: 628.1309204101562 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 437.188720703125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 1012.8915405273438 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.9385, loss_val: nan, pos_over_neg: 1508.66943359375 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.9682, loss_val: nan, pos_over_neg: 707.5682373046875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.9514, loss_val: nan, pos_over_neg: 2308.309814453125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.9552, loss_val: nan, pos_over_neg: 646.00927734375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.9484, loss_val: nan, pos_over_neg: 264.76849365234375 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.9734, loss_val: nan, pos_over_neg: 225.53656005859375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.9534, loss_val: nan, pos_over_neg: 336.7052917480469 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.9545, loss_val: nan, pos_over_neg: 451.4533386230469 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.9529, loss_val: nan, pos_over_neg: 1289.8048095703125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.9826, loss_val: nan, pos_over_neg: 391.23565673828125 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.9428, loss_val: nan, pos_over_neg: 755.0454711914062 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.9838, loss_val: nan, pos_over_neg: 214.05239868164062 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.9376, loss_val: nan, pos_over_neg: 233.96868896484375 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.9555, loss_val: nan, pos_over_neg: 281.789794921875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.9342, loss_val: nan, pos_over_neg: 351.9046630859375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.945, loss_val: nan, pos_over_neg: 444.179931640625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 735.59423828125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 246.84422302246094 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.9492, loss_val: nan, pos_over_neg: 442.0066833496094 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.9337, loss_val: nan, pos_over_neg: 600.1708374023438 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.9505, loss_val: nan, pos_over_neg: 269.0769348144531 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.945, loss_val: nan, pos_over_neg: 387.5591735839844 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.9487, loss_val: nan, pos_over_neg: 639.8821411132812 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.9511, loss_val: nan, pos_over_neg: 557.4862670898438 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.949, loss_val: nan, pos_over_neg: 620.270263671875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.9595, loss_val: nan, pos_over_neg: -12579.5341796875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.9239, loss_val: nan, pos_over_neg: 3054.812255859375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.9499, loss_val: nan, pos_over_neg: 602.6878051757812 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 819.5792846679688 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: 2057.0302734375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.9458, loss_val: nan, pos_over_neg: 376.9891662597656 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 658.758056640625 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.9218, loss_val: nan, pos_over_neg: 292.41094970703125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.9532, loss_val: nan, pos_over_neg: 402.3589782714844 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.9448, loss_val: nan, pos_over_neg: 536.2177124023438 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.935, loss_val: nan, pos_over_neg: 2286.681640625 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.9174, loss_val: nan, pos_over_neg: 716.9614868164062 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.9403, loss_val: nan, pos_over_neg: 1432.234375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.9296, loss_val: nan, pos_over_neg: 1960.6727294921875 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.9451, loss_val: nan, pos_over_neg: 421.1434631347656 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.932, loss_val: nan, pos_over_neg: 262.03729248046875 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.9401, loss_val: nan, pos_over_neg: 237.3811492919922 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 310.3262634277344 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.9392, loss_val: nan, pos_over_neg: 660.9075317382812 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.9653, loss_val: nan, pos_over_neg: 1733.354248046875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.9292, loss_val: nan, pos_over_neg: 539.4072875976562 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.9601, loss_val: nan, pos_over_neg: 172.11282348632812 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.9712, loss_val: nan, pos_over_neg: 229.63034057617188 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.9291, loss_val: nan, pos_over_neg: 434.18426513671875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.9447, loss_val: nan, pos_over_neg: 328.008056640625 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.9444, loss_val: nan, pos_over_neg: 256.05792236328125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.9517, loss_val: nan, pos_over_neg: 543.5482788085938 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.9348, loss_val: nan, pos_over_neg: 315.7367858886719 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.9644, loss_val: nan, pos_over_neg: 968.1500244140625 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.9645, loss_val: nan, pos_over_neg: 188.84832763671875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.9715, loss_val: nan, pos_over_neg: 670.6665649414062 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 461.7056884765625 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.9479, loss_val: nan, pos_over_neg: 481.9519348144531 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.9402, loss_val: nan, pos_over_neg: 605.6566162109375 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.9369, loss_val: nan, pos_over_neg: 395.59332275390625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.9447, loss_val: nan, pos_over_neg: 712.3467407226562 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.9525, loss_val: nan, pos_over_neg: 644.8046875 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 471.3186340332031 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.9287, loss_val: nan, pos_over_neg: 288.3128967285156 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.9584, loss_val: nan, pos_over_neg: 233.4298095703125 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.9408, loss_val: nan, pos_over_neg: 531.8684692382812 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.9173, loss_val: nan, pos_over_neg: 589.423583984375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.9565, loss_val: nan, pos_over_neg: 668.7005004882812 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.9328, loss_val: nan, pos_over_neg: 838.3190307617188 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.9445, loss_val: nan, pos_over_neg: 1047.892333984375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.9623, loss_val: nan, pos_over_neg: 216.04815673828125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.9393, loss_val: nan, pos_over_neg: 475.6846008300781 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.939, loss_val: nan, pos_over_neg: 439.0572814941406 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.934, loss_val: nan, pos_over_neg: 410.89947509765625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.9563, loss_val: nan, pos_over_neg: 696.6547241210938 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.9409, loss_val: nan, pos_over_neg: 15494.80078125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: 436.80267333984375 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.9311, loss_val: nan, pos_over_neg: 317.0371398925781 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.9312, loss_val: nan, pos_over_neg: 745.5546875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 657.3705444335938 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 652.5260009765625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.9518, loss_val: nan, pos_over_neg: 1071.5513916015625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.9364, loss_val: nan, pos_over_neg: 1638.923583984375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: -5159.50927734375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 431.57452392578125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.9488, loss_val: nan, pos_over_neg: 849.8182373046875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.9488, loss_val: nan, pos_over_neg: 868.846923828125 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 384.78228759765625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.9603, loss_val: nan, pos_over_neg: 278.7498474121094 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.9353, loss_val: nan, pos_over_neg: 797.469482421875 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.9336, loss_val: nan, pos_over_neg: 1205.3511962890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.9493, loss_val: nan, pos_over_neg: 281.2925109863281 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.9475, loss_val: nan, pos_over_neg: 379.2812805175781 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.9428, loss_val: nan, pos_over_neg: 895.4895629882812 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.908, loss_val: nan, pos_over_neg: 482.017822265625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.9148, loss_val: nan, pos_over_neg: 311.84002685546875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.9499, loss_val: nan, pos_over_neg: 307.79266357421875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.955, loss_val: nan, pos_over_neg: 455.188232421875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.9398, loss_val: nan, pos_over_neg: -369848.3125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 5292.7890625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.9547, loss_val: nan, pos_over_neg: 1357.6771240234375 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.942, loss_val: nan, pos_over_neg: 2766.57666015625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 816.49609375 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.9273, loss_val: nan, pos_over_neg: 539.4733276367188 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.961, loss_val: nan, pos_over_neg: 163.15707397460938 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.9566, loss_val: nan, pos_over_neg: 232.56114196777344 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 1083.809814453125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.9346, loss_val: nan, pos_over_neg: 2254.384521484375 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.9485, loss_val: nan, pos_over_neg: 973.31494140625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 526.4390869140625 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.9456, loss_val: nan, pos_over_neg: 346.5412902832031 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.9254, loss_val: nan, pos_over_neg: 769.4743041992188 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.9369, loss_val: nan, pos_over_neg: 534.52587890625 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: -5804.0556640625 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.9439, loss_val: nan, pos_over_neg: 1032.957275390625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 379.6251220703125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.9467, loss_val: nan, pos_over_neg: 633.7762451171875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.9513, loss_val: nan, pos_over_neg: 277.4427185058594 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 2757.127685546875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.9451, loss_val: nan, pos_over_neg: 605.3677368164062 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.9527, loss_val: nan, pos_over_neg: 920.5164794921875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.9413, loss_val: nan, pos_over_neg: 426.78387451171875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.9188, loss_val: nan, pos_over_neg: 2337.00390625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.9111, loss_val: nan, pos_over_neg: 578.88525390625 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.9468, loss_val: nan, pos_over_neg: 694.9198608398438 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.9265, loss_val: nan, pos_over_neg: 381.28387451171875 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.934, loss_val: nan, pos_over_neg: 581.3336181640625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.9285, loss_val: nan, pos_over_neg: 990.1636352539062 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.9248, loss_val: nan, pos_over_neg: 1203.2001953125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.9257, loss_val: nan, pos_over_neg: 2354.39697265625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: -140202.40625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.9419, loss_val: nan, pos_over_neg: 945.7930297851562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 979.222900390625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 859.8779907226562 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.9319, loss_val: nan, pos_over_neg: 414.7882385253906 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.9219, loss_val: nan, pos_over_neg: 462.45562744140625 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.9446, loss_val: nan, pos_over_neg: 622.695068359375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.9357, loss_val: nan, pos_over_neg: 683.0396728515625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 34126.95703125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.9556, loss_val: nan, pos_over_neg: 1697.1766357421875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.9213, loss_val: nan, pos_over_neg: 2470.022705078125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.9406, loss_val: nan, pos_over_neg: 1285.5323486328125 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.9456, loss_val: nan, pos_over_neg: 310.6751403808594 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.9431, loss_val: nan, pos_over_neg: 561.8411254882812 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.9383, loss_val: nan, pos_over_neg: 902.3755493164062 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.9328, loss_val: nan, pos_over_neg: 302.3341979980469 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 343.3909606933594 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.9444, loss_val: nan, pos_over_neg: 718.5056762695312 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.9246, loss_val: nan, pos_over_neg: 3088.381103515625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.9355, loss_val: nan, pos_over_neg: 1511.33935546875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.9353, loss_val: nan, pos_over_neg: 281.54998779296875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 266.06707763671875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.9283, loss_val: nan, pos_over_neg: 536.0421752929688 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.938, loss_val: nan, pos_over_neg: 309.01861572265625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.9394, loss_val: nan, pos_over_neg: 546.9005737304688 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.9366, loss_val: nan, pos_over_neg: 422.80517578125 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.9309, loss_val: nan, pos_over_neg: 1195.3099365234375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.9164, loss_val: nan, pos_over_neg: 1058.77001953125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300000 [25:06<125525:33:55, 1506.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Iter: 0/889, loss_train: 5.9379, loss_val: nan, pos_over_neg: 836.7720947265625 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.9342, loss_val: nan, pos_over_neg: 302.8044738769531 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.9302, loss_val: nan, pos_over_neg: 2447.04052734375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.9191, loss_val: nan, pos_over_neg: 1052.4263916015625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.9281, loss_val: nan, pos_over_neg: 342.2099609375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 391.80157470703125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.9256, loss_val: nan, pos_over_neg: 349.9445495605469 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 673.9456176757812 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.9228, loss_val: nan, pos_over_neg: 675.8674926757812 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: 692.7626953125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 328.53765869140625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.9349, loss_val: nan, pos_over_neg: 524.168701171875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 642.7512817382812 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.9287, loss_val: nan, pos_over_neg: 391.97607421875 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.9409, loss_val: nan, pos_over_neg: 1047.9857177734375 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.9155, loss_val: nan, pos_over_neg: 716.3146362304688 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.9434, loss_val: nan, pos_over_neg: 585.4293823242188 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.9167, loss_val: nan, pos_over_neg: 804.0676879882812 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.9321, loss_val: nan, pos_over_neg: 439.32403564453125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.9157, loss_val: nan, pos_over_neg: 1403.9527587890625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.9283, loss_val: nan, pos_over_neg: 6646.0654296875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.928, loss_val: nan, pos_over_neg: 1356.577392578125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.9222, loss_val: nan, pos_over_neg: 867.9805297851562 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.9224, loss_val: nan, pos_over_neg: 382.5021667480469 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.9397, loss_val: nan, pos_over_neg: 643.0014038085938 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 852.8610229492188 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.9387, loss_val: nan, pos_over_neg: 708.6339721679688 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.9396, loss_val: nan, pos_over_neg: 519.7952880859375 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.9259, loss_val: nan, pos_over_neg: 657.2423706054688 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.9419, loss_val: nan, pos_over_neg: 15739.5419921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.9452, loss_val: nan, pos_over_neg: 434.9681091308594 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.9436, loss_val: nan, pos_over_neg: 896.3618774414062 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.9219, loss_val: nan, pos_over_neg: 514.2257080078125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.9448, loss_val: nan, pos_over_neg: 276.66357421875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 1417.314697265625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 922.880615234375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.9381, loss_val: nan, pos_over_neg: 480.6160888671875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 2447.803955078125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 616.6158447265625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.9363, loss_val: nan, pos_over_neg: 1373.2706298828125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.9386, loss_val: nan, pos_over_neg: 1007.4410400390625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.9212, loss_val: nan, pos_over_neg: 1002.2719116210938 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 573.6642456054688 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.925, loss_val: nan, pos_over_neg: 350.3515625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.9302, loss_val: nan, pos_over_neg: 587.5128784179688 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.9318, loss_val: nan, pos_over_neg: 529.5972900390625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.9286, loss_val: nan, pos_over_neg: 465.07537841796875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.9252, loss_val: nan, pos_over_neg: 347.78741455078125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.9471, loss_val: nan, pos_over_neg: 312.86993408203125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.9228, loss_val: nan, pos_over_neg: 1922.1448974609375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.9254, loss_val: nan, pos_over_neg: 961.8570556640625 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.9396, loss_val: nan, pos_over_neg: 736.1995239257812 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.9405, loss_val: nan, pos_over_neg: 778.7427978515625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: 656.5799560546875 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 568.0379638671875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.906, loss_val: nan, pos_over_neg: 265.7946472167969 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 851.4569702148438 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 635.6089477539062 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.9375, loss_val: nan, pos_over_neg: 370.6193542480469 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.9023, loss_val: nan, pos_over_neg: 449.1189270019531 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 338.3833923339844 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 488.46832275390625 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.9382, loss_val: nan, pos_over_neg: 440.984130859375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 481.4092712402344 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.9325, loss_val: nan, pos_over_neg: 397.1817626953125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.9469, loss_val: nan, pos_over_neg: 348.9867858886719 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: 748.935791015625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 602.3949584960938 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.9197, loss_val: nan, pos_over_neg: 503.21563720703125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.93, loss_val: nan, pos_over_neg: 362.3328857421875 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.9199, loss_val: nan, pos_over_neg: -8576.021484375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 678.69775390625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 628.1722412109375 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.9358, loss_val: nan, pos_over_neg: 434.1410827636719 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.9271, loss_val: nan, pos_over_neg: 534.5394897460938 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.9185, loss_val: nan, pos_over_neg: 625.1991577148438 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.9138, loss_val: nan, pos_over_neg: 678.7293701171875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.9227, loss_val: nan, pos_over_neg: 369.8231506347656 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.9373, loss_val: nan, pos_over_neg: 990.7454833984375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1818.6282958984375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.9305, loss_val: nan, pos_over_neg: 474.88446044921875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 446.0697326660156 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.9399, loss_val: nan, pos_over_neg: 1164.074951171875 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.9292, loss_val: nan, pos_over_neg: 1810.885986328125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.9262, loss_val: nan, pos_over_neg: 1182.030029296875 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 855.3295288085938 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.9185, loss_val: nan, pos_over_neg: 14242.3720703125 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 16662.546875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.9183, loss_val: nan, pos_over_neg: 236.33984375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 507.2308044433594 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 528.10888671875 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.9298, loss_val: nan, pos_over_neg: 459.5431213378906 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.9058, loss_val: nan, pos_over_neg: 483.2398681640625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.9284, loss_val: nan, pos_over_neg: 258.1907043457031 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 381.2937316894531 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.9307, loss_val: nan, pos_over_neg: 498.35235595703125 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.9532, loss_val: nan, pos_over_neg: 246.59312438964844 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 265.37188720703125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 1099.0328369140625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.9188, loss_val: nan, pos_over_neg: 430.0424499511719 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8999, loss_val: nan, pos_over_neg: 454.2769470214844 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 750.8400268554688 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.9265, loss_val: nan, pos_over_neg: 242.1654510498047 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.9274, loss_val: nan, pos_over_neg: 573.848388671875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 386.7127685546875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 276.1266174316406 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.9333, loss_val: nan, pos_over_neg: 570.3038330078125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 234.22250366210938 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 221.9892120361328 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.9171, loss_val: nan, pos_over_neg: 301.6824951171875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.9224, loss_val: nan, pos_over_neg: 308.85809326171875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.9303, loss_val: nan, pos_over_neg: 485.88543701171875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 2584.116455078125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 550.90234375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.9179, loss_val: nan, pos_over_neg: 488.98443603515625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.93, loss_val: nan, pos_over_neg: 724.1250610351562 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.936, loss_val: nan, pos_over_neg: 366.17291259765625 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 732.6065063476562 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.9294, loss_val: nan, pos_over_neg: 305.2129821777344 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 846.3099365234375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 464.6542053222656 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 2730.417236328125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.9246, loss_val: nan, pos_over_neg: 348.8503112792969 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 3450.462158203125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.9317, loss_val: nan, pos_over_neg: 2321.423583984375 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.9032, loss_val: nan, pos_over_neg: 3584.247802734375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.9073, loss_val: nan, pos_over_neg: 757.8285522460938 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 928.8526611328125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 653.1839599609375 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.9352, loss_val: nan, pos_over_neg: 265.913330078125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.9022, loss_val: nan, pos_over_neg: 643.4158935546875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.9231, loss_val: nan, pos_over_neg: 555.0843505859375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 909.6328735351562 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.9346, loss_val: nan, pos_over_neg: 257.1886901855469 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.9127, loss_val: nan, pos_over_neg: 3040.913330078125 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.9204, loss_val: nan, pos_over_neg: -36498.01953125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.916, loss_val: nan, pos_over_neg: 1076.2364501953125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 3265.083251953125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 435.6319274902344 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1012.1414794921875 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 527.3817749023438 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.916, loss_val: nan, pos_over_neg: 753.5291137695312 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.9325, loss_val: nan, pos_over_neg: 372.06884765625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 6173.318359375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.9167, loss_val: nan, pos_over_neg: 4062.07958984375 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.9266, loss_val: nan, pos_over_neg: 880.4725341796875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 428.5052490234375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 3362.160888671875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.9099, loss_val: nan, pos_over_neg: 341.9124755859375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 657.66943359375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 587.4418334960938 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.9127, loss_val: nan, pos_over_neg: 464.57061767578125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.915, loss_val: nan, pos_over_neg: 1386.825927734375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.9215, loss_val: nan, pos_over_neg: 838.4158325195312 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.9268, loss_val: nan, pos_over_neg: 907.239013671875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 266.9720764160156 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.9009, loss_val: nan, pos_over_neg: 1685.8013916015625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.9147, loss_val: nan, pos_over_neg: 553.1577758789062 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 2369.632568359375 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.9317, loss_val: nan, pos_over_neg: 1993.2061767578125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.9323, loss_val: nan, pos_over_neg: 1227.0330810546875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.9071, loss_val: nan, pos_over_neg: 800.1154174804688 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: 2332.80859375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 3327.6591796875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.926, loss_val: nan, pos_over_neg: 370.9113464355469 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 728.7630615234375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.9296, loss_val: nan, pos_over_neg: 603.7725830078125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.9088, loss_val: nan, pos_over_neg: 350.8837585449219 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.9264, loss_val: nan, pos_over_neg: 1959.49658203125 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.9001, loss_val: nan, pos_over_neg: 700.6461181640625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.9407, loss_val: nan, pos_over_neg: 824.3500366210938 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 1264.021484375 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.9213, loss_val: nan, pos_over_neg: 703.3492431640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8974, loss_val: nan, pos_over_neg: 698.569091796875 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.9186, loss_val: nan, pos_over_neg: 243.4152374267578 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 297.2059020996094 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.9315, loss_val: nan, pos_over_neg: 1432.5318603515625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.9193, loss_val: nan, pos_over_neg: 597.5227661132812 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: -4245.95947265625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 2991.484375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.9175, loss_val: nan, pos_over_neg: 581.7600708007812 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.9161, loss_val: nan, pos_over_neg: 332.427490234375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 266.0779724121094 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.9009, loss_val: nan, pos_over_neg: 318.3259582519531 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.9269, loss_val: nan, pos_over_neg: 572.08935546875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 1301.0682373046875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.9043, loss_val: nan, pos_over_neg: 608.990478515625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.9016, loss_val: nan, pos_over_neg: 5503.77392578125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.922, loss_val: nan, pos_over_neg: 713.4677124023438 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 1007.9620361328125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.9076, loss_val: nan, pos_over_neg: 581.1928100585938 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.9076, loss_val: nan, pos_over_neg: 1062.998779296875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.9282, loss_val: nan, pos_over_neg: 351.93170166015625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.9217, loss_val: nan, pos_over_neg: 2362.716552734375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.9395, loss_val: nan, pos_over_neg: 830.4714965820312 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 1130.898193359375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8994, loss_val: nan, pos_over_neg: 791.8644409179688 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 575.8211059570312 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8985, loss_val: nan, pos_over_neg: 563.4288330078125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.9215, loss_val: nan, pos_over_neg: 481.2333984375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 301.97723388671875 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 563.1827392578125 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.9137, loss_val: nan, pos_over_neg: 859.6358642578125 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.9097, loss_val: nan, pos_over_neg: 531.98388671875 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.9253, loss_val: nan, pos_over_neg: 1310.9434814453125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.9202, loss_val: nan, pos_over_neg: 451.3404846191406 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.9129, loss_val: nan, pos_over_neg: 300.8221435546875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.9203, loss_val: nan, pos_over_neg: 294.6104736328125 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 599.0208740234375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.9177, loss_val: nan, pos_over_neg: 526.5089111328125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.9172, loss_val: nan, pos_over_neg: 288.1439208984375 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 582.3744506835938 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1135.7994384765625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1162.361328125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 388.9736328125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.9082, loss_val: nan, pos_over_neg: 333.5173034667969 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8989, loss_val: nan, pos_over_neg: 668.428955078125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 1140.5013427734375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.9225, loss_val: nan, pos_over_neg: 441.86273193359375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.9005, loss_val: nan, pos_over_neg: 298.1125793457031 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.9166, loss_val: nan, pos_over_neg: 368.89013671875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 427.68310546875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.9083, loss_val: nan, pos_over_neg: 433.9444885253906 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.9088, loss_val: nan, pos_over_neg: 465.1457824707031 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 416.1618347167969 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.9184, loss_val: nan, pos_over_neg: 167.95101928710938 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.9094, loss_val: nan, pos_over_neg: 805.5147705078125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.9134, loss_val: nan, pos_over_neg: 724.4593505859375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8962, loss_val: nan, pos_over_neg: 503.0556945800781 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.9073, loss_val: nan, pos_over_neg: 648.3631591796875 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: -13926.732421875 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.9343, loss_val: nan, pos_over_neg: 361.7447204589844 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.9303, loss_val: nan, pos_over_neg: 250.5220947265625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8893, loss_val: nan, pos_over_neg: 759.2198486328125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.9426, loss_val: nan, pos_over_neg: 259.2240295410156 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 472.1707458496094 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.9148, loss_val: nan, pos_over_neg: 452.5403137207031 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.9141, loss_val: nan, pos_over_neg: 902.1175537109375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.924, loss_val: nan, pos_over_neg: 762.87255859375 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.9048, loss_val: nan, pos_over_neg: 789.8384399414062 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.9022, loss_val: nan, pos_over_neg: 455.28948974609375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.909, loss_val: nan, pos_over_neg: 300.75006103515625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.9243, loss_val: nan, pos_over_neg: 298.64666748046875 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.9145, loss_val: nan, pos_over_neg: 653.007568359375 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.9251, loss_val: nan, pos_over_neg: 341.9557800292969 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.9216, loss_val: nan, pos_over_neg: 1007.8546142578125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.924, loss_val: nan, pos_over_neg: 1989.4256591796875 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 803.5055541992188 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.9264, loss_val: nan, pos_over_neg: 622.5687255859375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 440.09857177734375 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 600.9822998046875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.9226, loss_val: nan, pos_over_neg: 355.56890869140625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.9153, loss_val: nan, pos_over_neg: 806.2166748046875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.903, loss_val: nan, pos_over_neg: 497.19970703125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.911, loss_val: nan, pos_over_neg: 494.87969970703125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.9115, loss_val: nan, pos_over_neg: 458.4769592285156 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.9131, loss_val: nan, pos_over_neg: 943.2338256835938 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.9187, loss_val: nan, pos_over_neg: 758.5281982421875 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.9233, loss_val: nan, pos_over_neg: 794.793212890625 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.9263, loss_val: nan, pos_over_neg: 364.1551818847656 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8872, loss_val: nan, pos_over_neg: 2993.190185546875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.9115, loss_val: nan, pos_over_neg: 705.3141479492188 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.9025, loss_val: nan, pos_over_neg: 384.8189392089844 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8927, loss_val: nan, pos_over_neg: 339.48638916015625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8879, loss_val: nan, pos_over_neg: 856.8070068359375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 2142.439453125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.9165, loss_val: nan, pos_over_neg: 369.1758728027344 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.891, loss_val: nan, pos_over_neg: 917.0775756835938 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.9102, loss_val: nan, pos_over_neg: 1852.9022216796875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.9131, loss_val: nan, pos_over_neg: 441.1456298828125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.9195, loss_val: nan, pos_over_neg: 378.017578125 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.9288, loss_val: nan, pos_over_neg: 349.5747375488281 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.9007, loss_val: nan, pos_over_neg: 1965.099365234375 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.9134, loss_val: nan, pos_over_neg: 482.1929016113281 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 642.3475341796875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.9024, loss_val: nan, pos_over_neg: 1247.8370361328125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 543.9309692382812 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1262.5943603515625 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 689.5642700195312 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.9204, loss_val: nan, pos_over_neg: 335.2655944824219 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.9139, loss_val: nan, pos_over_neg: 395.0549011230469 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.9058, loss_val: nan, pos_over_neg: 623.8757934570312 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.9015, loss_val: nan, pos_over_neg: 476.7777404785156 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 1595.641845703125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.9269, loss_val: nan, pos_over_neg: 246.2244873046875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 317.9728698730469 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.9105, loss_val: nan, pos_over_neg: 711.1519165039062 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 439.5408630371094 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.9089, loss_val: nan, pos_over_neg: 324.7552490234375 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 433.92620849609375 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.9123, loss_val: nan, pos_over_neg: 383.4340515136719 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 705.2322387695312 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8996, loss_val: nan, pos_over_neg: 1614.858154296875 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.9192, loss_val: nan, pos_over_neg: 662.3483276367188 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8943, loss_val: nan, pos_over_neg: 966.185791015625 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8965, loss_val: nan, pos_over_neg: 493.8673095703125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.9301, loss_val: nan, pos_over_neg: 345.3692932128906 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 212.27833557128906 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 459.727294921875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8972, loss_val: nan, pos_over_neg: 1488.4923095703125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.9235, loss_val: nan, pos_over_neg: -1805.9552001953125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.9001, loss_val: nan, pos_over_neg: 2488.4599609375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 1133.6903076171875 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 424.1064758300781 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.9307, loss_val: nan, pos_over_neg: 214.7949981689453 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 472.8448181152344 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 601.9361572265625 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8926, loss_val: nan, pos_over_neg: 617.9676513671875 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.912, loss_val: nan, pos_over_neg: 501.7853698730469 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.9025, loss_val: nan, pos_over_neg: -3926.016357421875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 408.6362609863281 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.9104, loss_val: nan, pos_over_neg: 368.0096130371094 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8983, loss_val: nan, pos_over_neg: 355.65966796875 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.9018, loss_val: nan, pos_over_neg: 3245.97216796875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.9162, loss_val: nan, pos_over_neg: 472.7054748535156 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.9155, loss_val: nan, pos_over_neg: 228.1803436279297 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: 368.1419982910156 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.9122, loss_val: nan, pos_over_neg: 315.02886962890625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 675.4744873046875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 764.31884765625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.9139, loss_val: nan, pos_over_neg: 264.2423400878906 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1647.3074951171875 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.9329, loss_val: nan, pos_over_neg: 443.44085693359375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.9236, loss_val: nan, pos_over_neg: 1314.28369140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.9347, loss_val: nan, pos_over_neg: 441.1961975097656 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.9061, loss_val: nan, pos_over_neg: 468.6865539550781 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 294.3618469238281 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 3025.01611328125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.913, loss_val: nan, pos_over_neg: 251.67755126953125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.9091, loss_val: nan, pos_over_neg: 289.49774169921875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.9114, loss_val: nan, pos_over_neg: 343.4283447265625 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.9093, loss_val: nan, pos_over_neg: 538.138427734375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.9107, loss_val: nan, pos_over_neg: 665.6597290039062 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8906, loss_val: nan, pos_over_neg: 3084.208251953125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 3777.653564453125 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.921, loss_val: nan, pos_over_neg: 790.4403076171875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.9169, loss_val: nan, pos_over_neg: 1091.9410400390625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.902, loss_val: nan, pos_over_neg: 1200.080322265625 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 909.4475708007812 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.9171, loss_val: nan, pos_over_neg: 214.6783905029297 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.9078, loss_val: nan, pos_over_neg: 530.8419189453125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.923, loss_val: nan, pos_over_neg: 250.23513793945312 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.9096, loss_val: nan, pos_over_neg: 333.6680603027344 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.9031, loss_val: nan, pos_over_neg: 651.7206420898438 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.9011, loss_val: nan, pos_over_neg: 1214.919677734375 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.9259, loss_val: nan, pos_over_neg: 542.5956420898438 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: 340.1953430175781 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8981, loss_val: nan, pos_over_neg: 459.4508361816406 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 611.3436889648438 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8994, loss_val: nan, pos_over_neg: 719.6273803710938 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 457.0719299316406 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8889, loss_val: nan, pos_over_neg: 1477.845947265625 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 281.46826171875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: 3291.576416015625 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.905, loss_val: nan, pos_over_neg: 1248.7017822265625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8948, loss_val: nan, pos_over_neg: 432.6325988769531 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 562.90478515625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8928, loss_val: nan, pos_over_neg: 365.4876403808594 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 462.6136779785156 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8965, loss_val: nan, pos_over_neg: 289.41998291015625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 5110.21630859375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 873.326171875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.9099, loss_val: nan, pos_over_neg: 445.5585632324219 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.9144, loss_val: nan, pos_over_neg: 1832.4661865234375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.9055, loss_val: nan, pos_over_neg: 357.7665100097656 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.9054, loss_val: nan, pos_over_neg: 451.162353515625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8917, loss_val: nan, pos_over_neg: 1179.101806640625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8934, loss_val: nan, pos_over_neg: 672.4605712890625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 649.1009521484375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.9044, loss_val: nan, pos_over_neg: 705.7579345703125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.9089, loss_val: nan, pos_over_neg: 469.4029541015625 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8896, loss_val: nan, pos_over_neg: 1788.9083251953125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8985, loss_val: nan, pos_over_neg: 637.358642578125 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 740.7902221679688 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.9014, loss_val: nan, pos_over_neg: 648.873779296875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1219.7626953125 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.9121, loss_val: nan, pos_over_neg: 348.27703857421875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.899, loss_val: nan, pos_over_neg: 586.7098999023438 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 688.9286499023438 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 473.3155212402344 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 407.1054992675781 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 842.805419921875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8894, loss_val: nan, pos_over_neg: 363.6236267089844 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 336.0245666503906 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.9306, loss_val: nan, pos_over_neg: 568.8128662109375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.904, loss_val: nan, pos_over_neg: 415.7455139160156 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8971, loss_val: nan, pos_over_neg: -7068.02587890625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 4348.74609375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 513.4353637695312 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.9074, loss_val: nan, pos_over_neg: 869.7265625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 2071.564697265625 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 479.8277282714844 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.9057, loss_val: nan, pos_over_neg: 299.1687316894531 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.9118, loss_val: nan, pos_over_neg: 408.4020690917969 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 890.6370849609375 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.9274, loss_val: nan, pos_over_neg: 253.2266845703125 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.9124, loss_val: nan, pos_over_neg: 362.6310729980469 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8997, loss_val: nan, pos_over_neg: 2986.9716796875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8806, loss_val: nan, pos_over_neg: 717.8519897460938 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 818.8295288085938 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.9081, loss_val: nan, pos_over_neg: 273.3731994628906 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.9065, loss_val: nan, pos_over_neg: 424.61810302734375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 1055.495361328125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.9133, loss_val: nan, pos_over_neg: 259.5829772949219 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: 406.7304992675781 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8971, loss_val: nan, pos_over_neg: 570.2938232421875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8885, loss_val: nan, pos_over_neg: 319.1236267089844 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.9098, loss_val: nan, pos_over_neg: 2183.00927734375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 2255.489013671875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.9074, loss_val: nan, pos_over_neg: 469.9479064941406 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8976, loss_val: nan, pos_over_neg: 266.5321350097656 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.919, loss_val: nan, pos_over_neg: 322.86474609375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8978, loss_val: nan, pos_over_neg: 544.290771484375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 832.5512084960938 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8909, loss_val: nan, pos_over_neg: 336.097412109375 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.9037, loss_val: nan, pos_over_neg: 2612.646484375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8947, loss_val: nan, pos_over_neg: 871.0689697265625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8942, loss_val: nan, pos_over_neg: 646.1151733398438 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 301.6468200683594 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.9146, loss_val: nan, pos_over_neg: 239.0704345703125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 450.3094482421875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8934, loss_val: nan, pos_over_neg: 785.4794921875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 811.147705078125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.9141, loss_val: nan, pos_over_neg: 349.08551025390625 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.9116, loss_val: nan, pos_over_neg: 1614.6036376953125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 685.7630615234375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 368.4942321777344 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 750.0967407226562 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.9015, loss_val: nan, pos_over_neg: 252.50306701660156 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 495.2933654785156 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.9084, loss_val: nan, pos_over_neg: 2630.3134765625 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1098.581787109375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1551.5762939453125 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8878, loss_val: nan, pos_over_neg: 288.0241394042969 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.914, loss_val: nan, pos_over_neg: 355.2582702636719 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.9049, loss_val: nan, pos_over_neg: 244.57888793945312 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.9211, loss_val: nan, pos_over_neg: 297.5679626464844 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 708.0805053710938 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8919, loss_val: nan, pos_over_neg: 865.46875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 260.3616638183594 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.9021, loss_val: nan, pos_over_neg: 846.401123046875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 720.33154296875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8945, loss_val: nan, pos_over_neg: 1048.3790283203125 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8975, loss_val: nan, pos_over_neg: 312.6240234375 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8938, loss_val: nan, pos_over_neg: 314.41290283203125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 521.9918823242188 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 588.972412109375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 2210.014404296875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8927, loss_val: nan, pos_over_neg: 579.4403686523438 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 834.2207641601562 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 2207.25634765625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 513.156494140625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 1269.5634765625 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 356.4333190917969 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.9179, loss_val: nan, pos_over_neg: 237.98411560058594 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.9154, loss_val: nan, pos_over_neg: 324.4672546386719 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8987, loss_val: nan, pos_over_neg: 480.8013000488281 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 1135.7733154296875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 433.6571350097656 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8723, loss_val: nan, pos_over_neg: 2438.426513671875 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.91, loss_val: nan, pos_over_neg: 283.6044006347656 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.9036, loss_val: nan, pos_over_neg: 264.21697998046875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.9059, loss_val: nan, pos_over_neg: 414.82379150390625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.9117, loss_val: nan, pos_over_neg: 608.3895874023438 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 3082.764892578125 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 833.8212890625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.9159, loss_val: nan, pos_over_neg: 385.780517578125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.9109, loss_val: nan, pos_over_neg: 1005.5333862304688 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 647.837890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.9138, loss_val: nan, pos_over_neg: 500.0395202636719 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8869, loss_val: nan, pos_over_neg: 458.4137268066406 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8921, loss_val: nan, pos_over_neg: 315.55706787109375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.904, loss_val: nan, pos_over_neg: 390.30303955078125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8989, loss_val: nan, pos_over_neg: 358.6590270996094 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 929.069580078125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 2693.594482421875 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.9023, loss_val: nan, pos_over_neg: 441.9612731933594 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.9059, loss_val: nan, pos_over_neg: 205.0383758544922 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.897, loss_val: nan, pos_over_neg: 403.0819396972656 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 594.8404541015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.9226, loss_val: nan, pos_over_neg: 446.7851867675781 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8817, loss_val: nan, pos_over_neg: 642.7399291992188 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.9046, loss_val: nan, pos_over_neg: 385.2395935058594 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 3116.760498046875 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.9046, loss_val: nan, pos_over_neg: 681.8577270507812 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 557.9224853515625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 815.8115234375 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8941, loss_val: nan, pos_over_neg: 1435.7008056640625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: -3572.52880859375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8986, loss_val: nan, pos_over_neg: 948.9005126953125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.908, loss_val: nan, pos_over_neg: 495.39227294921875 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 327.6160583496094 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 240.2061767578125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 1182.4661865234375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 393.3223876953125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.903, loss_val: nan, pos_over_neg: 473.62921142578125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 548.7803955078125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8903, loss_val: nan, pos_over_neg: 912.80712890625 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8865, loss_val: nan, pos_over_neg: 741.574462890625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 1045.1566162109375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.9072, loss_val: nan, pos_over_neg: 996.9354858398438 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8936, loss_val: nan, pos_over_neg: 714.3323364257812 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 4443.3203125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8881, loss_val: nan, pos_over_neg: 1672.697265625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: -4186.8828125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8992, loss_val: nan, pos_over_neg: 720.3392333984375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8942, loss_val: nan, pos_over_neg: 380.22381591796875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 304.23712158203125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 584.4807739257812 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.9053, loss_val: nan, pos_over_neg: 357.32904052734375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8892, loss_val: nan, pos_over_neg: 427.36572265625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 581.3705444335938 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8988, loss_val: nan, pos_over_neg: 775.3858032226562 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 3319.923828125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 786.3154907226562 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8834, loss_val: nan, pos_over_neg: 1110.768310546875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.9044, loss_val: nan, pos_over_neg: 559.1914672851562 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 1414.2291259765625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 622.9440307617188 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.9101, loss_val: nan, pos_over_neg: 371.6031494140625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8801, loss_val: nan, pos_over_neg: 2084.5986328125 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.9218, loss_val: nan, pos_over_neg: 539.111328125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 528.6455078125 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8982, loss_val: nan, pos_over_neg: 1113.7921142578125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 2485.893798828125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 2238.60107421875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.9002, loss_val: nan, pos_over_neg: -78314.8203125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 361.4979248046875 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8956, loss_val: nan, pos_over_neg: 1065.4598388671875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8958, loss_val: nan, pos_over_neg: 395.3732604980469 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 480.07452392578125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.887, loss_val: nan, pos_over_neg: 1355.6312255859375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1013.1441650390625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.9027, loss_val: nan, pos_over_neg: 631.55078125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8843, loss_val: nan, pos_over_neg: 2411.853271484375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.9052, loss_val: nan, pos_over_neg: 683.43017578125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 6179.0068359375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 1073.6466064453125 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 666.1695556640625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: -4215.62646484375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8894, loss_val: nan, pos_over_neg: 719.5487060546875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 1202.126220703125 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 835.7968139648438 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1273.21826171875 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8902, loss_val: nan, pos_over_neg: 834.9902954101562 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 1057.2138671875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 937.3350219726562 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 621.3346557617188 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.9019, loss_val: nan, pos_over_neg: 330.17047119140625 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 442.3929138183594 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.9137, loss_val: nan, pos_over_neg: 538.919677734375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 3538.275146484375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8827, loss_val: nan, pos_over_neg: 593.3997192382812 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8995, loss_val: nan, pos_over_neg: 389.2201843261719 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8833, loss_val: nan, pos_over_neg: 1455.8563232421875 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 1060.9171142578125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.9007, loss_val: nan, pos_over_neg: 456.54168701171875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 523.7120361328125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8979, loss_val: nan, pos_over_neg: 579.14208984375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 1767.9774169921875 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8975, loss_val: nan, pos_over_neg: 761.630859375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 316.27978515625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 407.7854919433594 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 9328.041015625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 1091.346923828125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 371.0654296875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 426.6227111816406 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 515.1836547851562 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.9085, loss_val: nan, pos_over_neg: 423.2566833496094 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 977.5902709960938 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 703.000244140625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 333.4101867675781 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 356.87066650390625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 7111.833984375 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8868, loss_val: nan, pos_over_neg: 656.5270385742188 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8813, loss_val: nan, pos_over_neg: 576.936767578125 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8885, loss_val: nan, pos_over_neg: 2044.09716796875 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8891, loss_val: nan, pos_over_neg: 545.208984375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 477.7451171875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 329.15045166015625 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: 603.1397705078125 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.9005, loss_val: nan, pos_over_neg: 340.0184631347656 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8762, loss_val: nan, pos_over_neg: 5127.49169921875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 464.58184814453125 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 15717.849609375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.9013, loss_val: nan, pos_over_neg: 596.1564331054688 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8906, loss_val: nan, pos_over_neg: 2840.1728515625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: -10099.3466796875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.9067, loss_val: nan, pos_over_neg: 2444.637939453125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8914, loss_val: nan, pos_over_neg: 729.0769653320312 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 1164.853271484375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8753, loss_val: nan, pos_over_neg: 735.99169921875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 1101.5916748046875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8756, loss_val: nan, pos_over_neg: 1886.3458251953125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 379.5143127441406 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 245.16830444335938 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 320.3105773925781 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 388.953369140625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 624.9525756835938 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8963, loss_val: nan, pos_over_neg: 1972.4825439453125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.9067, loss_val: nan, pos_over_neg: 615.9689331054688 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.9024, loss_val: nan, pos_over_neg: 7975.8974609375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.891, loss_val: nan, pos_over_neg: 581.0065307617188 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 3847.69482421875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8857, loss_val: nan, pos_over_neg: 2575.20751953125 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8955, loss_val: nan, pos_over_neg: 1230.100830078125 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8967, loss_val: nan, pos_over_neg: 498.73468017578125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8842, loss_val: nan, pos_over_neg: 512.35986328125 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 1124.7130126953125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 1650.504638671875 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8876, loss_val: nan, pos_over_neg: 473.10028076171875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8788, loss_val: nan, pos_over_neg: 587.9468994140625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8833, loss_val: nan, pos_over_neg: 337.086669921875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8823, loss_val: nan, pos_over_neg: 1082.449951171875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.889, loss_val: nan, pos_over_neg: 614.8693237304688 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8999, loss_val: nan, pos_over_neg: 951.01953125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8768, loss_val: nan, pos_over_neg: 561.9002685546875 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.9017, loss_val: nan, pos_over_neg: 759.2863159179688 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 403.1372985839844 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 543.1517944335938 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 555.5492553710938 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8871, loss_val: nan, pos_over_neg: 346.3279113769531 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 345.873046875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 493.41546630859375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8838, loss_val: nan, pos_over_neg: 337.18487548828125 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.88, loss_val: nan, pos_over_neg: 1545.266845703125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8939, loss_val: nan, pos_over_neg: 1547.3912353515625 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 531.322021484375 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8928, loss_val: nan, pos_over_neg: 357.82159423828125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8782, loss_val: nan, pos_over_neg: 272.5589294433594 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8741, loss_val: nan, pos_over_neg: 492.1314392089844 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8974, loss_val: nan, pos_over_neg: 326.48065185546875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 1216.85791015625 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.9297, loss_val: nan, pos_over_neg: 436.80645751953125 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8865, loss_val: nan, pos_over_neg: 893.5662231445312 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.892, loss_val: nan, pos_over_neg: 261.33734130859375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8888, loss_val: nan, pos_over_neg: 427.0809631347656 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8939, loss_val: nan, pos_over_neg: 831.1725463867188 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8951, loss_val: nan, pos_over_neg: 334.2983703613281 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8839, loss_val: nan, pos_over_neg: 531.466064453125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 981.5607299804688 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8915, loss_val: nan, pos_over_neg: 382.7576599121094 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 445.4129943847656 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 487.9630432128906 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8662, loss_val: nan, pos_over_neg: 5944.11474609375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8924, loss_val: nan, pos_over_neg: 415.8723449707031 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 248.1579132080078 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8945, loss_val: nan, pos_over_neg: 443.0042419433594 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.894, loss_val: nan, pos_over_neg: 499.30230712890625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8821, loss_val: nan, pos_over_neg: 927.263671875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 362.8209533691406 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 395.5877685546875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8908, loss_val: nan, pos_over_neg: 5053.0546875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 953.9458618164062 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.899, loss_val: nan, pos_over_neg: 763.0831909179688 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 349.7425231933594 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 991.4747924804688 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.9032, loss_val: nan, pos_over_neg: 477.1410217285156 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 347.8028564453125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 1368.3323974609375 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 1188.4520263671875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.907, loss_val: nan, pos_over_neg: 343.6553649902344 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 276.8127136230469 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8904, loss_val: nan, pos_over_neg: 338.57196044921875 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 369.64404296875 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8983, loss_val: nan, pos_over_neg: 215.98313903808594 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 461.5781555175781 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 21808.455078125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.9069, loss_val: nan, pos_over_neg: 536.5988159179688 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.879, loss_val: nan, pos_over_neg: 849.07080078125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 1079.9603271484375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 358.15484619140625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 401.9375915527344 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8737, loss_val: nan, pos_over_neg: 2590.065673828125 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.885, loss_val: nan, pos_over_neg: 234.39645385742188 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8857, loss_val: nan, pos_over_neg: 798.9495239257812 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8892, loss_val: nan, pos_over_neg: 652.3558959960938 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: -2335.291259765625 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8872, loss_val: nan, pos_over_neg: 722.4969482421875 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8953, loss_val: nan, pos_over_neg: 522.1619262695312 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 1327.9239501953125 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8834, loss_val: nan, pos_over_neg: 717.2515869140625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 241.59291076660156 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8789, loss_val: nan, pos_over_neg: 570.6428833007812 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 529.8562622070312 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 1039.93896484375 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8998, loss_val: nan, pos_over_neg: 391.42041015625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: 331.86016845703125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8777, loss_val: nan, pos_over_neg: 1220.574462890625 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8893, loss_val: nan, pos_over_neg: 420.43743896484375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.9003, loss_val: nan, pos_over_neg: 1553.9638671875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 522.7280883789062 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8734, loss_val: nan, pos_over_neg: 325.99017333984375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8993, loss_val: nan, pos_over_neg: 7282.27392578125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 830.2398071289062 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.9006, loss_val: nan, pos_over_neg: 9316.8876953125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8874, loss_val: nan, pos_over_neg: 776.4791259765625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 2068.126220703125 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8886, loss_val: nan, pos_over_neg: 2426.76904296875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 343.0646667480469 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8984, loss_val: nan, pos_over_neg: 446.21490478515625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 236.67648315429688 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8901, loss_val: nan, pos_over_neg: 425.5094299316406 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8969, loss_val: nan, pos_over_neg: 311.9241027832031 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 671.2755737304688 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8802, loss_val: nan, pos_over_neg: 663.5509033203125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 142149.0625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8824, loss_val: nan, pos_over_neg: 24404.373046875 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8935, loss_val: nan, pos_over_neg: 508.4715881347656 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8962, loss_val: nan, pos_over_neg: 491.8982849121094 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 483.77252197265625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8923, loss_val: nan, pos_over_neg: 548.2720947265625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 800.8330078125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 387.18310546875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8959, loss_val: nan, pos_over_neg: 373.8369140625 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 554.834228515625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 653.4716186523438 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8851, loss_val: nan, pos_over_neg: 242.2792205810547 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 1122.3299560546875 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: -2656.113525390625 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8826, loss_val: nan, pos_over_neg: 290.979248046875 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8758, loss_val: nan, pos_over_neg: 243.91746520996094 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.896, loss_val: nan, pos_over_neg: 346.87835693359375 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 766.84228515625 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8964, loss_val: nan, pos_over_neg: 332.3446044921875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8706, loss_val: nan, pos_over_neg: 809.9148559570312 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8689, loss_val: nan, pos_over_neg: 219.88645935058594 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.885, loss_val: nan, pos_over_neg: 1326.450927734375 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 70374.8046875 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 791.6876831054688 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1581.553466796875 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 578.4685668945312 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 571.5304565429688 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 1340.9154052734375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 3018.515380859375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 815.8659057617188 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8866, loss_val: nan, pos_over_neg: 1592.0966796875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.882, loss_val: nan, pos_over_neg: 2422.795166015625 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 988.4677124023438 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 799.4517211914062 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 341.09466552734375 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 1195.013671875 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: -9023.33984375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.9077, loss_val: nan, pos_over_neg: 1670.12158203125 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 922.445556640625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 644.0513305664062 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 446.95928955078125 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 784.9315795898438 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 1533.5712890625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8778, loss_val: nan, pos_over_neg: -40567.41015625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: -3958.721923828125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.882, loss_val: nan, pos_over_neg: 994.6589965820312 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8706, loss_val: nan, pos_over_neg: 539.5186767578125 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 673.7045288085938 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 5179.3525390625 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 329.97857666015625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 294.71075439453125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 525.23583984375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 291.6466064453125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 457.9380187988281 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 384.7767639160156 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 887.9707641601562 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8763, loss_val: nan, pos_over_neg: 645.5285034179688 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 1213.0953369140625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 1031.745361328125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.8674, loss_val: nan, pos_over_neg: 285.5088195800781 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8964, loss_val: nan, pos_over_neg: 202.79306030273438 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 265.3450927734375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 334.2889099121094 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 398.3818664550781 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.9035, loss_val: nan, pos_over_neg: 274.4458923339844 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8907, loss_val: nan, pos_over_neg: 442.0044860839844 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.898, loss_val: nan, pos_over_neg: 3885.30712890625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 506.64117431640625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 822.8489379882812 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8897, loss_val: nan, pos_over_neg: 319.5180969238281 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 205.345703125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8874, loss_val: nan, pos_over_neg: 296.01361083984375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 435.3320617675781 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8859, loss_val: nan, pos_over_neg: 910.041259765625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 1252.499755859375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 519.13671875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 390.9224548339844 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8889, loss_val: nan, pos_over_neg: 394.8148498535156 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.892, loss_val: nan, pos_over_neg: 345.1260070800781 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 888.9567260742188 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8618, loss_val: nan, pos_over_neg: 715.395263671875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 379.1897277832031 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8879, loss_val: nan, pos_over_neg: 277.3395690917969 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 564.8689575195312 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 498.3711853027344 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8936, loss_val: nan, pos_over_neg: 398.169189453125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 331.9107971191406 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 1419.0880126953125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 53185.87890625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 684.1962280273438 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 730.8394775390625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8754, loss_val: nan, pos_over_neg: 226.2207489013672 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 415.3807678222656 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 657.4954833984375 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8592, loss_val: nan, pos_over_neg: 368.73236083984375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8824, loss_val: nan, pos_over_neg: 472.2039794921875 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.9097, loss_val: nan, pos_over_neg: 333.66717529296875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 803.5322875976562 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8853, loss_val: nan, pos_over_neg: -32693.578125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8886, loss_val: nan, pos_over_neg: 3492.388427734375 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8864, loss_val: nan, pos_over_neg: 622.8615112304688 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 933.62451171875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8849, loss_val: nan, pos_over_neg: 4846.38671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 624.554443359375 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8909, loss_val: nan, pos_over_neg: 555.3707275390625 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8915, loss_val: nan, pos_over_neg: 305.4637145996094 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 250.62066650390625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 765.9854125976562 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.881, loss_val: nan, pos_over_neg: 587.4918823242188 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.895, loss_val: nan, pos_over_neg: 589.5160522460938 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8937, loss_val: nan, pos_over_neg: -36412.11328125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 4664.31201171875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8949, loss_val: nan, pos_over_neg: 4779.28076171875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8822, loss_val: nan, pos_over_neg: 524.1141357421875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8718, loss_val: nan, pos_over_neg: 913.1514892578125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 662.965087890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 2740.090576171875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8931, loss_val: nan, pos_over_neg: 554.7077026367188 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 1029.3900146484375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 305.09326171875 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 274.51763916015625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 1308.03955078125 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.9028, loss_val: nan, pos_over_neg: 377.28900146484375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8797, loss_val: nan, pos_over_neg: 429.542724609375 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8769, loss_val: nan, pos_over_neg: 682.3121337890625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8718, loss_val: nan, pos_over_neg: 896.4800415039062 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8848, loss_val: nan, pos_over_neg: 805.6010131835938 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 524.964111328125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 468.4932556152344 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8805, loss_val: nan, pos_over_neg: 499.4515380859375 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8775, loss_val: nan, pos_over_neg: 925.756103515625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 1139.93798828125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8803, loss_val: nan, pos_over_neg: 7139.84619140625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 106878.46875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8627, loss_val: nan, pos_over_neg: 969.404052734375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 439.95660400390625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8818, loss_val: nan, pos_over_neg: 378.12255859375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8875, loss_val: nan, pos_over_neg: 295.49029541015625 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.895, loss_val: nan, pos_over_neg: 325.7441711425781 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8916, loss_val: nan, pos_over_neg: 735.8640747070312 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8808, loss_val: nan, pos_over_neg: 1098.8482666015625 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.871, loss_val: nan, pos_over_neg: 10962.294921875 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: -9137.341796875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8933, loss_val: nan, pos_over_neg: 1206.1697998046875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8922, loss_val: nan, pos_over_neg: 976.2044067382812 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 1930.3194580078125 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 437.67852783203125 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 1759.302734375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8765, loss_val: nan, pos_over_neg: 789.0890502929688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8784, loss_val: nan, pos_over_neg: 318.4605712890625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 421.6560974121094 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8902, loss_val: nan, pos_over_neg: 341.195068359375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 300.76959228515625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 452.689697265625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 572.9445190429688 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 429.3912658691406 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 606.3817749023438 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8837, loss_val: nan, pos_over_neg: 713.945556640625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8757, loss_val: nan, pos_over_neg: 566.6756591796875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8779, loss_val: nan, pos_over_neg: 1806.934814453125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 6084.78369140625 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 2026.9635009765625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 1771.9432373046875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 791.0905151367188 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 3551.06689453125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8912, loss_val: nan, pos_over_neg: 670.3289794921875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.881, loss_val: nan, pos_over_neg: 268.8786926269531 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 399.42620849609375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 381.9920349121094 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8841, loss_val: nan, pos_over_neg: 2272.904296875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.891, loss_val: nan, pos_over_neg: -8961.0556640625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8849, loss_val: nan, pos_over_neg: 639.0338745117188 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 1582.318603515625 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8836, loss_val: nan, pos_over_neg: 910.5545043945312 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 1984.9639892578125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 535.238037109375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8829, loss_val: nan, pos_over_neg: 820.094482421875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 528.517578125 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.893, loss_val: nan, pos_over_neg: 292.9237976074219 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 773.1793823242188 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 656.4672241210938 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8762, loss_val: nan, pos_over_neg: 1130.901611328125 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1215.0367431640625 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/300000 [51:15<128593:49:30, 1543.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "Iter: 0/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 1075.176513671875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 6245.3291015625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8873, loss_val: nan, pos_over_neg: 355.1353454589844 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 1739.677978515625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1063.34521484375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 4997.0244140625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 4643.34716796875 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 2999.383544921875 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 543.8554077148438 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.871, loss_val: nan, pos_over_neg: 452.1960754394531 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 191.77870178222656 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 462.4013977050781 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 876.7869262695312 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 593.9888305664062 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 667.9849853515625 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 323.50323486328125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 1888.3741455078125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8932, loss_val: nan, pos_over_neg: 585.3035888671875 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 3394.8193359375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8719, loss_val: nan, pos_over_neg: 887.2806396484375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 432.22039794921875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8913, loss_val: nan, pos_over_neg: 270.9685974121094 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 545.8938598632812 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8822, loss_val: nan, pos_over_neg: 1583.386962890625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 1960.891357421875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8919, loss_val: nan, pos_over_neg: 432.4559020996094 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 717.6377563476562 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 960.196044921875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.864, loss_val: nan, pos_over_neg: 2022.40673828125 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 5089.40380859375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 602.0731811523438 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8851, loss_val: nan, pos_over_neg: 326.2461242675781 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 367.5066223144531 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 2383.094970703125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 567.879150390625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8662, loss_val: nan, pos_over_neg: 2275.517822265625 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 524.3797607421875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.886, loss_val: nan, pos_over_neg: 612.2474365234375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8785, loss_val: nan, pos_over_neg: 3342.60400390625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 1648.2421875 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8882, loss_val: nan, pos_over_neg: 4595.8603515625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.9106, loss_val: nan, pos_over_neg: 311.8813781738281 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 443.65875244140625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 1162.101318359375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 873.6826171875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1738.14697265625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: -6518.93994140625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 490.4224853515625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8765, loss_val: nan, pos_over_neg: 558.9411010742188 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8867, loss_val: nan, pos_over_neg: 799.1810302734375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 557.5020141601562 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 402.30548095703125 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8831, loss_val: nan, pos_over_neg: 591.1005249023438 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8863, loss_val: nan, pos_over_neg: 641.1077880859375 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1161.3863525390625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 3311.521728515625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1091.151123046875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 331.8443298339844 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 8991.48828125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 442.14501953125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 614.81640625 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 376.1037292480469 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 895.2864990234375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.858, loss_val: nan, pos_over_neg: 468.9333801269531 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8756, loss_val: nan, pos_over_neg: 601.141845703125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8741, loss_val: nan, pos_over_neg: 507.6368408203125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8884, loss_val: nan, pos_over_neg: 887.4764404296875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 572.3271484375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 513.8126220703125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.887, loss_val: nan, pos_over_neg: 933.9344482421875 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8815, loss_val: nan, pos_over_neg: 1297.7496337890625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 9654.0556640625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.879, loss_val: nan, pos_over_neg: 816.9501342773438 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.884, loss_val: nan, pos_over_neg: 831.2824096679688 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 923.8306884765625 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 933.5872802734375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: 628.0035400390625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 461.08941650390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 730.2784423828125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1391.01171875 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 403.7232666015625 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.876, loss_val: nan, pos_over_neg: 693.6778564453125 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 694.7367553710938 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 1107.1572265625 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 1257.7288818359375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 397.6473388671875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 542.8385009765625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.88, loss_val: nan, pos_over_neg: 2310.9765625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8688, loss_val: nan, pos_over_neg: 616.1572875976562 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8659, loss_val: nan, pos_over_neg: 426.4395446777344 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 817.4014282226562 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8862, loss_val: nan, pos_over_neg: 625.2652587890625 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1489.786865234375 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 2902.106689453125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 560.3138427734375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 2646.97998046875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8754, loss_val: nan, pos_over_neg: 2284.910888671875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8799, loss_val: nan, pos_over_neg: 3584.594482421875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8708, loss_val: nan, pos_over_neg: 602.47412109375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 374.5241394042969 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 273.1947937011719 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 934.3021240234375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 573.803466796875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8697, loss_val: nan, pos_over_neg: 284.7366638183594 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8918, loss_val: nan, pos_over_neg: 652.1111450195312 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 750.2005004882812 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 529.9183959960938 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 585.62060546875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 1765.7197265625 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 577.4762573242188 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8652, loss_val: nan, pos_over_neg: 2297.88671875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 943.4608154296875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8742, loss_val: nan, pos_over_neg: 1442.147705078125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8811, loss_val: nan, pos_over_neg: 348.2008056640625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 391.04168701171875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8877, loss_val: nan, pos_over_neg: 251.00904846191406 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 765.5843505859375 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1255.4815673828125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8929, loss_val: nan, pos_over_neg: 365.936279296875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 1356.2216796875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 805.7055053710938 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 980.322509765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 504.180908203125 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 1714.7630615234375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8905, loss_val: nan, pos_over_neg: 274.7056884765625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 205.72996520996094 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 369.5574645996094 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 563.0384521484375 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 3629.805908203125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 513.1337280273438 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8652, loss_val: nan, pos_over_neg: 358.1010437011719 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8746, loss_val: nan, pos_over_neg: 796.3998413085938 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 396.4903259277344 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 1003.0144653320312 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 714.5008544921875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 338.38897705078125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8861, loss_val: nan, pos_over_neg: 144.02392578125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 596.3695068359375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8887, loss_val: nan, pos_over_neg: 664.2579956054688 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 1022.3934936523438 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 351.6241149902344 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 481.7894287109375 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 1125.246826171875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 583.0029907226562 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 311.0412902832031 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 658.407470703125 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 490.03009033203125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 370.50830078125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 584.31103515625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 248.98089599609375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8804, loss_val: nan, pos_over_neg: 648.3477172851562 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8862, loss_val: nan, pos_over_neg: 539.1656494140625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 1903.995849609375 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 470.1190490722656 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 903.1780395507812 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 268.8365173339844 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 712.3817749023438 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8619, loss_val: nan, pos_over_neg: 940.0736694335938 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 639.8726196289062 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 364.759765625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 1295.3055419921875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8914, loss_val: nan, pos_over_neg: 952.1818237304688 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: -7391.7548828125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 719.8208618164062 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: 322.4218444824219 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 1211.4786376953125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8797, loss_val: nan, pos_over_neg: 510.75146484375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1396.8419189453125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 399.9798889160156 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 414.46978759765625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 436.56689453125 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 402.2867736816406 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8778, loss_val: nan, pos_over_neg: 298.0601806640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8791, loss_val: nan, pos_over_neg: 218.41915893554688 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1075.7552490234375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 508.6595458984375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.9042, loss_val: nan, pos_over_neg: 581.7366943359375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 540.5795288085938 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 1083.8953857421875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 425.75897216796875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 598.8929443359375 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 474.13671875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8731, loss_val: nan, pos_over_neg: 282.3034362792969 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 539.6220703125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 1666.28076171875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8761, loss_val: nan, pos_over_neg: 1210.2955322265625 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 749.868896484375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1133.2200927734375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8709, loss_val: nan, pos_over_neg: 511.7240295410156 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 1096.329833984375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 290.720947265625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.877, loss_val: nan, pos_over_neg: 556.4549560546875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 400.2999267578125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8734, loss_val: nan, pos_over_neg: 476.8496398925781 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.8753, loss_val: nan, pos_over_neg: 886.701904296875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8966, loss_val: nan, pos_over_neg: 310.0042419433594 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 521.3740844726562 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 1471.786865234375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 341.41680908203125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 2794.212890625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.89, loss_val: nan, pos_over_neg: 1592.7725830078125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 633.0692749023438 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 233.9544677734375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 309.1622314453125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 297.6532897949219 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 392.19610595703125 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 372.8125305175781 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8825, loss_val: nan, pos_over_neg: 1170.86474609375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8871, loss_val: nan, pos_over_neg: 689.5963745117188 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 918.1259765625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 887.2156372070312 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8709, loss_val: nan, pos_over_neg: 372.27044677734375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 299.1202087402344 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8606, loss_val: nan, pos_over_neg: 1054.76953125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8594, loss_val: nan, pos_over_neg: 439.40771484375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 302.4642639160156 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 339.68701171875 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 780.3733520507812 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 2231.85009765625 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8828, loss_val: nan, pos_over_neg: 914.815185546875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8823, loss_val: nan, pos_over_neg: 514.0079956054688 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 344.5271911621094 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 762.9468994140625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8773, loss_val: nan, pos_over_neg: 173.73849487304688 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.883, loss_val: nan, pos_over_neg: 284.9039001464844 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8844, loss_val: nan, pos_over_neg: 331.09716796875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 725.8280639648438 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8651, loss_val: nan, pos_over_neg: 590.068359375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 534.75146484375 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 323.7666931152344 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8618, loss_val: nan, pos_over_neg: 838.58642578125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8676, loss_val: nan, pos_over_neg: 1209.07763671875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 508.37548828125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 446.8233642578125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 531.4090576171875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 357.5298156738281 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 735.4611206054688 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 399.1441650390625 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 538.1569213867188 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -4625.30224609375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8854, loss_val: nan, pos_over_neg: 837.5424194335938 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 472.31817626953125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 722.05908203125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 718.9353637695312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 644.680908203125 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 1279.848876953125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8846, loss_val: nan, pos_over_neg: 526.2493896484375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1630.385498046875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1041.918701171875 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 558.5006713867188 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 357.6961669921875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.876, loss_val: nan, pos_over_neg: 433.65484619140625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 1319.66015625 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8727, loss_val: nan, pos_over_neg: 696.2008056640625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 1533.298583984375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 545.6492919921875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8747, loss_val: nan, pos_over_neg: 521.754150390625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1020.3488159179688 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 1938.970703125 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 384.027099609375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 623.4685668945312 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 501.5295104980469 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 695.5584716796875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8661, loss_val: nan, pos_over_neg: 2061.42822265625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 770.4571533203125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 2253.27783203125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 546.16064453125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 412.6590881347656 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 994.4027099609375 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 1317.000732421875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8737, loss_val: nan, pos_over_neg: 1429.2412109375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 336.7294921875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8621, loss_val: nan, pos_over_neg: 650.4860229492188 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8726, loss_val: nan, pos_over_neg: 534.0308227539062 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 968.074462890625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8816, loss_val: nan, pos_over_neg: 429.2969055175781 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 281.4535217285156 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: 220.11976623535156 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.8694, loss_val: nan, pos_over_neg: 213.42129516601562 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 315.1135559082031 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.856, loss_val: nan, pos_over_neg: 393.583984375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 226.6142578125 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8596, loss_val: nan, pos_over_neg: 589.6156616210938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 789.3516845703125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8794, loss_val: nan, pos_over_neg: 3459.49462890625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8722, loss_val: nan, pos_over_neg: 854.5340576171875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 1013.1727905273438 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8732, loss_val: nan, pos_over_neg: 221.9559326171875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.857, loss_val: nan, pos_over_neg: 362.2867736816406 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8671, loss_val: nan, pos_over_neg: 608.814453125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 703.6651000976562 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 1449.37646484375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8925, loss_val: nan, pos_over_neg: 187.80938720703125 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 605.8714599609375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.854, loss_val: nan, pos_over_neg: 590.1094360351562 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 1017.3925170898438 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8714, loss_val: nan, pos_over_neg: -3564.129638671875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 461.349853515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8786, loss_val: nan, pos_over_neg: 547.3106689453125 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8793, loss_val: nan, pos_over_neg: 909.7786865234375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 361.75067138671875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 863.6995849609375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 917.5386962890625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8713, loss_val: nan, pos_over_neg: 695.3216552734375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8774, loss_val: nan, pos_over_neg: 1690.034912109375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 2559.393310546875 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8832, loss_val: nan, pos_over_neg: 556.2385864257812 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 1304.87646484375 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8694, loss_val: nan, pos_over_neg: 960.646728515625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 644.548828125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8669, loss_val: nan, pos_over_neg: 434.55841064453125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8847, loss_val: nan, pos_over_neg: 333.318115234375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 401.7045593261719 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8576, loss_val: nan, pos_over_neg: 482.4705810546875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1051.75341796875 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 1006.8344116210938 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 1565.5108642578125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8814, loss_val: nan, pos_over_neg: 384.39727783203125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1029.7974853515625 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 360.1948547363281 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8819, loss_val: nan, pos_over_neg: 646.34130859375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 898.9514770507812 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 1403.008056640625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 1766.3531494140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: -4241.66943359375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: -4191.185546875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 621.191162109375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 408.08746337890625 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 514.2671508789062 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8766, loss_val: nan, pos_over_neg: 790.348388671875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 484.1797180175781 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 557.0962524414062 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1910.7216796875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 857.7520141601562 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: -2547094.0 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8839, loss_val: nan, pos_over_neg: 2844.94091796875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 1227.699951171875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 2043.2186279296875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: 1020.6309814453125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 683.3217163085938 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8843, loss_val: nan, pos_over_neg: 608.4768676757812 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 653.2766723632812 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 469.05657958984375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 321.680908203125 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8749, loss_val: nan, pos_over_neg: 602.4171752929688 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 597.5897827148438 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: 1155.3753662109375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 426.7418518066406 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8992, loss_val: nan, pos_over_neg: 165.90049743652344 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 516.0029296875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8721, loss_val: nan, pos_over_neg: 406.4738464355469 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 453.05389404296875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 905.6323852539062 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 549.3851928710938 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 632.2247314453125 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 303.576416015625 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8606, loss_val: nan, pos_over_neg: 720.5989379882812 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8807, loss_val: nan, pos_over_neg: 531.1820068359375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8809, loss_val: nan, pos_over_neg: 596.673095703125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8858, loss_val: nan, pos_over_neg: 418.83978271484375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 255.2589569091797 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8597, loss_val: nan, pos_over_neg: 193.5805206298828 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 1614.203369140625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8764, loss_val: nan, pos_over_neg: 394.97265625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 276.6116943359375 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 683.8222045898438 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8852, loss_val: nan, pos_over_neg: 266.56292724609375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8973, loss_val: nan, pos_over_neg: 217.8087615966797 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1331.6214599609375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 2289.9921875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 546.5974731445312 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8704, loss_val: nan, pos_over_neg: 301.5432434082031 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8911, loss_val: nan, pos_over_neg: 171.99488830566406 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8739, loss_val: nan, pos_over_neg: 291.4035339355469 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 3963.844482421875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8735, loss_val: nan, pos_over_neg: 877.3892211914062 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 568.612548828125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8702, loss_val: nan, pos_over_neg: 325.6177062988281 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 3438.592041015625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8729, loss_val: nan, pos_over_neg: 627.5899047851562 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 320.5360107421875 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 404.7117004394531 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 400.2588806152344 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 435.5526123046875 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: -39020.57421875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8674, loss_val: nan, pos_over_neg: 1962.2811279296875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8642, loss_val: nan, pos_over_neg: 884.6886596679688 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 679.3472900390625 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1209.349365234375 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8752, loss_val: nan, pos_over_neg: 468.0185546875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 31836.431640625 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 619.6639404296875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 252.29800415039062 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 485.5860595703125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: 1487.2535400390625 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8801, loss_val: nan, pos_over_neg: 494.8489074707031 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 4776.67529296875 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 945.213623046875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 937.6642456054688 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 897.7469482421875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 575.764892578125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 362.3784484863281 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8696, loss_val: nan, pos_over_neg: 263.7637634277344 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8783, loss_val: nan, pos_over_neg: 329.08087158203125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 579.288330078125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 450.0419921875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 519.9652709960938 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8749, loss_val: nan, pos_over_neg: 4639.177734375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 676.9046020507812 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 610.6914672851562 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 827.241943359375 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 1691.2491455078125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8767, loss_val: nan, pos_over_neg: 325.5877990722656 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 534.7273559570312 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1018.2996826171875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8657, loss_val: nan, pos_over_neg: 656.4041748046875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8888, loss_val: nan, pos_over_neg: 335.8367919921875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 763.5714111328125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 8946.2255859375 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8561, loss_val: nan, pos_over_neg: 1713.7073974609375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 673.1533203125 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 772.338623046875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8855, loss_val: nan, pos_over_neg: 291.40399169921875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 1137.8673095703125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 1777.3609619140625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 1531.0958251953125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8631, loss_val: nan, pos_over_neg: 7404.6552734375 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.856, loss_val: nan, pos_over_neg: 5691.376953125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.841, loss_val: nan, pos_over_neg: 1306.958740234375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 2171.59033203125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8772, loss_val: nan, pos_over_neg: 473.74273681640625 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 864.4778442382812 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 510.8460388183594 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8703, loss_val: nan, pos_over_neg: 258.12060546875 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 473.5105895996094 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 1335.9295654296875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 759.4105224609375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8684, loss_val: nan, pos_over_neg: -23276.330078125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 2882.6455078125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.878, loss_val: nan, pos_over_neg: 2012.438232421875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 2527.00048828125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -10330.49609375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8787, loss_val: nan, pos_over_neg: 372.5998840332031 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8687, loss_val: nan, pos_over_neg: 378.6512451171875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8715, loss_val: nan, pos_over_neg: 682.2481079101562 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 888.1846923828125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 825.2175903320312 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 849.0145874023438 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 1135.860107421875 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 1576.049072265625 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 785.7926025390625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8716, loss_val: nan, pos_over_neg: 359.757568359375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 749.1768798828125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8827, loss_val: nan, pos_over_neg: 339.3416442871094 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 1674.0111083984375 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 991.2174072265625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8656, loss_val: nan, pos_over_neg: -5276.25634765625 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 641.1976928710938 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 587.2557373046875 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 3479.39794921875 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 8065.04052734375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -2652.373291015625 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8661, loss_val: nan, pos_over_neg: 6887.51123046875 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 1067.0196533203125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 1235.5003662109375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 712.2490234375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 786.2131958007812 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 646.0137939453125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8554, loss_val: nan, pos_over_neg: 934.6567993164062 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8395, loss_val: nan, pos_over_neg: -6584.259765625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 9565.5517578125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -17760.73046875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1658.3154296875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 706.1901245117188 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 1191.3656005859375 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 393.9706115722656 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8602, loss_val: nan, pos_over_neg: 966.177001953125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: -12948.791015625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8611, loss_val: nan, pos_over_neg: 1049.5029296875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 955.2474365234375 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 669.9185791015625 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8522, loss_val: nan, pos_over_neg: 2085.2392578125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 727.03955078125 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8678, loss_val: nan, pos_over_neg: 600.527587890625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 1055.9547119140625 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8751, loss_val: nan, pos_over_neg: 490.3638610839844 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 409.5103454589844 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 306.95367431640625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: 1253.83984375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 13123.6376953125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 782.2640991210938 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8563, loss_val: nan, pos_over_neg: 1847.6956787109375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 517.2401123046875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1141.8946533203125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2101.15771484375 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8679, loss_val: nan, pos_over_neg: 630.656982421875 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8812, loss_val: nan, pos_over_neg: 572.68115234375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 721.4280395507812 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 995.5783081054688 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 2778.945068359375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8651, loss_val: nan, pos_over_neg: 413.7323913574219 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8695, loss_val: nan, pos_over_neg: 363.4141540527344 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 1324.236083984375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 2322.294677734375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: -6988.37548828125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 1339.4754638671875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 2587.89306640625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -3477.9404296875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.874, loss_val: nan, pos_over_neg: 485.7971496582031 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 715.3729858398438 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 904.8485717773438 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 661.2586059570312 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8771, loss_val: nan, pos_over_neg: 705.7615966796875 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 2202.97607421875 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8738, loss_val: nan, pos_over_neg: 346.07318115234375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 4252.22265625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 748.9725341796875 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 267.291259765625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 486.227294921875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 291.2694396972656 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1163.99560546875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 508.2518615722656 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 344.2172546386719 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 700.358642578125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 1142.7076416015625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 23885.443359375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8673, loss_val: nan, pos_over_neg: 3242.742431640625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1926.61572265625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 2134.08740234375 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 688.7275390625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 410.5346984863281 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 602.9505615234375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 471.8742370605469 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8645, loss_val: nan, pos_over_neg: 761.359375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 422.06683349609375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8699, loss_val: nan, pos_over_neg: 803.9366455078125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8728, loss_val: nan, pos_over_neg: 1189.305908203125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8592, loss_val: nan, pos_over_neg: 2384.121826171875 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8667, loss_val: nan, pos_over_neg: 3130.070556640625 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1340.395263671875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 739.5156860351562 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8594, loss_val: nan, pos_over_neg: 787.5612182617188 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 474.64581298828125 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8748, loss_val: nan, pos_over_neg: 338.9555969238281 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 292.4483642578125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 1267.7471923828125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 744.4164428710938 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 4232.38525390625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8733, loss_val: nan, pos_over_neg: 487.2979431152344 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 964.0324096679688 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 750.3941650390625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 463.85296630859375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8441, loss_val: nan, pos_over_neg: 527.4739379882812 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 782.6657104492188 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 12037.712890625 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 95061.203125 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 1736.31494140625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 392.7061767578125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 2195.28271484375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8551, loss_val: nan, pos_over_neg: 1448.6512451171875 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8603, loss_val: nan, pos_over_neg: -17139.326171875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8803, loss_val: nan, pos_over_neg: 288.756103515625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.869, loss_val: nan, pos_over_neg: 440.304443359375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 3650.73828125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 2671.7265625 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 944.5056762695312 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 403.895263671875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 1745.553955078125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 545.7738037109375 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 851.486083984375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8628, loss_val: nan, pos_over_neg: 872.2369384765625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8759, loss_val: nan, pos_over_neg: 354.57208251953125 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 678.7432861328125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 368.88201904296875 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1097.1959228515625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 628.8080444335938 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 1100.11962890625 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 1318.5677490234375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 937.3278198242188 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1028.36865234375 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 513.65625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 816.1243896484375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 1601.462646484375 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8795, loss_val: nan, pos_over_neg: 406.99822998046875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 1543.7552490234375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 409.19921875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8624, loss_val: nan, pos_over_neg: 332.855712890625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 340.5652770996094 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8509, loss_val: nan, pos_over_neg: 673.401123046875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 488.64288330078125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 474.0411071777344 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 683.2166137695312 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1117.888916015625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8705, loss_val: nan, pos_over_neg: 750.571044921875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 1000.20166015625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 868.0068969726562 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 416.4661865234375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 240.49478149414062 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 403.9413146972656 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8654, loss_val: nan, pos_over_neg: 616.9020385742188 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 1171.4473876953125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 473.29150390625 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1506.303466796875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 2033.2044677734375 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8625, loss_val: nan, pos_over_neg: 571.8095092773438 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 468.31488037109375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8346, loss_val: nan, pos_over_neg: 449.4470520019531 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 290.7755126953125 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 516.5377197265625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 516.1143798828125 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1392.5423583984375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 491.71295166015625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8666, loss_val: nan, pos_over_neg: 1802.03466796875 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: -3984.944091796875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: -5052.12451171875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 2600.091064453125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 931.1320190429688 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 716.1036376953125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 227.76443481445312 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 492.6982727050781 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 417.78277587890625 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 439.3492126464844 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 484.4576416015625 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 1572.0009765625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 458.2521057128906 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8724, loss_val: nan, pos_over_neg: 291.6383361816406 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 442.6464538574219 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 901.731689453125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 1178.8233642578125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 834.4254760742188 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 2110.198974609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: -6886.248046875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 459.7488708496094 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1252.6463623046875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8587, loss_val: nan, pos_over_neg: 577.9113159179688 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8561, loss_val: nan, pos_over_neg: 554.026123046875 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 688.0755004882812 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 629.7623901367188 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 698.0081787109375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8744, loss_val: nan, pos_over_neg: 413.25567626953125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 1214.0413818359375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: 617.390380859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 627.9589233398438 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8736, loss_val: nan, pos_over_neg: 788.986083984375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 1044.9739990234375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 286.29913330078125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.854, loss_val: nan, pos_over_neg: 540.586669921875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 507.27252197265625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 436.36810302734375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 975.802490234375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 1041.6104736328125 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 567.90283203125 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 402.948486328125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 565.4600830078125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 907.9468994140625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8647, loss_val: nan, pos_over_neg: 200.0150146484375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 502.3478698730469 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 1441.0572509765625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8646, loss_val: nan, pos_over_neg: 331.79388427734375 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 916.0939331054688 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1371.5533447265625 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 1321.8155517578125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8509, loss_val: nan, pos_over_neg: 581.592041015625 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 5875.29736328125 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8693, loss_val: nan, pos_over_neg: 986.5808715820312 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 1755.831298828125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 489.9143981933594 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 525.4821166992188 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 930.517822265625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 1393.100830078125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.872, loss_val: nan, pos_over_neg: 430.4989013671875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8603, loss_val: nan, pos_over_neg: 627.0699462890625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8615, loss_val: nan, pos_over_neg: 533.424072265625 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 3316.7216796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 1780.536865234375 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 3547.778564453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 520.5115356445312 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8681, loss_val: nan, pos_over_neg: 600.45654296875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8636, loss_val: nan, pos_over_neg: 605.0275268554688 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1279.73876953125 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 765.7999877929688 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 1320.394775390625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 5097.068359375 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 2572.69677734375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 2088.02392578125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 690.2446899414062 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8648, loss_val: nan, pos_over_neg: 466.8162841796875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8806, loss_val: nan, pos_over_neg: 371.36981201171875 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.862, loss_val: nan, pos_over_neg: 957.4724731445312 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8711, loss_val: nan, pos_over_neg: 521.1004638671875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 771.1326293945312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: -6475.9345703125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 4073.7275390625 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8597, loss_val: nan, pos_over_neg: 541.0107421875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 584.0819091796875 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8605, loss_val: nan, pos_over_neg: 672.967041015625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 418.9327087402344 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8781, loss_val: nan, pos_over_neg: 587.7807006835938 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 882.8037109375 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1376.7900390625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 16379.1376953125 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 3197.150390625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: -4083.77294921875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8685, loss_val: nan, pos_over_neg: 7642.314453125 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 863.5057983398438 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1105.347412109375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8555, loss_val: nan, pos_over_neg: 263.05712890625 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1946.9718017578125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 2868.06982421875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 860.8370361328125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1536.236572265625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 624.3844604492188 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 318.0038757324219 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 1338.1160888671875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 570.1878051757812 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 2222.86181640625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 767.2025146484375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1654.583984375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 2041.14306640625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8712, loss_val: nan, pos_over_neg: 490.2674560546875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.858, loss_val: nan, pos_over_neg: 1136.31103515625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 504.21478271484375 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 689.6544189453125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8519, loss_val: nan, pos_over_neg: 1861.0638427734375 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: -33460.58203125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1999.5877685546875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8755, loss_val: nan, pos_over_neg: 136.09275817871094 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 823.255615234375 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8578, loss_val: nan, pos_over_neg: 544.1290283203125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 476.7257080078125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 544.3353881835938 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 400.8596496582031 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 508.796630859375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8658, loss_val: nan, pos_over_neg: 2349.414794921875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 938.8298950195312 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 471.5934143066406 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1149.3248291015625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8586, loss_val: nan, pos_over_neg: 515.778564453125 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 796.0274658203125 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: -1224.8035888671875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 1246.9814453125 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 795.2605590820312 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 2910.092041015625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 594.3202514648438 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8683, loss_val: nan, pos_over_neg: 1655.443115234375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8476, loss_val: nan, pos_over_neg: 325.9455261230469 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 377.5336608886719 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 305.984619140625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 887.0179443359375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 1313.824951171875 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.875, loss_val: nan, pos_over_neg: 873.5665283203125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8608, loss_val: nan, pos_over_neg: 761.4998168945312 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: -7494.98779296875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 20331.470703125 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 2481.849609375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 1695.014892578125 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 560.004638671875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 332.9964294433594 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 441.9856872558594 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 833.6871948242188 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8632, loss_val: nan, pos_over_neg: 1520.2452392578125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 716.6887817382812 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8691, loss_val: nan, pos_over_neg: 1508.1102294921875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 780.6260986328125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 604.5587768554688 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 1159.70849609375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8739, loss_val: nan, pos_over_neg: 374.8650817871094 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 725.9254150390625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8569, loss_val: nan, pos_over_neg: 5862.5771484375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1763.5347900390625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2045.86865234375 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 743.9234619140625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.8677, loss_val: nan, pos_over_neg: 452.31304931640625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8522, loss_val: nan, pos_over_neg: 677.28564453125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: -4964.22998046875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.864, loss_val: nan, pos_over_neg: 1072.6767578125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8649, loss_val: nan, pos_over_neg: 950.5066528320312 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 3949.888671875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 388.2855224609375 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2158.423095703125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8617, loss_val: nan, pos_over_neg: 578.9176635742188 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 881.18701171875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2132.6708984375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1810.9583740234375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 542.3095703125 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 367.8229064941406 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 797.0142822265625 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 755.6251831054688 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1583.283447265625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 814.164794921875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 2079.4951171875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 443.7566223144531 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 6416.62890625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 1532.1329345703125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 8342.8251953125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 12060.2197265625 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 4934.2822265625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.867, loss_val: nan, pos_over_neg: 317.6824951171875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8643, loss_val: nan, pos_over_neg: 518.3070068359375 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 659.1494750976562 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: -4527.74462890625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 375.1302795410156 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 462.4061584472656 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 202.4540557861328 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: 2731.347900390625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 950.0594482421875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 2009.57470703125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 646.1814575195312 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 678.7001953125 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8588, loss_val: nan, pos_over_neg: 448.60223388671875 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1022.8273315429688 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 784.4420166015625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 486.7146911621094 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 731.7244262695312 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 565.64794921875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1618.5443115234375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8551, loss_val: nan, pos_over_neg: 844.4874877929688 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8664, loss_val: nan, pos_over_neg: 468.38763427734375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 2690.471435546875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 483.33905029296875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: -2812.01025390625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 552.3569946289062 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 389.1024169921875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1803.05908203125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 2103.60888671875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 733.1332397460938 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8635, loss_val: nan, pos_over_neg: 422.6601257324219 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 1146.8255615234375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 2255.81005859375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 884.72216796875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 711.2398681640625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 415.0723876953125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 386.3736572265625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 486.0013122558594 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 702.3699340820312 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 924.27392578125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 626.1954956054688 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8593, loss_val: nan, pos_over_neg: 677.1514892578125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 788.25341796875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 1450.8192138671875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8675, loss_val: nan, pos_over_neg: 5844.9560546875 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 863.898193359375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 873.5923461914062 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8514, loss_val: nan, pos_over_neg: 528.235107421875 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8395, loss_val: nan, pos_over_neg: 458.3009033203125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 989.3134155273438 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.859, loss_val: nan, pos_over_neg: 1727.3070068359375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8544, loss_val: nan, pos_over_neg: 577.0971069335938 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8743, loss_val: nan, pos_over_neg: 487.6715087890625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 1120.6617431640625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 396.3429260253906 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 2207.652587890625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 350.8941955566406 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 679.8090209960938 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 469.50927734375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: -3071.916015625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 1030.5872802734375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8583, loss_val: nan, pos_over_neg: 659.72412109375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 868.6398315429688 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8707, loss_val: nan, pos_over_neg: 263.8551025390625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 2648.52392578125 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.868, loss_val: nan, pos_over_neg: 875.9598388671875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 22878.89453125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8503, loss_val: nan, pos_over_neg: 2114.8251953125 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 697.412841796875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: 583.2543334960938 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 345.40576171875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 791.8844604492188 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 1251.1646728515625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.851, loss_val: nan, pos_over_neg: 363.63507080078125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1373.658447265625 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1632.10009765625 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 926.3179931640625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 1184.937744140625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 503.44232177734375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8474, loss_val: nan, pos_over_neg: -3979.78076171875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8686, loss_val: nan, pos_over_neg: 341.95489501953125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 2046.746826171875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8607, loss_val: nan, pos_over_neg: 505.71331787109375 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 421.24017333984375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8518, loss_val: nan, pos_over_neg: 439.4966735839844 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 1152.4095458984375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 272.582275390625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 905.1065063476562 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1414.36279296875 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 607.9937133789062 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 401.8050842285156 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8558, loss_val: nan, pos_over_neg: 3524.43994140625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 1396.581298828125 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/300000 [1:16:46<128159:55:32, 1537.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "Iter: 0/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 2466.065673828125 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8468, loss_val: nan, pos_over_neg: 1065.8143310546875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 981.7886962890625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 303.48638916015625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 666.6537475585938 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8663, loss_val: nan, pos_over_neg: 701.0667114257812 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1000.0113525390625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1011.1612548828125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 821.1480102539062 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 809.2012329101562 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 340.6093444824219 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 722.5845336914062 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: 716.59765625 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8622, loss_val: nan, pos_over_neg: 583.3740234375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8548, loss_val: nan, pos_over_neg: 575.225341796875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 1146.1351318359375 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 643.8851928710938 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 910.6577758789062 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 1495.2288818359375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.861, loss_val: nan, pos_over_neg: 892.4231567382812 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 13522.478515625 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 627.9716186523438 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 375.6837463378906 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 302.9676208496094 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1079.217041015625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 1847.1988525390625 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 475.89935302734375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8672, loss_val: nan, pos_over_neg: 450.49029541015625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 996.3467407226562 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 768.3350830078125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8624, loss_val: nan, pos_over_neg: 288.3725891113281 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8555, loss_val: nan, pos_over_neg: 636.4075927734375 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 563.66796875 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 318.2464294433594 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 236.73912048339844 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 332.60443115234375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1043.7447509765625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8668, loss_val: nan, pos_over_neg: 706.9942626953125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 1165.4049072265625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 444.47125244140625 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.849, loss_val: nan, pos_over_neg: 1335.633056640625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8578, loss_val: nan, pos_over_neg: 347.1168212890625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8556, loss_val: nan, pos_over_neg: 663.4349975585938 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 339.3305969238281 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 388.19244384765625 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 323.9368591308594 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 484.319580078125 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 659.2789916992188 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8539, loss_val: nan, pos_over_neg: 377.6083679199219 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 231.83258056640625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 343.3299255371094 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8575, loss_val: nan, pos_over_neg: 439.2105407714844 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 615.531005859375 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 631.3292236328125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 675.8602294921875 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8503, loss_val: nan, pos_over_neg: 411.11376953125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1011.9521484375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8665, loss_val: nan, pos_over_neg: 453.3864440917969 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 693.8598022460938 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 274.2830810546875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 570.2306518554688 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 374.52685546875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.846, loss_val: nan, pos_over_neg: 408.7663879394531 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 571.11474609375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 1940.18359375 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8574, loss_val: nan, pos_over_neg: 285.1263427734375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8557, loss_val: nan, pos_over_neg: 482.9710693359375 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 654.22998046875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8445, loss_val: nan, pos_over_neg: 521.6522216796875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8641, loss_val: nan, pos_over_neg: 378.4000244140625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8527, loss_val: nan, pos_over_neg: 489.22686767578125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 1469.46435546875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.8514, loss_val: nan, pos_over_neg: 324.0608825683594 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8547, loss_val: nan, pos_over_neg: 549.0965576171875 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 685.40771484375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 526.5121459960938 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 685.4268798828125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8612, loss_val: nan, pos_over_neg: 329.27557373046875 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 646.4334106445312 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 837.9232788085938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8675, loss_val: nan, pos_over_neg: 1767.0443115234375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8701, loss_val: nan, pos_over_neg: 248.678466796875 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8566, loss_val: nan, pos_over_neg: 385.500244140625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8717, loss_val: nan, pos_over_neg: 290.01434326171875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 684.43115234375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8591, loss_val: nan, pos_over_neg: 630.366455078125 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1153.9716796875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 2017.5330810546875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1944.6859130859375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 1333.1693115234375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 1624.154541015625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 417.08831787109375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 724.916015625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 1827.6571044921875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 553.3336181640625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 891.5404663085938 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8532, loss_val: nan, pos_over_neg: 501.3624572753906 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 502.1820068359375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 753.1337280273438 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 305.1260070800781 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 781.4602661132812 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 770.9437866210938 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1542.9141845703125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 810.179443359375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1420.693603515625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 552.921142578125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 330.7018127441406 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 921.185546875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 379.52838134765625 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 2345.016357421875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.86, loss_val: nan, pos_over_neg: 649.2520141601562 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 485.4549255371094 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 710.1654663085938 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: -1195.8045654296875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8792, loss_val: nan, pos_over_neg: 538.5342407226562 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8534, loss_val: nan, pos_over_neg: 575.2193603515625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 789.5206909179688 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 595.0471801757812 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8521, loss_val: nan, pos_over_neg: 6681.05419921875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 1058.447021484375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1674.7215576171875 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 841.6788330078125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 488.98590087890625 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 674.2462768554688 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 1516.1168212890625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8567, loss_val: nan, pos_over_neg: 976.5150756835938 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1155.6490478515625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 652.5096435546875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8526, loss_val: nan, pos_over_neg: 4547.23291015625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8474, loss_val: nan, pos_over_neg: 1617.8409423828125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 1627.3470458984375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 1441.416748046875 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8531, loss_val: nan, pos_over_neg: 430.79925537109375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 418.6505126953125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8457, loss_val: nan, pos_over_neg: 888.1944580078125 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8504, loss_val: nan, pos_over_neg: 806.5783081054688 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8523, loss_val: nan, pos_over_neg: 1402.408203125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 1850.4818115234375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 2477.26171875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 782.3867797851562 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.841, loss_val: nan, pos_over_neg: 1059.1868896484375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1108.6300048828125 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 18421.58984375 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 7327.00244140625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 7809.33642578125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 560.8322143554688 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 550.2835693359375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 477.3525390625 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 606.0150146484375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 526.7847900390625 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 497.5501708984375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 324.8565979003906 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 470.3383483886719 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8585, loss_val: nan, pos_over_neg: 1313.53125 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 1844.1109619140625 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 931.9253540039062 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1279.5902099609375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8601, loss_val: nan, pos_over_neg: 862.0852661132812 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 647.5396728515625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1041.5531005859375 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 1833.994384765625 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 2458.228271484375 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1709.4061279296875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 785.3495483398438 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 499.63385009765625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 499.8020324707031 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 448.64361572265625 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8616, loss_val: nan, pos_over_neg: 537.8912963867188 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 1046.84375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 3684.382080078125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8669, loss_val: nan, pos_over_neg: 947.9563598632812 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8445, loss_val: nan, pos_over_neg: 3581.1962890625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 1307.2242431640625 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 1381.4508056640625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 8697.7568359375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 330.56243896484375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1454.9625244140625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 787.3121337890625 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.844, loss_val: nan, pos_over_neg: 586.7781982421875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 286.914306640625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8623, loss_val: nan, pos_over_neg: 273.7283630371094 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 408.0048828125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 626.2324829101562 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2482.665283203125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8613, loss_val: nan, pos_over_neg: 399.8953552246094 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 502.36248779296875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8579, loss_val: nan, pos_over_neg: 736.0884399414062 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: 576.7042846679688 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8639, loss_val: nan, pos_over_neg: 247.2111053466797 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 623.8096923828125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8473, loss_val: nan, pos_over_neg: 465.56390380859375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8468, loss_val: nan, pos_over_neg: 884.849609375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 309.28387451171875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 566.1668701171875 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 533.5142822265625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 1054.186767578125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 325.82598876953125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 463.7735900878906 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: 326.084716796875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1143.609130859375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 852.2100830078125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 487.670654296875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8516, loss_val: nan, pos_over_neg: 552.83984375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 1188.759765625 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 3179.70751953125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 432.0575256347656 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 434.7867126464844 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 402.88824462890625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8655, loss_val: nan, pos_over_neg: 378.0176086425781 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 89334.609375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 530.9576416015625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8471, loss_val: nan, pos_over_neg: -16102.4345703125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 1014.0775146484375 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 11359.234375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 2220.265869140625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 713.1948852539062 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 629.9130249023438 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 3585.678955078125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8637, loss_val: nan, pos_over_neg: 739.8639526367188 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 810.7025146484375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.87, loss_val: nan, pos_over_neg: 1062.8092041015625 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 612.255615234375 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 1247.734619140625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: -4710.86767578125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 6036.40966796875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8584, loss_val: nan, pos_over_neg: 1733.173828125 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.845, loss_val: nan, pos_over_neg: -5322.30322265625 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 16969.662109375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 2198.506591796875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8609, loss_val: nan, pos_over_neg: 490.8798522949219 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 2473.53271484375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8568, loss_val: nan, pos_over_neg: 690.4813232421875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 544.22802734375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 3285.8193359375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 2066.093017578125 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 802.565185546875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 464.7518615722656 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 1453.5965576171875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8525, loss_val: nan, pos_over_neg: 678.2716674804688 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8533, loss_val: nan, pos_over_neg: 1106.312255859375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: -8482.4560546875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 1309.1710205078125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8479, loss_val: nan, pos_over_neg: 693.0930786132812 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: -6642.55322265625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.855, loss_val: nan, pos_over_neg: 1153.153076171875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1564.833740234375 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8438, loss_val: nan, pos_over_neg: 520.2141723632812 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 359.7380676269531 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 650.6002807617188 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 2243.603515625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 492.8252258300781 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: -2264.5830078125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 2500.97265625 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 667.2232055664062 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: -35938.42578125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 2091.744873046875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 798.3357543945312 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: -6324.39599609375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 784.411865234375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 535.0762939453125 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8537, loss_val: nan, pos_over_neg: 549.2537231445312 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8456, loss_val: nan, pos_over_neg: 867.857666015625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 713.4089965820312 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 1035.400634765625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 1136.5467529296875 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 1576.58544921875 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1020.9102172851562 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 2125.3818359375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1124.5675048828125 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 628.8384399414062 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 589.2125244140625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 901.6432495117188 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 961.0066528320312 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 943.8526611328125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 1437.9273681640625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 391.1870422363281 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 4655.23046875 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 438.8931579589844 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 1326.46728515625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 466.6582946777344 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 548.6790161132812 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8482, loss_val: nan, pos_over_neg: 1615.2908935546875 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8417, loss_val: nan, pos_over_neg: 1042.8602294921875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 7160.52294921875 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 590.4283447265625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 309.65228271484375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: 458.7528076171875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1620.9801025390625 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1264.638916015625 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 716.5562133789062 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 934.6000366210938 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 767.5760498046875 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 596.284912109375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 744.9610595703125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8577, loss_val: nan, pos_over_neg: 534.3975830078125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 644.2820434570312 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.844, loss_val: nan, pos_over_neg: 3469.015380859375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 1115.7762451171875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 328.0389404296875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8432, loss_val: nan, pos_over_neg: 754.116455078125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 415.804443359375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 774.8496704101562 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 421.8898010253906 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8644, loss_val: nan, pos_over_neg: 910.5714721679688 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 711.8775634765625 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 324.9044494628906 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8417, loss_val: nan, pos_over_neg: 993.8720092773438 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 622.6137084960938 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 1099.7025146484375 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8536, loss_val: nan, pos_over_neg: 438.8740234375 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 756.6986083984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 684.1202392578125 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8626, loss_val: nan, pos_over_neg: 495.3412780761719 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 399.3384704589844 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 447.5902404785156 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8546, loss_val: nan, pos_over_neg: 500.2681579589844 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 547.5894775390625 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 568.5001220703125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1019.30908203125 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 708.7605590820312 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 444.4467468261719 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 629.7454833984375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8634, loss_val: nan, pos_over_neg: 537.4309692382812 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 1212.77734375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: -3735.404541015625 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 558.4554443359375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 665.031005859375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 523.00146484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 550.6627197265625 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 622.0020141601562 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 1464.08642578125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8467, loss_val: nan, pos_over_neg: 1707.5489501953125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8355, loss_val: nan, pos_over_neg: 476.94219970703125 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 600.1363525390625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 913.9561157226562 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 805.003173828125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1594.533203125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 432.34259033203125 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 315.8262023925781 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 575.0591430664062 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8476, loss_val: nan, pos_over_neg: 364.2408447265625 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 675.6857299804688 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: -3162.071044921875 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 1986.5408935546875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 857.8855590820312 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1858.892578125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 470.7485656738281 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8499, loss_val: nan, pos_over_neg: -3577.987060546875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 3988.211669921875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: 275.4056396484375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 147.75738525390625 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1148.3934326171875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8598, loss_val: nan, pos_over_neg: 306.4760437011719 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 351.9121398925781 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 808.6481323242188 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 303.1787109375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 1033.8880615234375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 667.7832641601562 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1791.3602294921875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 466.7815246582031 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8499, loss_val: nan, pos_over_neg: 792.0918579101562 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 491.3172302246094 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8477, loss_val: nan, pos_over_neg: 626.5946655273438 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1204.974853515625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 497.3778381347656 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 530.962890625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8573, loss_val: nan, pos_over_neg: 1023.0457763671875 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8513, loss_val: nan, pos_over_neg: 336.5376892089844 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8342, loss_val: nan, pos_over_neg: 424.84893798828125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 464.3945007324219 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 839.7589721679688 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2096.83837890625 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 357.0068664550781 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 424.1230773925781 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 1779.2225341796875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1219.69287109375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 405.77911376953125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 1189.2669677734375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8589, loss_val: nan, pos_over_neg: 311.71832275390625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 347.0991516113281 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 507.1873474121094 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 621.3656616210938 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8534, loss_val: nan, pos_over_neg: 1581.539306640625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 535.4490356445312 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 612.4412841796875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1113.627685546875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 6439.58203125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.852, loss_val: nan, pos_over_neg: 495.595458984375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 528.7750854492188 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 441.1551513671875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 658.77490234375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 711.541015625 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 1140.2374267578125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 548.9352416992188 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 720.7538452148438 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8524, loss_val: nan, pos_over_neg: 317.7551574707031 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 314.1240539550781 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 2179.70947265625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 522.3953857421875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8483, loss_val: nan, pos_over_neg: 526.9987182617188 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8545, loss_val: nan, pos_over_neg: 452.9750061035156 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: -81467.328125 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8682, loss_val: nan, pos_over_neg: 1627.657958984375 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8572, loss_val: nan, pos_over_neg: 5564.8720703125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 4940.0537109375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 414.8348693847656 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 1668.6590576171875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 2856.72607421875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8451, loss_val: nan, pos_over_neg: 1594.701416015625 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 2070.817138671875 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 1286.407470703125 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 721.400390625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8443, loss_val: nan, pos_over_neg: 341.47552490234375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 528.67041015625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8517, loss_val: nan, pos_over_neg: 851.7413940429688 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 659.3936767578125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 779.270751953125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 889.4264526367188 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 2217.010986328125 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 10295.9208984375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 3292.642333984375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1622.749267578125 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 2195.85205078125 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8484, loss_val: nan, pos_over_neg: 982.9451904296875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8448, loss_val: nan, pos_over_neg: 559.9746704101562 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 402.71234130859375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 1292.1544189453125 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 2265.04345703125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 1111.1605224609375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8428, loss_val: nan, pos_over_neg: 529.8400268554688 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 896.9290771484375 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 461.690185546875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 2152.5859375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 806.2136840820312 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 1090.3818359375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 998.4944458007812 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 545.6782836914062 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 1100.720947265625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 2795.94140625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8429, loss_val: nan, pos_over_neg: 7370.236328125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 789.4108276367188 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 389.9306640625 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 374.3068542480469 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 359.81805419921875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 415.0178527832031 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8494, loss_val: nan, pos_over_neg: 578.8458251953125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 708.3109741210938 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 445.3155212402344 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 424.2566833496094 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 1339.904052734375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: -11575.1044921875 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 4821.4228515625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 923.3274536132812 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 851.358154296875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1286.8878173828125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 988.4202270507812 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 12277.55859375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 3477.5185546875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 300.2544860839844 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 274.83203125 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8506, loss_val: nan, pos_over_neg: 709.7845458984375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.822, loss_val: nan, pos_over_neg: -3669.516845703125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8595, loss_val: nan, pos_over_neg: 1733.7965087890625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 2426.72314453125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 2334.940185546875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.849, loss_val: nan, pos_over_neg: -3554.90087890625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: -2324.300537109375 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 397.2493591308594 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 807.6107788085938 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 809.52099609375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 689.3311157226562 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8538, loss_val: nan, pos_over_neg: 351.9349365234375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 732.9874267578125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 676.471923828125 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 911.7964477539062 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 457.6138916015625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 922.5213012695312 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 44806.5078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1797.6190185546875 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1982.554931640625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 564.9769287109375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1305.364013671875 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 401.23651123046875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 1618.1026611328125 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 623.4459838867188 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 732.1924438476562 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 1207.6453857421875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 3174.325439453125 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 679346.375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8391, loss_val: nan, pos_over_neg: 779.3519287109375 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 388.2063903808594 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 255.00186157226562 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1162.5164794921875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8638, loss_val: nan, pos_over_neg: 351.86590576171875 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1619.19189453125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8616, loss_val: nan, pos_over_neg: 301.7975769042969 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8491, loss_val: nan, pos_over_neg: 425.9989013671875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 984.8508911132812 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 565.9537353515625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 745.7958984375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 803.2741088867188 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 650.1586303710938 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 679.8803100585938 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8346, loss_val: nan, pos_over_neg: 2445.9765625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1597.0623779296875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8511, loss_val: nan, pos_over_neg: 582.2315063476562 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 416.11444091796875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 828.3270263671875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 2186.32568359375 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 787.950927734375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 734.278076171875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8219, loss_val: nan, pos_over_neg: 1077.8970947265625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 790.9722290039062 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 543.7063598632812 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8485, loss_val: nan, pos_over_neg: 386.8588562011719 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 457.2557373046875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 649.0579833984375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 1932.40625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 373.6579284667969 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 484.4259033203125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1454.6822509765625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8582, loss_val: nan, pos_over_neg: 495.0986633300781 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 1082.1192626953125 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 932.945068359375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1399.9481201171875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8549, loss_val: nan, pos_over_neg: 253.8992919921875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 929.3449096679688 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8426, loss_val: nan, pos_over_neg: 1283.1146240234375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: -2817.801025390625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 768.496337890625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1310.1851806640625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 1262.6724853515625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 1188.521728515625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 555.585693359375 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8427, loss_val: nan, pos_over_neg: 749.79248046875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 476.011962890625 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 1753.5433349609375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 1120.702880859375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 3431.432861328125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 476.10028076171875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8465, loss_val: nan, pos_over_neg: 1017.336669921875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.843, loss_val: nan, pos_over_neg: 735.6171875 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8412, loss_val: nan, pos_over_neg: 500.6279296875 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8556, loss_val: nan, pos_over_neg: 217.67420959472656 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 4178.767578125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 8150.6279296875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 333.9430847167969 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1290.340576171875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 5617.1064453125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8564, loss_val: nan, pos_over_neg: 388.009765625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 622.8070068359375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 2761.533447265625 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: -14254.8837890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 814.2296752929688 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8459, loss_val: nan, pos_over_neg: 549.0142211914062 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 765.8764038085938 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.84, loss_val: nan, pos_over_neg: 612.96923828125 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 5503.49267578125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.847, loss_val: nan, pos_over_neg: 859.7573852539062 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 2205.130126953125 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8553, loss_val: nan, pos_over_neg: 388.5873107910156 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 711.0162963867188 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8507, loss_val: nan, pos_over_neg: 486.5859680175781 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8465, loss_val: nan, pos_over_neg: 376.1567687988281 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 708.6525268554688 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8571, loss_val: nan, pos_over_neg: 367.11541748046875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 1179.21484375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8528, loss_val: nan, pos_over_neg: 399.55682373046875 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 853.6284790039062 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 547.6143188476562 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8604, loss_val: nan, pos_over_neg: 1713.5648193359375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 855.8114624023438 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8469, loss_val: nan, pos_over_neg: -28195.521484375 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1553.647705078125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 909.2193603515625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8633, loss_val: nan, pos_over_neg: 577.0303344726562 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 409.7355041503906 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 365.6449890136719 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 358.3708801269531 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 590.06787109375 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 1485.899169921875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 525.2886352539062 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 490.284423828125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 570.7391357421875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1057.3126220703125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 3117.6796875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 96734.8515625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 467.5924987792969 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 374.1377258300781 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 527.8285522460938 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 290.3365478515625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 806.6060180664062 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8423, loss_val: nan, pos_over_neg: 445.2655029296875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8486, loss_val: nan, pos_over_neg: 764.4059448242188 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8512, loss_val: nan, pos_over_neg: 706.302978515625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 14599.9443359375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.843, loss_val: nan, pos_over_neg: 1091.40673828125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1748.5369873046875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 1426.468017578125 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1978.681884765625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1372.9102783203125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 307.84814453125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 383.53533935546875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 494.9018859863281 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 1155.4132080078125 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 610.3407592773438 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8505, loss_val: nan, pos_over_neg: 509.1431884765625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 392.7252197265625 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 2553.58837890625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 4579.5478515625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1528.7767333984375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 389.9655456542969 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 438.1468200683594 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 554.9794311523438 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 368.5992126464844 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 473.1827392578125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8599, loss_val: nan, pos_over_neg: 303.03704833984375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 594.1932373046875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 4435.63232421875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 3515.16259765625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 3708.280029296875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 886.4738159179688 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8519, loss_val: nan, pos_over_neg: 554.1129760742188 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 9009.4521484375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 2896.758056640625 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8466, loss_val: nan, pos_over_neg: 1667.5611572265625 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 2363.19580078125 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 684.3338012695312 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 3254.8173828125 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1119.3282470703125 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 518.7921142578125 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 551.565185546875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 855.5878295898438 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 717.83544921875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 645.0936279296875 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 6003.28662109375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 456.9342956542969 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 627.532470703125 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 3383.049560546875 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 4892.3125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 969.0841674804688 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 283.5102844238281 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 283.6214904785156 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8463, loss_val: nan, pos_over_neg: 2775.30126953125 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.863, loss_val: nan, pos_over_neg: 493.46673583984375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8408, loss_val: nan, pos_over_neg: 490.3393859863281 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1266.3743896484375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 769.5452880859375 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.842, loss_val: nan, pos_over_neg: 513.1185302734375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 2256.309326171875 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1893.877685546875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 1067.7047119140625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 412.15362548828125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 463.7403259277344 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 644.994140625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 1727.7161865234375 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 9532.4853515625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 875.346923828125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 605.5092163085938 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 473.0350646972656 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1296.9822998046875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 1100.82763671875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 353.7183837890625 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 595.9369506835938 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 444.6109619140625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 981.930908203125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 22397.51953125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 757.7805786132812 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 402.51971435546875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8552, loss_val: nan, pos_over_neg: 540.29833984375 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 1197.019287109375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 681.7850952148438 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 619.8715209960938 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 667.62841796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 1307.4527587890625 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1153.9754638671875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 2238.455322265625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8487, loss_val: nan, pos_over_neg: 908.8714599609375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 527.1109619140625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 2099.566650390625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 440.9059753417969 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 260.4726257324219 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 502.53131103515625 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 732.3170776367188 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 1941.491455078125 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 396.1759033203125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 717.8726196289062 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 430.54168701171875 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 369.6706237792969 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 551.618896484375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8421, loss_val: nan, pos_over_neg: 1724.8726806640625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8529, loss_val: nan, pos_over_neg: 562.6636352539062 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 552.2647094726562 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 1835.9793701171875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 933.912841796875 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 1389.685791015625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 1036.8740234375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 618.9827270507812 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 451.8885498046875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 559.10107421875 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 300.4645080566406 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 381.7170104980469 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 259.7263488769531 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 234.59390258789062 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1161.0045166015625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 1838.2353515625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1169.3392333984375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 339.1943359375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 641.6795654296875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 1130.834716796875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 298.11212158203125 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 471.1530456542969 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 327.6316223144531 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 375.0145263671875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 519.331298828125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 963.9797973632812 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 880.903564453125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1987.6580810546875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 3115.99609375 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 439.205810546875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.85, loss_val: nan, pos_over_neg: 555.9591674804688 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8508, loss_val: nan, pos_over_neg: 233.52857971191406 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 874.135498046875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 838.9825439453125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1178.896484375 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 3748.817138671875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 1591.871826171875 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1955.5614013671875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 2790.80908203125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8397, loss_val: nan, pos_over_neg: 860.1641845703125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 1909.942626953125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8439, loss_val: nan, pos_over_neg: 403.9610900878906 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 735.1278076171875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 880.1467895507812 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 763.2576293945312 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 420.1824951171875 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1331.27685546875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: -6540.8740234375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 438.09600830078125 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: -24612.259765625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: -6548.05419921875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 12108.78515625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 952.3846435546875 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 1996.7025146484375 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8393, loss_val: nan, pos_over_neg: 417.44512939453125 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 314.4626770019531 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 363.3477478027344 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 768.4861450195312 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 3058.799560546875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 867.5380249023438 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8481, loss_val: nan, pos_over_neg: 595.100830078125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8501, loss_val: nan, pos_over_neg: 492.9255065917969 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8496, loss_val: nan, pos_over_neg: 674.5592651367188 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8372, loss_val: nan, pos_over_neg: 1729.6712646484375 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 882.2268676757812 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 502.1742248535156 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 1044.401611328125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 1624.058349609375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1652.8438720703125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 2202.600830078125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 600.2238159179688 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 465.08807373046875 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8424, loss_val: nan, pos_over_neg: 317.76690673828125 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: -3702.45556640625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8377, loss_val: nan, pos_over_neg: 1042.788818359375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 777.604736328125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1550.507568359375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 560.4097900390625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 536.1901245117188 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.845, loss_val: nan, pos_over_neg: 432.7978820800781 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 336.25933837890625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 2076.32861328125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 1698.605224609375 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 1196.750732421875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 476.4013977050781 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 628.350830078125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 2629.58447265625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 1704.42529296875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 3938.37060546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 1200.8004150390625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 820.3158569335938 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 2147.270263671875 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 316.7895202636719 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 442.4298095703125 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 343.6575012207031 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 572.017822265625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 625.3546752929688 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 602.9475708007812 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 554.7772827148438 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 588.4009399414062 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 4262.4365234375 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 1846.4801025390625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 1033.1083984375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 939.0433349609375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.853, loss_val: nan, pos_over_neg: 242.0231475830078 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 564.666259765625 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 584.539306640625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8431, loss_val: nan, pos_over_neg: 251.10121154785156 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 375.4867858886719 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8488, loss_val: nan, pos_over_neg: 1375.061767578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 1326.239990234375 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 355.15374755859375 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 394.46832275390625 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 2354.37890625 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 453.7239074707031 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 16159.6689453125 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 545.3340454101562 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8492, loss_val: nan, pos_over_neg: 283.58624267578125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 508.02880859375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 628.903564453125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 2590.46923828125 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 1538.30419921875 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 652.0650634765625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 231.41859436035156 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.848, loss_val: nan, pos_over_neg: 482.5071105957031 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 2474.676025390625 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 912.9873046875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 481.2782897949219 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8472, loss_val: nan, pos_over_neg: 404.90130615234375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 650.966796875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 590.3845825195312 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 566.7587280273438 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 384.2112731933594 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 511.9076843261719 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 539.928955078125 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1999.1993408203125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: -47961.13671875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 12861.1796875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 366.8192138671875 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 1500.4141845703125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 865.7409057617188 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: -11289.5966796875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1787.4931640625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 142093.65625 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8368, loss_val: nan, pos_over_neg: 10851.6123046875 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 1283.1925048828125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 1002.498291015625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 681.068115234375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 6602.8154296875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 562.9443969726562 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 505.9974365234375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 342.5184631347656 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 660.445068359375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 21920.7734375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 789.9207153320312 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 1622.00537109375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: -2688.64501953125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1013.42333984375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8515, loss_val: nan, pos_over_neg: 384.68310546875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 512.465576171875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 446.8092956542969 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 427.5144958496094 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 9132.5771484375 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 2756.40478515625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 8327.1982421875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 2612.109130859375 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 1082.7640380859375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8331, loss_val: nan, pos_over_neg: 726.0943603515625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 778.6622314453125 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 3136.812744140625 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 734.7959594726562 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 605.6522216796875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8498, loss_val: nan, pos_over_neg: 968.0166625976562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 514.6896362304688 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 5997.0654296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8449, loss_val: nan, pos_over_neg: 630.8977661132812 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.839, loss_val: nan, pos_over_neg: 1675.73779296875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 567.569580078125 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 287.4620666503906 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 645.3057861328125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 426.6215515136719 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 420.9722595214844 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 8603.6416015625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: -5819.16650390625 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 1052.7271728515625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 1019.291015625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 785.3504028320312 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 650.895751953125 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 467.1831970214844 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 326.357421875 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1046.7144775390625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 546.5318603515625 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1856.2225341796875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1462.39599609375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 601.7301025390625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 7238.6162109375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: -4811.86279296875 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 9197.0556640625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 1050.4422607421875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/300000 [1:42:51<129040:50:08, 1548.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "Iter: 0/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 1332.977294921875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 1484.685791015625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1532.5133056640625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 791.9417724609375 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1258.7066650390625 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1033.0068359375 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8401, loss_val: nan, pos_over_neg: 681.4507446289062 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 3315.596923828125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8535, loss_val: nan, pos_over_neg: 401.41900634765625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 909.55322265625 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: -2270.199951171875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 352.6422119140625 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 989.9049072265625 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 437.8656921386719 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 347.5080871582031 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 5131.91748046875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 561.5304565429688 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 892.8121948242188 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 83607.25 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1025.3372802734375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 5518.82958984375 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8565, loss_val: nan, pos_over_neg: 957.1326293945312 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 3253.087890625 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8416, loss_val: nan, pos_over_neg: 741.8074340820312 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 846.715087890625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8376, loss_val: nan, pos_over_neg: 1336.360107421875 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 799.9405517578125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8472, loss_val: nan, pos_over_neg: 218.47418212890625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 307.6674499511719 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 1878.96875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 605.0100708007812 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 800.904296875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 504.6259765625 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 4428.6435546875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1490.435791015625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8385, loss_val: nan, pos_over_neg: 582.2889404296875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 15329.2119140625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 1115.164794921875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8357, loss_val: nan, pos_over_neg: 311.1371154785156 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 17176.9296875 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 2041.363525390625 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: 1719.0323486328125 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1201.6900634765625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 2437.6591796875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 576.1251220703125 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 370.4081115722656 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1212.996337890625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 569.0320434570312 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1389.311767578125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 2821.35791015625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8404, loss_val: nan, pos_over_neg: -1686.302734375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.836, loss_val: nan, pos_over_neg: 1600.299072265625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 799.7322387695312 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 494.7958679199219 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 3665.700927734375 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 606.4779052734375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 459.72216796875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 474.4653625488281 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 211.4830780029297 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 487.33892822265625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 1742.9578857421875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.8394, loss_val: nan, pos_over_neg: 504.836669921875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8378, loss_val: nan, pos_over_neg: 813.88916015625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 3885.6220703125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 9105.9267578125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 2568.827880859375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 412.53759765625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 896.703857421875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 630.588134765625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: -3077.18798828125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8447, loss_val: nan, pos_over_neg: 710.553955078125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8462, loss_val: nan, pos_over_neg: 459.3966064453125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 508.8385314941406 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 594.2413940429688 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1304.4012451171875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: -3950.345458984375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 620.1224975585938 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 316.8851013183594 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 588.585205078125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 595.7777709960938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1864.3427734375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 457.7340393066406 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 401.5248718261719 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 564.1952514648438 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 470.8522033691406 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 833.9461669921875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 469.420166015625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 319.4328918457031 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1319.4583740234375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 565.3174438476562 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 1065.6973876953125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 499.2113952636719 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 576.6680908203125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8339, loss_val: nan, pos_over_neg: 456.38653564453125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1950.5179443359375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 489.10064697265625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: -3969.060546875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 3785.95849609375 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 1472.166015625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8456, loss_val: nan, pos_over_neg: 897.1627197265625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8324, loss_val: nan, pos_over_neg: 397.0415954589844 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 732.1874389648438 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1009.7817993164062 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 2334.243896484375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 624.2557983398438 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 515.2042236328125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 746.38916015625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 4882.1298828125 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8305, loss_val: nan, pos_over_neg: 898.7767944335938 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1390.61572265625 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 7728.29443359375 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 1164.8193359375 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 708.223388671875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 1693.4833984375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 544.0813598632812 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 463.2973327636719 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 927.7211303710938 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 300.08941650390625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 543.7174682617188 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 340.3421630859375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8493, loss_val: nan, pos_over_neg: 332.3893127441406 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 697.1359252929688 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8348, loss_val: nan, pos_over_neg: 1523.2572021484375 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 404.6497497558594 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 254.81692504882812 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 631.4634399414062 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 571.6429443359375 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 784.202392578125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8444, loss_val: nan, pos_over_neg: 873.0725708007812 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 687.8368530273438 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 330.37249755859375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8435, loss_val: nan, pos_over_neg: 381.4262390136719 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 816.7776489257812 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1240.5089111328125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 669.846435546875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 378.4947509765625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 313.7902526855469 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8363, loss_val: nan, pos_over_neg: 267.8889465332031 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 326.2873229980469 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 378.5701599121094 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8502, loss_val: nan, pos_over_neg: 385.5609436035156 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 653.7935791015625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 923.7322387695312 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 661.9517822265625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1131.3475341796875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 225.22218322753906 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 372.5697937011719 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1299.2449951171875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8418, loss_val: nan, pos_over_neg: 361.9891052246094 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 372.64276123046875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 517.1666870117188 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 316.275634765625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 532.7857055664062 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.818, loss_val: nan, pos_over_neg: -11127.8623046875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: -9591.6982421875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8454, loss_val: nan, pos_over_neg: 1750.571044921875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 739.1272583007812 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 347.0636901855469 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 1202.305908203125 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 3752.77001953125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8458, loss_val: nan, pos_over_neg: 938.3859252929688 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 2831.962158203125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 577.5415649414062 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 544.5472412109375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 2621.414306640625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 1901.5020751953125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 415.2477111816406 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 845.399169921875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 557.46630859375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 835.154052734375 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8437, loss_val: nan, pos_over_neg: 401.537353515625 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 751.5904541015625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8382, loss_val: nan, pos_over_neg: 212.2986602783203 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1441.261474609375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 471.0311279296875 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 789.8221435546875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 501.2950439453125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 631.1616821289062 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 1643.36376953125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 2113.071044921875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8415, loss_val: nan, pos_over_neg: 1493.432861328125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 327.859130859375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8478, loss_val: nan, pos_over_neg: 286.7003479003906 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 637.7052612304688 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 921.6363525390625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 496.11883544921875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 417.8884582519531 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 478.02783203125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 1286.2841796875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 877.2488403320312 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1277.7572021484375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 796.6677856445312 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 566.4200439453125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 1302.802734375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 3927.8544921875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: -68023.6953125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 1066.2122802734375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: -28340.412109375 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: -6726.63134765625 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 540.6618041992188 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 426.5282897949219 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 247.16477966308594 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1319.8985595703125 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2015.4466552734375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1276.0423583984375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 986.5070190429688 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1057.2901611328125 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 595.474365234375 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 3254.940185546875 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: -1307.309814453125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 5404.859375 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 779.8331298828125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1159.2725830078125 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 1136.4420166015625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 701.583251953125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 435.496826171875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 930.9888916015625 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8366, loss_val: nan, pos_over_neg: 1902.067626953125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 791.2398071289062 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 901.8023071289062 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 632.0923461914062 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 606.8876953125 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 747.881591796875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 2199.23974609375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 1121.7476806640625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 630.2175903320312 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 648.1704711914062 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 849.5804443359375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 752.3195190429688 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 355.8598937988281 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 1141.11181640625 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1028.32861328125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 854.9646606445312 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8403, loss_val: nan, pos_over_neg: 739.0265502929688 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.8581, loss_val: nan, pos_over_neg: 343.7023620605469 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 657.5521850585938 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1222.4049072265625 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: -5756.69677734375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 7403.9794921875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 419.2969665527344 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 378.849365234375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 259.0028381347656 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1280.659912109375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 7152.5751953125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 877.4642333984375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 837.91845703125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 1906.8060302734375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 1240.8394775390625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 1351.5032958984375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 891.2208251953125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 679.0038452148438 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1625.0465087890625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 502.2060546875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1259.7958984375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 1296.81201171875 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 2101.664794921875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 915.6483764648438 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 480.78277587890625 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 303.39862060546875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 354.5715026855469 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1321.5576171875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1007.8419799804688 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 388.9189758300781 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 385.59063720703125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: -12114.3408203125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2699.5947265625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1173.9462890625 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1062.7423095703125 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 953.7328491210938 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8414, loss_val: nan, pos_over_neg: 279.8558044433594 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.8386, loss_val: nan, pos_over_neg: 398.865966796875 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1002.6803588867188 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 888.5496826171875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 1108.00537109375 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 712.3785400390625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8433, loss_val: nan, pos_over_neg: 362.43780517578125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.827, loss_val: nan, pos_over_neg: 658.3473510742188 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: -16483.34375 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.826, loss_val: nan, pos_over_neg: -1588.6041259765625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 10929.3740234375 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: -2304.967529296875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1100.7484130859375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8355, loss_val: nan, pos_over_neg: 814.3383178710938 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 422.5269775390625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 339.8081359863281 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 546.6654052734375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 303.2270812988281 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 821.5541381835938 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1553.032470703125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 9717.5009765625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -7948.88134765625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 6319.552734375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: -2283.120361328125 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8273, loss_val: nan, pos_over_neg: 599.8659057617188 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8442, loss_val: nan, pos_over_neg: 401.37738037109375 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 978.6744995117188 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: -4063.55224609375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 8283.8232421875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 3989.233154296875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 2094.40771484375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 680.2874755859375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 9410.626953125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1668.599609375 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 2670.482177734375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8497, loss_val: nan, pos_over_neg: 506.2670593261719 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 403.44903564453125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 302.38116455078125 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 4065.39306640625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: -27553.623046875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 814.8827514648438 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 782.4811401367188 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 3073.23974609375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 1893.6221923828125 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 498.51214599609375 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 458.96722412109375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 476.3731689453125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 627.7702026367188 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 941.4517211914062 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 604.1649169921875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 4814.6865234375 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1764.650634765625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 780.093505859375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 589.0476684570312 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 3231.92724609375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 11459.15234375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 60174.46484375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 1013.8057250976562 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1045.774658203125 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 862.708251953125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 718.5961303710938 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8307, loss_val: nan, pos_over_neg: 382.5260314941406 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 1415.4755859375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 313.9555969238281 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: -2822.6357421875 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 458038.875 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 613.0665283203125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 1035.779541015625 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8314, loss_val: nan, pos_over_neg: 490.7485046386719 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 640.5882568359375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 645.2023315429688 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 554.1783447265625 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 576.4287719726562 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1363.1292724609375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 500.84228515625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 936.8641967773438 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 1686.1806640625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 651.2060546875 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 2390.88232421875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1319.705322265625 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 308.09906005859375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 506.1529541015625 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 531.0397338867188 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 803.3409423828125 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8425, loss_val: nan, pos_over_neg: 525.9942626953125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 1032.3323974609375 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 595.1182250976562 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 2564.89404296875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 2595.46923828125 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 47113.73046875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 4172.24267578125 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.833, loss_val: nan, pos_over_neg: 494.31256103515625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 326.4202880859375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 487.2355041503906 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 579.195068359375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8295, loss_val: nan, pos_over_neg: 797.814697265625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 843.0720825195312 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8352, loss_val: nan, pos_over_neg: 443.0742492675781 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 3575.9970703125 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8294, loss_val: nan, pos_over_neg: 3330.836669921875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 11410.5888671875 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8475, loss_val: nan, pos_over_neg: 377.5028991699219 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8453, loss_val: nan, pos_over_neg: 2220.286865234375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 928.0565185546875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 2600.8798828125 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.83, loss_val: nan, pos_over_neg: 502.9117126464844 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 12454.291015625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 560.6082763671875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 465.0947265625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 837.607177734375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1502.166015625 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8542, loss_val: nan, pos_over_neg: 253.49305725097656 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 1280.677978515625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 499.69989013671875 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 726.1085815429688 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 362.8819885253906 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 584.115966796875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 867.0152587890625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 713.7159423828125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 347.0951843261719 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 714.865966796875 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 283.6891174316406 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 968.9070434570312 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 2049.837890625 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 1689.0792236328125 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 799.9360961914062 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8452, loss_val: nan, pos_over_neg: 1178.43310546875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 992.7079467773438 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 1629.3170166015625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 531.9442138671875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1467.8265380859375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8285, loss_val: nan, pos_over_neg: 1176.0802001953125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 639.2720947265625 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 539.3677978515625 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 1043.039794921875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1194.6895751953125 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 1231.6241455078125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8402, loss_val: nan, pos_over_neg: 680.9986572265625 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 803.8125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1877.201904296875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 979.7880859375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 1440.1942138671875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 416.0003662109375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1018.6922607421875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1123.80029296875 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 491.3175964355469 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 386.236328125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 421.0166320800781 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1292.3651123046875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.836, loss_val: nan, pos_over_neg: 1894.5389404296875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 1336.094482421875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8411, loss_val: nan, pos_over_neg: 636.9359130859375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 1034.3328857421875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 402.6874694824219 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 814.5682373046875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 680.26611328125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 2562.435546875 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 716.2886352539062 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 951.3850708007812 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1049.9688720703125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 531.50830078125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 1112.760498046875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 2978.051025390625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 559.7365112304688 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 659.80126953125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 1736.0706787109375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 764.88134765625 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 254.31153869628906 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 777.1689453125 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.8387, loss_val: nan, pos_over_neg: 513.7797241210938 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 882.8681030273438 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 845.223388671875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 352.8528747558594 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1227.42724609375 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 466.5658264160156 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 1158.400634765625 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 402.0037536621094 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 224.29251098632812 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 318.07489013671875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 833.8655395507812 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.83, loss_val: nan, pos_over_neg: 980.285888671875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 569.3095092773438 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8319, loss_val: nan, pos_over_neg: 3018.128662109375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 2475.52734375 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 452.05303955078125 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8356, loss_val: nan, pos_over_neg: 548.4697265625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8422, loss_val: nan, pos_over_neg: 445.1709899902344 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 1081.7984619140625 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 1179.7601318359375 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 679.058349609375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 863.1838989257812 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 495.3688659667969 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 749.0232543945312 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1547.3509521484375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 547.1207885742188 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 560.7462768554688 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8334, loss_val: nan, pos_over_neg: 618.8585205078125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 951.8281860351562 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 2383.976806640625 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 13491.0087890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 727.2293701171875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 898.4384155273438 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 480.94891357421875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 1965.9822998046875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8353, loss_val: nan, pos_over_neg: 1541.4703369140625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8407, loss_val: nan, pos_over_neg: 388.7671813964844 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 644.2405395507812 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.834, loss_val: nan, pos_over_neg: 366.14111328125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 529.2120361328125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8362, loss_val: nan, pos_over_neg: 1450.700927734375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1612.4420166015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 585.9899291992188 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8363, loss_val: nan, pos_over_neg: 1210.927001953125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 631.8440551757812 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 380.18609619140625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 883.8700561523438 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 494.0263366699219 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 759.343017578125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 627.7665405273438 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 1592.46337890625 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 751.2334594726562 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 6453.40087890625 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1515.7825927734375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 1196.05615234375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 620.2299194335938 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1053.162841796875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 946.5365600585938 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8371, loss_val: nan, pos_over_neg: 486.86553955078125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 845.4725341796875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1349.424560546875 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 721.1851806640625 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 1416.4140625 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1533.6082763671875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 685.8843994140625 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 457.4696350097656 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 741.773193359375 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 721.0914916992188 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8419, loss_val: nan, pos_over_neg: 824.1072387695312 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 195.5948944091797 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8406, loss_val: nan, pos_over_neg: 482.1943664550781 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 542.8978881835938 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 656.5103759765625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 1117.93505859375 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 477.54827880859375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8344, loss_val: nan, pos_over_neg: 752.3211059570312 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 549.9222412109375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: -2703.631103515625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1459.6956787109375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 4371.1572265625 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 467.76422119140625 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 338.7753601074219 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 348.56390380859375 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 744.920166015625 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 324.9574279785156 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 382.504638671875 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 409.9117431640625 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 602.6578979492188 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 405.940673828125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 3874.65234375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 496.09393310546875 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 302.9256896972656 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 559.2955322265625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 936.3646240234375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 889.7716674804688 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 517.2539672851562 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 518.5484619140625 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 325.6572570800781 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 2511.072265625 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1090.8873291015625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 765.6608276367188 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 226.51593017578125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 355.09698486328125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 419.7475280761719 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 486.6337890625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 724.6478881835938 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1246.854736328125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1338.7808837890625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 6115.4755859375 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1676.28662109375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: -3626.93408203125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 261.805908203125 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 397.7799987792969 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 488.2709655761719 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1341.5465087890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8436, loss_val: nan, pos_over_neg: 337.1330261230469 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 604.7180786132812 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 1045.193115234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 350.6950378417969 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 908.7991333007812 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8399, loss_val: nan, pos_over_neg: 2545.064208984375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 482.57269287109375 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 209.14430236816406 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 694.9153442382812 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1917.8463134765625 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 1239.0733642578125 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 1243.1544189453125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8361, loss_val: nan, pos_over_neg: 366.3664855957031 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 714.1109008789062 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8283, loss_val: nan, pos_over_neg: 398.4012145996094 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1081.8419189453125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 3145.56201171875 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1602.341064453125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8392, loss_val: nan, pos_over_neg: 719.291015625 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 547.235107421875 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 7101.02294921875 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1474.9754638671875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8409, loss_val: nan, pos_over_neg: 258.9355773925781 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8333, loss_val: nan, pos_over_neg: 1207.8402099609375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 726.1583251953125 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 573.9296264648438 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 546.7186889648438 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 437.2215881347656 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 795.926025390625 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1249.3272705078125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 2381.31103515625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 548.3674926757812 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 965.8153076171875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1255.9437255859375 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1082.0987548828125 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 1709.52490234375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: -6785.328125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 1032.308837890625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 931.8612060546875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1003.1881103515625 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8347, loss_val: nan, pos_over_neg: 807.4564819335938 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 808.9000854492188 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1267.2177734375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8287, loss_val: nan, pos_over_neg: 1500.719970703125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 476.7751159667969 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 565.9683837890625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 508.9958190917969 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 920.8079833984375 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 3640.538330078125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 716.5108642578125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8413, loss_val: nan, pos_over_neg: 1131.8572998046875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1278.05322265625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 4701.22900390625 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 2395.8095703125 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 1335.98193359375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 952.525146484375 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 475.32769775390625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8326, loss_val: nan, pos_over_neg: 1254.8372802734375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 859.349365234375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 32169.06640625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1057.35107421875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 442.54779052734375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 670.5680541992188 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: -7778.9970703125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1547.025390625 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1343.5230712890625 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 853.6646118164062 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 50528.828125 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1733.6746826171875 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8306, loss_val: nan, pos_over_neg: 2310.287841796875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 329.6921691894531 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1273.511474609375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 488.6058044433594 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 982.937255859375 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 997.4935913085938 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 675.2134399414062 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 467.65399169921875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 731.4187622070312 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 666.2911376953125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 34672.17578125 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 3862.48779296875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: -2921.830322265625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 420.83856201171875 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 325.919189453125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 330.2157897949219 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 2042.6505126953125 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1090.47021484375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 3764.365966796875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8281, loss_val: nan, pos_over_neg: 372.6037292480469 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8253, loss_val: nan, pos_over_neg: 1232.249267578125 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 3684.033447265625 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 821.157958984375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 5613.544921875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8317, loss_val: nan, pos_over_neg: 314.56494140625 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 525.1023559570312 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 452.8927307128906 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 383.921142578125 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8256, loss_val: nan, pos_over_neg: 506.7235412597656 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 450.0738830566406 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1785.3173828125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8301, loss_val: nan, pos_over_neg: 3559.0224609375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1391.57763671875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 409.33843994140625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1126.043212890625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1954.8409423828125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 826.6649780273438 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 2649.703125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 1484.2552490234375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 493.2529602050781 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1690.2982177734375 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 5908.67724609375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 749.0816040039062 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 918.1444091796875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.838, loss_val: nan, pos_over_neg: 254.39796447753906 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 546.6922607421875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 1084.2186279296875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 433.2651062011719 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 980.2667236328125 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 8323.0654296875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 971.7371826171875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 1436.3021240234375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8434, loss_val: nan, pos_over_neg: 1059.8087158203125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 731.1739501953125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 606.4662475585938 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 790.1185302734375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: -1502.8934326171875 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1094.00439453125 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 10075.6650390625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 731.3272705078125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 2806.200439453125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 453.24163818359375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 209.1885528564453 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8335, loss_val: nan, pos_over_neg: 279.67230224609375 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 1221.248291015625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 604.142578125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 831.346923828125 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 1365.7718505859375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 4080.95751953125 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 713.7479248046875 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 761.8110961914062 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8373, loss_val: nan, pos_over_neg: 380.55743408203125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 534.9170532226562 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8446, loss_val: nan, pos_over_neg: 357.8876953125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 715.7265014648438 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: -7877.53271484375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 2076.7158203125 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 2522.14990234375 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 685.4041748046875 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1024.259033203125 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8379, loss_val: nan, pos_over_neg: 493.9466247558594 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 1280.528076171875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 971.5358276367188 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: -9400.3564453125 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 926.1844482421875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1805.360595703125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 407.2812805175781 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 345.639892578125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 1016.3150634765625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8229, loss_val: nan, pos_over_neg: 2793.54638671875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1003.8060302734375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 1154.865234375 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 770.142333984375 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 2673.192626953125 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8235, loss_val: nan, pos_over_neg: 788.3577270507812 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 1147.5419921875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8243, loss_val: nan, pos_over_neg: 557.9237670898438 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 442.4327392578125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 704.52197265625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8383, loss_val: nan, pos_over_neg: 409.6075439453125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 810.2393798828125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 998.1534423828125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8359, loss_val: nan, pos_over_neg: 286.4176025390625 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1333.9254150390625 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 872.1337280273438 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 2080.51513671875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: -5753.6064453125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8388, loss_val: nan, pos_over_neg: 815.810791015625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 807.3743896484375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 2471.053955078125 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8264, loss_val: nan, pos_over_neg: 777.4789428710938 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1248.837646484375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 718.5150146484375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8461, loss_val: nan, pos_over_neg: 339.1204528808594 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1233.649169921875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 7672.80322265625 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 578.0138549804688 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 873.5293579101562 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8219, loss_val: nan, pos_over_neg: 392.9130554199219 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 490.6878356933594 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 1336.8072509765625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 2429.114501953125 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 698.3017578125 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 899.4212646484375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 471.9472961425781 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 468.79620361328125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 813.2880859375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: -3084.616943359375 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 1338.36376953125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 572.1549682617188 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 803.8450317382812 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1785.6363525390625 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1580.8192138671875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 867.9439086914062 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1437.771484375 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1298.640625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1360.4356689453125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 3096.537353515625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: -35339.4453125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 1306.58837890625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 525.2317504882812 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 851.48193359375 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1511.3214111328125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 2414.15576171875 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1478.142333984375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8296, loss_val: nan, pos_over_neg: 470.63824462890625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1272.480712890625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 825.371826171875 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 393.428955078125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 517.967041015625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1819.80517578125 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 3761.033203125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 561.3451538085938 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 1127.1248779296875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.809, loss_val: nan, pos_over_neg: -12340.3388671875 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 2022.857666015625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 791.08935546875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 443.3926086425781 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 416.7926940917969 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 575.5101318359375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 280.31878662109375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 7608.77294921875 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 762.8257446289062 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 596.711669921875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 1033.37109375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1161.166259765625 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.806, loss_val: nan, pos_over_neg: -2876.95361328125 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 742.2205810546875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 1310.3504638671875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 493.163330078125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1574.25244140625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 419.4698791503906 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 743.2050170898438 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 377.2100524902344 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 547.3235473632812 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 718.1007690429688 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2448.676513671875 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 1447.86767578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 1219.9705810546875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8375, loss_val: nan, pos_over_neg: 259.4275817871094 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 712.9760131835938 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 495.9917907714844 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 792.7701416015625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 1053.4498291015625 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 727.1722412109375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 583.0320434570312 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 591.0133666992188 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 513.263916015625 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 609.3976440429688 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 1022.3169555664062 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 723.693359375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 549.00634765625 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8341, loss_val: nan, pos_over_neg: 525.310546875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 519.880126953125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 334.1871643066406 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 733.1582641601562 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 3406.626953125 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1514.104736328125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 659.0872192382812 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 832.5717163085938 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1846.41162109375 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 863.5179443359375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8396, loss_val: nan, pos_over_neg: 399.843505859375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8222, loss_val: nan, pos_over_neg: 728.6557006835938 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 1283.4254150390625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.8292, loss_val: nan, pos_over_neg: 1752.5843505859375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8322, loss_val: nan, pos_over_neg: 359.6402893066406 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 743.2840576171875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 328.158935546875 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: -3044.572265625 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 2116.5693359375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 974.1813354492188 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1145.23876953125 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 480.22698974609375 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.8271, loss_val: nan, pos_over_neg: 2529.034423828125 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 681.5668334960938 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1462.25341796875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1113.8033447265625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 956.2608032226562 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.8369, loss_val: nan, pos_over_neg: 428.37744140625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 580.15771484375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1339.3927001953125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 1953.729736328125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8351, loss_val: nan, pos_over_neg: 491.23297119140625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.8304, loss_val: nan, pos_over_neg: 356.0769958496094 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 2283.753662109375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 1208.8043212890625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 627.02197265625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1059.1715087890625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 2910.15478515625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1015.304931640625 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 3688.19580078125 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 737.9661865234375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 3803.105224609375 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8328, loss_val: nan, pos_over_neg: 634.0650634765625 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8336, loss_val: nan, pos_over_neg: 503.62359619140625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 475.3443908691406 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 959.2413940429688 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8398, loss_val: nan, pos_over_neg: 717.6209716796875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 518.14697265625 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 522.8428344726562 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 10528.8603515625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 940.0108642578125 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 10926.9501953125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 2137.220703125 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: -2514.683837890625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.8312, loss_val: nan, pos_over_neg: 515.2659912109375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 3050.4599609375 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 4118.826171875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 2677.645751953125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 5134.279296875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 517.522705078125 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 545.656494140625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 389.4539489746094 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 557.54345703125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 347.1061096191406 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 798.2303466796875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8286, loss_val: nan, pos_over_neg: 1637.0140380859375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 1770.296142578125 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 879.233642578125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 3178.623046875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8374, loss_val: nan, pos_over_neg: 572.02001953125 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 373.94287109375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 370.78778076171875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8276, loss_val: nan, pos_over_neg: 402.81195068359375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 2250.693603515625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8354, loss_val: nan, pos_over_neg: 689.7123413085938 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/300000 [2:08:51<129366:41:38, 1552.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5\n",
      "Iter: 0/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 989.0838623046875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8311, loss_val: nan, pos_over_neg: 1221.1290283203125 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 845.54150390625 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 856.6835327148438 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 1451.4873046875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 725.66845703125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1577.31640625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 379.6065979003906 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 728.5592041015625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 473.2219543457031 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 6219.79833984375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: -1939.3387451171875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 902.352294921875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 3025.76708984375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8309, loss_val: nan, pos_over_neg: 667.1117553710938 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1438.897216796875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 393.3723449707031 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1006.2974243164062 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 645.5481567382812 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 1037.7734375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 1036.6669921875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 721.7974853515625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1226.8751220703125 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8316, loss_val: nan, pos_over_neg: 428.1856384277344 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: -2745.848388671875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -4366.19189453125 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 746.9970092773438 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8343, loss_val: nan, pos_over_neg: 617.6188354492188 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2038.8172607421875 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 5960.6044921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 33598.578125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8455, loss_val: nan, pos_over_neg: 359.9823303222656 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 514.6065673828125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8495, loss_val: nan, pos_over_neg: 269.8260803222656 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 630.8137817382812 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8315, loss_val: nan, pos_over_neg: 298.63720703125 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 742.7550048828125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 2744.17138671875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 633.1292114257812 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 2028.5001220703125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 2521.55859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 721.40478515625 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 1253.45263671875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 900.5717163085938 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1471.5789794921875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 591.1007690429688 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 1118.99609375 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 596.9722900390625 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 695.9600830078125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 446.3424072265625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 769.5116577148438 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.8384, loss_val: nan, pos_over_neg: 704.1669311523438 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 4838.81494140625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 373.81207275390625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 1945.6944580078125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 6126.73486328125 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.8405, loss_val: nan, pos_over_neg: 555.9190063476562 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 688.4104614257812 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: -61502.5078125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: -10040.1611328125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 737.0696411132812 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.821, loss_val: nan, pos_over_neg: 873.8475341796875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 487.5276794433594 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 933.648193359375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 606.934326171875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: -6118.1728515625 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 533.809326171875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 556.9136352539062 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 885.8910522460938 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 1192.863525390625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 5838.81787109375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 698.15673828125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1839.8804931640625 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 8469.21484375 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8244, loss_val: nan, pos_over_neg: 769.3206787109375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 958.5166625976562 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 1149.531982421875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 219.64019775390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 815.3682861328125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 3472.9609375 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 2328.48583984375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 2532.648193359375 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 6622.4580078125 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 517.2139892578125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: -21550.728515625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 2159.064697265625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 957.1785278320312 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.8268, loss_val: nan, pos_over_neg: 1204.307861328125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 1892.5426025390625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 402.7742614746094 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1491.9019775390625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: -8181.9453125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: -9846.703125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: -3188.872802734375 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: -3075.926513671875 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 767.5653686523438 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 692.1511840820312 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 295.2706604003906 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1153.834228515625 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1916.5980224609375 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1518.50537109375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -3841.612548828125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.802, loss_val: nan, pos_over_neg: -8326.583984375 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 920.7249145507812 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.8234, loss_val: nan, pos_over_neg: 1089.9783935546875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 1074.1956787109375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 2924.68115234375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 6430.12744140625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1436.8076171875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 328.5530090332031 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 601.1483154296875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 907.4114379882812 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1833.2265625 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 1001.1336059570312 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 7316.6865234375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 1048.370849609375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8261, loss_val: nan, pos_over_neg: 613.882080078125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 1580.9652099609375 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: -11855.9716796875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 573.5512084960938 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 3105.189208984375 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 1750.41552734375 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 675.9606323242188 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 647.955078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 808.9297485351562 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 668.4891967773438 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 941.5262451171875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1132.1966552734375 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 871.5074462890625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 615.7044677734375 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 1001.192626953125 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 613.576416015625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 870.52880859375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8338, loss_val: nan, pos_over_neg: 282.1803894042969 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 605.514404296875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 348.60308837890625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 575.6675415039062 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8256, loss_val: nan, pos_over_neg: 813.82568359375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 947.0191650390625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 494.40093994140625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 1342.822265625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 235.46902465820312 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1477.4390869140625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 4115.97900390625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8267, loss_val: nan, pos_over_neg: 708.1360473632812 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1600.1307373046875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 422.4645080566406 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 407.4665832519531 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 1054.983154296875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1395.5731201171875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8302, loss_val: nan, pos_over_neg: 622.7532958984375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: -8203.056640625 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 1011.4118041992188 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 472.635498046875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: -3278.173583984375 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 5061.2333984375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1141.1544189453125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 511.05194091796875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 458.4585266113281 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 3020.554931640625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8284, loss_val: nan, pos_over_neg: 369.2686462402344 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 619.0753173828125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 2340.090576171875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 703.989990234375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 763.6962890625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8337, loss_val: nan, pos_over_neg: 540.1134643554688 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 947.9881591796875 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 1420.829345703125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 913.5267944335938 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 624.5708618164062 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 831.8857421875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8216, loss_val: nan, pos_over_neg: 1275.4666748046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8297, loss_val: nan, pos_over_neg: 1984.084716796875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 698.4678955078125 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 526.9450073242188 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 842.3514404296875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 754.8310546875 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: -4780.2802734375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 3140.106689453125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 702.9301147460938 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.8212, loss_val: nan, pos_over_neg: 571.289794921875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 1079.470947265625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 522.232177734375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.826, loss_val: nan, pos_over_neg: 346.78466796875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 1044490.8125 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 763.9404296875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: -6302.93408203125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 8952.9541015625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: -5456.05712890625 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 5631.83740234375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 15906.96875 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 4404.63427734375 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1038.915283203125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 5586.08642578125 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2070.622802734375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1086.4285888671875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1320.5150146484375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 893.3076171875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 979.0860595703125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 398.8391418457031 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.832, loss_val: nan, pos_over_neg: 415.7236022949219 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 763.0564575195312 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 814.6751098632812 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1983.533203125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1153.6746826171875 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 2970.417724609375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 697.7621459960938 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 550.1764526367188 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 1259.14892578125 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 933.90576171875 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 373.3641662597656 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 464.2696533203125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 8741.1337890625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8277, loss_val: nan, pos_over_neg: 1290.6024169921875 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 1842.1715087890625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 878.77587890625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 510.6976623535156 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 2816.7255859375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 1045.8779296875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1562.9930419921875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1634.4482421875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 550.4178466796875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8358, loss_val: nan, pos_over_neg: 232.38778686523438 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 392.75030517578125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 517.3832397460938 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 330.51763916015625 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 610.9219970703125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 416.8913879394531 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 472.3199768066406 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 876.02197265625 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 730.3228149414062 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 2746.151611328125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 394.1680908203125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 492.0835876464844 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1192.31787109375 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 1102.0919189453125 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 2447.51513671875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8464, loss_val: nan, pos_over_neg: 399.6695861816406 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 740.656494140625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 351.8692321777344 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 4350.54248046875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 795.9613037109375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 683.093994140625 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 519.0671997070312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 602.498779296875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: -6188.74658203125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 566.5847778320312 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 6499.0263671875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 1541.2086181640625 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1537.9073486328125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 267.85321044921875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 1046.7523193359375 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 449.7471923828125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 752.7919921875 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 849.641845703125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 484.8435974121094 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 801.2799072265625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8233, loss_val: nan, pos_over_neg: 475.2546081542969 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8298, loss_val: nan, pos_over_neg: 793.8125610351562 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: -4941.22900390625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 571.7847900390625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1143.84326171875 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1109.5457763671875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 4258.8125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 819.0072021484375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1004.7711791992188 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 446.5078125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 782.5926513671875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.8349, loss_val: nan, pos_over_neg: 487.06744384765625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 514.9410400390625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1139.679931640625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 860.5619506835938 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1862.9254150390625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 742.805908203125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 7637.7060546875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 959.9682006835938 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 772.1672973632812 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 525.9911499023438 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 4502.02978515625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.8227, loss_val: nan, pos_over_neg: 429.7242736816406 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 2352.127685546875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 664.6831665039062 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 11749.9892578125 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 2752.31494140625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1446.08251953125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 3045.39990234375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8325, loss_val: nan, pos_over_neg: 393.0926818847656 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8269, loss_val: nan, pos_over_neg: 759.8732299804688 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 2717.077392578125 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 580.5446166992188 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 616.1874389648438 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 264.5180358886719 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 1312.36865234375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 545.5335083007812 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.835, loss_val: nan, pos_over_neg: 288.67169189453125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 960.1719970703125 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 984.6744384765625 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 2072.802978515625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 437.6721496582031 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: -8274.1787109375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: -24457.98046875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: -8639.673828125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: -32695.900390625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: -2226.66796875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 668.988037109375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 494.0677185058594 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.831, loss_val: nan, pos_over_neg: 504.84130859375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 344.0785217285156 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 782.6946411132812 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 626.8689575195312 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 457.0069580078125 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 358.7001647949219 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 453.82220458984375 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 563.6161499023438 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 777.8683471679688 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 716.8230590820312 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1223.997314453125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1079.70947265625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 712.2247924804688 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 1043.2615966796875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 2466.78369140625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1751.818359375 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: -5468.5 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 543.9286499023438 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 920.7670288085938 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 750.187255859375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1102.91796875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 664.554443359375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 189022.796875 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 1908.3350830078125 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8365, loss_val: nan, pos_over_neg: 435.2919616699219 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 861.5009155273438 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1066.3016357421875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 1192.3594970703125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 526.9826049804688 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 367.1149597167969 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 959.9210205078125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 426.16192626953125 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 756.305419921875 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 663.0230712890625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8215, loss_val: nan, pos_over_neg: 1159.021484375 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 27298.796875 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 4075.52490234375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.8255, loss_val: nan, pos_over_neg: 268.86199951171875 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 819.5276489257812 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 696.24658203125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 1139.919189453125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 2335.444091796875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 3160.25927734375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8203, loss_val: nan, pos_over_neg: 445.9900817871094 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 402.0126953125 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 409.17578125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 401.4203796386719 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8345, loss_val: nan, pos_over_neg: 343.34033203125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 518.2886352539062 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 1332.2764892578125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: -3264.57177734375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.792, loss_val: nan, pos_over_neg: -5294.25390625 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 5456.5263671875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2149.199462890625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.8226, loss_val: nan, pos_over_neg: 283.80731201171875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 911.2183227539062 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 867.2531127929688 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 400.1981201171875 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 657.1654663085938 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2032.0531005859375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 776.1989135742188 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: -3288.656005859375 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: -4618.47119140625 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 1533.218017578125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1531.49267578125 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 6250.4521484375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 488.9037780761719 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 531.93701171875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 363.1971740722656 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8291, loss_val: nan, pos_over_neg: 269.5426025390625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 624.8916015625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 2928.243896484375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1163.73046875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8146, loss_val: nan, pos_over_neg: 842.5173950195312 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: -39939.203125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 34318.85546875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 544.1770629882812 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 612.152099609375 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1618.37158203125 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 834.5174560546875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1196.6971435546875 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 2250.756103515625 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 2814.8154296875 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 951.8792724609375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 560.840576171875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 477.95123291015625 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.829, loss_val: nan, pos_over_neg: 405.9033508300781 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8265, loss_val: nan, pos_over_neg: 1209.0057373046875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.8308, loss_val: nan, pos_over_neg: 1099.379638671875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 2632.77099609375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 492.83282470703125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 5125.751953125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 1924.7099609375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 2155.036376953125 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: -16556.828125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 656.9439086914062 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 394.4543151855469 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1171.0994873046875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 986.7650146484375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 425.4579162597656 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1452.149169921875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 725.3106689453125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 849.0238037109375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1121.33984375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 2485.110595703125 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1208.1182861328125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 519.7550048828125 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 538.4800415039062 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1321.7791748046875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 1446.2086181640625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 812.723876953125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1054.0931396484375 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 2018.68994140625 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 872.8599243164062 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1412.3538818359375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 487.0447692871094 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 998.0599365234375 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 521.2340698242188 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 558.584716796875 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: -22426.9609375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 866.8582153320312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 652.5545043945312 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 327.6346740722656 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1735.41943359375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 2140.690673828125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 698.630615234375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 790.93359375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 11209.30859375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 743.9518432617188 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 501.45574951171875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8195, loss_val: nan, pos_over_neg: 890.6032104492188 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 2173.403076171875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1200.4388427734375 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 27890.89453125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8245, loss_val: nan, pos_over_neg: 1057.709228515625 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 790.4782104492188 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1362.3701171875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 893.6851806640625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 3837.7607421875 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1644.451416015625 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1283.8111572265625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1763.264892578125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 552.1909790039062 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1619.0477294921875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 685.036865234375 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 566.498046875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 532.7372436523438 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: -2737.662353515625 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8313, loss_val: nan, pos_over_neg: 674.5321655273438 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 943.02490234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 34862.5859375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1978.1109619140625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1971.5628662109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1793.9898681640625 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 2565.711669921875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1097.9249267578125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 359.8708801269531 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 3358.6611328125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8262, loss_val: nan, pos_over_neg: 835.593505859375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 754.477783203125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 462.0625 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 803.0433959960938 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: -2838.40087890625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 1007.6670532226562 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: -8960.771484375 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.8303, loss_val: nan, pos_over_neg: 631.9276123046875 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1703.5810546875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 836.2304077148438 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1524.1309814453125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 1001.7554321289062 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 868.4405517578125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 3016.395751953125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 954.399658203125 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 815.857666015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 708.5703735351562 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8254, loss_val: nan, pos_over_neg: 663.5621948242188 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 579.053955078125 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 726.0764770507812 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 815.40380859375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1762.272216796875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 791.80908203125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 729.1907958984375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 455.0909118652344 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8258, loss_val: nan, pos_over_neg: 281.1910095214844 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 487.6465148925781 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 614.9044799804688 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 544.4749755859375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8178, loss_val: nan, pos_over_neg: 274.4103698730469 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 531.6182861328125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 432.9576416015625 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 19540.34765625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 740.0353393554688 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 701.748291015625 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 626.9752807617188 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8321, loss_val: nan, pos_over_neg: 1012.873291015625 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 617.541259765625 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 1053.378173828125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 407.46051025390625 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 374.93194580078125 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 576.7437133789062 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 3339.076171875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 604.8191528320312 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 2395.2041015625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8288, loss_val: nan, pos_over_neg: 502.1728820800781 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 733.6921997070312 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 726.47509765625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 696.3665771484375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1729.435791015625 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1237.19677734375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 815.1459350585938 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 1271.6162109375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 2722.3828125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: -19527.2578125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 2991.54736328125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8251, loss_val: nan, pos_over_neg: 1059.1051025390625 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1971.1630859375 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 2093.420166015625 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8332, loss_val: nan, pos_over_neg: 1899.4200439453125 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 1410.0521240234375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: -3974.259521484375 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 636.8259887695312 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 770.55224609375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 730.2357177734375 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -12744.697265625 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1628.566162109375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 3342.928466796875 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 2488.260009765625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 470.32196044921875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 4427.46240234375 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 2434.118896484375 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 569.25048828125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 1255.9381103515625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1177.6683349609375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 7272.1240234375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: -197274.296875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 10372.9326171875 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 757.7459716796875 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 830.8389892578125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8289, loss_val: nan, pos_over_neg: 933.2930297851562 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 725.6134033203125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 497.6959228515625 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 383.8552551269531 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8176, loss_val: nan, pos_over_neg: 455.8186950683594 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 3160.534912109375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 3061.375 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 665.2655639648438 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 2090.069580078125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1039.237548828125 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 436.93743896484375 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 890.0452880859375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8221, loss_val: nan, pos_over_neg: 663.5007934570312 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 926.9168090820312 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 2257.828125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1207.5426025390625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1405.6483154296875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.8266, loss_val: nan, pos_over_neg: 364.9641418457031 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 625.7122802734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 734.2770385742188 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 2012.214111328125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 620.745361328125 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 580.5598754882812 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.8139, loss_val: nan, pos_over_neg: 645.889892578125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1376.0115966796875 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 558.9572143554688 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: -14873.5078125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2539.953857421875 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 772.8643798828125 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8163, loss_val: nan, pos_over_neg: 613.1735229492188 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 597.281982421875 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 777.1474609375 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -23346.798828125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 1126.6175537109375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.828, loss_val: nan, pos_over_neg: 1116.247314453125 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1605.6923828125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 2281.96484375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 776.1629028320312 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 694.19482421875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 635.7605590820312 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 10239.4853515625 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1514.57568359375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 485.9275207519531 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 5756.06201171875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2332.788818359375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 762.2400512695312 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 691.7085571289062 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 709.1434326171875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 626.3883666992188 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1278.208251953125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 401.60205078125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 557.0281372070312 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 5744.66845703125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.815, loss_val: nan, pos_over_neg: -4156.41259765625 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 533.3236083984375 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 717.9639892578125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.8293, loss_val: nan, pos_over_neg: 289.4179382324219 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 851.9334106445312 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 1029.574951171875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 2975.542236328125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1116.433837890625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 1986.61962890625 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 2010.102294921875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 871.3245239257812 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 464.37481689453125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: -46369.34375 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 536.26904296875 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 619.6314697265625 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 503.070068359375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 397.0839538574219 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 594.532958984375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 1666.2305908203125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 7156.5439453125 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1175.934326171875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: -28339.966796875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1010.4156494140625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1890.658203125 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 9646.0751953125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 430.01812744140625 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1805.4990234375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 560.109375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 723.7644653320312 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 380.5801086425781 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 502.8731994628906 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 502.1993103027344 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 689.369384765625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 492.1241149902344 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.831, loss_val: nan, pos_over_neg: -2589.49609375 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 2935.1904296875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 2039.52734375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2387.474365234375 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 8045.869140625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 1126.4154052734375 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 449.5064697265625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 587.3463134765625 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 3915.50830078125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1744.441162109375 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 3155.6318359375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 493.842041015625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1970.5775146484375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 533.4970703125 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 900.6524658203125 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 729.169677734375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 758.6554565429688 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 632.5350341796875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 758.5281372070312 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1710.213623046875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.8323, loss_val: nan, pos_over_neg: 2086.177978515625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 2388.03076171875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 960.8485717773438 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: -2708.377197265625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 643.9366455078125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 594.1307983398438 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 1103.0538330078125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 686.2318725585938 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1804.457763671875 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: -10575.740234375 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 407.03070068359375 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 869.8771362304688 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 819.4110107421875 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 259.4411315917969 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 983.0987548828125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 484.352294921875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 459.8370361328125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 580.8311767578125 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 355.9336242675781 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 829.3580322265625 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 667.089599609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 464.75701904296875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1215.3516845703125 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1491.651123046875 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: -38087.71484375 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 2540.42724609375 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 730.8328247070312 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 1010.2291870117188 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 588.65087890625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 2210.6591796875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 1368.845947265625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 667.2794799804688 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: -9867.955078125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8201, loss_val: nan, pos_over_neg: 1771.4261474609375 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8389, loss_val: nan, pos_over_neg: 1385.861328125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1763.489990234375 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 740.0537109375 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 3574.099365234375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 2603.84423828125 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8211, loss_val: nan, pos_over_neg: 429.02935791015625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 754.4320068359375 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1887.0374755859375 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 501.6921081542969 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 2672.47021484375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1439.0081787109375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1824.828857421875 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.825, loss_val: nan, pos_over_neg: 681.7651977539062 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: -8929.6318359375 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 583.4266967773438 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 303.5416564941406 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 774.4972534179688 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 575.3872680664062 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 5106.79248046875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1114.0977783203125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 697.4171752929688 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1375.1156005859375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: -2390.177490234375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1081.122802734375 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 335.034423828125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1028.61279296875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1440.0985107421875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: -4549.3330078125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 487.725830078125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1078.606689453125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2264.98583984375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 18240.58203125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 3268.240966796875 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 8420.2353515625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 583.5051879882812 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1362.439697265625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1551.830078125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 3728.397216796875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 734.4082641601562 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 665.7411499023438 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 285.79833984375 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 4854.22900390625 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 456.6615295410156 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8329, loss_val: nan, pos_over_neg: 570.9395751953125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.8274, loss_val: nan, pos_over_neg: 335.8350524902344 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.8147, loss_val: nan, pos_over_neg: 5533.82470703125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.8115, loss_val: nan, pos_over_neg: 1215.31103515625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 2543.143310546875 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 1135.98193359375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 569.00146484375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: -3957.360595703125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: -4815.84375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: -13200.6708984375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 576.6307373046875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 456.93463134765625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 724.4415893554688 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 395.3525695800781 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1760.052734375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 507.9880065917969 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1447.968505859375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 384.0812072753906 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 948.8369140625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: 618.4566650390625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1205.6412353515625 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 1648.4385986328125 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 887.1535034179688 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1356.9613037109375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 3169.8115234375 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8279, loss_val: nan, pos_over_neg: 808.9480590820312 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1064.7669677734375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 11607.8984375 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 1517.557373046875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 2002.6834716796875 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8249, loss_val: nan, pos_over_neg: 372.890380859375 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1484.2117919921875 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 3725.77099609375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 447.615234375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 490.040771484375 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 30675.06640625 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2322.7001953125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.8299, loss_val: nan, pos_over_neg: 379.8511657714844 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.8367, loss_val: nan, pos_over_neg: 409.9866638183594 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8187, loss_val: nan, pos_over_neg: 2526.408935546875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: -2115.37353515625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1750.7880859375 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 695.1790161132812 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 750.36279296875 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2025.041259765625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 672.623779296875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: -6797.2294921875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 1437.9180908203125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1802.9931640625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1240.6964111328125 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1564.6732177734375 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 825.7591552734375 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1030.050048828125 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 237.64691162109375 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1152.6400146484375 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 551.5109252929688 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 506.0106201171875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1354.660400390625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 2164.59423828125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 993.8792114257812 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 9325.6357421875 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 572.1201782226562 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1008.5089111328125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1288.40771484375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 466.74267578125 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: -1913.1505126953125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 1523.4580078125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 347.8162536621094 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1384.3834228515625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: -12146.8798828125 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 828.3737182617188 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 900.9503173828125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 602.5916137695312 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 838.0308227539062 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 376.11920166015625 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 425.5256652832031 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 592.5563354492188 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: -7726.43896484375 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8236, loss_val: nan, pos_over_neg: 1854.9525146484375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 2186.9814453125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1013.1942138671875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 723.4630126953125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 2935.57177734375 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 717.93212890625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 295.162841796875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 2980.05859375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 3282.092529296875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 1250.2159423828125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 5792.12890625 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 1149.9254150390625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 2867.792724609375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 1250.1064453125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 2798.9111328125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 491.6922912597656 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 437.7845153808594 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 507.1268005371094 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1134.4930419921875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1401.178955078125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 53184.08203125 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 1592.207763671875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 523.0501708984375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 527.623779296875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 839.9202880859375 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 739.9036254882812 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: -77431.5234375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1589.4307861328125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 825.5154418945312 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: -2371.8251953125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 3726.345947265625 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1777.417724609375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1008.2501220703125 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 536.8624877929688 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 601.7926635742188 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 4183.14599609375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1759.8658447265625 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 5407.87158203125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8196, loss_val: nan, pos_over_neg: -19817.970703125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.798, loss_val: nan, pos_over_neg: -5268.755859375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 3767.807373046875 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 8663.1259765625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 771.703369140625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 314.2196044921875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 481.7801818847656 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 528.3541259765625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 810.2987670898438 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 531.7193603515625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 796.2882690429688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 2176.011962890625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 488.4314270019531 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1260.7237548828125 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 2602.62451171875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 7204.9814453125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 866.37255859375 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 4382.18359375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 406.0932922363281 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 699.0784301757812 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 962.165771484375 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 613.1130981445312 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 561.7879638671875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 705.447021484375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1178.883056640625 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 380.66748046875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.8169, loss_val: nan, pos_over_neg: 3858.39306640625 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 500.7049255371094 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 2891.6064453125 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.802, loss_val: nan, pos_over_neg: -2154.655517578125 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 751.4204711914062 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 496.177001953125 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1722.3212890625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 665.0659790039062 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 425.7177429199219 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 1330.94189453125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 418.6142883300781 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 596.8331909179688 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 690.2052001953125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: -3521.959716796875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: -6390.9375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 564.5350341796875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1177.7896728515625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -4362.50927734375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.8232, loss_val: nan, pos_over_neg: 975.410888671875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/300000 [2:34:07<128353:43:49, 1540.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6\n",
      "Iter: 0/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: -63498.40234375 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: -1404.64404296875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 778.8506469726562 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 431.81549072265625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.823, loss_val: nan, pos_over_neg: 2049.84912109375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 669.4655151367188 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 955.7408447265625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 400.49609375 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.8063, loss_val: nan, pos_over_neg: 3292.92236328125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.8218, loss_val: nan, pos_over_neg: 795.2947387695312 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: -3135.500244140625 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: -4443.685546875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2673.175537109375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 433.3121337890625 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 314.2413330078125 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 7659.306640625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1056.6776123046875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 3514.2490234375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 8905.005859375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 21763.220703125 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 986.9786376953125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 969.1070556640625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 837.5093383789062 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 175.03005981445312 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 344.68414306640625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 986.8167114257812 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1522.5943603515625 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.8209, loss_val: nan, pos_over_neg: 368.8453674316406 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 522.6987915039062 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 602.3378295898438 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 3696.603271484375 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1034.20947265625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8164, loss_val: nan, pos_over_neg: 535.6829833984375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8247, loss_val: nan, pos_over_neg: 361.4577941894531 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8272, loss_val: nan, pos_over_neg: 327.8190612792969 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 752.0994262695312 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 490.8089904785156 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 789.1947021484375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 477.7741394042969 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 4837.41552734375 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 13019.80859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 642.1928100585938 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 297.7781677246094 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1646.5860595703125 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 613.0484619140625 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 843.032958984375 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1274.6502685546875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 594.1945190429688 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 567.4609375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 894.8012084960938 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 6177.12158203125 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 834.978515625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 500.0151062011719 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 956.7249145507812 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1651.908203125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 589.8341674804688 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 644.89306640625 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 825.6864624023438 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 919.1619873046875 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 679.3697509765625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 2354.1923828125 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 2054.32177734375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 1026.1640625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 3256.171630859375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 690.1441650390625 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1097.2044677734375 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 771.5187377929688 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 2318.27734375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 728.4010620117188 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 482.4165344238281 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 500.2852478027344 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 2342.86962890625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 3502.05810546875 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 889.9651489257812 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 2476.060546875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 1543.55908203125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: -5054.220703125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 8188.79052734375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1227.62255859375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 546.9125366210938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 4936.55615234375 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 545.0326538085938 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.8248, loss_val: nan, pos_over_neg: 275.7101745605469 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1955.202392578125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 610.1934204101562 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 570.9957275390625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 508.02655029296875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 856.9032592773438 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1180.514404296875 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 808.3939208984375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 932.9619140625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 927.6397094726562 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 502.5204772949219 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 1504.6876220703125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 782.722900390625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.8327, loss_val: nan, pos_over_neg: 1334.5184326171875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.793, loss_val: nan, pos_over_neg: -33255.0859375 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 2466.36962890625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 396.7500305175781 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 229.98016357421875 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 323.9969177246094 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1161.4857177734375 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 66007.90625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 15195.6357421875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 6034.3779296875 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8116, loss_val: nan, pos_over_neg: 30852.10546875 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 1074.2105712890625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 775.174072265625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 1271718.75 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 362.1174011230469 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8167, loss_val: nan, pos_over_neg: 828.5946655273438 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8239, loss_val: nan, pos_over_neg: 389.8658447265625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 450.5089416503906 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 734.4364624023438 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 998.560302734375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 700.1484375 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1857.8173828125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 819.9573364257812 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.8193, loss_val: nan, pos_over_neg: 1158.5223388671875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 3885.04931640625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 848.8702392578125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.8122, loss_val: nan, pos_over_neg: 395.37310791015625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 589.8599243164062 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 335.6944580078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 411.1377258300781 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 1384.515380859375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 2289.143310546875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 852.7817993164062 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 296.4571838378906 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 543.2518310546875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 397.6554260253906 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 627.78759765625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 975.158203125 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 769.6995239257812 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 318.01409912109375 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1198.5057373046875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 2261.6103515625 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 436.81280517578125 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 443.4547119140625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 477.98052978515625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.8282, loss_val: nan, pos_over_neg: 1090.1102294921875 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1545.3961181640625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.8184, loss_val: nan, pos_over_neg: 945.1915893554688 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 862.65625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 509.18939208984375 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 66345.296875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1123.2803955078125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1573.069580078125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 564.0637817382812 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 772.4330444335938 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8197, loss_val: nan, pos_over_neg: 370.7440490722656 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 446.2478942871094 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 1404.16796875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.813, loss_val: nan, pos_over_neg: 1401.9129638671875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: -1787.317626953125 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.8318, loss_val: nan, pos_over_neg: 428.2508544921875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 71613.5625 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 6772.77001953125 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 635.1737060546875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1355.0281982421875 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 626.7200927734375 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 2251.3486328125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 18647.607421875 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 615.5497436523438 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 624.041015625 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 385.3411560058594 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 645.4514770507812 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 474.4189453125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 794.4808349609375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 559.95263671875 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 46387.59375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 1076.6697998046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 594.71630859375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1320.42626953125 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 770.8413696289062 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: -15059.7041015625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1782.0509033203125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 514.9129028320312 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 611.9830322265625 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1365.000732421875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 540.1697387695312 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1355.299072265625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 699.5899658203125 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 976.2774047851562 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 701.0338745117188 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 923.2644653320312 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 1829.689453125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.805, loss_val: nan, pos_over_neg: -109610.7421875 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 764.6834106445312 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 308.6045837402344 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 383.5429992675781 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 426.5934753417969 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2173.305419921875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 957.2387084960938 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 874.745849609375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 542.9815063476562 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 657.271240234375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 580.8350219726562 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1751.392578125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 647.4856567382812 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.8263, loss_val: nan, pos_over_neg: 1067.75048828125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 841.9443969726562 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1141.2164306640625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 602.6083374023438 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 554.4554443359375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 821.479248046875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 557.0206909179688 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 2124.082275390625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1008.0726928710938 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: -1569.28759765625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -7024.76416015625 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: -5476.3056640625 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 2922.562744140625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 409.5931396484375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 1361.8626708984375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 1339.951171875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1245.224365234375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 814.1017456054688 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.8142, loss_val: nan, pos_over_neg: 422.3680725097656 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 3019.611328125 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 565.1367797851562 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8143, loss_val: nan, pos_over_neg: 1162.681884765625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 436.7347412109375 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: -8957.9365234375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 570.22705078125 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1826.3426513671875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.8117, loss_val: nan, pos_over_neg: 683.8556518554688 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 1802.29296875 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 1970.7918701171875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1690.4202880859375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 377.8922424316406 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 804.94921875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1112.5986328125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 10492.3486328125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1136.4002685546875 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 350.0618896484375 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8214, loss_val: nan, pos_over_neg: 335.9877014160156 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 4977.697265625 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 741.0404663085938 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 2560.99951171875 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 2477.67822265625 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 1290.3646240234375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 330.02178955078125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 457.3935852050781 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 7053.30029296875 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 3005.173828125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 8876.5458984375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 641.5742797851562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 440.0751953125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 682.5281982421875 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 451.3333740234375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1781.6910400390625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 909.5783081054688 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 6702.25634765625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 7496.71337890625 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 1009.8355712890625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 3391.54931640625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: -3640.755859375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 691.9104614257812 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 896.2503051757812 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1075.78466796875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 987.2369384765625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1261.477783203125 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 3910.02197265625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1025.741455078125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 759.9296264648438 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 3585.173583984375 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.8194, loss_val: nan, pos_over_neg: 1292.9847412109375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 2096.53466796875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1154.648681640625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1131.4691162109375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 455.58612060546875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 537.3103637695312 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 665.1536865234375 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 799.5388793945312 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 668.581298828125 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.8259, loss_val: nan, pos_over_neg: 511.65460205078125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 2092.059326171875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 991.4310913085938 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 2267.2509765625 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.8172, loss_val: nan, pos_over_neg: 1236.62109375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 2138.695556640625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: 1716.574951171875 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.816, loss_val: nan, pos_over_neg: 1314.3165283203125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 442.5426330566406 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 454.0780029296875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 646.5370483398438 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 772.8195190429688 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 2100.334228515625 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1017.0369873046875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8155, loss_val: nan, pos_over_neg: 449.43951416015625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1434.7083740234375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 792.5833740234375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 678.6654052734375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.8185, loss_val: nan, pos_over_neg: 458.33758544921875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 898.6173706054688 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: -3266.9287109375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1482.7069091796875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 4211.78466796875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 856.9585571289062 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 533.3529052734375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.8237, loss_val: nan, pos_over_neg: 951.2367553710938 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 2156.69189453125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 3002.432373046875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 690.951904296875 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 727.4552612304688 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 559.5714721679688 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 730.1099853515625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 513.123291015625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 537.9579467773438 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: -2901.7958984375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 837.4459838867188 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 885.4942016601562 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 1736.7890625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 3261.197998046875 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 1079.3392333984375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 3364.860107421875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.8097, loss_val: nan, pos_over_neg: 483.61798095703125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 467.3896179199219 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 626.209228515625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 784.8660888671875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 363.7149353027344 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 245.8863067626953 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 353.7322692871094 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1004.8829956054688 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 978.402099609375 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1548.8348388671875 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 2046.43896484375 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 7050.88134765625 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1715.014892578125 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 852.7529296875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 805.6820678710938 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 661.08349609375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 513.146728515625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 686.1314697265625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.837, loss_val: nan, pos_over_neg: 298.9138488769531 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1894.7020263671875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: 858.5812377929688 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 629.7376708984375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 828.5289306640625 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: -4577.22607421875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 254766.34375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 674.4103393554688 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 846.69140625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1022.12451171875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 2390.629638671875 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 809.4371948242188 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 970.8272705078125 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 1547.991943359375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 829.3427734375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 614.5259399414062 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1829.554443359375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1852.558837890625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 2336.687744140625 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 749.7560424804688 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1109.2320556640625 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: -8500.072265625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 28792.02734375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 2097.7470703125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8038, loss_val: nan, pos_over_neg: 598.8587646484375 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2183.456787109375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 374.5457763671875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2319.73828125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 459.6758117675781 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 1374.0767822265625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 820.22119140625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 7301.765625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 2271.572998046875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 577.9366455078125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 759.5474243164062 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 2591.30078125 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 510.37200927734375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: -131802.65625 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 11707.1484375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8165, loss_val: nan, pos_over_neg: 330.88226318359375 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1370.9085693359375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.794, loss_val: nan, pos_over_neg: -2877.7041015625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 2172.7890625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 750.858154296875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 767.4074096679688 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.8199, loss_val: nan, pos_over_neg: 505.8196716308594 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8223, loss_val: nan, pos_over_neg: 571.7809448242188 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 8018.91943359375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.8174, loss_val: nan, pos_over_neg: 917.4183959960938 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: -5262.7509765625 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 740.373046875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 432.48529052734375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: -18777.599609375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 4764.18798828125 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: -2695.98583984375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 354.2461242675781 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 4428.7705078125 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1185.583984375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 542.7811279296875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: -3717.813232421875 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1123.4168701171875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1406.032470703125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1665.03955078125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 3001.246826171875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 918.7979125976562 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 557.0603637695312 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 533.2225341796875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 921.9916381835938 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 748.8779296875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 610.3912353515625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1422.93017578125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 1032.373046875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 6142.32861328125 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 592.9151000976562 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1696.3623046875 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 792.5807495117188 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 406.8873596191406 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 1148.4752197265625 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 614.239501953125 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 624.7342529296875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 572.8217163085938 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 532.748779296875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1151.20361328125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 1198.190673828125 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.8242, loss_val: nan, pos_over_neg: 289.9647216796875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 394.7234802246094 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 645.1820678710938 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 751.9295654296875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 2891.845703125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 7055.19970703125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 2051.553955078125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 1453.8302001953125 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2243.802734375 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8189, loss_val: nan, pos_over_neg: 321.03363037109375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 459.6149597167969 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 518.8468627929688 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1613.62451171875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 814.6253662109375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 655.5267944335938 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 867.70654296875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 17000.279296875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 2055.490478515625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 4160.91357421875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1249.201416015625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 986.7255859375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 12593.162109375 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1195.603271484375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 386.1304931640625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1403.2662353515625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 859.9807739257812 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 933.9893798828125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 1390.8570556640625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1366.340576171875 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 38826.2265625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 489.67547607421875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 853.2550659179688 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 1606.6942138671875 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 586.46142578125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 505.93572998046875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 494.5060729980469 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 2298.389404296875 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 3320.38232421875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 759.9004516601562 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2171.25537109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 361.7827453613281 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.8231, loss_val: nan, pos_over_neg: 534.2008666992188 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1639.098388671875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8198, loss_val: nan, pos_over_neg: 1077.38525390625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1758.748046875 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 664.0082397460938 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 962.6096801757812 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1150.0067138671875 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1549.5462646484375 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 868.1378173828125 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 592.3863525390625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 1858.0831298828125 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 392.9716491699219 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1201.75 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1129.94775390625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 836.1619262695312 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 1791.099853515625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 5645.607421875 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 530.710205078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.8275, loss_val: nan, pos_over_neg: 562.8409423828125 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 4378.34521484375 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 512.8820190429688 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 2604.318359375 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 723.5054931640625 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8238, loss_val: nan, pos_over_neg: 1033.51318359375 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 816.5736083984375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 10818.5283203125 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 783.2006225585938 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 845.482177734375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 691.052490234375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: -6368.7431640625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 525.3049926757812 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 426.94488525390625 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 726.5780639648438 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 762.234375 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: -2739.56201171875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 6835.173828125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 593.3592529296875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8144, loss_val: nan, pos_over_neg: 606.7504272460938 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 916.6737670898438 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 4459.5693359375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.8084, loss_val: nan, pos_over_neg: 987.4756469726562 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 452.6397705078125 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1364.4432373046875 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 482.0149230957031 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 948.973388671875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 854.1087036132812 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1610.6510009765625 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 803.3443603515625 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 907.0459594726562 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 733.392578125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1269.9219970703125 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1568.229736328125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1802.36474609375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 490.0534973144531 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 2484.24951171875 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 602.3790893554688 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 9597.939453125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 301.9348449707031 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 306.8106689453125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1058.095703125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 979.5873413085938 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 458.46563720703125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 668.3240966796875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 976.5011596679688 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 1727.754638671875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: -10456.7470703125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 699.001953125 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 1678.176025390625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8188, loss_val: nan, pos_over_neg: 6074.2265625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1528.131591796875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.8154, loss_val: nan, pos_over_neg: 1007.0841064453125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 508.3394470214844 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 980.5523681640625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1774.9588623046875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.8208, loss_val: nan, pos_over_neg: 558.9774169921875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 487.21026611328125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 1670.16748046875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 3068.245849609375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 19259.580078125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 3270.048828125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 2764.908447265625 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 1310.6614990234375 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 1233.3333740234375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 604.0872802734375 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 638.7528686523438 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 4642.20458984375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1268.2403564453125 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1338.9716796875 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 575.092529296875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 6754.8857421875 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 2008.11962890625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.793, loss_val: nan, pos_over_neg: -3756.110107421875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: -17429.625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 356.93365478515625 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 513.8682861328125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1204.990234375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 767.279052734375 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 1186.288818359375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1323.49609375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 743.5001831054688 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 311.86328125 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1532.587890625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 7476.2177734375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 600.1199951171875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 1605.2462158203125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 425.22515869140625 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 484.6129150390625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1920.95703125 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 3082.03955078125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 3429.383056640625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1875.0966796875 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1006.7883911132812 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: -5818.66357421875 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 36208.0703125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 803.4931030273438 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 282.3153381347656 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1621.066162109375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 4055.978759765625 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 3282.948974609375 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 1137.7889404296875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1328.923583984375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 620.6174926757812 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 887.0232543945312 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 811.566650390625 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 803.5430908203125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.8101, loss_val: nan, pos_over_neg: 993.8070068359375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 2861.668701171875 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 973.3035888671875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: -2495.940185546875 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1351.11279296875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1719.6636962890625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1242.2799072265625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1061.1875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 903.077392578125 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 468.3442077636719 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 432.63037109375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 820.14404296875 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 875.0997314453125 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 1755.028076171875 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 706.664306640625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: -19287.419921875 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2375.81005859375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 2187.46240234375 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 512.824951171875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 792.62646484375 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 631.9173583984375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 449.45025634765625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 872.989990234375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 847.7382202148438 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.8152, loss_val: nan, pos_over_neg: 527.450439453125 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 994.3861694335938 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1400.280029296875 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 3001.210693359375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1025.3270263671875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8183, loss_val: nan, pos_over_neg: 673.4222412109375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1622.7994384765625 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 878.1315307617188 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 1364.8138427734375 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 641.70458984375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 15440.3623046875 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 2260.721435546875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 922.5271606445312 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 671.0785522460938 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1111.274658203125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 620.4762573242188 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: 955.1898193359375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 800.2273559570312 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 738.2505493164062 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 647.98388671875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1738.6575927734375 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 559.2777099609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 474.6025695800781 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 757.6766967773438 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 2117.593505859375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1024.153564453125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 446.3888244628906 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 751.158935546875 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 614.2450561523438 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1023.9210815429688 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 608.3289184570312 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.822, loss_val: nan, pos_over_neg: 366.9979553222656 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.8149, loss_val: nan, pos_over_neg: 616.4905395507812 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 1740.9176025390625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 665.5348510742188 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 753.06005859375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 576.676513671875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 665.609619140625 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 622.9171142578125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 552.5098876953125 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 544.1439208984375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 393.9538269042969 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 465.9170837402344 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 798.0965576171875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1690.6651611328125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 6191.83642578125 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 470.3009338378906 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.794, loss_val: nan, pos_over_neg: -4910.5517578125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: -6316.48193359375 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1119.3155517578125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 747.5202026367188 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 686.8466796875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1466.72705078125 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 574.6102905273438 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.8364, loss_val: nan, pos_over_neg: 465.63958740234375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 816.8513793945312 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1014.0184326171875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: -8887.685546875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 6134.0693359375 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 531.3616943359375 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: -11295.068359375 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1047.58935546875 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: -2374.389404296875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1046.3956298828125 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.8141, loss_val: nan, pos_over_neg: 816.7796630859375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1725.5653076171875 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 2939.059814453125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 52850.203125 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1923.245361328125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 751.2413940429688 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 1006.40625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 828.8707275390625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 2294.338134765625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: -7332.9970703125 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1238.376708984375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 4489.60546875 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 868.9010620117188 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 820.6192626953125 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1133.1461181640625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 4681.51123046875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 361.4461364746094 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 737.4475708007812 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1240.77197265625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 575.0955200195312 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 493.2822570800781 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8159, loss_val: nan, pos_over_neg: 310.0205383300781 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 784.193359375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 994.8886108398438 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1741.574951171875 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 1338.97265625 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2246.092529296875 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 4567.68994140625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1453.8636474609375 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 1381.121826171875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 8225.3271484375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: -7804.58447265625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.824, loss_val: nan, pos_over_neg: 575.8727416992188 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1703.7696533203125 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8257, loss_val: nan, pos_over_neg: 313.4212646484375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8191, loss_val: nan, pos_over_neg: 544.9563598632812 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.8224, loss_val: nan, pos_over_neg: 660.0844116210938 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1060.7681884765625 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 2062.4814453125 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 395.7106018066406 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 448.0409240722656 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 696.9360961914062 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1272.4185791015625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 835.239501953125 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 691.7456665039062 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 517.0181274414062 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8192, loss_val: nan, pos_over_neg: 1140.1607666015625 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 486.67437744140625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1434.550048828125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 629.0292358398438 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8065, loss_val: nan, pos_over_neg: 397.1302185058594 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1625.5811767578125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 997.7932739257812 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 501.8439025878906 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 647.5287475585938 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 1479.3365478515625 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1767.5965576171875 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1632.59130859375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 608.7339477539062 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 659.6034545898438 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 365.6700439453125 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 4275.078125 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 496.3498840332031 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1814.7469482421875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 922.0151977539062 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1234.7479248046875 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.8059, loss_val: nan, pos_over_neg: 1345.40966796875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 4095.68701171875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1433.103759765625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 787.6798706054688 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 788.225341796875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 687.22509765625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 527.0403442382812 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8136, loss_val: nan, pos_over_neg: 257.7713928222656 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 884.0192260742188 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 568.7479248046875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 562.1410522460938 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 1104.09619140625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 531.97802734375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: -9103.060546875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1547.0057373046875 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 1317.6787109375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 3611.9853515625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8252, loss_val: nan, pos_over_neg: 443.3130187988281 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1212.90283203125 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 856.2432250976562 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 664.853759765625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 824.3702392578125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 811.4064331054688 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1001.9569702148438 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1369.954345703125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1133.918212890625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 2235.712158203125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1004.308349609375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: 716.4976806640625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 2572.965576171875 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 1643.1007080078125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 283.6566467285156 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 502.9090881347656 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 3588.712158203125 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 672.0611572265625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 413.3063659667969 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 627.9054565429688 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 2355.552001953125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1465.416015625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 781.7328491210938 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1193.858154296875 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 998.5091552734375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 761.2626953125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 588.9302978515625 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: -119158.015625 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 846.7883911132812 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 712.54150390625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 684.87158203125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 793.5741577148438 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 966.4264526367188 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 966.3330688476562 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1289.2564697265625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 2331.548583984375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 619.3016357421875 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1356.515625 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 906.8331909179688 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 535.7337036132812 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1381.6104736328125 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2022.869873046875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 301.3951110839844 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 347.5727233886719 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 867.8706665039062 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1322.774658203125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 449.409423828125 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 761.7055053710938 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 606.7384033203125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 727.0296020507812 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 1361.9483642578125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.8205, loss_val: nan, pos_over_neg: 2198.949951171875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1104.6585693359375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: -12815.4892578125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 440.9384460449219 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 963.2166137695312 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 936.3597412109375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 792.7920532226562 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1128.07177734375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: -3845.621826171875 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 2522.64697265625 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 10509.884765625 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: -1938.308349609375 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 713.2323608398438 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 805.439697265625 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 437.2855529785156 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 626.0494995117188 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 584.2904052734375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 475.8515319824219 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 501.5838928222656 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: -4838.79541015625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 19364.912109375 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: -64386.7734375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 392.3672790527344 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2301.289306640625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 14649.4267578125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1314.8482666015625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 545.5765991210938 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 903.9563598632812 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1299.1239013671875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 868.5971069335938 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 435.499267578125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 475.9422302246094 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 321.62042236328125 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1740.5782470703125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: -6930.9580078125 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 2111.82861328125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 270.9315185546875 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 641.5280151367188 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2164.36474609375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2441.27001953125 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 555.2803955078125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 807.4930419921875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1179.82373046875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 605.2902221679688 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 606.2072143554688 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 774.9041748046875 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1533.8118896484375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: -4061.68017578125 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 1575635.5 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1326.8994140625 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1770.8631591796875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 583.8974609375 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.8151, loss_val: nan, pos_over_neg: 401.96990966796875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 888.6469116210938 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1067.8624267578125 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1599.1925048828125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1090.7177734375 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 643.86279296875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 842.5670166015625 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 801.8231201171875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 2853.75146484375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 7558.4921875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.815, loss_val: nan, pos_over_neg: 611.513916015625 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 673.5589599609375 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.799, loss_val: nan, pos_over_neg: -3855.38427734375 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: -3189.50146484375 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 10405.4921875 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 876.9636840820312 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 348.2359619140625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 822.7430419921875 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1652.9315185546875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1014.23193359375 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 39972.44140625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 768.8846435546875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.8104, loss_val: nan, pos_over_neg: 3315.307861328125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: -6004.58935546875 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 941.2816162109375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 1165.6787109375 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 671.3544921875 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 430.5423278808594 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2255.256103515625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 622.8724365234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/300000 [3:00:07<128873:59:37, 1546.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7\n",
      "Iter: 0/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 428.9358825683594 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 697.8335571289062 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1150.2855224609375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 913.9742431640625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 772.8807373046875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2086.922607421875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 1660.8465576171875 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 436.9815368652344 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 978.3173828125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 526.50732421875 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2694.52001953125 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 537.5466918945312 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: -7479.37841796875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 635.6077880859375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 690.6222534179688 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 563.780517578125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 381.62872314453125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 503.6488037109375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 428.4697570800781 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 917.7058715820312 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 612.5421142578125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 883.3790283203125 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1138.4781494140625 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: -6110.99560546875 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.8213, loss_val: nan, pos_over_neg: 887.5757446289062 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1328.2447509765625 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 771.7894287109375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 1888.6058349609375 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1584.2965087890625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.8204, loss_val: nan, pos_over_neg: 579.3897705078125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 972.3845825195312 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1616.6739501953125 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1543.902099609375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.8099, loss_val: nan, pos_over_neg: 1948.293212890625 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1665.3843994140625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.8083, loss_val: nan, pos_over_neg: 1276.6629638671875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 527.681884765625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 653.396728515625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 360.1602783203125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 397.4815673828125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1400.1624755859375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 820.457763671875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.817, loss_val: nan, pos_over_neg: 752.024658203125 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.8113, loss_val: nan, pos_over_neg: 1922.426025390625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 3063.03271484375 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 2424.543212890625 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 520.7303466796875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1135.3470458984375 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1952.3404541015625 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 594.1839599609375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1405.9984130859375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 439.0948181152344 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 352.40777587890625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 37152.86328125 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1449.5152587890625 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 469.5511169433594 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 739.2529907226562 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 604.7349853515625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 528.3257446289062 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 329.6050720214844 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 722.1385498046875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 930.3552856445312 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1119.329833984375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 535.0650634765625 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 616.567138671875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1683.496826171875 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1784.888916015625 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 730.9036254882812 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8076, loss_val: nan, pos_over_neg: 831.4407958984375 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1078.921142578125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.8089, loss_val: nan, pos_over_neg: 632.2325439453125 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1408.2044677734375 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 717.071044921875 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 631.8228149414062 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 3694.9912109375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1464.51171875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1083.5924072265625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.804, loss_val: nan, pos_over_neg: -1773.3355712890625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 2370.790283203125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1696.8955078125 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1846.906494140625 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 429.0865783691406 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 694.5978393554688 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 1102.4539794921875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: -4083.661865234375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.8168, loss_val: nan, pos_over_neg: 458.9219665527344 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 657.19482421875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 2273.007568359375 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1147.3109130859375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 795.0779418945312 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1427.0699462890625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1373.8673095703125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1836.293212890625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 960.2999267578125 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 552.9671630859375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.796, loss_val: nan, pos_over_neg: -6391.87646484375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 731.1671142578125 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 896.9277954101562 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 602.88818359375 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 705.3189086914062 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 686.193359375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1422.95556640625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 721.16357421875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 501.11395263671875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 2906.50146484375 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 549.36474609375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 483.41973876953125 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 911.9810180664062 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 540.1123046875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 66713.828125 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -6137.42041015625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 456.30010986328125 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 844.8178100585938 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.814, loss_val: nan, pos_over_neg: 1665.5413818359375 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 4963.1142578125 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 528.2459716796875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1452.23095703125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1614.896484375 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 364.45068359375 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 370.260498046875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 786.4751586914062 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 565.1906127929688 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 784.0635375976562 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 1042.645263671875 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.8109, loss_val: nan, pos_over_neg: -2566.99853515625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 12851.939453125 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 752.3949584960938 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: -6062.107421875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 2050.845703125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 10519.1748046875 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1493.0867919921875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.8179, loss_val: nan, pos_over_neg: 1201.5826416015625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1934.3111572265625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 767.5126342773438 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 936.364990234375 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2383.99072265625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 1618.658203125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 546.4389038085938 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 740.718994140625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: -83907.328125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 985.8588256835938 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 446.6208801269531 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 552.083984375 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 6888.2744140625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 7888.6904296875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 3429.420166015625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 2321.193115234375 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 1269.43115234375 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1569.4462890625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 647.0580444335938 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.8171, loss_val: nan, pos_over_neg: 379.95294189453125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 3709.048828125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 937.6557006835938 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: -1576.2484130859375 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 515.7042846679688 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 487.8143005371094 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 977.7310180664062 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 797.2882690429688 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 2649.50244140625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 476.7662048339844 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 965.9124145507812 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1181.5313720703125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2079.99853515625 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -3816.63525390625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: -1894.2913818359375 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1884.459228515625 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -3350.841552734375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1991.629638671875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 2555.37646484375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 637.829833984375 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: -3673.357421875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 377.8650207519531 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 922.77685546875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8182, loss_val: nan, pos_over_neg: 529.1988525390625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 899.443115234375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1532.7294921875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: -1647.5322265625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.8105, loss_val: nan, pos_over_neg: 829.6309814453125 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 432.7832336425781 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.8129, loss_val: nan, pos_over_neg: 740.9352416992188 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 3806.761474609375 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 565.2111206054688 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.8207, loss_val: nan, pos_over_neg: 576.7904052734375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 517.6797485351562 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 785.6031494140625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1131.0595703125 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1219.19775390625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 1099.1915283203125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 488.833251953125 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 767.6224975585938 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 680.2831420898438 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.8137, loss_val: nan, pos_over_neg: 1348.8056640625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1259.6773681640625 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: -4745.80859375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 5461.07568359375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 11720.4765625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 774.0596313476562 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1294.8817138671875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 724.52294921875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1110.458251953125 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.811, loss_val: nan, pos_over_neg: 603.7266845703125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 605.8621826171875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1046.336181640625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: -8823.2021484375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1244.0343017578125 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 12788.654296875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1719.4315185546875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 592.818603515625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: -1913.859130859375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: -4722.68994140625 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 522.5916137695312 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 690.798828125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1843.544921875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1105.508056640625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 12835.9736328125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 1811.6866455078125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 1218.5728759765625 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 11018.765625 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: -3427.509765625 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1476.613525390625 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1313.4266357421875 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 538.670166015625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 364.5287170410156 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 1552.743408203125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.8181, loss_val: nan, pos_over_neg: 422.23583984375 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.778, loss_val: nan, pos_over_neg: -7776.11181640625 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 460.7677917480469 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1037.984375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 471.42449951171875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 718.0189208984375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1582.586181640625 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 5158.13427734375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 394.41473388671875 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 427.8536682128906 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 599.8798217773438 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 2287.007568359375 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: -2119.53271484375 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 6673.99951171875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 761.7661743164062 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.8131, loss_val: nan, pos_over_neg: 658.4805908203125 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 673.258544921875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.8217, loss_val: nan, pos_over_neg: 313.3067626953125 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 980.4190063476562 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 883.4021606445312 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 396.6631164550781 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 704.9381713867188 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1368.80322265625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 678.2012329101562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: -7014.08740234375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1227.4615478515625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1575.49609375 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1010.3299560546875 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 393.0118103027344 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.8162, loss_val: nan, pos_over_neg: 926.4903564453125 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 465.0843200683594 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1712.1986083984375 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1597.8729248046875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 624.1519165039062 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 933.4010009765625 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 974.5264282226562 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1205.128173828125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8206, loss_val: nan, pos_over_neg: 9147.5869140625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 9475.1328125 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 2051.7705078125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 1509.77197265625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 2625.31640625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 711.1364135742188 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 580.8555297851562 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 780.76904296875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 875.2508544921875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 445.3579406738281 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 488.5940246582031 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 428.31988525390625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1573.429931640625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8132, loss_val: nan, pos_over_neg: 1796.359619140625 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 3017.851806640625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 2968.292724609375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1796.0906982421875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 539.0194702148438 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 767.386962890625 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 2223.392822265625 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 3985.248291015625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 3584.44677734375 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 4385.32080078125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.8128, loss_val: nan, pos_over_neg: 1094.7176513671875 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 566.288818359375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.8166, loss_val: nan, pos_over_neg: 559.4588623046875 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 676.59033203125 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: -10575.0966796875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 1426.5076904296875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 4538.88134765625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 1513.7890625 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 823.6185913085938 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 558.0458984375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 1071.451171875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 534.5881958007812 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 2961.960205078125 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 414.68328857421875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 668.7031860351562 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 433.7723388671875 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1465.3232421875 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 4843.8359375 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 1107.41259765625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 4191.64111328125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: -9412.3251953125 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.8078, loss_val: nan, pos_over_neg: 571.4972534179688 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 468.9420471191406 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 609.752197265625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 394.3424987792969 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 3638.52685546875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 753.8265380859375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 781.2509155273438 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1911.822509765625 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.8228, loss_val: nan, pos_over_neg: 369.447509765625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1185.222412109375 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 412.6603698730469 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1527.4268798828125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: -2817.8740234375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 728.3587646484375 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: -11911.3564453125 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1324.6512451171875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8092, loss_val: nan, pos_over_neg: 955.4996337890625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1202.02099609375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 756.0938110351562 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 737.55224609375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 5872.43798828125 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 4817.99658203125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1396.5516357421875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 912.0610961914062 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 3374.256103515625 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1003.866455078125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 2428.65869140625 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 2939.701416015625 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2724.62353515625 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 7491.28271484375 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 668.3619995117188 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 4195.955078125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 3039.54638671875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 308.14727783203125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 840.6764526367188 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 2625.63330078125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: -2745.31689453125 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 2804.6953125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 2691.818603515625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 1671.82177734375 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8118, loss_val: nan, pos_over_neg: 644.3760986328125 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: -2287.344482421875 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: -20943.083984375 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 347.6964111328125 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1393.4532470703125 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 764.9035034179688 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: -24780.52734375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 3789.2685546875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1496.8157958984375 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1363.8138427734375 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: -4111.2705078125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 1276.9873046875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 805.3074951171875 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.8153, loss_val: nan, pos_over_neg: 474.0218200683594 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 6983.0185546875 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 738.7198486328125 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: -12129.65625 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 616.255126953125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1924.2666015625 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.8088, loss_val: nan, pos_over_neg: 452.36163330078125 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1730.7940673828125 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1089.67333984375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 421.09869384765625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 617.9453125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 2117.635009765625 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 852.4578247070312 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: -16992.03125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 357.0045471191406 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 3698.367919921875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 2238.535400390625 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 752.00732421875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 5238.2080078125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 5820.07373046875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 6015.05517578125 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.8028, loss_val: nan, pos_over_neg: 521.9140625 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1866.7630615234375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 545.6673583984375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 491.5013122558594 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 448.3458251953125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 742.8543090820312 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1179.1024169921875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 1093.7686767578125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1259.8868408203125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 809.4418334960938 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 2410.3447265625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 745.2150268554688 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 626.1849365234375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 800.22412109375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 747.7406616210938 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1323.8482666015625 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 622.9674072265625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 426.49200439453125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 361.19256591796875 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 385.49285888671875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.8111, loss_val: nan, pos_over_neg: 1052.20068359375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.8106, loss_val: nan, pos_over_neg: 662.35302734375 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1878.501220703125 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 608.7166137695312 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 678.0479736328125 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1173.6617431640625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 474.1816101074219 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 502.66180419921875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 580.0399169921875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 840.6097412109375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 3846.734375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 746.0570678710938 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 665.8095092773438 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 506.88092041015625 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: -2432.431640625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1222.33056640625 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 2609.641357421875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1392.4781494140625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 1041.1395263671875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1184.4716796875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: -28173.23828125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 485.9824523925781 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 278.99029541015625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 644.5386352539062 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 6062.1083984375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 970.5446166992188 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1763.4986572265625 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 820.8494262695312 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1200.04345703125 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1141.322998046875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 3083.2236328125 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 2552.2998046875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 4209.416015625 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1324.57763671875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 663.5140380859375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 726.4305419921875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1295.2330322265625 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1015.5042724609375 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 801.7958984375 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 715.336181640625 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 645.594482421875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 728.6096801757812 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 8423.6201171875 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 908.8882446289062 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 678.6666259765625 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1337.5234375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 478.3893127441406 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 305.4659729003906 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 369.3839416503906 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 611.5647583007812 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 28139.322265625 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: -23636.158203125 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 511.7979736328125 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 488.51007080078125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 905.6531982421875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 911.4586181640625 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 982.7443237304688 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 856.9407958984375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1126.565185546875 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 5295.412109375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 2287.43505859375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 630.584228515625 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 748.6232299804688 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 862.0775146484375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 3509.78759765625 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: -12133.607421875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 756.2989501953125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 390.10845947265625 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 781.8380737304688 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1841.4539794921875 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 856.7907104492188 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1003.4679565429688 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 4693.34814453125 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 3470.660888671875 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: -5669.931640625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2063.74072265625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.8107, loss_val: nan, pos_over_neg: 1055.0738525390625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 920.77978515625 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: -3044.81640625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 637.724853515625 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 2969.688720703125 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.8241, loss_val: nan, pos_over_neg: 343.1136474609375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 437.0045166015625 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1453.9031982421875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 988.6363525390625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1468.8133544921875 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 3120.544189453125 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 594.1304321289062 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 3163.64453125 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 26648.23828125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 530.4164428710938 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.8202, loss_val: nan, pos_over_neg: 653.61376953125 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 4176.9560546875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 964.1487426757812 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 426.72442626953125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 2710.89794921875 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1317.6871337890625 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 749.3316650390625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 486.4423522949219 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 759.6563720703125 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 5131.107421875 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 643.6033325195312 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1197.5244140625 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 897.130859375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 966.3455810546875 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.8156, loss_val: nan, pos_over_neg: 393.6354064941406 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 693.9427490234375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 450.16058349609375 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1076.294921875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 106919.8984375 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: -3397.315673828125 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: -5197.123046875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 3934.0126953125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 946.2369384765625 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 766.0114135742188 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1807.034912109375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 12769.0576171875 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1674.4249267578125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 731.5914916992188 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 467.43280029296875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1002.1162719726562 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 8935.2607421875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 572.7791748046875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 2614.563232421875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8125, loss_val: nan, pos_over_neg: 419.3586730957031 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 2327.079345703125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1043.0584716796875 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: -2085.707763671875 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 650.7530517578125 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 619.3650512695312 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1717.6756591796875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1620.150146484375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 9542.1181640625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1029.22314453125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 2185.114013671875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 425.0171813964844 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1944.7720947265625 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.8126, loss_val: nan, pos_over_neg: 1649.0714111328125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 724.064208984375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 476.6556396484375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: -2422.6826171875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 741.0624389648438 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 466.5634460449219 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 30195.052734375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1487.59326171875 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 4326.4765625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2362.31640625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 812.2713623046875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 825.9065551757812 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 615.0906982421875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: -1879.7386474609375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1843.1119384765625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1716.8936767578125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 1090.2593994140625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 12748.916015625 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1136.0177001953125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 676.0687255859375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 434.2704162597656 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 612.4511108398438 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 2180.54443359375 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 346.45355224609375 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1080.908203125 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 920.37109375 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 562.7703247070312 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 492989.15625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.82, loss_val: nan, pos_over_neg: 338.01220703125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: -16869.29296875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 902.6575927734375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1750.488525390625 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 804.6975708007812 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 1690.6673583984375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 850.6165771484375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 279.65325927734375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: -9493.49609375 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1462.52001953125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 584.7715454101562 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: -44395.796875 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1340.50439453125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 381.44903564453125 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 29656.103515625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1506.8211669921875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1661.3121337890625 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 874.22802734375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 459.77276611328125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1152.4139404296875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1173.9412841796875 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 775.2019653320312 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 755.7506713867188 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 982.4976196289062 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1751.183837890625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 742.9842529296875 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 1132.0208740234375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.8043, loss_val: nan, pos_over_neg: 708.4740600585938 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.8096, loss_val: nan, pos_over_neg: 1403.37646484375 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 2365.86279296875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 12226.8056640625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1175.0821533203125 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 486.5912780761719 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 642.9802856445312 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1431.580322265625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 943.405029296875 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1273.89208984375 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8148, loss_val: nan, pos_over_neg: 1046.049560546875 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: -2569.3681640625 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 979.517333984375 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1102.0234375 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 883.7259521484375 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 644.7359619140625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 341.05194091796875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 990.8992919921875 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 833.6663208007812 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 896.5862426757812 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1572.7034912109375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 3110.22265625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.8119, loss_val: nan, pos_over_neg: 1459.8543701171875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.8112, loss_val: nan, pos_over_neg: 420.33123779296875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 2667.950439453125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 2758.93505859375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 2750.984375 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 5058.8603515625 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 2687.55859375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 745.8306274414062 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.8114, loss_val: nan, pos_over_neg: 292.27337646484375 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 7375.783203125 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 679.443603515625 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 560.0426025390625 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 1442.7279052734375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 814.7990112304688 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 4156.22314453125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.8009, loss_val: nan, pos_over_neg: 3432.33935546875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1294.2523193359375 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 2624.499755859375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1746.5245361328125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 44452.32421875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1320.74755859375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1119.55615234375 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 4867.84375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1174.25634765625 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1271.60498046875 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 888.5654907226562 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 473.99493408203125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 434.701904296875 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 2701.19140625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 787.5861206054688 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1294.389404296875 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1135.5089111328125 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 6277.41748046875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 2264.95849609375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.8067, loss_val: nan, pos_over_neg: 902.8145751953125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1519.2254638671875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1427.1536865234375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 2347.820068359375 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1821.9781494140625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 341.1569519042969 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 660.5829467773438 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 604.0094604492188 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 1667.6143798828125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 497.332275390625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 606.27197265625 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.8102, loss_val: nan, pos_over_neg: 492.1229248046875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: -6072.42041015625 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 718.251220703125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 1881.002197265625 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 2575.7001953125 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1007.7611083984375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 891.9595336914062 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 395.7416076660156 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 686.8917846679688 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 423.3965759277344 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1512.119140625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 1102.4959716796875 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1364.230224609375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.8075, loss_val: nan, pos_over_neg: 1520.9395751953125 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 449865.6875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 208.90234375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 756.8041381835938 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 955.8714599609375 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 5030.36572265625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 2046.95751953125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1181.051025390625 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1086.593017578125 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 636.0056762695312 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 514.889404296875 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1871.567138671875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 608.7701416015625 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 507.4560241699219 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 591.3200073242188 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 949.9481811523438 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1003.61083984375 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1875.06982421875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 33236.65625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 484.3481140136719 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 8392.205078125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1549.953857421875 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 2205.156982421875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.8021, loss_val: nan, pos_over_neg: 1487.2054443359375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 2869.2666015625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1407.003173828125 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 918.4607543945312 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 3249.57080078125 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1139.490478515625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 575.0432739257812 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1263.3544921875 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 894.7313232421875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 2500.28857421875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.822, loss_val: nan, pos_over_neg: -5983.017578125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: -3666.220458984375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 970.4864501953125 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.8069, loss_val: nan, pos_over_neg: 1518.7015380859375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 619.5143432617188 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 571.2394409179688 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 3065.466796875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 313.0130310058594 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1065.1107177734375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 941.3533325195312 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 550.4016723632812 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1597.7947998046875 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.8055, loss_val: nan, pos_over_neg: -18949.755859375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 3789.200927734375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 9251.6337890625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8079, loss_val: nan, pos_over_neg: 1104.802001953125 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: 843.340087890625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 5157.8056640625 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1435.2236328125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.8042, loss_val: nan, pos_over_neg: 908.7479858398438 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 1021.6717529296875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1644.4586181640625 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 378.939453125 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 482.21295166015625 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 1809.06298828125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.806, loss_val: nan, pos_over_neg: 3942.036376953125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: -2452.406982421875 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 12453.48828125 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1513.49267578125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.8127, loss_val: nan, pos_over_neg: 662.9401245117188 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 855.6112670898438 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 911.5366821289062 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1266.597900390625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 1127.70849609375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 584.5131225585938 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 541.8765869140625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.801, loss_val: nan, pos_over_neg: -5941.9296875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1926.4161376953125 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8177, loss_val: nan, pos_over_neg: 1321.3314208984375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.8072, loss_val: nan, pos_over_neg: 33150.1015625 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: -3951.2529296875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 3113.808837890625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8278, loss_val: nan, pos_over_neg: 1034.947265625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1886.599853515625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 7650.62255859375 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 641.672119140625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 380.525634765625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.8134, loss_val: nan, pos_over_neg: 303.25958251953125 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 434.6826171875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.8071, loss_val: nan, pos_over_neg: 609.5687866210938 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 728.359130859375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1485.2608642578125 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.8145, loss_val: nan, pos_over_neg: 411.47076416015625 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.8093, loss_val: nan, pos_over_neg: 479.25518798828125 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 6002.9404296875 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1446.573486328125 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 770.0738525390625 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 752.0462646484375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 28797.40234375 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 2067.618408203125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1185.3394775390625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 8645.9443359375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1600.52734375 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 797.2920532226562 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 573.787353515625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 862.7802734375 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 8957.259765625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 3399.297119140625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 2190.15478515625 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2635.987060546875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 544.3974609375 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1479.0372314453125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 547.315673828125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.8173, loss_val: nan, pos_over_neg: 736.537841796875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1370.3603515625 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1582.0089111328125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: -8764.9443359375 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 3354.2509765625 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 458.3153991699219 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 503.6397399902344 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 3532.61181640625 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1076.7056884765625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 998.082275390625 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1227.3348388671875 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 332.5079650878906 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 734.0576171875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 5279.69873046875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 2138.21142578125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 412.93157958984375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1087.586181640625 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 578.6959228515625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1062.64892578125 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 446.4833984375 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 786.9579467773438 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 538.3494262695312 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1321.43017578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2627.92041015625 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 951.69970703125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1202.435546875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1764.9647216796875 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 1142.3721923828125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 992.2572631835938 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1277.7899169921875 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1553.0784912109375 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1958.446533203125 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 950.8450317382812 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: -5268.517578125 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.8124, loss_val: nan, pos_over_neg: 410.45758056640625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 735.5425415039062 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 769.5172729492188 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 580.5279541015625 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 708.423095703125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 681.7199096679688 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 961.4198608398438 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.8175, loss_val: nan, pos_over_neg: 326.5954284667969 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2731.1328125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 402.5243225097656 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 5884.27197265625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: -2909.037353515625 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1056.8409423828125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 680.3994750976562 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 910.8079833984375 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 731.0872192382812 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 1056.2197265625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 2002.6817626953125 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 844.7901611328125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1324.359619140625 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 749.7598266601562 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 778.6880493164062 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 14166.3095703125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 631.8912963867188 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 295.91900634765625 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 504.6229248046875 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8086, loss_val: nan, pos_over_neg: 1065.810302734375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 792.8189086914062 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 677.406982421875 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 733.2405395507812 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 585.2723999023438 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 389.31146240234375 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 743.6594848632812 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 434.5022277832031 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 510.7799987792969 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 302.8984069824219 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 451.7978515625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 386.3437805175781 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 614.189208984375 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 459.47052001953125 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 693.482666015625 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.8158, loss_val: nan, pos_over_neg: 1002.3206787109375 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 994.5934448242188 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 797.3042602539062 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1528.22314453125 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 782.243896484375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 412.3659362792969 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 577.9100341796875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 622.5469360351562 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.8095, loss_val: nan, pos_over_neg: 711.6615600585938 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: -40751.99609375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 800.9760131835938 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 538.6214599609375 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 871.4259033203125 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 1506.3646240234375 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 3579.287109375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1458.6492919921875 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1218.874755859375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 468.5707702636719 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1837.4395751953125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1361.056640625 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 733.2672119140625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 525.130126953125 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 583.2566528320312 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1758.3280029296875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.8108, loss_val: nan, pos_over_neg: 407.4639587402344 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 447.5179138183594 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 1000.0250854492188 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.8064, loss_val: nan, pos_over_neg: 1035.368408203125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 2517.343994140625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 11507.20703125 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 8122.099609375 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 3012.6513671875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 475.06304931640625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1624.328125 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 807.0089721679688 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 867.7469482421875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 713.5565185546875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/300000 [3:25:28<128194:32:29, 1538.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8\n",
      "Iter: 0/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 574.9132690429688 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1261.04443359375 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 667.598876953125 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 537.9375610351562 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1142.9503173828125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 565.7656860351562 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 266.8775939941406 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 857.7625122070312 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 348.0204772949219 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: -3387.114501953125 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: -2806.65966796875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 5127.74072265625 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 2594.5810546875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 796.4017333984375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 2337.589111328125 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 576.5182495117188 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 577.2894897460938 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 904.2962036132812 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 1462.7681884765625 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 536.7822875976562 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 2373.2783203125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 459.0380554199219 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 750.1854858398438 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 20046.5078125 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 3437.96826171875 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 522.7824096679688 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 598.8435668945312 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 582.344970703125 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 2183.46826171875 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 870.6742553710938 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 342.3628234863281 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 915.4598388671875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -2053.0927734375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 942.9266357421875 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 3994.83642578125 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1918.6351318359375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: -25359.7578125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.784, loss_val: nan, pos_over_neg: -7560.42578125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 5238.86376953125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 783.529296875 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 344.4831237792969 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 594.0056762695312 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 370.5230712890625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 886.8531494140625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 487.8527526855469 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 304.3106689453125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 381.9113464355469 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 1068.8406982421875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 623.4267578125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 4559.5771484375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: -2240.182861328125 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: -4826.07080078125 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1639.9610595703125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 1075.1763916015625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 575.6478881835938 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 909.4198608398438 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 984.2634887695312 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 987.2828369140625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 394.57843017578125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 544.364990234375 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 719.6019287109375 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -34474.59765625 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 2531.509521484375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 509.669921875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1536.6934814453125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 580.11962890625 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: -3503.282958984375 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.784, loss_val: nan, pos_over_neg: -3679.606689453125 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8186, loss_val: nan, pos_over_neg: 344.37371826171875 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 470.0282897949219 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 9294.3935546875 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 2024.6123046875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -12319.1064453125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: -2866.640625 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 677.2742309570312 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1827.501953125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 2474.705810546875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1154.466552734375 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 898.58544921875 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 405.1467590332031 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 646.0560913085938 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 657.3645629882812 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 444.4388427734375 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 413.9692687988281 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 3595.854736328125 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 2242.415771484375 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: 1805.0418701171875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 1966.281494140625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: -1520.0540771484375 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1004.9712524414062 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: 1180.560791015625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 453.98675537109375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 853.6674194335938 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 958.9879150390625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 803.20947265625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 8812.6494140625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 819.2518920898438 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1129.199951171875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1424.4598388671875 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 643.8283081054688 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 820.8999633789062 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1343.7205810546875 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 333.8875427246094 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1000.9881591796875 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 478.27691650390625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1547.05712890625 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1031.233154296875 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 712.3208618164062 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1060.8839111328125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 449.1432800292969 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.8225, loss_val: nan, pos_over_neg: 1527.061767578125 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: -5003.51416015625 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1259.6727294921875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.8085, loss_val: nan, pos_over_neg: 1981.2763671875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 549.4644775390625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.8036, loss_val: nan, pos_over_neg: -5895.3046875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 716.4105834960938 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 823.3232421875 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 2406.908447265625 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 675.1873779296875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 4185.84033203125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 699.8892211914062 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 597.8571166992188 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1114.8115234375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 454.71221923828125 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 710.0340576171875 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 2988.628662109375 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1007.0933837890625 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.8026, loss_val: nan, pos_over_neg: 8391.9541015625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.808, loss_val: nan, pos_over_neg: 678.7363891601562 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1876.5926513671875 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 810.592041015625 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 859.8970336914062 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.8073, loss_val: nan, pos_over_neg: 524.12109375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 410.0513916015625 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1627.25634765625 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 36399.18359375 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1213.885009765625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 16334.7939453125 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1881.17138671875 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 2709.125244140625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1006.5558471679688 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1893.4449462890625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: -6835.65185546875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.8058, loss_val: nan, pos_over_neg: 855.2466430664062 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 541.9918212890625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 618.906494140625 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1287.3734130859375 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 942.8402709960938 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 4079.28662109375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 3530.0966796875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 1427.353271484375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 457.00152587890625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 542.684326171875 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1343.850341796875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 391.5028381347656 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 482.6078186035156 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 2041.8045654296875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 3504.79638671875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1698.4342041015625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 939.6660766601562 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: -4777.9404296875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 4179.521484375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2532.8369140625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 2898.642578125 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 791.1085205078125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 1397.948974609375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 690.5950927734375 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 681.37939453125 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 662.4468994140625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.8121, loss_val: nan, pos_over_neg: 288.0870056152344 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1447.235107421875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 983.600341796875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 968.9669189453125 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 466.241455078125 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 822.351806640625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 3174.314208984375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 520.77685546875 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 691.1007690429688 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 2196.65869140625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 2128.9306640625 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1526.75732421875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 991.373291015625 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 2121.317138671875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 466.1673583984375 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: -3145.34130859375 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 2368.556396484375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1378.3287353515625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1068.9539794921875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 988.3927001953125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1127.3328857421875 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 818.4559326171875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1373.234130859375 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 3360.50927734375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.8133, loss_val: nan, pos_over_neg: 520.6586303710938 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 1217.9632568359375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2855.126953125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 850.9071044921875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.79, loss_val: nan, pos_over_neg: -8428.1376953125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: -19988.810546875 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 1559.310791015625 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 2686.096435546875 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1411.6982421875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.8094, loss_val: nan, pos_over_neg: 500.3116149902344 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 9607.822265625 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 9819.2646484375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1919.7869873046875 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2272.541015625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8024, loss_val: nan, pos_over_neg: 515.400390625 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.787, loss_val: nan, pos_over_neg: -1796.1888427734375 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: -31573.0546875 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1939.4569091796875 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: -67445.609375 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: -11521.5947265625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1807.0791015625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: -4297.93115234375 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 2973.442626953125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1375.2662353515625 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2481.41357421875 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1046.1064453125 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1810.76953125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 613.40625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1172.0985107421875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 7641.279296875 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 876.3241577148438 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 674.6119995117188 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: -6214.59375 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 557.4016723632812 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 863.343505859375 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 1756.1064453125 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 956.23095703125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 666.8819580078125 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 517.4078369140625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 641.5077514648438 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1654.1446533203125 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 973.2208862304688 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 6195.5966796875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: -8621.1162109375 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 6280.04248046875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1892.770751953125 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1522.5849609375 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1565.620849609375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 353.23095703125 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1022.6377563476562 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 314.88568115234375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 504.2319030761719 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1711.662841796875 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 457.2923889160156 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 991.7348022460938 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1216.3321533203125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 9205.06640625 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: -65974.015625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1571.3597412109375 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 720.4010620117188 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 1198.5693359375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 2465.631103515625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.8138, loss_val: nan, pos_over_neg: 504.10748291015625 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1282.09375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 635.783935546875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.8049, loss_val: nan, pos_over_neg: 641.1438598632812 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 405.2198791503906 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 2080.161865234375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1030.5638427734375 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 928.7171020507812 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 580.2562255859375 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 679.7339477539062 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 917.6973266601562 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 5134.29150390625 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 3636.528076171875 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1487.3228759765625 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 976.3012084960938 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 497.3512268066406 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1085.2744140625 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1051.3897705078125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.8054, loss_val: nan, pos_over_neg: 736.4164428710938 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1206.7135009765625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 30540.037109375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 909.3292236328125 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1268.468017578125 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 838.6065063476562 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 3706.765380859375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 938.7970581054688 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1870.1884765625 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 169.4188995361328 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 514.482177734375 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1029.575927734375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 558.7788696289062 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1019.504638671875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 904.4017944335938 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1184.77587890625 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 690.5401611328125 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1399.8955078125 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1562.93505859375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 713.0108032226562 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 780.5618286132812 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 711.1199951171875 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 1081.095703125 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1115.94091796875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 416.958251953125 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 1663.9105224609375 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 3058.57080078125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 715.8049926757812 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 2435.70703125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1507.9056396484375 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 317.7430725097656 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 895.835205078125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1495.163818359375 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 2407.222900390625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 849.6723022460938 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 2285.543212890625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 2071.16552734375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2088.233642578125 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2130.137939453125 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 552.084228515625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 915.486328125 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 3612.870849609375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 892.2352294921875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 967.8742065429688 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 209358.296875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 429.1549072265625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 58724.41796875 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 3659.986328125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: -9101.087890625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 804.6146240234375 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 2358.43603515625 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 667.9613647460938 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 430.3116760253906 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 765.97607421875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 301.4579772949219 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1233.3416748046875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 393.70196533203125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1222.2254638671875 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 548.1546630859375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: -3426.096923828125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1315.1004638671875 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1042.2796630859375 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 1094.471435546875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 460.5437316894531 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1257.78662109375 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1942.82470703125 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 904.2462768554688 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.8013, loss_val: nan, pos_over_neg: 1046.9993896484375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 654.1581420898438 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 1701.3385009765625 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7925, loss_val: nan, pos_over_neg: 1339.0135498046875 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1378.0101318359375 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 6177.388671875 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 2737.314453125 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 520.1536254882812 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.8006, loss_val: nan, pos_over_neg: 630.4810180664062 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1239.297607421875 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 7535.611328125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.8074, loss_val: nan, pos_over_neg: -11434.9873046875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 649.9609985351562 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 564.9559936523438 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1161.7784423828125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 2548.48583984375 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 823.469482421875 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1231.124755859375 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8, loss_val: nan, pos_over_neg: 734.219970703125 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 513.1205444335938 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 5900.5966796875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 685.46630859375 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 1700.7445068359375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 10580.9931640625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 4209.279296875 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 65139.66015625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 665.735107421875 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 2297.638671875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 473.9453430175781 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 470.5205993652344 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1788.9705810546875 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 2366.273681640625 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 4947.912109375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 466.6045837402344 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1561.5035400390625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 5310.14306640625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1177.867919921875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1671.9849853515625 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1088.2647705078125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 666.8117065429688 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 1975.827880859375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 770.8712158203125 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 989.2923583984375 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.774, loss_val: nan, pos_over_neg: -10584.55859375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2946.925048828125 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: -2218.3388671875 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 19572.876953125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 464.03863525390625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 816.7315063476562 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 317.2429504394531 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1659.90576171875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 747.1800537109375 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1007.2496337890625 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1990.41064453125 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 2546.576171875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1469.4451904296875 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2188.47265625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 709.2974243164062 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1098.287841796875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 556.578369140625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 602.2366943359375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 505.6789855957031 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 1566.9820556640625 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 700.2505493164062 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 656.101806640625 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 528.4354248046875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.8048, loss_val: nan, pos_over_neg: 995.682373046875 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1829.7763671875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1203.5396728515625 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1416.44775390625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 2597.519287109375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 636.426513671875 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1988.51025390625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 4813.90869140625 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 2794.25146484375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1671.7301025390625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 2417.031982421875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 4200.1044921875 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 12544.38671875 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 473.9686584472656 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 394.1541748046875 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 495.39764404296875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 533.7440185546875 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 1178.5465087890625 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: -6021.78662109375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 968.2319946289062 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 880.8587646484375 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 871.6199951171875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 3813.349365234375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 2506.427001953125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 863.5159912109375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 17294.029296875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 7282.55810546875 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 544.9154052734375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 2380.073974609375 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: -16917.048828125 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 817.0096435546875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 640.1034545898438 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 910.1978759765625 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 542.482421875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 3200.228759765625 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1610.9873046875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 563.2666625976562 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 1793.5576171875 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 954.353759765625 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 2692.024169921875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: -3392.002197265625 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.8057, loss_val: nan, pos_over_neg: 1550.921142578125 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 508.60174560546875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 822.1168212890625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 496.1192321777344 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1099.4658203125 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 2764.32861328125 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 409.9981689453125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 3556.476318359375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 877.4204711914062 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 3075.078369140625 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 688.7718505859375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 711.0532836914062 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 1631.7615966796875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: -2757.32177734375 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 364.1319274902344 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 3387.08203125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 1891.0128173828125 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1979.28564453125 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1002.033203125 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1743.1502685546875 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 722.6824951171875 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 499.1318359375 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1577.6239013671875 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2501.814208984375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 2441.1806640625 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1796.30078125 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1167.98291015625 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 502.22039794921875 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1388.58984375 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.8019, loss_val: nan, pos_over_neg: 431.9206848144531 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 818.7943115234375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 3455.04541015625 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1510.295654296875 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.809, loss_val: nan, pos_over_neg: 977.2894897460938 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 622.4397583007812 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 3039.351806640625 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 6005.6875 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 1573.203857421875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 21853.97265625 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1055.0347900390625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 322.3481140136719 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1048.3330078125 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 1551.2464599609375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 435.06878662109375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1404.6123046875 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 655.6023559570312 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 632.7133178710938 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 9572.845703125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 2330.219970703125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1173.0711669921875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 322.1579284667969 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.819, loss_val: nan, pos_over_neg: 744.4818115234375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 739.80419921875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 2529.693359375 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: -8226.484375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -2386.58740234375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 636.254150390625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 481.3249206542969 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 877.6735229492188 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 795.0172119140625 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 689.11572265625 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 640.1258544921875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1149.420166015625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: -2758.015869140625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1184.34716796875 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 561.748779296875 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1264.8509521484375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 1241.3262939453125 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 5210.1376953125 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 4339.50341796875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1076.82861328125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 508.8249816894531 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 3877.928466796875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 443.3504333496094 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 2137.661376953125 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 355.7962646484375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 6252.95263671875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2170.018798828125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1332.9700927734375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 5718.12744140625 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 1406.5712890625 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: -41624.10546875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 627.147705078125 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 1164.4654541015625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 591.3814697265625 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1544.3515625 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 8540.0263671875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 961.8956298828125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1265.9393310546875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 559.8189086914062 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 756.0426635742188 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7623, loss_val: nan, pos_over_neg: 2516.8388671875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1076.1414794921875 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 719.9248657226562 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 755.8110961914062 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 4638.90869140625 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 347.000732421875 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.8157, loss_val: nan, pos_over_neg: 812.0721435546875 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 683.8455810546875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 754.3134765625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 872.5296630859375 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 2204.1494140625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 462.010986328125 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.8015, loss_val: nan, pos_over_neg: 1035.9686279296875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 655.6090087890625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 3816.098388671875 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7991, loss_val: nan, pos_over_neg: 1130.8477783203125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 436.88397216796875 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.8246, loss_val: nan, pos_over_neg: 498.0749816894531 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 2759.919189453125 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.803, loss_val: nan, pos_over_neg: 1257.4698486328125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 317.838134765625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.802, loss_val: nan, pos_over_neg: 6238.56787109375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 2363.70703125 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: -45701.64453125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 388.3179626464844 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 634.0877685546875 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 1236.2467041015625 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 873.572998046875 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: -1759.35009765625 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 986.4898681640625 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 1764.9317626953125 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 488.9993591308594 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 537.4489135742188 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 590.7633056640625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 1230.4278564453125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 551.5512084960938 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 1582.708251953125 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1701.165283203125 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 498.789794921875 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1681.7127685546875 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 8439.2080078125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 13794.0341796875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 800.9846801757812 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 763.1649169921875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1075.7747802734375 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 789.0804443359375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 855.951904296875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 445.5603332519531 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1494.209716796875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 3934.853515625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.804, loss_val: nan, pos_over_neg: 551.2326049804688 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 774.2413940429688 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 604.4560546875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.812, loss_val: nan, pos_over_neg: 335.3678894042969 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 522.4066772460938 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1160.911865234375 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1504.2554931640625 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 607.6875610351562 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 492.3089599609375 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 227484.171875 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1940.0650634765625 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 948.1698608398438 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.8031, loss_val: nan, pos_over_neg: 833.429931640625 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: -4875.8525390625 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1603.5439453125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 332.10260009765625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 383.9453430175781 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.8046, loss_val: nan, pos_over_neg: 424.8648681640625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 629.3258666992188 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 906.4561157226562 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 495.0616760253906 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 417.2828369140625 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 968.4118041992188 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 736.5056762695312 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 588.0448608398438 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1109.2919921875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 2713.955810546875 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 2005.69775390625 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.8082, loss_val: nan, pos_over_neg: 381.6543273925781 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 745.7528076171875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 492.1584777832031 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: -36035.96484375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 4398.18408203125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 755.4085693359375 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1063.5323486328125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 801.4895629882812 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 728.3883056640625 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1166.857421875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 861.8745727539062 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1414.646484375 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 1674.0029296875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 712.81982421875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: 1401.3212890625 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: -2283.1796875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7614, loss_val: nan, pos_over_neg: 10889.966796875 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 996.348876953125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 612.5435791015625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 751.6865844726562 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 2288.08837890625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1044.5997314453125 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1733.140625 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 470.8803405761719 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 274.75634765625 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 873.3453979492188 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1183.89990234375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 773.5349731445312 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 2886.3046875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 724.0071411132812 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 1468.97265625 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: -2688.52294921875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 2430.16552734375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 664.7254638671875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 1519.317138671875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 558.1337890625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 763.5740966796875 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 409.6871337890625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1218.828857421875 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 374.2215576171875 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 684.5217895507812 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1247.619873046875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1618.36279296875 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7997, loss_val: nan, pos_over_neg: 910.94873046875 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 893.6763305664062 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 440.1312561035156 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 921.0413818359375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 1172.835693359375 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1547.0589599609375 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: -6071.11474609375 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 2086.373779296875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 446.6485900878906 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 496.9787902832031 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 599.8516235351562 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 1080.2196044921875 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 317.62933349609375 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 417.8835754394531 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 808.76953125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 320.326904296875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 497.885498046875 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 617.5741577148438 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1615.739501953125 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1801.603515625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 446.0389404296875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1347.2779541015625 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1784.649169921875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 529.5509033203125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 1364.234619140625 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.8037, loss_val: nan, pos_over_neg: 393.9358215332031 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 404.5643005371094 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1887.0743408203125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1143.9189453125 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 531.0897827148438 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 1741.8388671875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 813.6095581054688 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1802.752197265625 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 711.9454956054688 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 817.2672119140625 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 3196.56787109375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2458.464111328125 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1918.9940185546875 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 3725.547607421875 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 2735.474853515625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 482.443359375 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 1607.5712890625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 2461.392333984375 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 1580.54296875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 9318.6962890625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 1423.8134765625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 717.6980590820312 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 337.64111328125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 3195.31640625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 15707.04296875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 3538.036865234375 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1025.6890869140625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1795.4146728515625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 2355.827392578125 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 841.5985717773438 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 521.1285400390625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 8174.67822265625 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 879.786865234375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 900.2199096679688 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 830.8034057617188 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7976, loss_val: nan, pos_over_neg: 672.1063842773438 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: -18934.814453125 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: -4457.734375 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1461.5460205078125 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.818, loss_val: nan, pos_over_neg: 354.9115295410156 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 527.306884765625 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1912.681396484375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 585.8231811523438 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 891.3523559570312 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 1680.8221435546875 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 892.804443359375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1385.1649169921875 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 50454.8359375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 3894.835693359375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 4528.8056640625 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 496.79571533203125 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 381.2512512207031 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 479.2602233886719 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 457.79498291015625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 751.7630004882812 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 803.5889282226562 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 619.9309692382812 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.8022, loss_val: nan, pos_over_neg: 577.476318359375 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1087.64404296875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7967, loss_val: nan, pos_over_neg: 996.990478515625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1619.0841064453125 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.8103, loss_val: nan, pos_over_neg: 314.2150573730469 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 380.8450622558594 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.8081, loss_val: nan, pos_over_neg: 403.8335266113281 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 492.2614440917969 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 424.0779113769531 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.7986, loss_val: nan, pos_over_neg: 3484.871337890625 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 852.1851196289062 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: -43222.06640625 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: -44318.9296875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 4936.716796875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: -2245.85205078125 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1737.467041015625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 650.921142578125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 59353.49609375 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 602.0499267578125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1245.2430419921875 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 417.1294250488281 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 505.8967590332031 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 420.1133117675781 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 462.7090148925781 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 2416.000732421875 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 409.8302001953125 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1614.287109375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1108.448486328125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1074.8741455078125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1752.08203125 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 341.80419921875 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1814.1072998046875 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 756.7125854492188 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 881.4664306640625 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 362.4024963378906 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 707.3868408203125 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 887.5376586914062 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: -4666.7861328125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 702.5626831054688 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 1783.3951416015625 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 616.745361328125 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 3179.21875 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 573.4238891601562 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1322.0169677734375 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1114.961181640625 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 483.5140380859375 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 947.8844604492188 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 2138.556396484375 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 8107.61572265625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 2546.20361328125 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 876.4984741210938 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1963.066650390625 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 3690.639892578125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1008.6340942382812 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 583.6005859375 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2469.61474609375 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 853.1420288085938 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 880.7215576171875 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 804.2439575195312 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 485.600341796875 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 3383.517333984375 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: -8914.3916015625 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 588.3422241210938 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 584.3663940429688 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 4581.44287109375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1187.1224365234375 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 394.99560546875 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 453.35870361328125 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1279.298583984375 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 778.1541137695312 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 917.4625244140625 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 784.649169921875 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 4371.08642578125 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 583.0181884765625 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.8098, loss_val: nan, pos_over_neg: 1379.5986328125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1025.639404296875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 651.67041015625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 673.3013916015625 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1332.5067138671875 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1943.4766845703125 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 922.0862426757812 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1191.6051025390625 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 618.4359130859375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 725.998291015625 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: -8949.505859375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 803.0255737304688 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 680.7308349609375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 775.7630615234375 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 746.9522705078125 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1469.1571044921875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1076.9669189453125 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 639.5025024414062 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1487.792724609375 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1109.90087890625 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 738.1226196289062 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 4093.122314453125 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1545.5665283203125 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 1997.4947509765625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 822.0528564453125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 870.651611328125 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 776.042236328125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 341.3298645019531 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 4496.28955078125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 836.2560424804688 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 9688.287109375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 995.1353759765625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 948.0288696289062 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 731.629150390625 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 1647.5064697265625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 839.7796020507812 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 871.7970581054688 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 3750.09716796875 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.8034, loss_val: nan, pos_over_neg: 724.3634643554688 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 923.7989501953125 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 845.522705078125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.784, loss_val: nan, pos_over_neg: -12675.6357421875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 1524.651123046875 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: -6081.71826171875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 586.2615966796875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: -6170.771484375 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 717.5474853515625 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1952.5054931640625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1766.0377197265625 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 574.8933715820312 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 1048.5989990234375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 2757.37109375 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1234.6328125 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 898.7811889648438 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1818.9007568359375 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 8015.67138671875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1175.0419921875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 546.5908813476562 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 913.2728881835938 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 1030.518798828125 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: -36472.515625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.81, loss_val: nan, pos_over_neg: 388.390625 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: -2024.5748291015625 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1169.2598876953125 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 477.58941650390625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1504.8446044921875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 486.134765625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 886.3836669921875 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1118.4736328125 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 597.105224609375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 3626.044677734375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 2144.748046875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 668.0755615234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/300000 [3:50:51<127815:39:07, 1533.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9\n",
      "Iter: 0/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 485.3573913574219 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 610.9149169921875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 495.6209411621094 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1354.791748046875 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 618.1541137695312 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1678.892822265625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 442.46038818359375 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 2437.3525390625 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 701.01416015625 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1733.4007568359375 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 796.8336791992188 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 675.077392578125 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.8062, loss_val: nan, pos_over_neg: 1183.599853515625 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: -47881.21484375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1794.8660888671875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1112.2730712890625 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 4260.8408203125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 2516.988525390625 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 630.110595703125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 555.6727294921875 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 557.5623779296875 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 571.0492553710938 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 991.5748901367188 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 827.4724731445312 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 585.4571533203125 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 675.1815185546875 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 23060.470703125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 1895.5946044921875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: -20556.576171875 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1259.4886474609375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 826.40673828125 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 449.85107421875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 2161.68017578125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 850.1935424804688 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 535.0211181640625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1302.3988037109375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1457.630126953125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 770.73828125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 896.4412231445312 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 4987.9833984375 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 2669.849609375 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 1549.170654296875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1299.8365478515625 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 1896.9166259765625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 742.1754760742188 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2614.190673828125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 996.3463134765625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 1201.2620849609375 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 3274.068359375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: -181063.109375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 10708.693359375 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 735.94384765625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 544.1508178710938 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1032.5804443359375 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 619.0343627929688 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 525.2982177734375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 3498.12451171875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 475.4244079589844 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -4565.333984375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 2016.065185546875 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 469.20977783203125 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 497.57989501953125 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 1519.1578369140625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: -6887.9990234375 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1497.46533203125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1313.673828125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.8017, loss_val: nan, pos_over_neg: 28720.158203125 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1927.3916015625 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1235.0523681640625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1049.984375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 914.9762573242188 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 789.1868286132812 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1552.4044189453125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 4440.00927734375 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.8091, loss_val: nan, pos_over_neg: 679.4783935546875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 409.4766845703125 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 3022.03515625 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: -29031.978515625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 2308.448486328125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7978, loss_val: nan, pos_over_neg: 1152.7408447265625 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 321.2712707519531 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 509.86102294921875 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 828.4398803710938 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 1468.8333740234375 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 10175.916015625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 3167.32666015625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 19198.119140625 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 2446.90869140625 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 847.9754028320312 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 28188.751953125 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 968.3179931640625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2644.96875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1316.264892578125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 962.2023315429688 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 21161.123046875 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 645.8316040039062 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 709.69384765625 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 693.3907470703125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 584.7384033203125 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 955.7753295898438 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 451.311767578125 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 484.7633972167969 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 794.616455078125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1021.2623291015625 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.785, loss_val: nan, pos_over_neg: -1649.689697265625 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 2178.874755859375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 2099.5859375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: -12906.4765625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.8002, loss_val: nan, pos_over_neg: 3791.98876953125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 1067.2176513671875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 505.2400207519531 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 1456.639404296875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 8000.93408203125 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 9511.7353515625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 541.5875854492188 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1102.0164794921875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 859.9363403320312 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.8051, loss_val: nan, pos_over_neg: 714.49462890625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: -2388.35888671875 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 2172.169677734375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 631.6436157226562 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 441.3839111328125 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 554.9432983398438 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1146.6217041015625 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.795, loss_val: nan, pos_over_neg: 668.6673583984375 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 639.1209106445312 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 9285.4833984375 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1333.77783203125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 727.3547973632812 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: -3760.19287109375 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 2359.113037109375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: -2069.388427734375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1409.0999755859375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 912.908935546875 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1992.2039794921875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 930.2215576171875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 540.2686157226562 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 2438.58056640625 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 691.49609375 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1645.8804931640625 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 430.95050048828125 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7955, loss_val: nan, pos_over_neg: 1153.002685546875 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1203.2684326171875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1243.73779296875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7931, loss_val: nan, pos_over_neg: 874.7344970703125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 686.8052978515625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2931.08203125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 5633.70263671875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 2880.857421875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 484.9893798828125 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1789.887451171875 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: -6911.0263671875 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 1787.7613525390625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1267.7349853515625 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 904.6395263671875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1123.2398681640625 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1120.6378173828125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1239.22119140625 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 212.8785858154297 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 420.2721862792969 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1285.1343994140625 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 3804.575439453125 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 45568.98828125 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1218.041015625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 441.1502685546875 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 499.9308776855469 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 11232.4658203125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: -8468.0400390625 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 912.0482788085938 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1411.9893798828125 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 523.6937866210938 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1020.6774291992188 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 384.2426452636719 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.8135, loss_val: nan, pos_over_neg: 332.056640625 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1348.9222412109375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 5230.279296875 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 515.076171875 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7965, loss_val: nan, pos_over_neg: 979.6782836914062 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 604.5453491210938 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 806.5502319335938 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 3281.217529296875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 14349.4912109375 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 677.6102905273438 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 792.8057861328125 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 722.77197265625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7948, loss_val: nan, pos_over_neg: 868.6996459960938 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 625.0383911132812 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 806.254638671875 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1178.0914306640625 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 789.3336181640625 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 726.889892578125 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 6959.72265625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: -40408.984375 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1166.7940673828125 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 362.1484375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 784.0391845703125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 995.1549072265625 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 1203.5467529296875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1714.5498046875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 79797.5390625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 805.27001953125 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 1307.818359375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 14982.9775390625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 155078.328125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 457.4379577636719 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 6371.9619140625 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 690.2183227539062 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 766.9501953125 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 610.2792358398438 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 562.8587036132812 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 473.5418701171875 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.8008, loss_val: nan, pos_over_neg: 462.30267333984375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 902.8699951171875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 3388.613525390625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 860.3545532226562 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1897.18310546875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1396.9256591796875 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 554.2738647460938 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 2082.628662109375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 6185.66943359375 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 900.8165283203125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 8050.81640625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 2831.362060546875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1446.7010498046875 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 999.03515625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 779.8541259765625 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1133.262939453125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 721.7517700195312 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1837.4705810546875 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2172.724853515625 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 312.0010681152344 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: -2277.951416015625 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1099.9964599609375 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 789.3204345703125 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1223.3021240234375 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 832.0913696289062 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: -9671.7412109375 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 243.39370727539062 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 3558.76123046875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 907.6527099609375 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 909.4172973632812 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 2379.72412109375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: -17330.671875 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 4882.64111328125 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1263.2025146484375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 2018.7310791015625 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 757.3365478515625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 2733.898681640625 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 4261.14208984375 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1528.9053955078125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1526.6219482421875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 1793.0279541015625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 8164.3359375 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 16113.443359375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 902.0849609375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.8041, loss_val: nan, pos_over_neg: 849.5930786132812 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 632.62255859375 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 689.89892578125 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 680.3563232421875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 673.414794921875 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 807.964111328125 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1608.220947265625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1066.430419921875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 541.3059692382812 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: -8185.26806640625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 543.9697265625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 3255.055908203125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 8973.1875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 4209.4462890625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 415.9446105957031 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 590.3523559570312 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1311.89306640625 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 2315.520263671875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.774, loss_val: nan, pos_over_neg: -21995.09765625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: -1964.246826171875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7933, loss_val: nan, pos_over_neg: 1690.36572265625 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 3678.63818359375 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 18091.046875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 713.5274658203125 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1419.517578125 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1764.074951171875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 15327.421875 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 3034.251953125 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 451.32720947265625 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1957.1094970703125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 670.8773193359375 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 566.2063598632812 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.8123, loss_val: nan, pos_over_neg: 262.28436279296875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 872.7874755859375 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 770.8121948242188 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 755.2928466796875 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 858.8228149414062 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 851.0503540039062 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 1500.440673828125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 913.2296142578125 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 1194.4429931640625 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 550.7156982421875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 779.8668823242188 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 2143.90185546875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1091.23779296875 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 812.7229614257812 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1418.694580078125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 553.564697265625 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 2708.4560546875 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 1254.202880859375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2025.4620361328125 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 652.6114501953125 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 661.0358276367188 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1712.81640625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 671.0523071289062 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 627.5252685546875 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 669.1099243164062 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 660.2564086914062 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 1765.5482177734375 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.794, loss_val: nan, pos_over_neg: -9135.4853515625 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: -2279.794677734375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 904.37060546875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 9596.3564453125 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1289.764892578125 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1206.54345703125 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 525.1619262695312 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1327.659423828125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 603.3336181640625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 457.1375732421875 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 630.6725463867188 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 897.185546875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 785.5723266601562 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 589.9542846679688 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7927, loss_val: nan, pos_over_neg: 346.4182434082031 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 99109.8359375 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 2018.6920166015625 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1038.7908935546875 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 692.7172241210938 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 695.371826171875 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.8011, loss_val: nan, pos_over_neg: 343.6180114746094 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 3861.8623046875 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 2991.37451171875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1289.888671875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.8087, loss_val: nan, pos_over_neg: 710.4654541015625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 624.06982421875 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 4306.4580078125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 641.3250732421875 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1159.0931396484375 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 4881.75 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: -146529.75 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.8052, loss_val: nan, pos_over_neg: 2355.724853515625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1015.9150390625 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1320.727294921875 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 728.8153686523438 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 364.1044921875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 2061.133544921875 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: -3222.970703125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 634.3853759765625 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 4419.1171875 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 1893.357421875 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 834.7913818359375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2006.4246826171875 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 3267.823974609375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: -3962.491455078125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.8053, loss_val: nan, pos_over_neg: 550.0634765625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 962.6302490234375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 434.17816162109375 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 575.511962890625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 2034.9415283203125 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 943.9182739257812 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 625.661865234375 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: -130449.796875 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 467.9420166015625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 550.393798828125 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 2278.46630859375 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 607.6629638671875 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 617.1627807617188 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1076.5692138671875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 10019.5498046875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 1058.7314453125 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1098.613525390625 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 544.3829956054688 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1123.5517578125 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 49582.21484375 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: -1813.072998046875 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1103.9227294921875 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 936.199951171875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 796.3815307617188 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 670.2451782226562 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 674.1539306640625 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1226.954345703125 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 7316.73828125 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 838.6752319335938 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 3147.15625 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 613.29736328125 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 355.65380859375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1259.751708984375 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1474.009033203125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 553.6536865234375 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 603.0189819335938 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 379.80316162109375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7984, loss_val: nan, pos_over_neg: 732.3540649414062 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 363.6631164550781 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1018.518798828125 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 3594.548583984375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1625.105712890625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 754.1863403320312 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 80063.953125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 2928.754150390625 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: -4005.97607421875 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: -3246.141357421875 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 466.9422607421875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 1286.92333984375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 520.4209594726562 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 2566.337646484375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 501.5740966796875 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1395.2496337890625 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1883.480224609375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 421.9229736328125 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7972, loss_val: nan, pos_over_neg: 2000.16357421875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 936.9111328125 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 532.52685546875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.7952, loss_val: nan, pos_over_neg: 8936.923828125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 706.6741333007812 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 963.1427001953125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 4253.31396484375 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 658.2177124023438 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1336.341796875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 641.8458251953125 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.8023, loss_val: nan, pos_over_neg: 1280.4649658203125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 536.2877807617188 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 503.6772155761719 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 451.3187255859375 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: -58445.0390625 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1257.9381103515625 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1191.00341796875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1792.287109375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1902.734619140625 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 635.10205078125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 18304.87109375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 7445.1533203125 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 941.7706909179688 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1404.95068359375 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 784.8784790039062 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 334.06622314453125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 1364.0184326171875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1398.4736328125 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 4622.71044921875 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 598.327880859375 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 543.708251953125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 783.7505493164062 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: -3335.4541015625 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 1075.658203125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.805, loss_val: nan, pos_over_neg: 504.0094299316406 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1332.388671875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1640.689453125 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.8004, loss_val: nan, pos_over_neg: 3967.8583984375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 885.112060546875 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 2701.7841796875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 486.051513671875 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1531.679443359375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1675.7042236328125 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.8044, loss_val: nan, pos_over_neg: 579.5281372070312 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 904.1856079101562 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 417.0025634765625 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1202.14453125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1075.58642578125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 479.04193115234375 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 2396.49853515625 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 988.8713989257812 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 929.5099487304688 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1459.4232177734375 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 2527.181396484375 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1099.94873046875 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 2477.56591796875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 2141.973388671875 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1193.3062744140625 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1231.376220703125 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 868.0209350585938 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 1261.750732421875 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: -4039.38232421875 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 2028.05517578125 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 733.4957885742188 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 583.2975463867188 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 551.9353637695312 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1011.25146484375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 367.7348937988281 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 781.0594482421875 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 468.4811706542969 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 604.1702880859375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 1505.205322265625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1559.3612060546875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 570.2792358398438 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 630.2020263671875 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 4969.822265625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: -4248.64208984375 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1232.81640625 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 938.1702270507812 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 480.1588134765625 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7461, loss_val: nan, pos_over_neg: 10508.8837890625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 820.3223876953125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1291.0064697265625 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 853.117919921875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 29929.34375 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 670.6466064453125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1011.235107421875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 753.4981689453125 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 831.7081909179688 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 763.8138427734375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 640.8648681640625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 261.50286865234375 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1101.5185546875 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: -7094.79541015625 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7906, loss_val: nan, pos_over_neg: 410.7794494628906 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 18431.529296875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 919.9210205078125 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 595.5264282226562 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: -7771.3662109375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 913.1190795898438 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: -2551.0693359375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1094.105224609375 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 824.4994506835938 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 724.6319580078125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1787.079345703125 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1612.377197265625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 798.410888671875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 297.3055419921875 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 53580.8359375 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 823.7614135742188 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 7906.73095703125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 2161.688720703125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1091.6109619140625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: -2686.945068359375 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 744.8659057617188 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 2275.96630859375 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 625.933837890625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1913.823974609375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 3549.060546875 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1600.257080078125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 835.2321166992188 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 2115.41162109375 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 448.52825927734375 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 96124.03125 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1337.7979736328125 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 689.0922241210938 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1344.9591064453125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: -8785.853515625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 3875.832275390625 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1118.4315185546875 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 616.2492065429688 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 3298.097900390625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 4426.4482421875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1242.407958984375 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1359.578125 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 3105.98193359375 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: -9267.728515625 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 820.7724609375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 719.9451293945312 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 904.2813720703125 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1252.26953125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 5968.37255859375 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 955.871826171875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 5851.943359375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 2337.311767578125 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1580.90478515625 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 889.4791870117188 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 628.9680786132812 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 2209.0517578125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 2024.8387451171875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 3684.944580078125 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 551.03076171875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 567.6482543945312 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1124.93896484375 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 12738.689453125 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1987.0689697265625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1646.85009765625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.8077, loss_val: nan, pos_over_neg: 257.23858642578125 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 475.3504333496094 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1047.1005859375 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1018.3189086914062 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 2708.33251953125 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 3283.263671875 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 7315.0087890625 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 1973.214111328125 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 1504.5213623046875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 18562.755859375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 2258.99267578125 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 524.2368774414062 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1159.098388671875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 973.6986083984375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1462.248779296875 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 833.9968872070312 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 366.8475036621094 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1140.1500244140625 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 308.2357177734375 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: -8050.23583984375 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 888.7139282226562 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1475.5428466796875 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.8061, loss_val: nan, pos_over_neg: 534.7049560546875 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 610.1083374023438 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 619.7255249023438 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 562.7017822265625 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 595.700927734375 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 4781.78173828125 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1832.3314208984375 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1771.054443359375 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 2615.24609375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: -8573.8876953125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.8025, loss_val: nan, pos_over_neg: 1067.837158203125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.8033, loss_val: nan, pos_over_neg: 514.019775390625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 15989.1455078125 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 788.1536254882812 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 590.0763549804688 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: -14685.59765625 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 823.8381958007812 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 605.1412353515625 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 9007.5849609375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: -3729.34814453125 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1240.054443359375 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 635.7864990234375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: -2436.546142578125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1286.93896484375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: -91970.71875 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 846.638671875 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1680.4444580078125 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 613.0370483398438 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5941.30078125 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 883.9736938476562 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 1277.5433349609375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 2863.83349609375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 948.591552734375 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 874.6480712890625 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 3503.53515625 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 900.640380859375 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1521.4532470703125 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 525.980224609375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 2343.161376953125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 717.9011840820312 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1693.5579833984375 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: -3069.23876953125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 4197.6435546875 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: -25941.361328125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1181.9276123046875 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1207.8936767578125 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: -4688.69677734375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 2626.385986328125 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.763, loss_val: nan, pos_over_neg: 6158.4208984375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 8218.5478515625 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 680.3419189453125 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1182.809814453125 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 865.6527099609375 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 1131.6732177734375 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 595.6680908203125 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 1383.7528076171875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 623.1221313476562 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1075.434326171875 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 12408.0419921875 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 878.0590209960938 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 745.0110473632812 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: -2216.962890625 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 995.1661376953125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: -2353.110595703125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 65051.84375 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 30950.998046875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7975, loss_val: nan, pos_over_neg: 2383.90087890625 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1517.378173828125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: -5442.4853515625 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 23535.89453125 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 11384.4912109375 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 3019.052490234375 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 967.4904174804688 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 12489.439453125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1728.1318359375 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1714.1201171875 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 823.3256225585938 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1493.7933349609375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 582.4550170898438 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1218.3328857421875 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 3916.485107421875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 871.26123046875 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: -21335.91015625 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 5253.86962890625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1500.7294921875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 828.8206176757812 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 640.3035278320312 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 13390.9580078125 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1081.356689453125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 667.891357421875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 285.413818359375 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 709.2810668945312 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 537.4381713867188 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1378.88037109375 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: -32428.0078125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 4680.0478515625 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 30629.462890625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 3018.030517578125 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 835.9656982421875 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 517.2251586914062 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 507.6973571777344 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 2039.169677734375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: -10912.140625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 2013.433349609375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 889.8245849609375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 47956.0 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1043.62890625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1046.31640625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 2286.0009765625 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: -13147.677734375 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1157.2852783203125 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 322.5697326660156 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7999, loss_val: nan, pos_over_neg: 247.53916931152344 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 350.5848388671875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 1206.3521728515625 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: 824.4395141601562 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 759.4030151367188 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 1036.7835693359375 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.782, loss_val: nan, pos_over_neg: -10766.5947265625 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 988.9771118164062 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1178.0244140625 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: -2835.431396484375 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: -3710.653564453125 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 549.8450317382812 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 854.6641845703125 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 629.6522216796875 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.8032, loss_val: nan, pos_over_neg: 467.4601745605469 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1699.23583984375 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 1632.0067138671875 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 3588.31640625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1477.2747802734375 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1184.8670654296875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1755.6953125 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 2485.7333984375 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 7399.2119140625 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 14569.9775390625 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 598.5048828125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -12087.9814453125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 432.232666015625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 2161.60986328125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: -6920.08544921875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 974.7600708007812 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 513.6805419921875 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 2290.494384765625 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 351.7042541503906 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 3182.767578125 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2994.0419921875 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 2309.579833984375 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 850.8873901367188 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 773.734619140625 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1450.6990966796875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: -5433.091796875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7996, loss_val: nan, pos_over_neg: 2971.349609375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 675.7669067382812 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1278.8355712890625 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.7993, loss_val: nan, pos_over_neg: 797.436279296875 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 643.4387817382812 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1116.6068115234375 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 918.3580322265625 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 516.6388549804688 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 348.8461608886719 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 1074.5494384765625 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7981, loss_val: nan, pos_over_neg: 3904.27099609375 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1249.8843994140625 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 531.7698974609375 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 864.6262817382812 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 733.6624755859375 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 727.626220703125 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1747.2801513671875 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1187.719482421875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 872.2601928710938 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 2027.068115234375 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 533.0000610351562 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 452.2391662597656 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 903.842529296875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1784.62451171875 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 923.3822631835938 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1228.451904296875 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 691.7272338867188 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 614.0213623046875 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 2121.17578125 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 16962.2109375 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 13099.4150390625 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 3942.24072265625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2241.234375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 569.95654296875 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 677.8937377929688 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 620.862060546875 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 801.6218872070312 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: -8734.185546875 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 452.56585693359375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 926.2801513671875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1218.9554443359375 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1215.954345703125 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 525.5172119140625 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7995, loss_val: nan, pos_over_neg: 501.2879638671875 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 659.5095825195312 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 3144.09716796875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 77751.8515625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 2980.26416015625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1428.48193359375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7614, loss_val: nan, pos_over_neg: 719.5140380859375 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 481.25311279296875 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 441.267822265625 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 2352.836669921875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1507.65478515625 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 4478.08203125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 835.876220703125 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7935, loss_val: nan, pos_over_neg: 1562.1671142578125 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 490.083740234375 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 1906.8057861328125 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: -13955.4580078125 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 958.2879638671875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 1589.5374755859375 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 749.1293334960938 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1144.21533203125 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 456.2997131347656 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 870.8031005859375 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 3095.760009765625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1066.8048095703125 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1184.1378173828125 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 4063.4716796875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: 819.0133666992188 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7974, loss_val: nan, pos_over_neg: 1502.5888671875 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 953.186279296875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 807.006591796875 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 948.7753295898438 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 792.5078735351562 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 1286.0650634765625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 2361.81982421875 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7555, loss_val: nan, pos_over_neg: 14520.943359375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 451.602783203125 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7962, loss_val: nan, pos_over_neg: 784.954345703125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 472.2746276855469 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 792.2186279296875 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 1091.397705078125 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 853.1723022460938 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 1066.19091796875 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 604.185546875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 2609.5126953125 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1514.0831298828125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: -14128.2548828125 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 852.62158203125 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 1024.333740234375 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 601.57958984375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 857.0382690429688 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 457.91156005859375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 1044.82568359375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1983.2744140625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 823.9834594726562 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3793.81689453125 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 933.0963134765625 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2044.59130859375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 7250.89599609375 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 4014.887939453125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3886.09619140625 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 620.4157104492188 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 662.2369995117188 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 256.241943359375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 527.7267456054688 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 1090.259765625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 571.8805541992188 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 1431.2431640625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 906.7511596679688 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1493.107177734375 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 425.7317199707031 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 680.993896484375 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1088.731201171875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7971, loss_val: nan, pos_over_neg: 805.1004638671875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1011.3856811523438 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1863.1265869140625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 1300.1923828125 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1480.2122802734375 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1407.82958984375 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 853.26123046875 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7911, loss_val: nan, pos_over_neg: 976.5040893554688 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1727.172607421875 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 782.205078125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1096.7169189453125 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 628.6445922851562 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.8029, loss_val: nan, pos_over_neg: 493.9333190917969 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1098.5087890625 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 6676.50537109375 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: 6567.93017578125 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1972.1259765625 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 34787.23046875 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 701.3928833007812 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 760.7877807617188 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 515.28271484375 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 987.1604614257812 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 424.5844421386719 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 612.9940185546875 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 603.6783447265625 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1001.2747802734375 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1128.78662109375 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 528.3472290039062 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/300000 [4:16:39<128161:11:08, 1537.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10\n",
      "Iter: 0/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 305.7594909667969 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 691.5698852539062 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1710.261474609375 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1404.9296875 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 1866.737548828125 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 689.5929565429688 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 9876.3974609375 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 420.05859375 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 639.0506591796875 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 955.7430419921875 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 563.9635620117188 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 507.1581726074219 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 552.343017578125 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 13370.7412109375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 4112.82421875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 964.7485961914062 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1333.932861328125 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 908.73583984375 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 944.3077392578125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: -12842.138671875 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 648.9258422851562 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 390.20404052734375 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 565.3509521484375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1303.36865234375 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 735.1546020507812 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 863.074462890625 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 762.7583618164062 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 2006.9322509765625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2595.152587890625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 5173.28662109375 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 728.6187133789062 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 438.53106689453125 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 956.9779663085938 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 962.5621948242188 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1012.81640625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: -6307.46435546875 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1091.3353271484375 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 559.6417236328125 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1248.973388671875 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 420.1698303222656 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2757.250732421875 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1619.1826171875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 373.2477111816406 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 1047.6278076171875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.7577, loss_val: nan, pos_over_neg: 1146.8104248046875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 781.4166870117188 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 423.45867919921875 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 1124.6651611328125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 998.0323486328125 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 620.0067749023438 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 977.0487060546875 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1303.264404296875 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1110.23095703125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 600.1065063476562 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 563.2263793945312 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 2343.9375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 1628.613037109375 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: -4764.37890625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 7532.51123046875 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 3852.842041015625 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 501.3135986328125 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 5022.17919921875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1076.0325927734375 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1458.5006103515625 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: -109591.8828125 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 549.8370971679688 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 395.9158630371094 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1388.9793701171875 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 442.1939697265625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 2644.220947265625 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1341.82666015625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1462.783203125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 5813.17626953125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 477.7565612792969 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 992.1204223632812 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 8832.9130859375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7994, loss_val: nan, pos_over_neg: 589.2489013671875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: -47497.67578125 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 531.869873046875 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 570.2514038085938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 6074.6875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 5347.02587890625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1303.6650390625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 1305.4412841796875 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 541.4249267578125 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 869.8792724609375 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1175.9661865234375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 4909.32568359375 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: -7509.51611328125 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.8066, loss_val: nan, pos_over_neg: 679.7943725585938 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 7074.25341796875 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 727.8801879882812 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 3603.232421875 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 2753.79296875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 647.1715087890625 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 4969.1806640625 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 600.9131469726562 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1865.81103515625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 814.509033203125 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: -6565.09716796875 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: -2555.217529296875 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: -5342.6962890625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 481.249267578125 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7929, loss_val: nan, pos_over_neg: 413.0177001953125 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 755.2379760742188 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 626.1036376953125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.807, loss_val: nan, pos_over_neg: 433.9659729003906 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 3649.263671875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1005.48583984375 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 942.974365234375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 673.0938110351562 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 3767.18505859375 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 939.1610107421875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 535.0744018554688 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: -1681.9105224609375 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 658.4791259765625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 24259.1875 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: -17317.017578125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1222.78515625 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 2042.1077880859375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 5531.56298828125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1563.549560546875 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 591.9127807617188 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 2026.41455078125 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 566.4691772460938 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 591.3397827148438 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.796, loss_val: nan, pos_over_neg: 1067.8876953125 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 731.3291625976562 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1185.24267578125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1083.042236328125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 7593.11962890625 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 5781.564453125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1131.8326416015625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 2188.234375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1972.6832275390625 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 1352.4921875 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 3292.593505859375 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 3162.8662109375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1687.3433837890625 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1563.155517578125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 322788.71875 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1820.4237060546875 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1042.2281494140625 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 1271.7742919921875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1043.40869140625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 384.76959228515625 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7928, loss_val: nan, pos_over_neg: 876.2145385742188 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: -5801.7158203125 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 4233.03173828125 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 626.3287353515625 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7977, loss_val: nan, pos_over_neg: 1058.48095703125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 867.4806518554688 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1096.072509765625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 478.9458312988281 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1434.631103515625 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 2721.5185546875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 3800.25146484375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: -16520.12890625 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.781, loss_val: nan, pos_over_neg: -5408.20556640625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 990.7320556640625 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 4304.42529296875 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 303.92742919921875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 471.09173583984375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 775.984375 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1234.2498779296875 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 3625.7724609375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 821.7190551757812 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 678.9639892578125 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 413.974609375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 616.1215209960938 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1476.7554931640625 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1324.400634765625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1568.81787109375 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 715.3640747070312 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 373.85589599609375 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: -15548.146484375 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 501.9563903808594 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 717.2964477539062 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 885.4141845703125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1579.6295166015625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 731.3358154296875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 525.9633178710938 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 2835.07958984375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: -4903.515625 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 1643.5604248046875 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 719.0250854492188 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 472.5718688964844 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1770.9036865234375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 3855.70263671875 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1443.067626953125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 920.1326904296875 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1102.61767578125 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1560.2147216796875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 669.825439453125 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2948.654541015625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 435.43646240234375 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 2033.5283203125 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 1999.186767578125 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1317.0147705078125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 884.44287109375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 932.7933959960938 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 523.1156005859375 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 905.7943725585938 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: -6123.23193359375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 1773.05029296875 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 501.9826354980469 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1540.120361328125 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 870.9486694335938 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.8068, loss_val: nan, pos_over_neg: 592.2963256835938 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1234.3494873046875 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1371.7462158203125 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.77, loss_val: nan, pos_over_neg: -49040.0546875 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 787.3409423828125 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1347.125244140625 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: -3515.202392578125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 987.5078125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -2415.580078125 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 515.2842407226562 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1967.2630615234375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 4583.44970703125 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 840.8936767578125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1875.7867431640625 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 643.6975708007812 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 391.069580078125 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1081.262939453125 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 3373.38623046875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1078.7086181640625 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1777.13671875 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1094.09619140625 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1136.9075927734375 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 786.3551635742188 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 899.98681640625 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 2367.0556640625 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 2484.66162109375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 951.6314697265625 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1236.826416015625 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 968.9647827148438 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 549.7069702148438 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 777.4193115234375 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 622.8964233398438 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 714.1116333007812 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 3513.8056640625 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 679.3218994140625 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1245.75927734375 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 271.5898742675781 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 947.6884765625 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 17240.064453125 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1426.190185546875 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 719.3236694335938 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 4707.5830078125 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 7673.32373046875 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1021.588134765625 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 4335.22705078125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 3568.327880859375 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 650.637939453125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 474.4671936035156 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 895.5654296875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 336.5604553222656 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 583.5895385742188 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 2953.822265625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 3520.343994140625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 1102.3294677734375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 2412.1044921875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 4765.2197265625 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1384.0076904296875 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 647.6607666015625 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 705.868408203125 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1644.41650390625 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 598.5866088867188 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1300.6693115234375 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: -2684.324462890625 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 783.5181274414062 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 696.0443725585938 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 734.3065795898438 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 2421.00732421875 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 6621.2294921875 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: -3411.655517578125 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 2276.72900390625 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.799, loss_val: nan, pos_over_neg: 894.059326171875 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 598.4661254882812 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 2137.377685546875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 616.9511108398438 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 981.6605224609375 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 674.8546142578125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 317.2331848144531 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1553.653564453125 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 995.6094360351562 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 2277.217529296875 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 710.508544921875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 392.779296875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 2717.755126953125 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1378.7115478515625 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 3641.42919921875 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 2752.939208984375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 484.4638671875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 656.8748779296875 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 2123.65087890625 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 710.681884765625 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 606.5379638671875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 191447.140625 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 14208.416015625 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: -6932.203125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 2358.629638671875 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 608.0280151367188 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 891.8947143554688 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1146.6253662109375 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 702.2820434570312 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 2095.845947265625 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 3108.11767578125 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.771, loss_val: nan, pos_over_neg: -12213.3232421875 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 924.3113403320312 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 934.7681884765625 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1600.6651611328125 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 689.8795166015625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 2567.0546875 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 947.9259033203125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 959.4461669921875 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 996.9011840820312 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: -17370.41796875 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1046.19287109375 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2096.896240234375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 498.34698486328125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 520.9224853515625 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7907, loss_val: nan, pos_over_neg: 629.5004272460938 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1101.817626953125 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7946, loss_val: nan, pos_over_neg: 442.6805419921875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 1542.235595703125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 4986.0029296875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 3601.376953125 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 451.4508972167969 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 753.782470703125 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 512.8601684570312 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 647.3597412109375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 670.0174560546875 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 489.80609130859375 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 2322.265380859375 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 29974.740234375 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 700.35546875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1540.889892578125 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.781, loss_val: nan, pos_over_neg: -4585.46240234375 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.8007, loss_val: nan, pos_over_neg: 369.0278015136719 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 656.4334106445312 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1536.17431640625 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 833.5127563476562 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1029.7618408203125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 545.6728515625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 598.9603271484375 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 561.6218872070312 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 607.3607177734375 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 587.6267700195312 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 871.6925659179688 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1864.446044921875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 454.6933288574219 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 271.0614929199219 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1134.365478515625 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: -36115.4453125 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1112.587158203125 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1434.1781005859375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7998, loss_val: nan, pos_over_neg: 376.80047607421875 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 656.9552612304688 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 10580.888671875 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 351.80694580078125 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 933.8375244140625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 7982.83544921875 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 914.9006958007812 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 14923.8916015625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: -148844.375 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 10132.75390625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 931.6212768554688 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 284.8933410644531 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: -14330.2685546875 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.8005, loss_val: nan, pos_over_neg: 380.5300598144531 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 455.2587585449219 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 2636.763671875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 585.0916748046875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1055.6678466796875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 1291.451416015625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: -39808.6640625 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 650.0360107421875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1019.500244140625 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 16837.2890625 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: -6251.54052734375 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 927.2672119140625 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1130.35546875 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: -66778.2109375 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 2190.944580078125 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 7499.50927734375 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 553.5243530273438 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 913.8108520507812 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 717.18896484375 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 3861.1904296875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 783.23779296875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2900.0322265625 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 43548.09765625 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 574.0598754882812 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 688.2720947265625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1854.4903564453125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: -2380.4853515625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: -3019.229736328125 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2205.669189453125 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 709.314453125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7949, loss_val: nan, pos_over_neg: 292.0893249511719 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 530.516357421875 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -10369.3935546875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 933.8515014648438 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 4737.5146484375 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 409.9437561035156 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1823.9329833984375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1474.2161865234375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1386.4571533203125 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 892.0239868164062 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1243.2425537109375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 613.0404663085938 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 644.333984375 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1041.3021240234375 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2364.826904296875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1247.0845947265625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 497.35986328125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 560.02099609375 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 919.0733642578125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 427.2315368652344 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 840.5140380859375 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 644.0045166015625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 982.92333984375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1224.0228271484375 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1137.328857421875 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: -8244.03515625 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 915.8391723632812 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1805.4774169921875 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 726.82568359375 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 2457.729736328125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 557.9448852539062 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7944, loss_val: nan, pos_over_neg: 2875.526611328125 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1636.3363037109375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 2674.6669921875 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 572.3875122070312 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 3021.348876953125 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7922, loss_val: nan, pos_over_neg: 676.17041015625 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: -4305.43310546875 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1512.262939453125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 26860.861328125 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 306.8122253417969 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 675.01416015625 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 761.1004638671875 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 947.876953125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 2446.58349609375 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 509.798828125 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 4304.43505859375 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 286.4011535644531 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 461.7397766113281 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 298.2874450683594 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1418.92236328125 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1614.697509765625 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 985.0031127929688 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 931.3208618164062 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 4735.15234375 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 876.5146484375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 842.8819580078125 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 734.0077514648438 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 747.9296875 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 1046.2366943359375 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 726.854736328125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 833.6073608398438 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7941, loss_val: nan, pos_over_neg: 1394.0223388671875 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: 2819.83984375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1285.1959228515625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1017.2456665039062 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 2043.5489501953125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 5940.4296875 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1802.4796142578125 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2998.134765625 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 753.4508666992188 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 636.1781616210938 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 641.0332641601562 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1339.503173828125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 4654.42822265625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 521.5906372070312 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 722.6485595703125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1063.603515625 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 954.157470703125 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1245.4290771484375 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1786.8648681640625 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 1896.3568115234375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1389.048583984375 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 569.9424438476562 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1643.780029296875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7969, loss_val: nan, pos_over_neg: 306.5945739746094 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7964, loss_val: nan, pos_over_neg: 1322.4310302734375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1547.8099365234375 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1696.963134765625 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 9600.455078125 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2625.90283203125 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1654.7276611328125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 940.9630126953125 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 2383.841552734375 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: -2585.253662109375 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 860.7975463867188 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.7883, loss_val: nan, pos_over_neg: 713.8565673828125 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 590.7429809570312 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 60519.83203125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 2283.15185546875 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 797.6060180664062 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 780.5970458984375 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7914, loss_val: nan, pos_over_neg: -4441.5751953125 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: -1317.06640625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 1505.813232421875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 729.8587646484375 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 3391.200439453125 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 627.9171142578125 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 659.9080810546875 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 999.9674072265625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 2625.331298828125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1006.5323486328125 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 482.3100891113281 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 678.8817138671875 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1041.5870361328125 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 2979.591552734375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 818.005126953125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: -6288.4072265625 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 16419.244140625 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 704.4067993164062 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 433.3077697753906 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1755.0400390625 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.8018, loss_val: nan, pos_over_neg: 824.3268432617188 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 2665.359375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1033.1962890625 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7936, loss_val: nan, pos_over_neg: 1053.315185546875 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1431.1505126953125 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1173.766845703125 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 658.3418579101562 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 942.66552734375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 485.9244689941406 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 506.86224365234375 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 2033.8502197265625 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 343.7548828125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 359.23663330078125 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1974.33544921875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: -2426.16748046875 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1316.9300537109375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 920.0579833984375 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 661.0542602539062 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 698.9596557617188 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 787.4970092773438 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 739.0885009765625 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.8016, loss_val: nan, pos_over_neg: 717.9528198242188 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1085.6409912109375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 2277.441162109375 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 398.85308837890625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 720.4739379882812 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 582.606201171875 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 862.3104248046875 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 650.7120971679688 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.8014, loss_val: nan, pos_over_neg: 1031.25244140625 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 3167.19677734375 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1626.9527587890625 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 883.687744140625 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 658.299072265625 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 617.5624389648438 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1164.86279296875 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 787.4431762695312 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 729.9137573242188 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 398.5068359375 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 606.7318115234375 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 312.0640869140625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1581.10546875 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 1001.1632690429688 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1365.351806640625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 824.2337036132812 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 643.914306640625 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1322.161865234375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1046.580322265625 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 2246.339599609375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 3056.34619140625 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7982, loss_val: nan, pos_over_neg: 1056.3131103515625 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1218.83349609375 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 494.4547424316406 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 731.0253295898438 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 509.046630859375 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 871.6170043945312 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 732.3065185546875 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 36744.91796875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 1287.0560302734375 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 484.375732421875 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 635.2552490234375 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1429.9801025390625 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 635.31494140625 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 776.9299926757812 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1139.0438232421875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 400.4571533203125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 395.50640869140625 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 984.0885009765625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 331.8885498046875 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 435.88861083984375 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 890.8626098632812 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 910.4730224609375 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 890.94091796875 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 794.3746337890625 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1211.940185546875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1780.467529296875 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 756.9200439453125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 551.1675415039062 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 504.1238098144531 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 306.4779968261719 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 2434.18017578125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1481.38916015625 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 354.6770935058594 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1588.848388671875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 507.04949951171875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1046.2667236328125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 645.8447265625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1997.177490234375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 459.34326171875 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1055.941162109375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 798.682373046875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 272.64093017578125 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 457.775634765625 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 369.8939208984375 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 532.4138793945312 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1157.736083984375 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1615.2821044921875 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: -5016.38427734375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7909, loss_val: nan, pos_over_neg: 339.98834228515625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1568.2645263671875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 530.1660766601562 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 45814.84375 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7923, loss_val: nan, pos_over_neg: 644.1689453125 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1434.9862060546875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: -38363.2578125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1550.484375 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1047.236328125 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.8056, loss_val: nan, pos_over_neg: 414.67181396484375 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 2578.346435546875 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 6918.78857421875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 492.4610290527344 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 853.9330444335938 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 1208.896484375 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 5681.658203125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 2925.296630859375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 474.14007568359375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7953, loss_val: nan, pos_over_neg: 454.567626953125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1836.674072265625 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.7912, loss_val: nan, pos_over_neg: 522.656005859375 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 2041.7093505859375 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1983.40283203125 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 823.731201171875 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 676.7327270507812 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 910.4143676757812 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 976.2409057617188 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1312.6116943359375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: -3215.92431640625 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2384.64599609375 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 700.9312133789062 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 4790.4833984375 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 3656.099853515625 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 594.1062622070312 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1309.631591796875 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1968.922119140625 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 384086.84375 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 553.0517578125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 562.48291015625 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 353.0862731933594 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 588.8074340820312 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 603.8212280273438 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 748.4578247070312 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 1187.07421875 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1340.639404296875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: -4448.513671875 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 814.6363525390625 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1674.538330078125 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 849.0587158203125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 25400.3359375 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 855.8522338867188 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 344.9455871582031 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 635.3944091796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7622, loss_val: nan, pos_over_neg: 682.8939208984375 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 5064.4287109375 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 392.9644775390625 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 505.36279296875 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 458.8343811035156 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 1833.6572265625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 21833.296875 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 665.382080078125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 359.1601867675781 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1766.13671875 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 884.8182983398438 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2331.97607421875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1511.651611328125 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 7826.72509765625 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7905, loss_val: nan, pos_over_neg: 717.426513671875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 979.8953247070312 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.778, loss_val: nan, pos_over_neg: -57249.1875 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 1301.91064453125 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 1743.8380126953125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1260.018798828125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 688.7559204101562 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1787.59375 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 511.4471435546875 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 595.7699584960938 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 649.4671020507812 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 643.3271484375 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1855.847900390625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 476273.09375 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2608.944580078125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 348.43853759765625 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: -5126.59912109375 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 3395.84130859375 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 746.8885498046875 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: -4490.1474609375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.7937, loss_val: nan, pos_over_neg: 3558.916015625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1240.2674560546875 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 653.05810546875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 3056.489990234375 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 5383.30126953125 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 794.6537475585938 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1179.343994140625 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 651.8030395507812 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 1937.0689697265625 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1376.816162109375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 753.5701904296875 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 400.27117919921875 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1868.59765625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1836.2254638671875 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: -41843.79296875 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1025.018310546875 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 2368.168701171875 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 418.14947509765625 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1706.5093994140625 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1054.2296142578125 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1722.8946533203125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 570.8106689453125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7951, loss_val: nan, pos_over_neg: 7313.43505859375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 659.839111328125 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 779.8409423828125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 2426.598388671875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 896.1043090820312 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: -5799.04248046875 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 3901.038330078125 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 857.5780029296875 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 667.8968505859375 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 856.501953125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1784.0162353515625 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 813.7572631835938 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 335.8621520996094 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: -3539.955810546875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: 2680.349853515625 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 439.8446044921875 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.798, loss_val: nan, pos_over_neg: 571.2713012695312 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1020.1524658203125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 784.6553955078125 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 4014.748291015625 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: 1378.167724609375 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 887.2772827148438 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 587.3583374023438 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1030.55712890625 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 1689.8262939453125 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 579.7275390625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 730.1829223632812 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1956.950439453125 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1131.876708984375 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 294.4001770019531 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1216.4124755859375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 1113.857177734375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 1080.6202392578125 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 676.3034057617188 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -32464.017578125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1419.2738037109375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 2478.0947265625 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1444.84228515625 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 38456.734375 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1646.7176513671875 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1596.4913330078125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 3436.967041015625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 614.834716796875 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 3673.412109375 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1068.64404296875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 632.72705078125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 533.83349609375 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 1189.5732421875 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 595.3243408203125 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 414.79541015625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 955.4232788085938 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 431.8034973144531 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 487.78631591796875 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1199.07861328125 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 784.3488159179688 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 808.8634643554688 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 1276.1815185546875 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 890.2507934570312 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.771, loss_val: nan, pos_over_neg: -6118.10693359375 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.8001, loss_val: nan, pos_over_neg: 933.215576171875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 2296.515869140625 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 1867.8641357421875 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 30039.380859375 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1208.63232421875 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 491.2130126953125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 912.7994995117188 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 320.1686706542969 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 623.227294921875 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 3024.2919921875 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.768, loss_val: nan, pos_over_neg: -708129.625 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 2099.531982421875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1141.929931640625 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 548.7614135742188 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 356.94720458984375 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1429.7884521484375 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 554.01611328125 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 627.2606811523438 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 815.828857421875 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 507.7752990722656 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 572.2395629882812 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 18842.16796875 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 753.44384765625 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 2838.7001953125 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 4440.49365234375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1033.8021240234375 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.8039, loss_val: nan, pos_over_neg: 370.202392578125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 364.3575134277344 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1255.430419921875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 852.9542846679688 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1263.3912353515625 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.8003, loss_val: nan, pos_over_neg: 580.3655395507812 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1439.8939208984375 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.8047, loss_val: nan, pos_over_neg: 348.0125427246094 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7924, loss_val: nan, pos_over_neg: 776.7313232421875 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 840.5286865234375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7956, loss_val: nan, pos_over_neg: 1656.6766357421875 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 796.3748779296875 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 890.1790161132812 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: -3191.51708984375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 8201.8203125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 363.06048583984375 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 688.8594970703125 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1292.321044921875 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 676.8829956054688 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 806.8578491210938 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1009.5728149414062 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1067.2777099609375 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 997.115234375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.752, loss_val: nan, pos_over_neg: 1356.86279296875 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2112.855712890625 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 181.12081909179688 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 296.3960876464844 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7932, loss_val: nan, pos_over_neg: 607.3875122070312 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1608.341552734375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 7123.66552734375 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 300.12164306640625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 847.3410034179688 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 3269.827880859375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7536, loss_val: nan, pos_over_neg: -40847.421875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1370.3226318359375 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 720.5943603515625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: -4300.2060546875 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 880.90087890625 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1160.6915283203125 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1001.8822021484375 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 660.8737182617188 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: -6035.53125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 5779.26025390625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1692.959228515625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 484.15020751953125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 748.3779907226562 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1987.1795654296875 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: -11747.8955078125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 3723.359619140625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: -30144.701171875 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 15997.9501953125 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1197.0487060546875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 925.7158813476562 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2401.23046875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1201.6177978515625 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 3701.42529296875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: -16831.390625 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 765.7879638671875 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 693.6395263671875 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 2313.263427734375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: -3203.095458984375 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1947.8466796875 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1066.19091796875 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 870.309814453125 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 7500.0966796875 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 753.7694091796875 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 592.5361938476562 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7623, loss_val: nan, pos_over_neg: 3938.54541015625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2077.25390625 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 573.00244140625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 1053.68798828125 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 395.3387145996094 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1858.1746826171875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: -11041.7490234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/300000 [4:42:19<128232:11:06, 1538.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11\n",
      "Iter: 0/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 533.7620239257812 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1973.099365234375 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 559.4988403320312 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 6345.9765625 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1617.118896484375 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 654.3795166015625 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 3581.626708984375 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: -2935.131103515625 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: -9395.7294921875 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 907.2526245117188 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 1290.3402099609375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1159.3045654296875 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 3701.839599609375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: -1753.6865234375 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 823.3758544921875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1169.522705078125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 860.5545654296875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 1182.2388916015625 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7968, loss_val: nan, pos_over_neg: 1614.25927734375 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 797.376708984375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1777.122314453125 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 2855.85009765625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 532.933349609375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 366.0649108886719 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 5956.189453125 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 2276.04638671875 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 3028.257080078125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: -3394.04248046875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 497.6155700683594 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1342.27294921875 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: -1878.743896484375 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 4102.15478515625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 47162.5 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 399.75970458984375 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 428.6536865234375 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1313.3863525390625 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 2886.226806640625 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 2275.87353515625 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 4669.2509765625 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: -13318.36328125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 945.7640991210938 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1098.225341796875 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 404.0496826171875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1146.407958984375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 5771.65966796875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 1239.959716796875 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 943.1044311523438 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 6201.6220703125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: -9675.716796875 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7992, loss_val: nan, pos_over_neg: 456.6920166015625 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 859.3450317382812 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2182.424560546875 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1108.9259033203125 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 50087.234375 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 453.3161926269531 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 3338.58984375 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 10397.7783203125 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 1206.230224609375 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 1767.08984375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.8035, loss_val: nan, pos_over_neg: -3242.116455078125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: -6626.80029296875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 603.2188110351562 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1168.9471435546875 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7511, loss_val: nan, pos_over_neg: 6742.5810546875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 8587.5107421875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 617.9380493164062 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 3542.091064453125 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.762, loss_val: nan, pos_over_neg: -2786.513427734375 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: -7884.2099609375 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 2552.66796875 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 465.07757568359375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1509.6268310546875 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 411.3050537109375 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 652.01708984375 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 501.6178283691406 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 896.4614868164062 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 3717.88330078125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: -5354.2900390625 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.8027, loss_val: nan, pos_over_neg: 1383.552734375 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1949.387939453125 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 9449.7294921875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2311.792236328125 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2611.559326171875 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 490.80255126953125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 727.1031494140625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 1148.9111328125 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 848.09423828125 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2550.24267578125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 472.49871826171875 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 7703.98388671875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1255.901611328125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 5827.90185546875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.8161, loss_val: nan, pos_over_neg: 482.814697265625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 3219.544921875 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: -12827.4716796875 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 11218.662109375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 901.2838134765625 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 532.0197143554688 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2114.6923828125 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 447.2951354980469 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 299.724365234375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 574.9150390625 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 533.9429931640625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1107.3804931640625 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 1764.9630126953125 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 4476.822265625 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: -3545.062255859375 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7613, loss_val: nan, pos_over_neg: 2306.76513671875 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1132.064208984375 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: -3907.54638671875 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 7053.6376953125 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1240.6624755859375 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 460.4431457519531 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 350.6685791015625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1761.9071044921875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1160.576904296875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 59307.6796875 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 883.4998168945312 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: -4476.0283203125 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: -30779.603515625 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 1848.442626953125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7913, loss_val: nan, pos_over_neg: 996.5963134765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 13340.3330078125 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 682.5797729492188 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 3160.1767578125 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.755, loss_val: nan, pos_over_neg: 824.1515502929688 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1200.4998779296875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 1838.299072265625 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 643.2716064453125 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.8012, loss_val: nan, pos_over_neg: 679.5143432617188 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1486.4954833984375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 17374.908203125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 32370.078125 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 817.6585693359375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 498.64569091796875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 542.5546264648438 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1085.0665283203125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: -67122.4375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1841.1639404296875 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 562.5806274414062 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 449.58111572265625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 962.9729614257812 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1252.3477783203125 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 1191.98583984375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 3067.1220703125 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 605.9481201171875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 663.658935546875 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7875, loss_val: nan, pos_over_neg: 705.7018432617188 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1121.769775390625 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1045.3909912109375 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1350.85791015625 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 648.007080078125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 2187.66015625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1690.7744140625 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1518.1727294921875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 7416.10302734375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 2451.884033203125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1662.8548583984375 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 1035.12890625 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 879.4032592773438 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 619.32080078125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 767.3824462890625 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 8819.259765625 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 1580.3387451171875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 3899.40673828125 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 933.37109375 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7859, loss_val: nan, pos_over_neg: 19205.861328125 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1386.63525390625 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 1114.40185546875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 11570.9951171875 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1149.5946044921875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 13121.0986328125 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7601, loss_val: nan, pos_over_neg: 752.8719482421875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7966, loss_val: nan, pos_over_neg: 383.1199035644531 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 708.049560546875 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.793, loss_val: nan, pos_over_neg: 773.0380249023438 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1067.2884521484375 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 356.87701416015625 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 472.17266845703125 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1054.5733642578125 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 606.0090942382812 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1179.5418701171875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1778.3126220703125 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 721.146240234375 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 641.80322265625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 960.7210693359375 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 434.5096130371094 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1135.3101806640625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 460.6794128417969 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 568.0911865234375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 520.3882446289062 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 3024.694091796875 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 371.38934326171875 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7622, loss_val: nan, pos_over_neg: 2759.5693359375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 564.9662475585938 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 1242.8221435546875 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 3610.331787109375 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 735.385498046875 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 855.1180419921875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: 1033.0496826171875 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 484.57611083984375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 874.6202392578125 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1489.5428466796875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1684.2393798828125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 775.0661010742188 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1637.27490234375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 388.2622985839844 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 727.1060180664062 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: 924.9241943359375 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 986.6539916992188 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 418.1088562011719 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.801, loss_val: nan, pos_over_neg: 485.19854736328125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 547.39697265625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2111.058349609375 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 3547.158447265625 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 6418.609375 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 2113.416015625 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 553.7264404296875 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 492.9675598144531 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 615.1614379882812 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 382.6338195800781 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 778.9629516601562 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7903, loss_val: nan, pos_over_neg: -32758.654296875 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7959, loss_val: nan, pos_over_neg: 765.2545776367188 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7471, loss_val: nan, pos_over_neg: 6656.12060546875 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: -15958.078125 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1705.8609619140625 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2250.60009765625 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 1204.79052734375 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 3837.316650390625 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 925.9946899414062 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 1274.7669677734375 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 905.6309814453125 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 565.7415771484375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1276.056396484375 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 644.2914428710938 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 521.278564453125 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 34672.6953125 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1223.4700927734375 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 672.1177368164062 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 4545.03564453125 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 1782.0662841796875 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 1385.9300537109375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2058.541259765625 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: -3180.70751953125 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 8471.9541015625 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 2791.554931640625 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1422.08203125 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 5022.93017578125 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: -28000.71484375 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 768.0260620117188 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 797.8604125976562 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1542.671875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 874.0557250976562 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 1195.187255859375 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1309.304931640625 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 494.11602783203125 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1140.1297607421875 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: -17589.18359375 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 941.066162109375 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: -11573.21484375 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 821.1728515625 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1277.0616455078125 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7979, loss_val: nan, pos_over_neg: 415.8241271972656 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1157.323486328125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1884.3194580078125 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 776.7661743164062 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1642.3388671875 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 749.7902221679688 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 2208.135498046875 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 1663.2376708984375 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1114.3070068359375 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 4210.45751953125 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 566.736572265625 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1419.4346923828125 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 1486.2685546875 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 751.7066040039062 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 434.72686767578125 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 2383.295166015625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 8403.9189453125 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: -46683.171875 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: -161895.90625 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.777, loss_val: nan, pos_over_neg: -14282.0283203125 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 1751.1885986328125 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 854.175048828125 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 990.24951171875 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: 950.0189208984375 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 442.3836364746094 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 898.3986206054688 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 9256.033203125 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 6367.71630859375 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: -13451.548828125 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1402.66015625 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 2183.474365234375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1035.6239013671875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: -2239.80615234375 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1849.9442138671875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 781.26171875 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: -2784.050537109375 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 850.9055786132812 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 475.7572937011719 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 362.2841491699219 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1169.475830078125 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: -2145.7626953125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1439.118896484375 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 655.883544921875 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: -51906.69921875 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: -353166.0 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 1248.7208251953125 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 680.1939697265625 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1427.5125732421875 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1326.393798828125 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 479.549072265625 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: 4021.271728515625 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 566.7601928710938 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 641.80224609375 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7585, loss_val: nan, pos_over_neg: 4460.3330078125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1746.140869140625 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 486.7501525878906 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1016.8463134765625 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 968.620849609375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1066.381103515625 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1361.1474609375 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 2737.250244140625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1051.0111083984375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 905.1480712890625 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1432.1263427734375 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 719.3386840820312 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 869.5917358398438 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 2809.171630859375 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 1006.710693359375 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 426.9947814941406 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 474.89910888671875 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 626.4620361328125 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 918.7305297851562 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 3662.85400390625 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1386.1533203125 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1018.83837890625 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 534.9019775390625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 841.0197143554688 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 816.1121826171875 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7541, loss_val: nan, pos_over_neg: 3353.78076171875 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 486.8682861328125 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 382.3175964355469 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 490.0348815917969 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 2265.126708984375 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: 542.3201293945312 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 425.4682922363281 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1605.787841796875 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 1774.0360107421875 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 452.7657470703125 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 633.7845458984375 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 766.8670043945312 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 257.8659973144531 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 350.6292419433594 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 933.4458618164062 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 44528.9453125 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.7593, loss_val: nan, pos_over_neg: 13386.0859375 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 3100.896728515625 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 1528.0489501953125 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: -10765.3349609375 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 2001.5274658203125 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 1610.964111328125 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1470.98974609375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3082.627685546875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: -3018.78759765625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 10314.6962890625 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -8444.28515625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 375.12030029296875 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 890.1802368164062 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 281.5353088378906 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 793.130615234375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 2928.900390625 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 446.6367492675781 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 3154.572509765625 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7585, loss_val: nan, pos_over_neg: 1746.361083984375 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1165.221435546875 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 4349.63232421875 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 860.0537719726562 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 953.8770751953125 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 2514.221435546875 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: -27283.587890625 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 1781.821044921875 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1989.499267578125 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 702.3244018554688 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 817.029296875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 603.9234008789062 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1423.7626953125 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 646.8787841796875 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 521.9180297851562 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 415.880615234375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 1573.9207763671875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 836.211181640625 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7963, loss_val: nan, pos_over_neg: 1443.6131591796875 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 281.1568908691406 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7901, loss_val: nan, pos_over_neg: 576.161865234375 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 604.1257934570312 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 1638.726806640625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1094.400390625 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1553.563232421875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: -111378.515625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 2392.217529296875 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1459.586181640625 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 452.1663513183594 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 868.2540893554688 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 652.0118408203125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1227.8570556640625 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 343.0510559082031 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7938, loss_val: nan, pos_over_neg: 436.490966796875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 854.0494384765625 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 2531.006103515625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 855.938720703125 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 531.8389282226562 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 972.2408447265625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 502.7951354980469 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 1227.44140625 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1908.9791259765625 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 520.555419921875 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 635.9505004882812 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 519.4697875976562 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 1141.975341796875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 502.7980651855469 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 2903.9228515625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1299.474609375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 686.7044677734375 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 371.5672912597656 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 353.8190612792969 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 3001.098388671875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 682.2328491210938 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 731.6401977539062 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 447.2154541015625 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 698.6703491210938 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 24042.654296875 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 916.712158203125 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 932.9629516601562 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 657.0028076171875 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: -7048.8203125 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 716.3856201171875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 577.6841430664062 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 2614.6669921875 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 2029.131103515625 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1642.101806640625 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 2117.374267578125 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 690.0226440429688 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 621.2058715820312 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 616.534912109375 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7496, loss_val: nan, pos_over_neg: 1406.806396484375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 4041.700439453125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 3303.230224609375 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 1247.4852294921875 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7855, loss_val: nan, pos_over_neg: 668.4058227539062 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 910.8841552734375 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: -9146.9931640625 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 893.7673950195312 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.7516, loss_val: nan, pos_over_neg: 20039.0 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 1637.03955078125 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1107.1937255859375 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 810.3917236328125 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: 1278.6944580078125 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7921, loss_val: nan, pos_over_neg: 449.0583190917969 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 494.9167175292969 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7495, loss_val: nan, pos_over_neg: 5659.64111328125 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1284.9420166015625 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1022.66845703125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 535.0516357421875 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 240.1951904296875 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 735.9768676757812 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: -8872.2470703125 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7889, loss_val: nan, pos_over_neg: 592.0162353515625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 592.104248046875 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 936.0659790039062 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 4919.646484375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1305.976318359375 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: -1904.6226806640625 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: -8303.2060546875 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1472.070556640625 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7887, loss_val: nan, pos_over_neg: 1541.0335693359375 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 6190.32666015625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.7498, loss_val: nan, pos_over_neg: 3478.93359375 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7987, loss_val: nan, pos_over_neg: 577.2020263671875 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7588, loss_val: nan, pos_over_neg: 973.3338012695312 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: -34013.26953125 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 2754.456298828125 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 792.2431030273438 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: -6394.72998046875 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 5447.51318359375 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1716.4814453125 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 2531.8193359375 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7957, loss_val: nan, pos_over_neg: 1620.4093017578125 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 519.844482421875 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1126.0728759765625 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 916.0026245117188 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1135.2960205078125 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: -2953.68701171875 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1482.64453125 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1326.7435302734375 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.8045, loss_val: nan, pos_over_neg: 438.60711669921875 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 765.3673706054688 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 2474.913330078125 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1328.493408203125 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 3422.009521484375 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 7117.53466796875 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 814.2915649414062 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 1302.2410888671875 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 693.8843994140625 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: -12819.26171875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 823.3881225585938 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 536.6979370117188 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 821.5383911132812 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.786, loss_val: nan, pos_over_neg: 539.5742797851562 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: -3066.32666015625 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 1177.7237548828125 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1371.84912109375 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 497.87335205078125 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 469.5499572753906 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 487.2423095703125 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 647.9512329101562 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1105.2227783203125 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 5407.30810546875 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 1804.95068359375 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 3935.446044921875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: -4979.7509765625 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7985, loss_val: nan, pos_over_neg: 1183.3076171875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: -5572.39794921875 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1141.2801513671875 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 5309.8876953125 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 16625.759765625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 470.8527526855469 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 842.810302734375 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 519.701171875 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7622, loss_val: nan, pos_over_neg: -3734.306884765625 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 720.8922119140625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 402.05401611328125 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 2725.32275390625 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2264.861328125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: -3460.701904296875 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 2024.975830078125 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: -4499.4609375 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1255.0859375 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: -3239.26513671875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1321.8538818359375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 1084.30322265625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1515.0521240234375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7989, loss_val: nan, pos_over_neg: 305.1948547363281 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1316.667236328125 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 3023.2333984375 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 3330.462158203125 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1958.6353759765625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 909.2440185546875 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 526.2281494140625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 480.8301696777344 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1250.7315673828125 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 629.8723754882812 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 790.1807250976562 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 575.515625 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 985.1039428710938 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 595.6378784179688 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 787.383056640625 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 1153.9041748046875 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 2413.51220703125 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 640.5379638671875 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: -3101.8056640625 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 2091.134033203125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: -4687.58447265625 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 1585.4696044921875 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 874.4852294921875 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1553.5455322265625 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1213.1822509765625 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.7868, loss_val: nan, pos_over_neg: 336.3793640136719 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1500.4739990234375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 903.1610107421875 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 5729.95849609375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 2670.074951171875 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2262.839599609375 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 4843.1650390625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 2098.46435546875 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 671.08447265625 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 645.9019775390625 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 857.4561157226562 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 500.7697448730469 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 482.50616455078125 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1959.7442626953125 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1063.2296142578125 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 378.8685607910156 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: -5632.8583984375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: -12223.7080078125 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 4201.60888671875 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 34610.7109375 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 3719.46240234375 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -8531.548828125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 25921.373046875 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 1485.137939453125 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 9763.5830078125 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 702.3273315429688 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: -39793.7421875 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 491.27191162109375 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 790.028564453125 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 7604.19921875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1763.9879150390625 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1649.8118896484375 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1694.80517578125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 3349.77880859375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1706.364013671875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 778.00048828125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 592.9647216796875 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 792.97314453125 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1587.85693359375 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 5305.966796875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1773.132568359375 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 1339.1806640625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1936.474853515625 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 806.46240234375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 618.6492309570312 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 39486.265625 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 734.0888671875 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: -3038.915283203125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 2359.692138671875 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 946.7743530273438 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7899, loss_val: nan, pos_over_neg: 419.5698547363281 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 604.0970458984375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 957.6776123046875 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1412.4312744140625 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 3896.934326171875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 934.3255615234375 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7536, loss_val: nan, pos_over_neg: 545.6735229492188 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 646.1500854492188 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1687000.875 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 15296.423828125 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 2950.08154296875 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1422.142333984375 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 832.248046875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1028.8135986328125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 660.7841186523438 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1029.5787353515625 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 644.6455078125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 2034.04931640625 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 713.3919677734375 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1059.214599609375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1341.450927734375 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 574.274658203125 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1954.206298828125 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 529.1178588867188 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 2868.65576171875 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 1140.0062255859375 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1444.5601806640625 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.7856, loss_val: nan, pos_over_neg: 527.957275390625 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1373.831298828125 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 979.1561889648438 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 4194.2353515625 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 850.37646484375 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 3095.318115234375 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 13618.0390625 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1087.9031982421875 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2099.083740234375 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 1028.77490234375 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1458.0860595703125 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 549.891845703125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7593, loss_val: nan, pos_over_neg: -4594.798828125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 6908.4580078125 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 731.7764892578125 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 2657.9580078125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1079.904296875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7841, loss_val: nan, pos_over_neg: 492.6966247558594 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 899.43603515625 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1630.56494140625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 890.2493286132812 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1607.44140625 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 603.675048828125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 4894.162109375 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 3486.8486328125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 11279.376953125 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 3479.58447265625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1028.3985595703125 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 7994.7412109375 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 830.0269165039062 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2260.450927734375 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1003.3776245117188 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 547.2662353515625 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 710.5645751953125 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 800.5689697265625 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 815.4263916015625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7563, loss_val: nan, pos_over_neg: 5065.25439453125 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: -6434.375 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 1887.954345703125 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7844, loss_val: nan, pos_over_neg: 884.3818359375 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1119.182861328125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 5201.90185546875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1305.6649169921875 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1800.615478515625 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 955.102294921875 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1133.377197265625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 257.38824462890625 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 4459.93798828125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 1684.6571044921875 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 462.3291931152344 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 785.9234619140625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 2593.9853515625 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: -13094.859375 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 984.6054077148438 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 916.4896240234375 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 1015.2659912109375 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1847.071533203125 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 1695.73046875 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 17945.83984375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 8633.5048828125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 254.82174682617188 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 527.5208740234375 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1473.2628173828125 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1953.3521728515625 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 847.624267578125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1452.84521484375 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2395.920654296875 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 2740.342041015625 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1336.65380859375 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1091.417236328125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: -8649.140625 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: -68753.9375 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 2797.07568359375 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 3064.58837890625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 628.1175537109375 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1147.5472412109375 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 600.251708984375 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 624.9654541015625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 829.6978149414062 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 803.1554565429688 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 575.4844970703125 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1526.8350830078125 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7572, loss_val: nan, pos_over_neg: 714.38232421875 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2209.626953125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 797.6422119140625 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1490.7425537109375 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7601, loss_val: nan, pos_over_neg: 1401.0865478515625 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 5426.802734375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 438.457763671875 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 796.2752685546875 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 361.1999206542969 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1120.6094970703125 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1016.4056396484375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1130.2041015625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 587.390869140625 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 816.5536499023438 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7523, loss_val: nan, pos_over_neg: 465.28094482421875 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 380.2940368652344 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1094.6669921875 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: -19072.673828125 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1258.0223388671875 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1608.369140625 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1544.1221923828125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 7295.41455078125 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1298.8485107421875 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: -2636.78564453125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 4183.9375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 607.1549072265625 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 743.3488159179688 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 920.214599609375 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7544, loss_val: nan, pos_over_neg: -142622.625 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 542.0841674804688 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 3231.49169921875 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 741.1033935546875 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 2041.8480224609375 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 658.9489135742188 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 2247.13916015625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 6650.15576171875 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 512.1141357421875 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 407.0567932128906 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2218.258056640625 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: -3520.783447265625 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 1957.073974609375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1587.7115478515625 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 1276.4727783203125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 432.9664306640625 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 412.6923522949219 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 556.7772827148438 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 535.5617065429688 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 4361.26171875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1122.8406982421875 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1172.840087890625 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 516.9096069335938 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1773.87548828125 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1570.4898681640625 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 929.8587036132812 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 1216.632080078125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 1339.7861328125 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2300.148193359375 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 542.1123046875 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1451.59228515625 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: -4151.92333984375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: -284854.875 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 709.8942260742188 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1037.3538818359375 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 31281.505859375 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 432.7812805175781 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2340.748291015625 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1116.0413818359375 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 371.93707275390625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 377.37396240234375 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 2202.251708984375 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 942.5869750976562 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 2916.78662109375 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 1301.223876953125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7529, loss_val: nan, pos_over_neg: 8678.1982421875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 876.9517822265625 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: 8916.5693359375 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1139.4468994140625 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: -3793.22802734375 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 339.7830505371094 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 1136.67626953125 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 517.84033203125 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 3361.896240234375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7877, loss_val: nan, pos_over_neg: 449.4166564941406 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 851.4247436523438 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: 668.0784301757812 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: -2689.38427734375 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 1441.85693359375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7447, loss_val: nan, pos_over_neg: -3833.219482421875 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 4452.16552734375 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 3045.9033203125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 4073.916748046875 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1056.92626953125 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1274.4266357421875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 1792.76708984375 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: -3407.86865234375 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 891.4242553710938 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 599.5247802734375 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: -8830.3544921875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1214.654052734375 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 7090.16552734375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 791.1307983398438 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 4203.7822265625 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7878, loss_val: nan, pos_over_neg: 2769.766845703125 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7884, loss_val: nan, pos_over_neg: 567.2202758789062 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: 6787.0380859375 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1730.9287109375 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2733.581787109375 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7532, loss_val: nan, pos_over_neg: 2287.12255859375 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 4369.63525390625 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 732.4552612304688 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 1788.9427490234375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1275.1480712890625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 482.6146240234375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 2265.9638671875 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: -4264.72509765625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: -15032.91015625 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 952.5294799804688 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 796.2075805664062 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 3429.378173828125 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 895.2711791992188 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 3543.25244140625 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 2937.3408203125 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1373.0145263671875 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 473.8650817871094 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1994.462646484375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 757.5642700195312 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 432.6732177734375 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 860.8758544921875 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 363.3482360839844 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1383.6536865234375 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7973, loss_val: nan, pos_over_neg: 1560.775390625 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 461.3597106933594 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 774.78466796875 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 340.9951477050781 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: 728.854736328125 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 618.2935180664062 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1167.933837890625 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 656.6097412109375 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 1238.5501708984375 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 697.8575439453125 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1894.4195556640625 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 578.07177734375 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 379.0108337402344 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 2531.37158203125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 1639.6151123046875 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 13064.900390625 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 1532.77685546875 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 5296.234375 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 1358.521484375 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: -3986.40478515625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 1436.040771484375 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 4566.6328125 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: -6428.61083984375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 859.6376342773438 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 551.0836181640625 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 528.7283325195312 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: -19148.4609375 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1120.7733154296875 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 598.0376586914062 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 904.5169067382812 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7867, loss_val: nan, pos_over_neg: 2362.145263671875 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 2264.20068359375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/300000 [5:08:05<128392:03:06, 1540.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12\n",
      "Iter: 0/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 606.3856201171875 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 587.760009765625 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 563.0917358398438 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 854.2704467773438 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 7669.46826171875 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 753.807373046875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 2290.82421875 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 1270.7130126953125 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: -2169.974609375 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 967.1152954101562 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 1659.4287109375 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1126.4603271484375 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 807.8712768554688 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 226.1916961669922 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 505.02630615234375 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1339.63720703125 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1123.7269287109375 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 380.0787048339844 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 5472.04931640625 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.797, loss_val: nan, pos_over_neg: 4496.06005859375 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 1534.861083984375 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 4886.22265625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1014.4685668945312 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 778.2763061523438 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 609.1587524414062 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 868.3491821289062 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 2617.022705078125 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1815.578125 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 558.209228515625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1330.4200439453125 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: -2371.476806640625 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 1093.43994140625 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.7556, loss_val: nan, pos_over_neg: -5409.0654296875 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1823.43017578125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 687.8292236328125 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 577.275634765625 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7522, loss_val: nan, pos_over_neg: 3310.766357421875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: -170598.984375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7503, loss_val: nan, pos_over_neg: 9062.0380859375 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: -7259.52783203125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 772.4280395507812 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1010.6489868164062 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 3047.373779296875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 542.8006591796875 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 283.9901428222656 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 634.6280517578125 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 557.4739379882812 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1617.879638671875 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1470.0975341796875 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.753, loss_val: nan, pos_over_neg: 3728.24755859375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: -71137.46875 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 6477.8037109375 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 4162.1083984375 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 479.5934753417969 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7891, loss_val: nan, pos_over_neg: 920.8479614257812 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: -21041.634765625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 780.4114379882812 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: -5984.7900390625 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 897.1278686523438 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 817.622802734375 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 775.936767578125 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 448.818359375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 5217.50634765625 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: -56789.0078125 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 659.1336059570312 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 805.4902954101562 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 831.7833251953125 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1224.5126953125 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2032.56298828125 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.751, loss_val: nan, pos_over_neg: 1498.1817626953125 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7533, loss_val: nan, pos_over_neg: 587.447265625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 612.957275390625 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 1355.2923583984375 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 895.5855102539062 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 900.9589233398438 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 722.0906982421875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 702.7861328125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 6256.41845703125 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 2705.698486328125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 651.5609741210938 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 2624.094482421875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 700.333984375 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 2970.617431640625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1293.1666259765625 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1899.3070068359375 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 4414.337890625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1613.4046630859375 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1187.7608642578125 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 992.8543090820312 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 13458.4443359375 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 2540.736083984375 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 798.7855224609375 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 3467.46728515625 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: -16268.166015625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 604.3331909179688 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1702.3546142578125 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 3335.02197265625 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1376.9080810546875 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 968.243408203125 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 4814.0791015625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1717.0113525390625 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1298.70263671875 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 409.20806884765625 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 447.2659912109375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 972.7510375976562 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 14324.2275390625 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 946.5992431640625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1689.4140625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 1250.047607421875 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 2300.339599609375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 2347.0322265625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1512.3546142578125 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 555.80029296875 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 373.2819519042969 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 1110.7427978515625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 30704.369140625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7834, loss_val: nan, pos_over_neg: 1140.916748046875 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7863, loss_val: nan, pos_over_neg: 1079.4462890625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7833, loss_val: nan, pos_over_neg: 525.75 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 1451.7493896484375 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: -21245.966796875 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 4498.634765625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 2328.689453125 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7918, loss_val: nan, pos_over_neg: 922.2376708984375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 900.5318603515625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.7866, loss_val: nan, pos_over_neg: 537.8312377929688 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 3380.055419921875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: -9239.39453125 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1750.7349853515625 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1178.72119140625 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 940.4025268554688 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 709.1614379882812 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 809.2760009765625 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: -3362.16943359375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 3970.405029296875 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1548.0421142578125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 356.9612121582031 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 459.1383361816406 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 836.9342651367188 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 845.5497436523438 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1933.228515625 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 2008.0953369140625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 765.8455810546875 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 603.8592529296875 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 600.1259765625 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 7828.68701171875 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 920.795654296875 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 108692.75 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1672.794189453125 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: -1524.6658935546875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -2356.66552734375 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.754, loss_val: nan, pos_over_neg: -3205.61474609375 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 782.189697265625 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 700.0374755859375 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 891.5153198242188 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 819.8416748046875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: 3922.473876953125 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 842.2661743164062 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1209.4307861328125 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2246.4033203125 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 5178.25634765625 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 2182.412353515625 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1169.1865234375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1815.2159423828125 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1039.0928955078125 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 1625.9678955078125 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 926.64013671875 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 798.8734741210938 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: 3187.976318359375 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1971.8387451171875 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 15287.9072265625 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 910.6923828125 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: -5415.25439453125 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 7778.333984375 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 870.55712890625 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 703.676025390625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1725.237548828125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1161.370361328125 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 762.1621704101562 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 601.215087890625 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 625.0224609375 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 5381.33447265625 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7904, loss_val: nan, pos_over_neg: 685.1329956054688 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 7521.43896484375 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 3445.041259765625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1727.0185546875 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 2010.6527099609375 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 1582.36767578125 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 665.2151489257812 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: -5858.423828125 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7939, loss_val: nan, pos_over_neg: 378.03515625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 1029.67578125 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1113.7845458984375 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1248.4644775390625 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 1454.0389404296875 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7869, loss_val: nan, pos_over_neg: 471.7522888183594 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1271.693603515625 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 358.6694030761719 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1751.0889892578125 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 895.0904541015625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 2246.9208984375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 2423.074462890625 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1165.662109375 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 3346.460693359375 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1072.4931640625 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.792, loss_val: nan, pos_over_neg: 492.7786865234375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1771.2882080078125 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 587.8715209960938 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 4127.67529296875 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2136.42138671875 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 416.0721130371094 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1454.2552490234375 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 610.2115478515625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 685.6299438476562 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 845.4201049804688 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1286.5015869140625 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.766, loss_val: nan, pos_over_neg: -4566.7763671875 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: -4853.66455078125 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 926.7700805664062 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1177.05810546875 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 524.2032470703125 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 12215.029296875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 811.4978637695312 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 2212.709228515625 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 3489.2900390625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1054.639404296875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 3161.4169921875 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7846, loss_val: nan, pos_over_neg: 787.064453125 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 673.1392211914062 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7947, loss_val: nan, pos_over_neg: 738.8848266601562 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 3267.349365234375 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 966.1796875 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: -43676.1171875 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1233.63232421875 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 1589.7698974609375 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 588.02099609375 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1959.677490234375 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1872.5321044921875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1352.252197265625 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1310.2635498046875 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 751.9091796875 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 869.6175537109375 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 367.11273193359375 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 730.7221069335938 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 949.8582153320312 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 796.3824462890625 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: 5396.40380859375 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 1656.972412109375 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 434.4393615722656 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 287.6557922363281 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 22841.89453125 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 670.9445190429688 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7908, loss_val: nan, pos_over_neg: 550.7598876953125 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7575, loss_val: nan, pos_over_neg: 1102.060546875 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 3077.823486328125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7766, loss_val: nan, pos_over_neg: 3478.47216796875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 4743.11328125 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7872, loss_val: nan, pos_over_neg: 217.45021057128906 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 1371.904296875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 614.818603515625 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1373.49560546875 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7601, loss_val: nan, pos_over_neg: 4963.65380859375 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7945, loss_val: nan, pos_over_neg: 2508.10107421875 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.7514, loss_val: nan, pos_over_neg: 4771.93359375 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 883.2408447265625 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1820.71630859375 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1626.570068359375 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 716.11865234375 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1045.7320556640625 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 386.3826599121094 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 962.2053833007812 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.7588, loss_val: nan, pos_over_neg: 6808.66357421875 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 981.229248046875 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: -2828.08251953125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 2912.989501953125 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 674.6333618164062 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7843, loss_val: nan, pos_over_neg: 983.7026977539062 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 2427.89013671875 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 2144.797119140625 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1966.2510986328125 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7536, loss_val: nan, pos_over_neg: 17750.115234375 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1226.489013671875 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 293.3619079589844 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1058.3656005859375 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 905.6025390625 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 615.3506469726562 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 1890.2672119140625 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.754, loss_val: nan, pos_over_neg: 1472.751953125 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 3206.365966796875 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 2045.6033935546875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.7535, loss_val: nan, pos_over_neg: 1138.9888916015625 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 2849.456787109375 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 463.2831115722656 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 817.821533203125 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 444.76336669921875 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 1835.871826171875 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1211.23583984375 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 891.1826782226562 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 4306.05224609375 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.7526, loss_val: nan, pos_over_neg: 2358.534423828125 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 1746.517578125 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 758.7967529296875 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 6809.7333984375 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 3947.10986328125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 510.8252868652344 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7549, loss_val: nan, pos_over_neg: 2299.3037109375 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1780.801513671875 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 779.7784423828125 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1804.6529541015625 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 2387.542236328125 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 2205.390869140625 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1120.2406005859375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1625.866943359375 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 645.7603149414062 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 558.0214233398438 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 755.8287963867188 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1914.235595703125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 3273.351318359375 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 555.5861206054688 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7523, loss_val: nan, pos_over_neg: 3023.973388671875 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: -5079.61962890625 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 539.8682250976562 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 578.4764404296875 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 2927.09814453125 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 2069.718505859375 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 2295.27294921875 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2190.205078125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 2998.14404296875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1080.5472412109375 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 649.3660888671875 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 836.0344848632812 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 3592.349365234375 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1371.1025390625 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 437.53509521484375 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7832, loss_val: nan, pos_over_neg: 808.9039306640625 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.765, loss_val: nan, pos_over_neg: -11716.4501953125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 1317.9139404296875 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5284.00146484375 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: -3503.25341796875 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 11778.0048828125 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1747.3770751953125 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 840.0003662109375 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7548, loss_val: nan, pos_over_neg: 1465.1103515625 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1481.1895751953125 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 3118.751220703125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 824.1001586914062 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 642.602783203125 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 654.3001098632812 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 323.0205383300781 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 608.5189819335938 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 750.0950927734375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1082.8836669921875 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1415.6669921875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 849.1196899414062 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 512.1267700195312 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 13846.2646484375 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 1935.15625 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 603.1231689453125 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 179036.203125 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1173.9852294921875 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 506.3559875488281 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7763, loss_val: nan, pos_over_neg: 784.4309692382812 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7541, loss_val: nan, pos_over_neg: 875.7633056640625 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 1292.9486083984375 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 3578.08935546875 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 2397.846435546875 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 520.96533203125 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 965.2965087890625 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 376.20111083984375 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 12822.6767578125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 2563.674560546875 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7542, loss_val: nan, pos_over_neg: -5275.4599609375 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: -4035.138671875 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 1124.97607421875 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 512.5740966796875 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 5536.59326171875 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: -8403.8017578125 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: -8515.7255859375 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: 3442.1689453125 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 518.8403930664062 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 543.075439453125 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 783.9290161132812 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 632.4440307617188 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 772.4827270507812 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 1117.2998046875 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 938.21044921875 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 2430.795166015625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 7898.255859375 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.7441, loss_val: nan, pos_over_neg: 2089.4912109375 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: -14339.5517578125 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1065.7215576171875 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 612.0643310546875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1466.8018798828125 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 553.064208984375 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 677.1399536132812 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1945.369140625 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1871.5438232421875 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 3344.617431640625 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: -4407.45458984375 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 1059.7772216796875 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7623, loss_val: nan, pos_over_neg: 3382.071533203125 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 3619.739990234375 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 466.6189880371094 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1222.2451171875 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1340.351806640625 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1345.241455078125 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 925.3602294921875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 4046.816650390625 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 5482.58349609375 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 894.5084228515625 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 668.4515380859375 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1069.3060302734375 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 3386.265380859375 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 3635.6328125 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1349.451904296875 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 729.801513671875 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 3479.47705078125 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1953.3018798828125 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 823.0172729492188 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 2602.532958984375 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 402.0379638671875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 538.4124145507812 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1109.73388671875 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1307.329833984375 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1168.0802001953125 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 1593.835205078125 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 716.15185546875 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7865, loss_val: nan, pos_over_neg: 824.725341796875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7847, loss_val: nan, pos_over_neg: 1196.12646484375 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: -2422.148681640625 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.755, loss_val: nan, pos_over_neg: 2484.305908203125 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7539, loss_val: nan, pos_over_neg: -3534.950927734375 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 1807.6156005859375 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1076.78271484375 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1571.980224609375 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7942, loss_val: nan, pos_over_neg: 596.8998413085938 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 808.0054931640625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 985.5731811523438 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 466.22735595703125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 3247.08203125 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1956.9097900390625 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 3406.0849609375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1571.6953125 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 576.8407592773438 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 590.761474609375 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 450.9357604980469 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 2747.7255859375 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 8255.818359375 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7516, loss_val: nan, pos_over_neg: -1149.5709228515625 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 6055.7333984375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 2109.079833984375 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1268.3121337890625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1753.1170654296875 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 833.270263671875 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 429.57208251953125 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 1584.1392822265625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 6538.732421875 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 722.3001708984375 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 736.0189208984375 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7801, loss_val: nan, pos_over_neg: 641.4202880859375 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1962.989501953125 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 5715.2919921875 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 2728.42919921875 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7577, loss_val: nan, pos_over_neg: 10436.828125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 763.4081420898438 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -2412.78369140625 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2049671.875 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 15886.8427734375 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: -103953.6015625 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1056.7867431640625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 555.7492065429688 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1577.6937255859375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 947.6879272460938 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 3856.89501953125 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 738.4384155273438 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 3443.56689453125 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 750.327880859375 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 955.301025390625 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 546.6780395507812 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 574.6082153320312 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1639.41650390625 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 611.5072021484375 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1362.228515625 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: -4265.60693359375 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 737.4099731445312 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 4355.02978515625 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 618.8328857421875 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 580.0281372070312 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.784, loss_val: nan, pos_over_neg: 421.2507019042969 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1004.8731079101562 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: 689.5634155273438 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 626.3460693359375 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1199.4105224609375 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 879.9812622070312 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 691.8120727539062 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 521.953125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1688.7279052734375 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 644.610107421875 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 985.7821655273438 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7807, loss_val: nan, pos_over_neg: 1055.458984375 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 760.0686645507812 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 417.4602966308594 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 1952.1405029296875 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 656.47412109375 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7518, loss_val: nan, pos_over_neg: 3219.695068359375 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 901.2122192382812 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: 851.0950927734375 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1001.5120239257812 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.7496, loss_val: nan, pos_over_neg: 853.3886108398438 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1566.611083984375 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 305.6900939941406 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 694.3052978515625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.7879, loss_val: nan, pos_over_neg: 872.0357055664062 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 1175.0408935546875 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7531, loss_val: nan, pos_over_neg: 1182.091552734375 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7839, loss_val: nan, pos_over_neg: 672.867431640625 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 1243.8704833984375 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 2390.967529296875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 511.3590393066406 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1012.2312622070312 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7481, loss_val: nan, pos_over_neg: 801.1879272460938 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: -15831.4697265625 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7494, loss_val: nan, pos_over_neg: 1405.957763671875 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1033.6800537109375 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 2189.73876953125 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: -1915.0601806640625 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 393.258056640625 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -4617.5732421875 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 427.7665710449219 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 482.1463928222656 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 3193.080810546875 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 1057.457275390625 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 2697.922607421875 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1512.7919921875 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 3291.541748046875 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1341.9013671875 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 405.8229064941406 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7465, loss_val: nan, pos_over_neg: 5771.509765625 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1387.3865966796875 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 4834.94189453125 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6023.05908203125 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: -23009.48046875 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 1095.5921630859375 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2104.231689453125 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: 2072.008056640625 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 884.0240478515625 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1572.516845703125 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1420.37744140625 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 2209.12646484375 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 3204.77197265625 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 3699.77783203125 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1921.422607421875 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 5514.314453125 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: 961.71435546875 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 642.5241088867188 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 531.4054565429688 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 782.206298828125 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7857, loss_val: nan, pos_over_neg: 395.48193359375 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 347.6226501464844 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 1149.4144287109375 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 619.0069580078125 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 1156.185546875 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1115.813720703125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1387.2830810546875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: -5681.9794921875 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 749.06005859375 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 366.5870361328125 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7888, loss_val: nan, pos_over_neg: 692.6921997070312 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1062.8685302734375 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1443.279052734375 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 774.3193359375 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7894, loss_val: nan, pos_over_neg: 622.6458740234375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 580.7256469726562 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 720.7218627929688 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7809, loss_val: nan, pos_over_neg: 1094.1806640625 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 1016.5021362304688 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 650.2320556640625 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7873, loss_val: nan, pos_over_neg: 855.7759399414062 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: 984.607666015625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1183.4317626953125 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7502, loss_val: nan, pos_over_neg: -721605.875 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1843.243408203125 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.7551, loss_val: nan, pos_over_neg: -25676.9609375 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 638.62060546875 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1077.859130859375 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 688.7308349609375 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 568.123291015625 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 612.4871826171875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 941.7764892578125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.7532, loss_val: nan, pos_over_neg: 4552.07470703125 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 1939.342041015625 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7528, loss_val: nan, pos_over_neg: 992.3907470703125 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 523.3770141601562 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 574.82958984375 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 22851.42578125 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: -10938.5400390625 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1614.3050537109375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 704.4519653320312 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 998.7693481445312 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1155.5496826171875 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 1115.2755126953125 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 610.251220703125 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1401.3153076171875 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 901.2314453125 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 1071.623779296875 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.778, loss_val: nan, pos_over_neg: 1604.2391357421875 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 4458.53369140625 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -1479.5941162109375 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 1062.2303466796875 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7958, loss_val: nan, pos_over_neg: 592.2064208984375 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 751.8340454101562 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: -9024.9052734375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7566, loss_val: nan, pos_over_neg: 7191.5458984375 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7593, loss_val: nan, pos_over_neg: -15030.2060546875 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 1520.6650390625 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 2655.68408203125 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 1823.2674560546875 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 1446.003173828125 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1008.272705078125 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1338.2691650390625 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 866.90380859375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7602, loss_val: nan, pos_over_neg: 1808.5311279296875 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 3250.403564453125 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 524.1804809570312 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 302.6040954589844 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 410.2102355957031 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 9866.603515625 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 106724.5546875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1041.1402587890625 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: -92418.9296875 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 12601.5087890625 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 510.33917236328125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 1112.7176513671875 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 820.4380493164062 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7861, loss_val: nan, pos_over_neg: 1913.7442626953125 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 998.8286743164062 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7552, loss_val: nan, pos_over_neg: 639.3067626953125 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 976.7835693359375 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1171.2283935546875 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7882, loss_val: nan, pos_over_neg: 520.4041748046875 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 805.2553100585938 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 787.2129516601562 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1233.2606201171875 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 363.3240051269531 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 16127.3271484375 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1523.7451171875 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1412.8721923828125 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1285.0926513671875 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.763, loss_val: nan, pos_over_neg: 548.59521484375 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.7943, loss_val: nan, pos_over_neg: 458.2992858886719 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 395.9898986816406 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7561, loss_val: nan, pos_over_neg: -3413.30517578125 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 692.16650390625 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.755, loss_val: nan, pos_over_neg: 9482.1064453125 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7522, loss_val: nan, pos_over_neg: 4185.80810546875 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 374.2748718261719 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1758.81005859375 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 1456.5225830078125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: -6935.12060546875 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: -31343.37890625 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 1726.673828125 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7464, loss_val: nan, pos_over_neg: 1561.1536865234375 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 490.13934326171875 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: -10364.1337890625 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 68838.765625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 4494.63671875 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 3184.205078125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 418.0438232421875 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1856.9405517578125 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 645.1585083007812 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 616.25634765625 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 7358.76025390625 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 853.564453125 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: -6749.22216796875 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 2594.84033203125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 5797.4169921875 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 631.5126342773438 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1272.4425048828125 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7885, loss_val: nan, pos_over_neg: 554.0265502929688 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 34995.546875 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1180.475830078125 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 883.0470581054688 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.789, loss_val: nan, pos_over_neg: 322.72125244140625 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 531.0419311523438 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1378.025390625 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 555.8182373046875 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1220.4188232421875 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1940.7169189453125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 1143.235107421875 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1040.279052734375 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 971.7166137695312 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 3441.65380859375 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7897, loss_val: nan, pos_over_neg: 1298.4598388671875 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1070.037109375 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 289.1808776855469 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 2451.946533203125 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 2587.316162109375 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 508.9186706542969 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.79, loss_val: nan, pos_over_neg: 873.8313598632812 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: -3464.12890625 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.755, loss_val: nan, pos_over_neg: 966.5695190429688 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 557.4570922851562 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 6859.6943359375 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7776, loss_val: nan, pos_over_neg: 1037.873779296875 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1741.9239501953125 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2751.69873046875 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 545.0487670898438 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 679.2481079101562 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: -12246.5185546875 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 643.9960327148438 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1171.9552001953125 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 466.7007141113281 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 852.6688842773438 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 779.9788208007812 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 2044.532470703125 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1009.7191162109375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 869.2123413085938 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7572, loss_val: nan, pos_over_neg: -8259.837890625 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 536.4437866210938 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 592.2595825195312 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1804.3255615234375 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.7802, loss_val: nan, pos_over_neg: 852.5534057617188 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.7557, loss_val: nan, pos_over_neg: 3780.000244140625 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.753, loss_val: nan, pos_over_neg: 4325.96728515625 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1243.80078125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 634.0719604492188 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1029.1220703125 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 717.711181640625 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 555.6767578125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 1272.137939453125 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7566, loss_val: nan, pos_over_neg: 1729.304931640625 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 10716.6162109375 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 930.522705078125 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1119.79931640625 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1007.3441162109375 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 831.1470336914062 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: -212789.171875 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1135.82568359375 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 3215.78759765625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 611.4814453125 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1232.430419921875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1126.371826171875 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: -28610.01953125 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 837.50244140625 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 3125.364013671875 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 3698.4228515625 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 2076.763671875 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1373.7296142578125 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 663.7487182617188 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 558.3534545898438 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: -4124.28759765625 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 3497.4833984375 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 967.6904907226562 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 1728.1923828125 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.7835, loss_val: nan, pos_over_neg: 723.1141357421875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 988.6209716796875 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1592.348388671875 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 3165.03515625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: -5303.18798828125 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 2407.927978515625 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 569.236083984375 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: -6583.31884765625 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 1045.630126953125 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -27045.4609375 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 3465.525634765625 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 365.4232177734375 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 2561.6728515625 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 621.606201171875 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 3389.653564453125 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1520.8638916015625 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 482.04083251953125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1136.52734375 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 500.4695129394531 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 1191.5152587890625 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1008.620361328125 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1294.0322265625 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: 503.3807678222656 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 788.6494140625 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 2741.667236328125 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: -3536.278564453125 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 7369.826171875 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 6231.12548828125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 936.9028930664062 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1079.6661376953125 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 1706.4105224609375 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 529.8713989257812 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 648.8753662109375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 605.1924438476562 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7893, loss_val: nan, pos_over_neg: 582.299072265625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 999.0492553710938 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7438, loss_val: nan, pos_over_neg: 2145.840576171875 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 862.906005859375 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7826, loss_val: nan, pos_over_neg: 563.9873046875 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 609.3910522460938 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1393.8551025390625 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 3320.3095703125 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 1332.417236328125 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 759.2603149414062 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 1446.0889892578125 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 1667.555419921875 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 650.7086791992188 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 967.43115234375 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 4219.26513671875 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 1368.402099609375 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7555, loss_val: nan, pos_over_neg: 1545.791015625 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 2556.3310546875 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 1704.0152587890625 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: 1288.353759765625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1994.22412109375 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 2148.174072265625 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 598.68359375 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 908.84521484375 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1636.8914794921875 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 914.1099853515625 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 509.6577453613281 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: 929.8781127929688 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 746.4749755859375 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 2547.628662109375 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: 596.1199340820312 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.752, loss_val: nan, pos_over_neg: -3580.63623046875 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 623.53564453125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7585, loss_val: nan, pos_over_neg: 4829.6728515625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 663.4987182617188 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 2334.536376953125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7874, loss_val: nan, pos_over_neg: 463.15863037109375 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 681.1255493164062 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 1172.7259521484375 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2026.6666259765625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 4593.85693359375 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 7006.2294921875 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 592.8897094726562 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1878.3533935546875 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 967.1002197265625 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 3652.1767578125 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7572, loss_val: nan, pos_over_neg: -5092.85302734375 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7616, loss_val: nan, pos_over_neg: 2093.42529296875 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 8111.28955078125 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: 741.3656005859375 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 2303.28125 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1768.2259521484375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7458, loss_val: nan, pos_over_neg: 923.94970703125 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 926.1318359375 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1052.9415283203125 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 531.75537109375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2333.094970703125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1238.163818359375 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 1031.5631103515625 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1562.55126953125 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 992.4100952148438 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 758.0517578125 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: 4732.84765625 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 939.87255859375 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 5341.10791015625 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: -16693.91015625 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: -2673.606689453125 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 623.7496337890625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.747, loss_val: nan, pos_over_neg: 590.169189453125 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 722.8001098632812 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 2806.65478515625 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 659.0503540039062 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 862.3084716796875 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: 696.6036376953125 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 800.9024658203125 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 915.4089965820312 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 697.4246215820312 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 3085.267822265625 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1477.7451171875 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 4004.0908203125 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 2740.709716796875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 841.443115234375 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7781, loss_val: nan, pos_over_neg: 2036.5714111328125 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 1659.8343505859375 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 686.4104614257812 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1909.3577880859375 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 2707.309814453125 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7536, loss_val: nan, pos_over_neg: -27540.857421875 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 2891.60009765625 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1654.4049072265625 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 471.65948486328125 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 1546.888427734375 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 980.834228515625 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 6032.0322265625 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 620.5440673828125 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 1220.3394775390625 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 1050.2799072265625 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 2138.55859375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7474, loss_val: nan, pos_over_neg: 4058.390625 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: 1731.7978515625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 832.6458740234375 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/300000 [5:33:21<127769:52:25, 1533.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13\n",
      "Iter: 0/889, loss_train: 5.7506, loss_val: nan, pos_over_neg: 2604.572021484375 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1769.1912841796875 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 350.11102294921875 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 1566.380859375 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 634.4264526367188 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1388.289794921875 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 508.18804931640625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 9553.59375 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1485.350830078125 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.788, loss_val: nan, pos_over_neg: 729.8001098632812 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7499, loss_val: nan, pos_over_neg: 3905.50341796875 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 4023.0302734375 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 1018.310302734375 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 630.2415161132812 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7508, loss_val: nan, pos_over_neg: 1454.9207763671875 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 753.197998046875 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 1746.608154296875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 748.9171142578125 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 491.0824279785156 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1001.1500244140625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 7638.96484375 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 760.1076049804688 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 617.2611083984375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: -6659.1103515625 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 2946.38134765625 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 927.8407592773438 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7541, loss_val: nan, pos_over_neg: 661.8216552734375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 1075.5015869140625 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7701, loss_val: nan, pos_over_neg: 775.2197875976562 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 619.7835083007812 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 667.3414916992188 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 2318.34716796875 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.766, loss_val: nan, pos_over_neg: -7055.771484375 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 715.7152099609375 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1199.4185791015625 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -7082.33251953125 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7565, loss_val: nan, pos_over_neg: -6121.0673828125 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 1224.3773193359375 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 719.6007080078125 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 900.5400390625 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.7819, loss_val: nan, pos_over_neg: 737.2992553710938 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1341.6856689453125 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 1003.294921875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 3991.4619140625 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1187.2261962890625 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2498.521484375 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7529, loss_val: nan, pos_over_neg: 1879.685791015625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 5282.58251953125 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 3027.4287109375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1086.4505615234375 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 647.24267578125 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7788, loss_val: nan, pos_over_neg: 991.9710083007812 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 497.2755432128906 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1051.752197265625 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7571, loss_val: nan, pos_over_neg: 6936.15283203125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 967.949462890625 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 1420.2994384765625 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7849, loss_val: nan, pos_over_neg: 1425.21875 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 2162.434814453125 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 794.732177734375 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 22366.0546875 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: -2280.81591796875 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7542, loss_val: nan, pos_over_neg: -18485.673828125 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1181.305419921875 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: -14931.2294921875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 1821.45361328125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 4272.8974609375 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 3767.596923828125 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 21081.947265625 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1309.0472412109375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 2020.93896484375 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7988, loss_val: nan, pos_over_neg: 538.9724731445312 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 2350.326904296875 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 924.3551635742188 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7495, loss_val: nan, pos_over_neg: -3183.240966796875 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 766.6380615234375 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 322.239013671875 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.794, loss_val: nan, pos_over_neg: 287.6348571777344 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 997.0567626953125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 984.7761840820312 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7784, loss_val: nan, pos_over_neg: 1445.694580078125 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 4419.59619140625 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 2012.5660400390625 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7473, loss_val: nan, pos_over_neg: 1246.5830078125 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1513.6834716796875 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 477.0799560546875 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 590.5320434570312 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 911.5438842773438 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 485.6622619628906 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 865.2332763671875 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7722, loss_val: nan, pos_over_neg: 1215.428955078125 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.7446, loss_val: nan, pos_over_neg: 1211.587158203125 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1998.4525146484375 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 2186.656494140625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 12434.58984375 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 875.5462646484375 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 3181.81787109375 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 613.4478759765625 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 289.92626953125 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 872.9039306640625 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 380.8889465332031 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.7614, loss_val: nan, pos_over_neg: 855.6046142578125 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 5165.74560546875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 651.77197265625 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 15460.724609375 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 2547.5302734375 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 947.8598022460938 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 1434.33837890625 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 1751.9456787109375 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1894.1339111328125 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1263.7418212890625 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 969.71435546875 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 439.8299255371094 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 667.0516357421875 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1629.324462890625 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: 4765.76123046875 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 162555.015625 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7919, loss_val: nan, pos_over_neg: 1034.9302978515625 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: -2928.234130859375 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.7805, loss_val: nan, pos_over_neg: 500.9668273925781 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 7142.595703125 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: -20419.447265625 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 2648.798583984375 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 4438.599609375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 533.1337890625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1603.87939453125 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 919.9459228515625 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 595.4771118164062 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 872.6192016601562 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 645.0462036132812 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.7616, loss_val: nan, pos_over_neg: 3635.64208984375 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 809.298583984375 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1506.4681396484375 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: 2157.333984375 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7484, loss_val: nan, pos_over_neg: -7379.1611328125 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 954.3822021484375 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 809.2528686523438 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1458.53271484375 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 769.81298828125 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: -76755.8125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: -1880.749755859375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: -7864.291015625 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: -2122.923828125 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 427.01715087890625 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 527.8512573242188 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 418.3545227050781 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 700.0703125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 749.9390258789062 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 421.18499755859375 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: 881.6117553710938 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1888.2257080078125 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7658, loss_val: nan, pos_over_neg: 2372.998779296875 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 12118.3935546875 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 2019.45703125 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1394.256591796875 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 347.36297607421875 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 660.0238037109375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 1185.3597412109375 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7535, loss_val: nan, pos_over_neg: 2386.772216796875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7858, loss_val: nan, pos_over_neg: 568.591552734375 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 711.83447265625 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7443, loss_val: nan, pos_over_neg: 77341.046875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 754.060302734375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 2293.195556640625 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 554.5128173828125 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 395.468994140625 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 9260.3740234375 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 6599.46826171875 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 11636.7138671875 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1284.6314697265625 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7769, loss_val: nan, pos_over_neg: 1967.58349609375 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 611.8961181640625 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1134.3768310546875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 687.8949584960938 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 529.4462280273438 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7544, loss_val: nan, pos_over_neg: 1448.3629150390625 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1199.1226806640625 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 5049.75341796875 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.774, loss_val: nan, pos_over_neg: -34774.80859375 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -1486.7449951171875 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7602, loss_val: nan, pos_over_neg: 3060.428466796875 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7829, loss_val: nan, pos_over_neg: 2248.904296875 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 1550.5086669921875 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: -2708.4873046875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 2965.92431640625 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 1276.104736328125 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 983.4501953125 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 3122.55224609375 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 493.4919738769531 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1346.6470947265625 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 430.8101806640625 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: -313917.40625 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 746.2916870117188 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1702.9451904296875 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: 2718.52587890625 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7748, loss_val: nan, pos_over_neg: 1091.331298828125 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 1664.9012451171875 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1750.4351806640625 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1175.530029296875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7531, loss_val: nan, pos_over_neg: 2198.472900390625 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7455, loss_val: nan, pos_over_neg: 4133.07568359375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 2941.709228515625 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 2420.127197265625 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 2163.024658203125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7827, loss_val: nan, pos_over_neg: 843.5911865234375 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7511, loss_val: nan, pos_over_neg: -8302.427734375 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 538.7943725585938 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 427.05572509765625 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 472.9901123046875 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.761, loss_val: nan, pos_over_neg: -9723.8232421875 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 2246.49267578125 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 1267.6485595703125 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 2462.319091796875 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 2843.620361328125 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 8377.1162109375 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 780.145751953125 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 4801.5537109375 lr: 0.00031623\n",
      "Iter: 217/889, loss_train: 5.7509, loss_val: nan, pos_over_neg: -5020.8583984375 lr: 0.00031623\n",
      "Iter: 218/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 2016.097412109375 lr: 0.00031623\n",
      "Iter: 219/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: -2983.43017578125 lr: 0.00031623\n",
      "Iter: 220/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 776.3359375 lr: 0.00031623\n",
      "Iter: 221/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 7181.29248046875 lr: 0.00031623\n",
      "Iter: 222/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 687.01416015625 lr: 0.00031623\n",
      "Iter: 223/889, loss_train: 5.7557, loss_val: nan, pos_over_neg: 1183.45458984375 lr: 0.00031623\n",
      "Iter: 224/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1216.3922119140625 lr: 0.00031623\n",
      "Iter: 225/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1239.9873046875 lr: 0.00031623\n",
      "Iter: 226/889, loss_train: 5.7864, loss_val: nan, pos_over_neg: 1738.9434814453125 lr: 0.00031623\n",
      "Iter: 227/889, loss_train: 5.7471, loss_val: nan, pos_over_neg: 1731.9456787109375 lr: 0.00031623\n",
      "Iter: 228/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 976.0770874023438 lr: 0.00031623\n",
      "Iter: 229/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: 6137.12890625 lr: 0.00031623\n",
      "Iter: 230/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 2976.10205078125 lr: 0.00031623\n",
      "Iter: 231/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2718.812744140625 lr: 0.00031623\n",
      "Iter: 232/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 618.9844360351562 lr: 0.00031623\n",
      "Iter: 233/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 1329.534912109375 lr: 0.00031623\n",
      "Iter: 234/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 1678.87548828125 lr: 0.00031623\n",
      "Iter: 235/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1688.6522216796875 lr: 0.00031623\n",
      "Iter: 236/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 3751.365966796875 lr: 0.00031623\n",
      "Iter: 237/889, loss_train: 5.755, loss_val: nan, pos_over_neg: 151234.71875 lr: 0.00031623\n",
      "Iter: 238/889, loss_train: 5.7551, loss_val: nan, pos_over_neg: 1056.9500732421875 lr: 0.00031623\n",
      "Iter: 239/889, loss_train: 5.7481, loss_val: nan, pos_over_neg: 3926.599853515625 lr: 0.00031623\n",
      "Iter: 240/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 929.2378540039062 lr: 0.00031623\n",
      "Iter: 241/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 687.4828491210938 lr: 0.00031623\n",
      "Iter: 242/889, loss_train: 5.7488, loss_val: nan, pos_over_neg: 862.6510620117188 lr: 0.00031623\n",
      "Iter: 243/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 674.9633178710938 lr: 0.00031623\n",
      "Iter: 244/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 2680.599365234375 lr: 0.00031623\n",
      "Iter: 245/889, loss_train: 5.7488, loss_val: nan, pos_over_neg: 3343.257080078125 lr: 0.00031623\n",
      "Iter: 246/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 772.6552124023438 lr: 0.00031623\n",
      "Iter: 247/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 702.9619750976562 lr: 0.00031623\n",
      "Iter: 248/889, loss_train: 5.7793, loss_val: nan, pos_over_neg: 417.6564636230469 lr: 0.00031623\n",
      "Iter: 249/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 757.2935791015625 lr: 0.00031623\n",
      "Iter: 250/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1290.0826416015625 lr: 0.00031623\n",
      "Iter: 251/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: -4152.8017578125 lr: 0.00031623\n",
      "Iter: 252/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 2879.841796875 lr: 0.00031623\n",
      "Iter: 253/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 715.4263916015625 lr: 0.00031623\n",
      "Iter: 254/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 5859.7080078125 lr: 0.00031623\n",
      "Iter: 255/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: 5407.98046875 lr: 0.00031623\n",
      "Iter: 256/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 2642.641357421875 lr: 0.00031623\n",
      "Iter: 257/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: -1903.3187255859375 lr: 0.00031623\n",
      "Iter: 258/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: -18296.796875 lr: 0.00031623\n",
      "Iter: 259/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 2572.8095703125 lr: 0.00031623\n",
      "Iter: 260/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 671.2281494140625 lr: 0.00031623\n",
      "Iter: 261/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 953.1406860351562 lr: 0.00031623\n",
      "Iter: 262/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 657.8711547851562 lr: 0.00031623\n",
      "Iter: 263/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 2387.77392578125 lr: 0.00031623\n",
      "Iter: 264/889, loss_train: 5.7791, loss_val: nan, pos_over_neg: 1303.7120361328125 lr: 0.00031623\n",
      "Iter: 265/889, loss_train: 5.7512, loss_val: nan, pos_over_neg: 622.471435546875 lr: 0.00031623\n",
      "Iter: 266/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 1586.129638671875 lr: 0.00031623\n",
      "Iter: 267/889, loss_train: 5.785, loss_val: nan, pos_over_neg: 1427.7242431640625 lr: 0.00031623\n",
      "Iter: 268/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 658.7045288085938 lr: 0.00031623\n",
      "Iter: 269/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 1484.036376953125 lr: 0.00031623\n",
      "Iter: 270/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 1130.1055908203125 lr: 0.00031623\n",
      "Iter: 271/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 929.6719360351562 lr: 0.00031623\n",
      "Iter: 272/889, loss_train: 5.751, loss_val: nan, pos_over_neg: 5912.87939453125 lr: 0.00031623\n",
      "Iter: 273/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1166.9464111328125 lr: 0.00031623\n",
      "Iter: 274/889, loss_train: 5.7424, loss_val: nan, pos_over_neg: 1214.7288818359375 lr: 0.00031623\n",
      "Iter: 275/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 649.5599975585938 lr: 0.00031623\n",
      "Iter: 276/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 758.1255493164062 lr: 0.00031623\n",
      "Iter: 277/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 420.5811767578125 lr: 0.00031623\n",
      "Iter: 278/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 369.51214599609375 lr: 0.00031623\n",
      "Iter: 279/889, loss_train: 5.7521, loss_val: nan, pos_over_neg: 1413.587158203125 lr: 0.00031623\n",
      "Iter: 280/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1676.036376953125 lr: 0.00031623\n",
      "Iter: 281/889, loss_train: 5.7786, loss_val: nan, pos_over_neg: 1526.6497802734375 lr: 0.00031623\n",
      "Iter: 282/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1883.6798095703125 lr: 0.00031623\n",
      "Iter: 283/889, loss_train: 5.7983, loss_val: nan, pos_over_neg: 291.58905029296875 lr: 0.00031623\n",
      "Iter: 284/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 420.3935852050781 lr: 0.00031623\n",
      "Iter: 285/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 756.1054077148438 lr: 0.00031623\n",
      "Iter: 286/889, loss_train: 5.7712, loss_val: nan, pos_over_neg: 779.6506958007812 lr: 0.00031623\n",
      "Iter: 287/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 818.4397583007812 lr: 0.00031623\n",
      "Iter: 288/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 339.1980285644531 lr: 0.00031623\n",
      "Iter: 289/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 743.7537841796875 lr: 0.00031623\n",
      "Iter: 290/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 797.7614135742188 lr: 0.00031623\n",
      "Iter: 291/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 2430.821533203125 lr: 0.00031623\n",
      "Iter: 292/889, loss_train: 5.7483, loss_val: nan, pos_over_neg: 3285.22021484375 lr: 0.00031623\n",
      "Iter: 293/889, loss_train: 5.791, loss_val: nan, pos_over_neg: 759.287109375 lr: 0.00031623\n",
      "Iter: 294/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 510.67474365234375 lr: 0.00031623\n",
      "Iter: 295/889, loss_train: 5.78, loss_val: nan, pos_over_neg: 700.6893920898438 lr: 0.00031623\n",
      "Iter: 296/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 11001.29296875 lr: 0.00031623\n",
      "Iter: 297/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 658.0492553710938 lr: 0.00031623\n",
      "Iter: 298/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: 3742.008544921875 lr: 0.00031623\n",
      "Iter: 299/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 669.5839233398438 lr: 0.00031623\n",
      "Iter: 300/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 2338.38037109375 lr: 0.00031623\n",
      "Iter: 301/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 892.335205078125 lr: 0.00031623\n",
      "Iter: 302/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 1454.0693359375 lr: 0.00031623\n",
      "Iter: 303/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 1192.116455078125 lr: 0.00031623\n",
      "Iter: 304/889, loss_train: 5.7881, loss_val: nan, pos_over_neg: 432.83306884765625 lr: 0.00031623\n",
      "Iter: 305/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 2839.396240234375 lr: 0.00031623\n",
      "Iter: 306/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 577.2745971679688 lr: 0.00031623\n",
      "Iter: 307/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 1573.355224609375 lr: 0.00031623\n",
      "Iter: 308/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 1778.7977294921875 lr: 0.00031623\n",
      "Iter: 309/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1604.3284912109375 lr: 0.00031623\n",
      "Iter: 310/889, loss_train: 5.7771, loss_val: nan, pos_over_neg: 1462.4779052734375 lr: 0.00031623\n",
      "Iter: 311/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1182.1878662109375 lr: 0.00031623\n",
      "Iter: 312/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 744.9085083007812 lr: 0.00031623\n",
      "Iter: 313/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 989.5966796875 lr: 0.00031623\n",
      "Iter: 314/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1277.1270751953125 lr: 0.00031623\n",
      "Iter: 315/889, loss_train: 5.7733, loss_val: nan, pos_over_neg: 904.936767578125 lr: 0.00031623\n",
      "Iter: 316/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 804.393798828125 lr: 0.00031623\n",
      "Iter: 317/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 730.369873046875 lr: 0.00031623\n",
      "Iter: 318/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 1497.9219970703125 lr: 0.00031623\n",
      "Iter: 319/889, loss_train: 5.7767, loss_val: nan, pos_over_neg: 1620.1378173828125 lr: 0.00031623\n",
      "Iter: 320/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 788.271484375 lr: 0.00031623\n",
      "Iter: 321/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 214649.453125 lr: 0.00031623\n",
      "Iter: 322/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 910.8828735351562 lr: 0.00031623\n",
      "Iter: 323/889, loss_train: 5.7528, loss_val: nan, pos_over_neg: 2470.40625 lr: 0.00031623\n",
      "Iter: 324/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 465.7878723144531 lr: 0.00031623\n",
      "Iter: 325/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 1883.2857666015625 lr: 0.00031623\n",
      "Iter: 326/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 424.28253173828125 lr: 0.00031623\n",
      "Iter: 327/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1183.18310546875 lr: 0.00031623\n",
      "Iter: 328/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1999.1468505859375 lr: 0.00031623\n",
      "Iter: 329/889, loss_train: 5.7556, loss_val: nan, pos_over_neg: 2447.089599609375 lr: 0.00031623\n",
      "Iter: 330/889, loss_train: 5.767, loss_val: nan, pos_over_neg: -202676.875 lr: 0.00031623\n",
      "Iter: 331/889, loss_train: 5.7916, loss_val: nan, pos_over_neg: 550.6905517578125 lr: 0.00031623\n",
      "Iter: 332/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 2065.25537109375 lr: 0.00031623\n",
      "Iter: 333/889, loss_train: 5.7585, loss_val: nan, pos_over_neg: -6260.57763671875 lr: 0.00031623\n",
      "Iter: 334/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 7262.57861328125 lr: 0.00031623\n",
      "Iter: 335/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1278.915283203125 lr: 0.00031623\n",
      "Iter: 336/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 844.7798461914062 lr: 0.00031623\n",
      "Iter: 337/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 786.3817138671875 lr: 0.00031623\n",
      "Iter: 338/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 2583.29931640625 lr: 0.00031623\n",
      "Iter: 339/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 752.6912231445312 lr: 0.00031623\n",
      "Iter: 340/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 534.6482543945312 lr: 0.00031623\n",
      "Iter: 341/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 225.58566284179688 lr: 0.00031623\n",
      "Iter: 342/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 509.5997009277344 lr: 0.00031623\n",
      "Iter: 343/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1634.939208984375 lr: 0.00031623\n",
      "Iter: 344/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 2285.294189453125 lr: 0.00031623\n",
      "Iter: 345/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 1427.0123291015625 lr: 0.00031623\n",
      "Iter: 346/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 934.0830688476562 lr: 0.00031623\n",
      "Iter: 347/889, loss_train: 5.7736, loss_val: nan, pos_over_neg: -9537.021484375 lr: 0.00031623\n",
      "Iter: 348/889, loss_train: 5.7623, loss_val: nan, pos_over_neg: 532.5045776367188 lr: 0.00031623\n",
      "Iter: 349/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 586.458740234375 lr: 0.00031623\n",
      "Iter: 350/889, loss_train: 5.7441, loss_val: nan, pos_over_neg: 4047.88037109375 lr: 0.00031623\n",
      "Iter: 351/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 810.579345703125 lr: 0.00031623\n",
      "Iter: 352/889, loss_train: 5.7862, loss_val: nan, pos_over_neg: 384.75848388671875 lr: 0.00031623\n",
      "Iter: 353/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 698.6923828125 lr: 0.00031623\n",
      "Iter: 354/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 672.2173461914062 lr: 0.00031623\n",
      "Iter: 355/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1309.3719482421875 lr: 0.00031623\n",
      "Iter: 356/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 1472.1221923828125 lr: 0.00031623\n",
      "Iter: 357/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 514.9636840820312 lr: 0.00031623\n",
      "Iter: 358/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 17383.7265625 lr: 0.00031623\n",
      "Iter: 359/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 3894.1572265625 lr: 0.00031623\n",
      "Iter: 360/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1068.9967041015625 lr: 0.00031623\n",
      "Iter: 361/889, loss_train: 5.7505, loss_val: nan, pos_over_neg: 18412.9921875 lr: 0.00031623\n",
      "Iter: 362/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 760.3703002929688 lr: 0.00031623\n",
      "Iter: 363/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 659.0573120117188 lr: 0.00031623\n",
      "Iter: 364/889, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1995.44384765625 lr: 0.00031623\n",
      "Iter: 365/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 5637.54541015625 lr: 0.00031623\n",
      "Iter: 366/889, loss_train: 5.7493, loss_val: nan, pos_over_neg: 1658.827392578125 lr: 0.00031623\n",
      "Iter: 367/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 794.0460205078125 lr: 0.00031623\n",
      "Iter: 368/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2271.58837890625 lr: 0.00031623\n",
      "Iter: 369/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 511.06689453125 lr: 0.00031623\n",
      "Iter: 370/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 812.499755859375 lr: 0.00031623\n",
      "Iter: 371/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1116.69970703125 lr: 0.00031623\n",
      "Iter: 372/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 1050.5543212890625 lr: 0.00031623\n",
      "Iter: 373/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1404.931396484375 lr: 0.00031623\n",
      "Iter: 374/889, loss_train: 5.7852, loss_val: nan, pos_over_neg: 448.32305908203125 lr: 0.00031623\n",
      "Iter: 375/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 1012.9797973632812 lr: 0.00031623\n",
      "Iter: 376/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 2857.508056640625 lr: 0.00031623\n",
      "Iter: 377/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 868.7086791992188 lr: 0.00031623\n",
      "Iter: 378/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 2971.365966796875 lr: 0.00031623\n",
      "Iter: 379/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 573.3681030273438 lr: 0.00031623\n",
      "Iter: 380/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 1107.9652099609375 lr: 0.00031623\n",
      "Iter: 381/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 517.112060546875 lr: 0.00031623\n",
      "Iter: 382/889, loss_train: 5.7563, loss_val: nan, pos_over_neg: 3391.462646484375 lr: 0.00031623\n",
      "Iter: 383/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 909.941162109375 lr: 0.00031623\n",
      "Iter: 384/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 3435.646728515625 lr: 0.00031623\n",
      "Iter: 385/889, loss_train: 5.7926, loss_val: nan, pos_over_neg: 289.238037109375 lr: 0.00031623\n",
      "Iter: 386/889, loss_train: 5.7602, loss_val: nan, pos_over_neg: 839.75390625 lr: 0.00031623\n",
      "Iter: 387/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 738.239013671875 lr: 0.00031623\n",
      "Iter: 388/889, loss_train: 5.777, loss_val: nan, pos_over_neg: 1865.3577880859375 lr: 0.00031623\n",
      "Iter: 389/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 692.1646728515625 lr: 0.00031623\n",
      "Iter: 390/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 1180.55615234375 lr: 0.00031623\n",
      "Iter: 391/889, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1208.166748046875 lr: 0.00031623\n",
      "Iter: 392/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 3944.16357421875 lr: 0.00031623\n",
      "Iter: 393/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: 3808.302978515625 lr: 0.00031623\n",
      "Iter: 394/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 2205.984619140625 lr: 0.00031623\n",
      "Iter: 395/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 1451.822998046875 lr: 0.00031623\n",
      "Iter: 396/889, loss_train: 5.7669, loss_val: nan, pos_over_neg: 2970.001953125 lr: 0.00031623\n",
      "Iter: 397/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 33673.5078125 lr: 0.00031623\n",
      "Iter: 398/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 3452.03857421875 lr: 0.00031623\n",
      "Iter: 399/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1116.572021484375 lr: 0.00031623\n",
      "Iter: 400/889, loss_train: 5.7438, loss_val: nan, pos_over_neg: 16382.0087890625 lr: 0.00031623\n",
      "Iter: 401/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1518.76806640625 lr: 0.00031623\n",
      "Iter: 402/889, loss_train: 5.7447, loss_val: nan, pos_over_neg: 7526.02783203125 lr: 0.00031623\n",
      "Iter: 403/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 339.52447509765625 lr: 0.00031623\n",
      "Iter: 404/889, loss_train: 5.7527, loss_val: nan, pos_over_neg: 5110.9833984375 lr: 0.00031623\n",
      "Iter: 405/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1126.2076416015625 lr: 0.00031623\n",
      "Iter: 406/889, loss_train: 5.7583, loss_val: nan, pos_over_neg: 1343.6004638671875 lr: 0.00031623\n",
      "Iter: 407/889, loss_train: 5.7845, loss_val: nan, pos_over_neg: 415.499755859375 lr: 0.00031623\n",
      "Iter: 408/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: -20691.748046875 lr: 0.00031623\n",
      "Iter: 409/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 55188.09375 lr: 0.00031623\n",
      "Iter: 410/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 1325.4156494140625 lr: 0.00031623\n",
      "Iter: 411/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 1051.8826904296875 lr: 0.00031623\n",
      "Iter: 412/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 757.0795288085938 lr: 0.00031623\n",
      "Iter: 413/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 3082.49072265625 lr: 0.00031623\n",
      "Iter: 414/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: -2749.710205078125 lr: 0.00031623\n",
      "Iter: 415/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 1361.892333984375 lr: 0.00031623\n",
      "Iter: 416/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 897.9393310546875 lr: 0.00031623\n",
      "Iter: 417/889, loss_train: 5.7917, loss_val: nan, pos_over_neg: 564.1625366210938 lr: 0.00031623\n",
      "Iter: 418/889, loss_train: 5.764, loss_val: nan, pos_over_neg: 349.7944641113281 lr: 0.00031623\n",
      "Iter: 419/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 391.0762939453125 lr: 0.00031623\n",
      "Iter: 420/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: 12509.9482421875 lr: 0.00031623\n",
      "Iter: 421/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 2628.195556640625 lr: 0.00031623\n",
      "Iter: 422/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 959.2593994140625 lr: 0.00031623\n",
      "Iter: 423/889, loss_train: 5.7632, loss_val: nan, pos_over_neg: 564.7477416992188 lr: 0.00031623\n",
      "Iter: 424/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 401.1898498535156 lr: 0.00031623\n",
      "Iter: 425/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 1273.4974365234375 lr: 0.00031623\n",
      "Iter: 426/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 6229.99658203125 lr: 0.00031623\n",
      "Iter: 427/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1162.7850341796875 lr: 0.00031623\n",
      "Iter: 428/889, loss_train: 5.7433, loss_val: nan, pos_over_neg: 1622.37939453125 lr: 0.00031623\n",
      "Iter: 429/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 877.3252563476562 lr: 0.00031623\n",
      "Iter: 430/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: -2402.6748046875 lr: 0.00031623\n",
      "Iter: 431/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2962.33544921875 lr: 0.00031623\n",
      "Iter: 432/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 4373.35791015625 lr: 0.00031623\n",
      "Iter: 433/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1992.5015869140625 lr: 0.00031623\n",
      "Iter: 434/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 2355.039306640625 lr: 0.00031623\n",
      "Iter: 435/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 818.3844604492188 lr: 0.00031623\n",
      "Iter: 436/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 2231.0556640625 lr: 0.00031623\n",
      "Iter: 437/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 870.2117919921875 lr: 0.00031623\n",
      "Iter: 438/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1158.4503173828125 lr: 0.00031623\n",
      "Iter: 439/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 1202.9769287109375 lr: 0.00031623\n",
      "Iter: 440/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2755.252685546875 lr: 0.00031623\n",
      "Iter: 441/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 7141.44677734375 lr: 0.00031623\n",
      "Iter: 442/889, loss_train: 5.7828, loss_val: nan, pos_over_neg: 821.9700927734375 lr: 0.00031623\n",
      "Iter: 443/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 510.9778747558594 lr: 0.00031623\n",
      "Iter: 444/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1621.7960205078125 lr: 0.00031623\n",
      "Iter: 445/889, loss_train: 5.7506, loss_val: nan, pos_over_neg: 3174.147705078125 lr: 0.00031623\n",
      "Iter: 446/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 2405.783935546875 lr: 0.00031623\n",
      "Iter: 447/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 4370.1611328125 lr: 0.00031623\n",
      "Iter: 448/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: -9476.3076171875 lr: 0.00031623\n",
      "Iter: 449/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 3491.39990234375 lr: 0.00031623\n",
      "Iter: 450/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 621.203369140625 lr: 0.00031623\n",
      "Iter: 451/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 364.41754150390625 lr: 0.00031623\n",
      "Iter: 452/889, loss_train: 5.7622, loss_val: nan, pos_over_neg: 460.91748046875 lr: 0.00031623\n",
      "Iter: 453/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 1192.572021484375 lr: 0.00031623\n",
      "Iter: 454/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 1289.6815185546875 lr: 0.00031623\n",
      "Iter: 455/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 814.4166259765625 lr: 0.00031623\n",
      "Iter: 456/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1475.874267578125 lr: 0.00031623\n",
      "Iter: 457/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 700.4202270507812 lr: 0.00031623\n",
      "Iter: 458/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 2425.399658203125 lr: 0.00031623\n",
      "Iter: 459/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 479.2325134277344 lr: 0.00031623\n",
      "Iter: 460/889, loss_train: 5.7743, loss_val: nan, pos_over_neg: 1127.2283935546875 lr: 0.00031623\n",
      "Iter: 461/889, loss_train: 5.7505, loss_val: nan, pos_over_neg: 1144.1890869140625 lr: 0.00031623\n",
      "Iter: 462/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: -4220.9345703125 lr: 0.00031623\n",
      "Iter: 463/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 9344.11328125 lr: 0.00031623\n",
      "Iter: 464/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 2067.488037109375 lr: 0.00031623\n",
      "Iter: 465/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 11598.7802734375 lr: 0.00031623\n",
      "Iter: 466/889, loss_train: 5.7796, loss_val: nan, pos_over_neg: 710.5636596679688 lr: 0.00031623\n",
      "Iter: 467/889, loss_train: 5.7684, loss_val: nan, pos_over_neg: 273.0484313964844 lr: 0.00031623\n",
      "Iter: 468/889, loss_train: 5.7585, loss_val: nan, pos_over_neg: 922.5321655273438 lr: 0.00031623\n",
      "Iter: 469/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1056.9693603515625 lr: 0.00031623\n",
      "Iter: 470/889, loss_train: 5.753, loss_val: nan, pos_over_neg: -2281.4443359375 lr: 0.00031623\n",
      "Iter: 471/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: 10685.8037109375 lr: 0.00031623\n",
      "Iter: 472/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 610.6012573242188 lr: 0.00031623\n",
      "Iter: 473/889, loss_train: 5.7707, loss_val: nan, pos_over_neg: 529.5863037109375 lr: 0.00031623\n",
      "Iter: 474/889, loss_train: 5.7487, loss_val: nan, pos_over_neg: 1846.7393798828125 lr: 0.00031623\n",
      "Iter: 475/889, loss_train: 5.7537, loss_val: nan, pos_over_neg: 2269.214111328125 lr: 0.00031623\n",
      "Iter: 476/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 440.1637878417969 lr: 0.00031623\n",
      "Iter: 477/889, loss_train: 5.7529, loss_val: nan, pos_over_neg: 2477.25830078125 lr: 0.00031623\n",
      "Iter: 478/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 608.1930541992188 lr: 0.00031623\n",
      "Iter: 479/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 708.0647583007812 lr: 0.00031623\n",
      "Iter: 480/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 420.7245178222656 lr: 0.00031623\n",
      "Iter: 481/889, loss_train: 5.757, loss_val: nan, pos_over_neg: 778.8972778320312 lr: 0.00031623\n",
      "Iter: 482/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 1109.4075927734375 lr: 0.00031623\n",
      "Iter: 483/889, loss_train: 5.7512, loss_val: nan, pos_over_neg: -9184.4111328125 lr: 0.00031623\n",
      "Iter: 484/889, loss_train: 5.7523, loss_val: nan, pos_over_neg: 1635.83984375 lr: 0.00031623\n",
      "Iter: 485/889, loss_train: 5.741, loss_val: nan, pos_over_neg: -5719.30419921875 lr: 0.00031623\n",
      "Iter: 486/889, loss_train: 5.7583, loss_val: nan, pos_over_neg: 2208.785400390625 lr: 0.00031623\n",
      "Iter: 487/889, loss_train: 5.7822, loss_val: nan, pos_over_neg: 406.53814697265625 lr: 0.00031623\n",
      "Iter: 488/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 594.4097290039062 lr: 0.00031623\n",
      "Iter: 489/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 570.2179565429688 lr: 0.00031623\n",
      "Iter: 490/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 828.3868408203125 lr: 0.00031623\n",
      "Iter: 491/889, loss_train: 5.7742, loss_val: nan, pos_over_neg: 1215.479248046875 lr: 0.00031623\n",
      "Iter: 492/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: -2813.2509765625 lr: 0.00031623\n",
      "Iter: 493/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 1947.7694091796875 lr: 0.00031623\n",
      "Iter: 494/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 2788.9619140625 lr: 0.00031623\n",
      "Iter: 495/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 2836.216064453125 lr: 0.00031623\n",
      "Iter: 496/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 1436.135986328125 lr: 0.00031623\n",
      "Iter: 497/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 2525.283203125 lr: 0.00031623\n",
      "Iter: 498/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1981.4661865234375 lr: 0.00031623\n",
      "Iter: 499/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 562.5206909179688 lr: 0.00031623\n",
      "Iter: 500/889, loss_train: 5.7533, loss_val: nan, pos_over_neg: -5705.24755859375 lr: 0.00031623\n",
      "Iter: 501/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 883.8526000976562 lr: 0.00031623\n",
      "Iter: 502/889, loss_train: 5.7525, loss_val: nan, pos_over_neg: 1053.0313720703125 lr: 0.00031623\n",
      "Iter: 503/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 803.4151000976562 lr: 0.00031623\n",
      "Iter: 504/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 630.1258544921875 lr: 0.00031623\n",
      "Iter: 505/889, loss_train: 5.7675, loss_val: nan, pos_over_neg: 1154.2186279296875 lr: 0.00031623\n",
      "Iter: 506/889, loss_train: 5.7488, loss_val: nan, pos_over_neg: 5067.87109375 lr: 0.00031623\n",
      "Iter: 507/889, loss_train: 5.7664, loss_val: nan, pos_over_neg: 3334.924560546875 lr: 0.00031623\n",
      "Iter: 508/889, loss_train: 5.757, loss_val: nan, pos_over_neg: 870.5349731445312 lr: 0.00031623\n",
      "Iter: 509/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 798.0098266601562 lr: 0.00031623\n",
      "Iter: 510/889, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1133.8768310546875 lr: 0.00031623\n",
      "Iter: 511/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 2601.226806640625 lr: 0.00031623\n",
      "Iter: 512/889, loss_train: 5.767, loss_val: nan, pos_over_neg: 896.0142211914062 lr: 0.00031623\n",
      "Iter: 513/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 697.7681884765625 lr: 0.00031623\n",
      "Iter: 514/889, loss_train: 5.7746, loss_val: nan, pos_over_neg: 9633.0078125 lr: 0.00031623\n",
      "Iter: 515/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 852.7398071289062 lr: 0.00031623\n",
      "Iter: 516/889, loss_train: 5.7552, loss_val: nan, pos_over_neg: 1575.3072509765625 lr: 0.00031623\n",
      "Iter: 517/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 1273.3631591796875 lr: 0.00031623\n",
      "Iter: 518/889, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1826.929931640625 lr: 0.00031623\n",
      "Iter: 519/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1664.2081298828125 lr: 0.00031623\n",
      "Iter: 520/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1504.246826171875 lr: 0.00031623\n",
      "Iter: 521/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 2517.8251953125 lr: 0.00031623\n",
      "Iter: 522/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1419.4124755859375 lr: 0.00031623\n",
      "Iter: 523/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 1094.103515625 lr: 0.00031623\n",
      "Iter: 524/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1014.63037109375 lr: 0.00031623\n",
      "Iter: 525/889, loss_train: 5.7792, loss_val: nan, pos_over_neg: 765.8054809570312 lr: 0.00031623\n",
      "Iter: 526/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 1436.247802734375 lr: 0.00031623\n",
      "Iter: 527/889, loss_train: 5.7489, loss_val: nan, pos_over_neg: 1638.4598388671875 lr: 0.00031623\n",
      "Iter: 528/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 3885.134033203125 lr: 0.00031623\n",
      "Iter: 529/889, loss_train: 5.7453, loss_val: nan, pos_over_neg: -5732.03125 lr: 0.00031623\n",
      "Iter: 530/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 12345.177734375 lr: 0.00031623\n",
      "Iter: 531/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 712.5023193359375 lr: 0.00031623\n",
      "Iter: 532/889, loss_train: 5.7643, loss_val: nan, pos_over_neg: 2280.613037109375 lr: 0.00031623\n",
      "Iter: 533/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 1292.600830078125 lr: 0.00031623\n",
      "Iter: 534/889, loss_train: 5.7851, loss_val: nan, pos_over_neg: 663.690673828125 lr: 0.00031623\n",
      "Iter: 535/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 731.8621826171875 lr: 0.00031623\n",
      "Iter: 536/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 9058.3623046875 lr: 0.00031623\n",
      "Iter: 537/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 976.1727294921875 lr: 0.00031623\n",
      "Iter: 538/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 6681.58154296875 lr: 0.00031623\n",
      "Iter: 539/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1919.8621826171875 lr: 0.00031623\n",
      "Iter: 540/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 3772.650146484375 lr: 0.00031623\n",
      "Iter: 541/889, loss_train: 5.7601, loss_val: nan, pos_over_neg: 15082.3212890625 lr: 0.00031623\n",
      "Iter: 542/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: -2367.430908203125 lr: 0.00031623\n",
      "Iter: 543/889, loss_train: 5.7838, loss_val: nan, pos_over_neg: 2762.617431640625 lr: 0.00031623\n",
      "Iter: 544/889, loss_train: 5.7824, loss_val: nan, pos_over_neg: 477.57440185546875 lr: 0.00031623\n",
      "Iter: 545/889, loss_train: 5.7506, loss_val: nan, pos_over_neg: -2505.014892578125 lr: 0.00031623\n",
      "Iter: 546/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 5096.30810546875 lr: 0.00031623\n",
      "Iter: 547/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1028.30908203125 lr: 0.00031623\n",
      "Iter: 548/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1073.458984375 lr: 0.00031623\n",
      "Iter: 549/889, loss_train: 5.7798, loss_val: nan, pos_over_neg: 775.8417358398438 lr: 0.00031623\n",
      "Iter: 550/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 730.2194213867188 lr: 0.00031623\n",
      "Iter: 551/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 742.962646484375 lr: 0.00031623\n",
      "Iter: 552/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 984.2025756835938 lr: 0.00031623\n",
      "Iter: 553/889, loss_train: 5.7619, loss_val: nan, pos_over_neg: 793.41259765625 lr: 0.00031623\n",
      "Iter: 554/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 7302.6767578125 lr: 0.00031623\n",
      "Iter: 555/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 22391.640625 lr: 0.00031623\n",
      "Iter: 556/889, loss_train: 5.7485, loss_val: nan, pos_over_neg: -28062.904296875 lr: 0.00031623\n",
      "Iter: 557/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 1369.1832275390625 lr: 0.00031623\n",
      "Iter: 558/889, loss_train: 5.7813, loss_val: nan, pos_over_neg: 522.1314086914062 lr: 0.00031623\n",
      "Iter: 559/889, loss_train: 5.7704, loss_val: nan, pos_over_neg: 769.2040405273438 lr: 0.00031623\n",
      "Iter: 560/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: -45060.74609375 lr: 0.00031623\n",
      "Iter: 561/889, loss_train: 5.7556, loss_val: nan, pos_over_neg: -10232.7763671875 lr: 0.00031623\n",
      "Iter: 562/889, loss_train: 5.7593, loss_val: nan, pos_over_neg: 3909.001953125 lr: 0.00031623\n",
      "Iter: 563/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: -5452.4716796875 lr: 0.00031623\n",
      "Iter: 564/889, loss_train: 5.7747, loss_val: nan, pos_over_neg: 1050.4779052734375 lr: 0.00031623\n",
      "Iter: 565/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 3312.61767578125 lr: 0.00031623\n",
      "Iter: 566/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 5561.55615234375 lr: 0.00031623\n",
      "Iter: 567/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 307.6850891113281 lr: 0.00031623\n",
      "Iter: 568/889, loss_train: 5.7599, loss_val: nan, pos_over_neg: 776.8598022460938 lr: 0.00031623\n",
      "Iter: 569/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 831.5103759765625 lr: 0.00031623\n",
      "Iter: 570/889, loss_train: 5.7569, loss_val: nan, pos_over_neg: -7517.05078125 lr: 0.00031623\n",
      "Iter: 571/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2735.5224609375 lr: 0.00031623\n",
      "Iter: 572/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 314.4857482910156 lr: 0.00031623\n",
      "Iter: 573/889, loss_train: 5.7507, loss_val: nan, pos_over_neg: -3963.292236328125 lr: 0.00031623\n",
      "Iter: 574/889, loss_train: 5.7582, loss_val: nan, pos_over_neg: -3177.103271484375 lr: 0.00031623\n",
      "Iter: 575/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 833.4690551757812 lr: 0.00031623\n",
      "Iter: 576/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5852.3193359375 lr: 0.00031623\n",
      "Iter: 577/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 3635.50830078125 lr: 0.00031623\n",
      "Iter: 578/889, loss_train: 5.771, loss_val: nan, pos_over_neg: 3043.48681640625 lr: 0.00031623\n",
      "Iter: 579/889, loss_train: 5.7489, loss_val: nan, pos_over_neg: -11246.6484375 lr: 0.00031623\n",
      "Iter: 580/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: -4354.02734375 lr: 0.00031623\n",
      "Iter: 581/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: -4597.86376953125 lr: 0.00031623\n",
      "Iter: 582/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 854.5560913085938 lr: 0.00031623\n",
      "Iter: 583/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 979.6804809570312 lr: 0.00031623\n",
      "Iter: 584/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: 1657.78564453125 lr: 0.00031623\n",
      "Iter: 585/889, loss_train: 5.7706, loss_val: nan, pos_over_neg: 1060.3135986328125 lr: 0.00031623\n",
      "Iter: 586/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 90854.203125 lr: 0.00031623\n",
      "Iter: 587/889, loss_train: 5.7547, loss_val: nan, pos_over_neg: 1359.00341796875 lr: 0.00031623\n",
      "Iter: 588/889, loss_train: 5.7555, loss_val: nan, pos_over_neg: 3936.800048828125 lr: 0.00031623\n",
      "Iter: 589/889, loss_train: 5.7961, loss_val: nan, pos_over_neg: 465.5804443359375 lr: 0.00031623\n",
      "Iter: 590/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: 978.1929321289062 lr: 0.00031623\n",
      "Iter: 591/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 3013.05712890625 lr: 0.00031623\n",
      "Iter: 592/889, loss_train: 5.7534, loss_val: nan, pos_over_neg: 1781.470458984375 lr: 0.00031623\n",
      "Iter: 593/889, loss_train: 5.7455, loss_val: nan, pos_over_neg: 11793.380859375 lr: 0.00031623\n",
      "Iter: 594/889, loss_train: 5.7737, loss_val: nan, pos_over_neg: 1049.869140625 lr: 0.00031623\n",
      "Iter: 595/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: -20451.044921875 lr: 0.00031623\n",
      "Iter: 596/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: -2536.800537109375 lr: 0.00031623\n",
      "Iter: 597/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 1061.072998046875 lr: 0.00031623\n",
      "Iter: 598/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 4313.40576171875 lr: 0.00031623\n",
      "Iter: 599/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2529.97314453125 lr: 0.00031623\n",
      "Iter: 600/889, loss_train: 5.7629, loss_val: nan, pos_over_neg: 8255.0810546875 lr: 0.00031623\n",
      "Iter: 601/889, loss_train: 5.7715, loss_val: nan, pos_over_neg: 1345.025146484375 lr: 0.00031623\n",
      "Iter: 602/889, loss_train: 5.7657, loss_val: nan, pos_over_neg: 1271.114501953125 lr: 0.00031623\n",
      "Iter: 603/889, loss_train: 5.7799, loss_val: nan, pos_over_neg: 1371.93701171875 lr: 0.00031623\n",
      "Iter: 604/889, loss_train: 5.7548, loss_val: nan, pos_over_neg: 7184.26220703125 lr: 0.00031623\n",
      "Iter: 605/889, loss_train: 5.7563, loss_val: nan, pos_over_neg: 5317.97998046875 lr: 0.00031623\n",
      "Iter: 606/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: 1166.770263671875 lr: 0.00031623\n",
      "Iter: 607/889, loss_train: 5.7691, loss_val: nan, pos_over_neg: 1230.840576171875 lr: 0.00031623\n",
      "Iter: 608/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 6883.328125 lr: 0.00031623\n",
      "Iter: 609/889, loss_train: 5.7753, loss_val: nan, pos_over_neg: 5280.93603515625 lr: 0.00031623\n",
      "Iter: 610/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 5443.865234375 lr: 0.00031623\n",
      "Iter: 611/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: -3451.31396484375 lr: 0.00031623\n",
      "Iter: 612/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 1060.154541015625 lr: 0.00031623\n",
      "Iter: 613/889, loss_train: 5.7749, loss_val: nan, pos_over_neg: 987.7616577148438 lr: 0.00031623\n",
      "Iter: 614/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 2730.587646484375 lr: 0.00031623\n",
      "Iter: 615/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 996.3488159179688 lr: 0.00031623\n",
      "Iter: 616/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 1659.2828369140625 lr: 0.00031623\n",
      "Iter: 617/889, loss_train: 5.7583, loss_val: nan, pos_over_neg: -4613.5625 lr: 0.00031623\n",
      "Iter: 618/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2045.9986572265625 lr: 0.00031623\n",
      "Iter: 619/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2556.5234375 lr: 0.00031623\n",
      "Iter: 620/889, loss_train: 5.7886, loss_val: nan, pos_over_neg: 1661.1630859375 lr: 0.00031623\n",
      "Iter: 621/889, loss_train: 5.7738, loss_val: nan, pos_over_neg: 455.7629089355469 lr: 0.00031623\n",
      "Iter: 622/889, loss_train: 5.7898, loss_val: nan, pos_over_neg: 664.279296875 lr: 0.00031623\n",
      "Iter: 623/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 1287.2403564453125 lr: 0.00031623\n",
      "Iter: 624/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 460.0603942871094 lr: 0.00031623\n",
      "Iter: 625/889, loss_train: 5.7782, loss_val: nan, pos_over_neg: 4993.2421875 lr: 0.00031623\n",
      "Iter: 626/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 25992.9375 lr: 0.00031623\n",
      "Iter: 627/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: 923.9110107421875 lr: 0.00031623\n",
      "Iter: 628/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: 740.6101684570312 lr: 0.00031623\n",
      "Iter: 629/889, loss_train: 5.7517, loss_val: nan, pos_over_neg: 2016.9400634765625 lr: 0.00031623\n",
      "Iter: 630/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 2640.211669921875 lr: 0.00031623\n",
      "Iter: 631/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: -14277.080078125 lr: 0.00031623\n",
      "Iter: 632/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 416.6468505859375 lr: 0.00031623\n",
      "Iter: 633/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 692.9556274414062 lr: 0.00031623\n",
      "Iter: 634/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1285.38916015625 lr: 0.00031623\n",
      "Iter: 635/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 594.3115844726562 lr: 0.00031623\n",
      "Iter: 636/889, loss_train: 5.7848, loss_val: nan, pos_over_neg: 478.30291748046875 lr: 0.00031623\n",
      "Iter: 637/889, loss_train: 5.7681, loss_val: nan, pos_over_neg: -10551.19140625 lr: 0.00031623\n",
      "Iter: 638/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 1078.5526123046875 lr: 0.00031623\n",
      "Iter: 639/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: 3667.18359375 lr: 0.00031623\n",
      "Iter: 640/889, loss_train: 5.7588, loss_val: nan, pos_over_neg: 1583.7564697265625 lr: 0.00031623\n",
      "Iter: 641/889, loss_train: 5.7821, loss_val: nan, pos_over_neg: 1172.1376953125 lr: 0.00031623\n",
      "Iter: 642/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: -6437.8671875 lr: 0.00031623\n",
      "Iter: 643/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1008.9281616210938 lr: 0.00031623\n",
      "Iter: 644/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 581.3844604492188 lr: 0.00031623\n",
      "Iter: 645/889, loss_train: 5.762, loss_val: nan, pos_over_neg: -2336.963134765625 lr: 0.00031623\n",
      "Iter: 646/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 478.656982421875 lr: 0.00031623\n",
      "Iter: 647/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 231.24632263183594 lr: 0.00031623\n",
      "Iter: 648/889, loss_train: 5.758, loss_val: nan, pos_over_neg: 1356.5877685546875 lr: 0.00031623\n",
      "Iter: 649/889, loss_train: 5.7896, loss_val: nan, pos_over_neg: 549.9022827148438 lr: 0.00031623\n",
      "Iter: 650/889, loss_train: 5.7682, loss_val: nan, pos_over_neg: 944.7353515625 lr: 0.00031623\n",
      "Iter: 651/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 2690.0234375 lr: 0.00031623\n",
      "Iter: 652/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 459.41973876953125 lr: 0.00031623\n",
      "Iter: 653/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 2363.687744140625 lr: 0.00031623\n",
      "Iter: 654/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 898.58349609375 lr: 0.00031623\n",
      "Iter: 655/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 69361.15625 lr: 0.00031623\n",
      "Iter: 656/889, loss_train: 5.776, loss_val: nan, pos_over_neg: -56415.26953125 lr: 0.00031623\n",
      "Iter: 657/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 2071.65283203125 lr: 0.00031623\n",
      "Iter: 658/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 1460.5650634765625 lr: 0.00031623\n",
      "Iter: 659/889, loss_train: 5.754, loss_val: nan, pos_over_neg: 1930.0306396484375 lr: 0.00031623\n",
      "Iter: 660/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 736.1604614257812 lr: 0.00031623\n",
      "Iter: 661/889, loss_train: 5.7507, loss_val: nan, pos_over_neg: 3326.274169921875 lr: 0.00031623\n",
      "Iter: 662/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 1354.3304443359375 lr: 0.00031623\n",
      "Iter: 663/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 938.0315551757812 lr: 0.00031623\n",
      "Iter: 664/889, loss_train: 5.7871, loss_val: nan, pos_over_neg: 860.7584228515625 lr: 0.00031623\n",
      "Iter: 665/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 468.26983642578125 lr: 0.00031623\n",
      "Iter: 666/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1225.06689453125 lr: 0.00031623\n",
      "Iter: 667/889, loss_train: 5.7816, loss_val: nan, pos_over_neg: -17808.408203125 lr: 0.00031623\n",
      "Iter: 668/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 1354.0660400390625 lr: 0.00031623\n",
      "Iter: 669/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: -7128.314453125 lr: 0.00031623\n",
      "Iter: 670/889, loss_train: 5.7618, loss_val: nan, pos_over_neg: 1442.3248291015625 lr: 0.00031623\n",
      "Iter: 671/889, loss_train: 5.7604, loss_val: nan, pos_over_neg: -16542.849609375 lr: 0.00031623\n",
      "Iter: 672/889, loss_train: 5.7842, loss_val: nan, pos_over_neg: 716.12109375 lr: 0.00031623\n",
      "Iter: 673/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 898.1356811523438 lr: 0.00031623\n",
      "Iter: 674/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 850.67578125 lr: 0.00031623\n",
      "Iter: 675/889, loss_train: 5.7806, loss_val: nan, pos_over_neg: 756.8486328125 lr: 0.00031623\n",
      "Iter: 676/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1569.344970703125 lr: 0.00031623\n",
      "Iter: 677/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 3692.59912109375 lr: 0.00031623\n",
      "Iter: 678/889, loss_train: 5.7639, loss_val: nan, pos_over_neg: 1554.7171630859375 lr: 0.00031623\n",
      "Iter: 679/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 639.1655883789062 lr: 0.00031623\n",
      "Iter: 680/889, loss_train: 5.7511, loss_val: nan, pos_over_neg: -2632.818603515625 lr: 0.00031623\n",
      "Iter: 681/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1387.868896484375 lr: 0.00031623\n",
      "Iter: 682/889, loss_train: 5.7694, loss_val: nan, pos_over_neg: 1088.551513671875 lr: 0.00031623\n",
      "Iter: 683/889, loss_train: 5.7772, loss_val: nan, pos_over_neg: 883.4527587890625 lr: 0.00031623\n",
      "Iter: 684/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 735.1926879882812 lr: 0.00031623\n",
      "Iter: 685/889, loss_train: 5.7666, loss_val: nan, pos_over_neg: 871.6220703125 lr: 0.00031623\n",
      "Iter: 686/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 2844.2431640625 lr: 0.00031623\n",
      "Iter: 687/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: 1527.58203125 lr: 0.00031623\n",
      "Iter: 688/889, loss_train: 5.7487, loss_val: nan, pos_over_neg: -2052.718505859375 lr: 0.00031623\n",
      "Iter: 689/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 2916.140869140625 lr: 0.00031623\n",
      "Iter: 690/889, loss_train: 5.761, loss_val: nan, pos_over_neg: -10173.41015625 lr: 0.00031623\n",
      "Iter: 691/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2159.534912109375 lr: 0.00031623\n",
      "Iter: 692/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 3485.960205078125 lr: 0.00031623\n",
      "Iter: 693/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 1731.6767578125 lr: 0.00031623\n",
      "Iter: 694/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: -7620.3271484375 lr: 0.00031623\n",
      "Iter: 695/889, loss_train: 5.7452, loss_val: nan, pos_over_neg: 1168.06494140625 lr: 0.00031623\n",
      "Iter: 696/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 519.4036254882812 lr: 0.00031623\n",
      "Iter: 697/889, loss_train: 5.7915, loss_val: nan, pos_over_neg: 433.5953674316406 lr: 0.00031623\n",
      "Iter: 698/889, loss_train: 5.7812, loss_val: nan, pos_over_neg: 448.8995361328125 lr: 0.00031623\n",
      "Iter: 699/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 487.00469970703125 lr: 0.00031623\n",
      "Iter: 700/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 732.8316650390625 lr: 0.00031623\n",
      "Iter: 701/889, loss_train: 5.7519, loss_val: nan, pos_over_neg: -4306.99755859375 lr: 0.00031623\n",
      "Iter: 702/889, loss_train: 5.754, loss_val: nan, pos_over_neg: 85871.4453125 lr: 0.00031623\n",
      "Iter: 703/889, loss_train: 5.7754, loss_val: nan, pos_over_neg: -9059.9833984375 lr: 0.00031623\n",
      "Iter: 704/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: 8838.119140625 lr: 0.00031623\n",
      "Iter: 705/889, loss_train: 5.7876, loss_val: nan, pos_over_neg: 1193.34765625 lr: 0.00031623\n",
      "Iter: 706/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 917.9619750976562 lr: 0.00031623\n",
      "Iter: 707/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 1339.1614990234375 lr: 0.00031623\n",
      "Iter: 708/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 516.4844970703125 lr: 0.00031623\n",
      "Iter: 709/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 1073.7354736328125 lr: 0.00031623\n",
      "Iter: 710/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 3081.123046875 lr: 0.00031623\n",
      "Iter: 711/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 496.0789489746094 lr: 0.00031623\n",
      "Iter: 712/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1210.626708984375 lr: 0.00031623\n",
      "Iter: 713/889, loss_train: 5.7575, loss_val: nan, pos_over_neg: 859.7430419921875 lr: 0.00031623\n",
      "Iter: 714/889, loss_train: 5.7685, loss_val: nan, pos_over_neg: 561.48046875 lr: 0.00031623\n",
      "Iter: 715/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1740.482177734375 lr: 0.00031623\n",
      "Iter: 716/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 919.9335327148438 lr: 0.00031623\n",
      "Iter: 717/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 3689.3056640625 lr: 0.00031623\n",
      "Iter: 718/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 3083.03369140625 lr: 0.00031623\n",
      "Iter: 719/889, loss_train: 5.7635, loss_val: nan, pos_over_neg: 2404.6552734375 lr: 0.00031623\n",
      "Iter: 720/889, loss_train: 5.7519, loss_val: nan, pos_over_neg: 309942.5625 lr: 0.00031623\n",
      "Iter: 721/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 826.4007568359375 lr: 0.00031623\n",
      "Iter: 722/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 407.317626953125 lr: 0.00031623\n",
      "Iter: 723/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 1191.5302734375 lr: 0.00031623\n",
      "Iter: 724/889, loss_train: 5.7721, loss_val: nan, pos_over_neg: 1029.9835205078125 lr: 0.00031623\n",
      "Iter: 725/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: 1921.0008544921875 lr: 0.00031623\n",
      "Iter: 726/889, loss_train: 5.7469, loss_val: nan, pos_over_neg: 2088.294921875 lr: 0.00031623\n",
      "Iter: 727/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 25277.611328125 lr: 0.00031623\n",
      "Iter: 728/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 2214.611328125 lr: 0.00031623\n",
      "Iter: 729/889, loss_train: 5.749, loss_val: nan, pos_over_neg: -7883.1982421875 lr: 0.00031623\n",
      "Iter: 730/889, loss_train: 5.7641, loss_val: nan, pos_over_neg: -87800.125 lr: 0.00031623\n",
      "Iter: 731/889, loss_train: 5.7652, loss_val: nan, pos_over_neg: 1271.282958984375 lr: 0.00031623\n",
      "Iter: 732/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 1261.1458740234375 lr: 0.00031623\n",
      "Iter: 733/889, loss_train: 5.7532, loss_val: nan, pos_over_neg: 1339.0169677734375 lr: 0.00031623\n",
      "Iter: 734/889, loss_train: 5.779, loss_val: nan, pos_over_neg: 222.59042358398438 lr: 0.00031623\n",
      "Iter: 735/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 1569.3367919921875 lr: 0.00031623\n",
      "Iter: 736/889, loss_train: 5.753, loss_val: nan, pos_over_neg: 5476.484375 lr: 0.00031623\n",
      "Iter: 737/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1050.2158203125 lr: 0.00031623\n",
      "Iter: 738/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 9298.478515625 lr: 0.00031623\n",
      "Iter: 739/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 23504.05859375 lr: 0.00031623\n",
      "Iter: 740/889, loss_train: 5.7615, loss_val: nan, pos_over_neg: 6729.03076171875 lr: 0.00031623\n",
      "Iter: 741/889, loss_train: 5.7513, loss_val: nan, pos_over_neg: 12932.3212890625 lr: 0.00031623\n",
      "Iter: 742/889, loss_train: 5.7683, loss_val: nan, pos_over_neg: 1403.4237060546875 lr: 0.00031623\n",
      "Iter: 743/889, loss_train: 5.7512, loss_val: nan, pos_over_neg: 1108.45947265625 lr: 0.00031623\n",
      "Iter: 744/889, loss_train: 5.7665, loss_val: nan, pos_over_neg: 1105.771484375 lr: 0.00031623\n",
      "Iter: 745/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: 1772.2835693359375 lr: 0.00031623\n",
      "Iter: 746/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1784.675537109375 lr: 0.00031623\n",
      "Iter: 747/889, loss_train: 5.769, loss_val: nan, pos_over_neg: 886.3488159179688 lr: 0.00031623\n",
      "Iter: 748/889, loss_train: 5.7797, loss_val: nan, pos_over_neg: 397.2140197753906 lr: 0.00031623\n",
      "Iter: 749/889, loss_train: 5.7613, loss_val: nan, pos_over_neg: 1051.9300537109375 lr: 0.00031623\n",
      "Iter: 750/889, loss_train: 5.7623, loss_val: nan, pos_over_neg: 2002.35595703125 lr: 0.00031623\n",
      "Iter: 751/889, loss_train: 5.7823, loss_val: nan, pos_over_neg: 918.6591186523438 lr: 0.00031623\n",
      "Iter: 752/889, loss_train: 5.749, loss_val: nan, pos_over_neg: 154663.921875 lr: 0.00031623\n",
      "Iter: 753/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -12625.671875 lr: 0.00031623\n",
      "Iter: 754/889, loss_train: 5.7497, loss_val: nan, pos_over_neg: 1612.0654296875 lr: 0.00031623\n",
      "Iter: 755/889, loss_train: 5.7693, loss_val: nan, pos_over_neg: 2185.89501953125 lr: 0.00031623\n",
      "Iter: 756/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 702.3853149414062 lr: 0.00031623\n",
      "Iter: 757/889, loss_train: 5.7787, loss_val: nan, pos_over_neg: 323.40582275390625 lr: 0.00031623\n",
      "Iter: 758/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 948.27197265625 lr: 0.00031623\n",
      "Iter: 759/889, loss_train: 5.7698, loss_val: nan, pos_over_neg: 585.9349365234375 lr: 0.00031623\n",
      "Iter: 760/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 871.1430053710938 lr: 0.00031623\n",
      "Iter: 761/889, loss_train: 5.7539, loss_val: nan, pos_over_neg: 2421.939208984375 lr: 0.00031623\n",
      "Iter: 762/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: -2753.327392578125 lr: 0.00031623\n",
      "Iter: 763/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1889.0909423828125 lr: 0.00031623\n",
      "Iter: 764/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 819.259033203125 lr: 0.00031623\n",
      "Iter: 765/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 1332.173828125 lr: 0.00031623\n",
      "Iter: 766/889, loss_train: 5.7645, loss_val: nan, pos_over_neg: 3881.610107421875 lr: 0.00031623\n",
      "Iter: 767/889, loss_train: 5.7741, loss_val: nan, pos_over_neg: 1726.2572021484375 lr: 0.00031623\n",
      "Iter: 768/889, loss_train: 5.7648, loss_val: nan, pos_over_neg: 2525.669921875 lr: 0.00031623\n",
      "Iter: 769/889, loss_train: 5.7439, loss_val: nan, pos_over_neg: 4048.6767578125 lr: 0.00031623\n",
      "Iter: 770/889, loss_train: 5.7517, loss_val: nan, pos_over_neg: -2332.567626953125 lr: 0.00031623\n",
      "Iter: 771/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 6752.06884765625 lr: 0.00031623\n",
      "Iter: 772/889, loss_train: 5.769, loss_val: nan, pos_over_neg: -7517.091796875 lr: 0.00031623\n",
      "Iter: 773/889, loss_train: 5.7831, loss_val: nan, pos_over_neg: -2111.64208984375 lr: 0.00031623\n",
      "Iter: 774/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 3556.11669921875 lr: 0.00031623\n",
      "Iter: 775/889, loss_train: 5.7549, loss_val: nan, pos_over_neg: 1723.710205078125 lr: 0.00031623\n",
      "Iter: 776/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 1625.97314453125 lr: 0.00031623\n",
      "Iter: 777/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 724.7698974609375 lr: 0.00031623\n",
      "Iter: 778/889, loss_train: 5.7536, loss_val: nan, pos_over_neg: 1586.1822509765625 lr: 0.00031623\n",
      "Iter: 779/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 475.7328186035156 lr: 0.00031623\n",
      "Iter: 780/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 1467.438720703125 lr: 0.00031623\n",
      "Iter: 781/889, loss_train: 5.7568, loss_val: nan, pos_over_neg: 989.885986328125 lr: 0.00031623\n",
      "Iter: 782/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 2381.732421875 lr: 0.00031623\n",
      "Iter: 783/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 509.72235107421875 lr: 0.00031623\n",
      "Iter: 784/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 823.5204467773438 lr: 0.00031623\n",
      "Iter: 785/889, loss_train: 5.7501, loss_val: nan, pos_over_neg: 771.80810546875 lr: 0.00031623\n",
      "Iter: 786/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 1233.6234130859375 lr: 0.00031623\n",
      "Iter: 787/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 49543.9375 lr: 0.00031623\n",
      "Iter: 788/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 1280.73681640625 lr: 0.00031623\n",
      "Iter: 789/889, loss_train: 5.7622, loss_val: nan, pos_over_neg: 3804.42919921875 lr: 0.00031623\n",
      "Iter: 790/889, loss_train: 5.7659, loss_val: nan, pos_over_neg: 984.111328125 lr: 0.00031623\n",
      "Iter: 791/889, loss_train: 5.7528, loss_val: nan, pos_over_neg: 803.195556640625 lr: 0.00031623\n",
      "Iter: 792/889, loss_train: 5.7729, loss_val: nan, pos_over_neg: 529.2785034179688 lr: 0.00031623\n",
      "Iter: 793/889, loss_train: 5.7687, loss_val: nan, pos_over_neg: 1766.4044189453125 lr: 0.00031623\n",
      "Iter: 794/889, loss_train: 5.7572, loss_val: nan, pos_over_neg: 989.7296142578125 lr: 0.00031623\n",
      "Iter: 795/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 1089.298828125 lr: 0.00031623\n",
      "Iter: 796/889, loss_train: 5.7587, loss_val: nan, pos_over_neg: 1285.5584716796875 lr: 0.00031623\n",
      "Iter: 797/889, loss_train: 5.7566, loss_val: nan, pos_over_neg: 563.0093994140625 lr: 0.00031623\n",
      "Iter: 798/889, loss_train: 5.753, loss_val: nan, pos_over_neg: -4481.73291015625 lr: 0.00031623\n",
      "Iter: 799/889, loss_train: 5.7608, loss_val: nan, pos_over_neg: -2419.892578125 lr: 0.00031623\n",
      "Iter: 800/889, loss_train: 5.7758, loss_val: nan, pos_over_neg: 16132.6435546875 lr: 0.00031623\n",
      "Iter: 801/889, loss_train: 5.7578, loss_val: nan, pos_over_neg: 13512.8154296875 lr: 0.00031623\n",
      "Iter: 802/889, loss_train: 5.7636, loss_val: nan, pos_over_neg: 855.8163452148438 lr: 0.00031623\n",
      "Iter: 803/889, loss_train: 5.7777, loss_val: nan, pos_over_neg: 465.0570983886719 lr: 0.00031623\n",
      "Iter: 804/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 527.0272827148438 lr: 0.00031623\n",
      "Iter: 805/889, loss_train: 5.7775, loss_val: nan, pos_over_neg: 574.2606811523438 lr: 0.00031623\n",
      "Iter: 806/889, loss_train: 5.7892, loss_val: nan, pos_over_neg: 424.285888671875 lr: 0.00031623\n",
      "Iter: 807/889, loss_train: 5.7456, loss_val: nan, pos_over_neg: 6715.2978515625 lr: 0.00031623\n",
      "Iter: 808/889, loss_train: 5.7513, loss_val: nan, pos_over_neg: 582.9319458007812 lr: 0.00031623\n",
      "Iter: 809/889, loss_train: 5.787, loss_val: nan, pos_over_neg: 274.0351867675781 lr: 0.00031623\n",
      "Iter: 810/889, loss_train: 5.7616, loss_val: nan, pos_over_neg: 788.0729370117188 lr: 0.00031623\n",
      "Iter: 811/889, loss_train: 5.7815, loss_val: nan, pos_over_neg: 657.3034057617188 lr: 0.00031623\n",
      "Iter: 812/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1071.1300048828125 lr: 0.00031623\n",
      "Iter: 813/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 1091.8939208984375 lr: 0.00031623\n",
      "Iter: 814/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 877.8181762695312 lr: 0.00031623\n",
      "Iter: 815/889, loss_train: 5.7625, loss_val: nan, pos_over_neg: 1739.3067626953125 lr: 0.00031623\n",
      "Iter: 816/889, loss_train: 5.7421, loss_val: nan, pos_over_neg: 2914.35205078125 lr: 0.00031623\n",
      "Iter: 817/889, loss_train: 5.7705, loss_val: nan, pos_over_neg: 907.7649536132812 lr: 0.00031623\n",
      "Iter: 818/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 1075.8199462890625 lr: 0.00031623\n",
      "Iter: 819/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: -2461.10986328125 lr: 0.00031623\n",
      "Iter: 820/889, loss_train: 5.7649, loss_val: nan, pos_over_neg: 663.7003173828125 lr: 0.00031623\n",
      "Iter: 821/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 1435.6636962890625 lr: 0.00031623\n",
      "Iter: 822/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 2301.218017578125 lr: 0.00031623\n",
      "Iter: 823/889, loss_train: 5.7714, loss_val: nan, pos_over_neg: 1488.9278564453125 lr: 0.00031623\n",
      "Iter: 824/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: -12068.7763671875 lr: 0.00031623\n",
      "Iter: 825/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: -3983.378173828125 lr: 0.00031623\n",
      "Iter: 826/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: -41726.84765625 lr: 0.00031623\n",
      "Iter: 827/889, loss_train: 5.7764, loss_val: nan, pos_over_neg: 3096.306640625 lr: 0.00031623\n",
      "Iter: 828/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 588.8171997070312 lr: 0.00031623\n",
      "Iter: 829/889, loss_train: 5.7744, loss_val: nan, pos_over_neg: 379.4333190917969 lr: 0.00031623\n",
      "Iter: 830/889, loss_train: 5.7755, loss_val: nan, pos_over_neg: 501.4087219238281 lr: 0.00031623\n",
      "Iter: 831/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: 699.2877807617188 lr: 0.00031623\n",
      "Iter: 832/889, loss_train: 5.7561, loss_val: nan, pos_over_neg: -2544.73388671875 lr: 0.00031623\n",
      "Iter: 833/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1232.55029296875 lr: 0.00031623\n",
      "Iter: 834/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: 484.3383483886719 lr: 0.00031623\n",
      "Iter: 835/889, loss_train: 5.7725, loss_val: nan, pos_over_neg: 1023.4450073242188 lr: 0.00031623\n",
      "Iter: 836/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 13917.529296875 lr: 0.00031623\n",
      "Iter: 837/889, loss_train: 5.7612, loss_val: nan, pos_over_neg: 4937.087890625 lr: 0.00031623\n",
      "Iter: 838/889, loss_train: 5.7728, loss_val: nan, pos_over_neg: -2617.150634765625 lr: 0.00031623\n",
      "Iter: 839/889, loss_train: 5.762, loss_val: nan, pos_over_neg: 1721.9486083984375 lr: 0.00031623\n",
      "Iter: 840/889, loss_train: 5.7502, loss_val: nan, pos_over_neg: 1799.939208984375 lr: 0.00031623\n",
      "Iter: 841/889, loss_train: 5.7757, loss_val: nan, pos_over_neg: 3668.437744140625 lr: 0.00031623\n",
      "Iter: 842/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1128.0828857421875 lr: 0.00031623\n",
      "Iter: 843/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 19359.802734375 lr: 0.00031623\n",
      "Iter: 844/889, loss_train: 5.7533, loss_val: nan, pos_over_neg: -5176.87158203125 lr: 0.00031623\n",
      "Iter: 845/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: -4578.41650390625 lr: 0.00031623\n",
      "Iter: 846/889, loss_train: 5.759, loss_val: nan, pos_over_neg: -3075.537841796875 lr: 0.00031623\n",
      "Iter: 847/889, loss_train: 5.7425, loss_val: nan, pos_over_neg: 2965.0693359375 lr: 0.00031623\n",
      "Iter: 848/889, loss_train: 5.7656, loss_val: nan, pos_over_neg: 1189.381591796875 lr: 0.00031623\n",
      "Iter: 849/889, loss_train: 5.7575, loss_val: nan, pos_over_neg: 3014.54443359375 lr: 0.00031623\n",
      "Iter: 850/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 645.1663818359375 lr: 0.00031623\n",
      "Iter: 851/889, loss_train: 5.754, loss_val: nan, pos_over_neg: 937.4749145507812 lr: 0.00031623\n",
      "Iter: 852/889, loss_train: 5.781, loss_val: nan, pos_over_neg: 493.0801696777344 lr: 0.00031623\n",
      "Iter: 853/889, loss_train: 5.7854, loss_val: nan, pos_over_neg: 648.8915405273438 lr: 0.00031623\n",
      "Iter: 854/889, loss_train: 5.7724, loss_val: nan, pos_over_neg: 843.7761840820312 lr: 0.00031623\n",
      "Iter: 855/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 839.9107666015625 lr: 0.00031623\n",
      "Iter: 856/889, loss_train: 5.7617, loss_val: nan, pos_over_neg: 832.4021606445312 lr: 0.00031623\n",
      "Iter: 857/889, loss_train: 5.7561, loss_val: nan, pos_over_neg: 4576.779296875 lr: 0.00031623\n",
      "Iter: 858/889, loss_train: 5.7745, loss_val: nan, pos_over_neg: 7222.13720703125 lr: 0.00031623\n",
      "Iter: 859/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 956.4591064453125 lr: 0.00031623\n",
      "Iter: 860/889, loss_train: 5.7672, loss_val: nan, pos_over_neg: 1169.7828369140625 lr: 0.00031623\n",
      "Iter: 861/889, loss_train: 5.7773, loss_val: nan, pos_over_neg: 1086.28564453125 lr: 0.00031623\n",
      "Iter: 862/889, loss_train: 5.773, loss_val: nan, pos_over_neg: 1118.015380859375 lr: 0.00031623\n",
      "Iter: 863/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 872.3773803710938 lr: 0.00031623\n",
      "Iter: 864/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 2468.058837890625 lr: 0.00031623\n",
      "Iter: 865/889, loss_train: 5.7542, loss_val: nan, pos_over_neg: 556.8370971679688 lr: 0.00031623\n",
      "Iter: 866/889, loss_train: 5.7533, loss_val: nan, pos_over_neg: 716.844970703125 lr: 0.00031623\n",
      "Iter: 867/889, loss_train: 5.783, loss_val: nan, pos_over_neg: 1302.0018310546875 lr: 0.00031623\n",
      "Iter: 868/889, loss_train: 5.7573, loss_val: nan, pos_over_neg: 2176.1875 lr: 0.00031623\n",
      "Iter: 869/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: -6212.43017578125 lr: 0.00031623\n",
      "Iter: 870/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: 1612.301513671875 lr: 0.00031623\n",
      "Iter: 871/889, loss_train: 5.756, loss_val: nan, pos_over_neg: 12321.5 lr: 0.00031623\n",
      "Iter: 872/889, loss_train: 5.7637, loss_val: nan, pos_over_neg: 1993.544677734375 lr: 0.00031623\n",
      "Iter: 873/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 915.4506225585938 lr: 0.00031623\n",
      "Iter: 874/889, loss_train: 5.7717, loss_val: nan, pos_over_neg: 822.668701171875 lr: 0.00031623\n",
      "Iter: 875/889, loss_train: 5.7508, loss_val: nan, pos_over_neg: 30296.181640625 lr: 0.00031623\n",
      "Iter: 876/889, loss_train: 5.7716, loss_val: nan, pos_over_neg: 651.4428100585938 lr: 0.00031623\n",
      "Iter: 877/889, loss_train: 5.7825, loss_val: nan, pos_over_neg: 649.0049438476562 lr: 0.00031623\n",
      "Iter: 878/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 900.9613037109375 lr: 0.00031623\n",
      "Iter: 879/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: 3657.08837890625 lr: 0.00031623\n",
      "Iter: 880/889, loss_train: 5.7702, loss_val: nan, pos_over_neg: 795.6753540039062 lr: 0.00031623\n",
      "Iter: 881/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1640.4178466796875 lr: 0.00031623\n",
      "Iter: 882/889, loss_train: 5.7837, loss_val: nan, pos_over_neg: 667.46728515625 lr: 0.00031623\n",
      "Iter: 883/889, loss_train: 5.768, loss_val: nan, pos_over_neg: 1247.8936767578125 lr: 0.00031623\n",
      "Iter: 884/889, loss_train: 5.7646, loss_val: nan, pos_over_neg: 952.5821533203125 lr: 0.00031623\n",
      "Iter: 885/889, loss_train: 5.7761, loss_val: nan, pos_over_neg: 1366.1546630859375 lr: 0.00031623\n",
      "Iter: 886/889, loss_train: 5.7789, loss_val: nan, pos_over_neg: 1361.4346923828125 lr: 0.00031623\n",
      "Iter: 887/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: -4436.24462890625 lr: 0.00031623\n",
      "Iter: 888/889, loss_train: 5.7586, loss_val: nan, pos_over_neg: -8227.1201171875 lr: 0.00031623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/300000 [5:58:42<127465:28:56, 1529.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14\n",
      "Iter: 0/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: -8780.4111328125 lr: 0.00031623\n",
      "Iter: 1/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 1167.5120849609375 lr: 0.00031623\n",
      "Iter: 2/889, loss_train: 5.7751, loss_val: nan, pos_over_neg: 684.522705078125 lr: 0.00031623\n",
      "Iter: 3/889, loss_train: 5.7774, loss_val: nan, pos_over_neg: 457.9812316894531 lr: 0.00031623\n",
      "Iter: 4/889, loss_train: 5.7624, loss_val: nan, pos_over_neg: 729.3207397460938 lr: 0.00031623\n",
      "Iter: 5/889, loss_train: 5.7703, loss_val: nan, pos_over_neg: 2223.71533203125 lr: 0.00031623\n",
      "Iter: 6/889, loss_train: 5.772, loss_val: nan, pos_over_neg: 1195.59765625 lr: 0.00031623\n",
      "Iter: 7/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 2251.201416015625 lr: 0.00031623\n",
      "Iter: 8/889, loss_train: 5.7803, loss_val: nan, pos_over_neg: 431.9714050292969 lr: 0.00031623\n",
      "Iter: 9/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 905.1828002929688 lr: 0.00031623\n",
      "Iter: 10/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 1687.1064453125 lr: 0.00031623\n",
      "Iter: 11/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 1540.6380615234375 lr: 0.00031623\n",
      "Iter: 12/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 1555.8006591796875 lr: 0.00031623\n",
      "Iter: 13/889, loss_train: 5.757, loss_val: nan, pos_over_neg: 1039.373291015625 lr: 0.00031623\n",
      "Iter: 14/889, loss_train: 5.7528, loss_val: nan, pos_over_neg: 1008.9568481445312 lr: 0.00031623\n",
      "Iter: 15/889, loss_train: 5.7808, loss_val: nan, pos_over_neg: 918.1898193359375 lr: 0.00031623\n",
      "Iter: 16/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: 2643.30810546875 lr: 0.00031623\n",
      "Iter: 17/889, loss_train: 5.7501, loss_val: nan, pos_over_neg: 6620.64453125 lr: 0.00031623\n",
      "Iter: 18/889, loss_train: 5.7676, loss_val: nan, pos_over_neg: 712.41845703125 lr: 0.00031623\n",
      "Iter: 19/889, loss_train: 5.7804, loss_val: nan, pos_over_neg: 835.238525390625 lr: 0.00031623\n",
      "Iter: 20/889, loss_train: 5.7695, loss_val: nan, pos_over_neg: 4997.19775390625 lr: 0.00031623\n",
      "Iter: 21/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 12248.44140625 lr: 0.00031623\n",
      "Iter: 22/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 673.6029052734375 lr: 0.00031623\n",
      "Iter: 23/889, loss_train: 5.7559, loss_val: nan, pos_over_neg: 648.1227416992188 lr: 0.00031623\n",
      "Iter: 24/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1481.78662109375 lr: 0.00031623\n",
      "Iter: 25/889, loss_train: 5.7756, loss_val: nan, pos_over_neg: 986.5429077148438 lr: 0.00031623\n",
      "Iter: 26/889, loss_train: 5.7818, loss_val: nan, pos_over_neg: 568.7513427734375 lr: 0.00031623\n",
      "Iter: 27/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 526.9891357421875 lr: 0.00031623\n",
      "Iter: 28/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1320.86962890625 lr: 0.00031623\n",
      "Iter: 29/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 4231.0087890625 lr: 0.00031623\n",
      "Iter: 30/889, loss_train: 5.7785, loss_val: nan, pos_over_neg: 8651.7822265625 lr: 0.00031623\n",
      "Iter: 31/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 3970.43505859375 lr: 0.00031623\n",
      "Iter: 32/889, loss_train: 5.782, loss_val: nan, pos_over_neg: 2372.384033203125 lr: 0.00031623\n",
      "Iter: 33/889, loss_train: 5.7594, loss_val: nan, pos_over_neg: 8892.8720703125 lr: 0.00031623\n",
      "Iter: 34/889, loss_train: 5.7688, loss_val: nan, pos_over_neg: -1992.3885498046875 lr: 0.00031623\n",
      "Iter: 35/889, loss_train: 5.7726, loss_val: nan, pos_over_neg: 432.39691162109375 lr: 0.00031623\n",
      "Iter: 36/889, loss_train: 5.7546, loss_val: nan, pos_over_neg: 2557.25341796875 lr: 0.00031623\n",
      "Iter: 37/889, loss_train: 5.766, loss_val: nan, pos_over_neg: 1154.04638671875 lr: 0.00031623\n",
      "Iter: 38/889, loss_train: 5.7727, loss_val: nan, pos_over_neg: 1645.4149169921875 lr: 0.00031623\n",
      "Iter: 39/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 678.3438720703125 lr: 0.00031623\n",
      "Iter: 40/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 475.1786804199219 lr: 0.00031623\n",
      "Iter: 41/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 387.2165832519531 lr: 0.00031623\n",
      "Iter: 42/889, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1477.6190185546875 lr: 0.00031623\n",
      "Iter: 43/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 528.7933349609375 lr: 0.00031623\n",
      "Iter: 44/889, loss_train: 5.757, loss_val: nan, pos_over_neg: -9785.0029296875 lr: 0.00031623\n",
      "Iter: 45/889, loss_train: 5.7778, loss_val: nan, pos_over_neg: 789.3894653320312 lr: 0.00031623\n",
      "Iter: 46/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 2021.537353515625 lr: 0.00031623\n",
      "Iter: 47/889, loss_train: 5.7811, loss_val: nan, pos_over_neg: 867.1928100585938 lr: 0.00031623\n",
      "Iter: 48/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 2026.66552734375 lr: 0.00031623\n",
      "Iter: 49/889, loss_train: 5.7554, loss_val: nan, pos_over_neg: 1323.90966796875 lr: 0.00031623\n",
      "Iter: 50/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 807.2654418945312 lr: 0.00031623\n",
      "Iter: 51/889, loss_train: 5.7545, loss_val: nan, pos_over_neg: 1452.681884765625 lr: 0.00031623\n",
      "Iter: 52/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1812.9508056640625 lr: 0.00031623\n",
      "Iter: 53/889, loss_train: 5.76, loss_val: nan, pos_over_neg: 868.1432495117188 lr: 0.00031623\n",
      "Iter: 54/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1072.08251953125 lr: 0.00031623\n",
      "Iter: 55/889, loss_train: 5.7451, loss_val: nan, pos_over_neg: -2427.0498046875 lr: 0.00031623\n",
      "Iter: 56/889, loss_train: 5.7524, loss_val: nan, pos_over_neg: 1882.4676513671875 lr: 0.00031623\n",
      "Iter: 57/889, loss_train: 5.7511, loss_val: nan, pos_over_neg: 1023.9122924804688 lr: 0.00031623\n",
      "Iter: 58/889, loss_train: 5.7562, loss_val: nan, pos_over_neg: -11730.0927734375 lr: 0.00031623\n",
      "Iter: 59/889, loss_train: 5.7667, loss_val: nan, pos_over_neg: 676.8116455078125 lr: 0.00031623\n",
      "Iter: 60/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: -8875.4306640625 lr: 0.00031623\n",
      "Iter: 61/889, loss_train: 5.7719, loss_val: nan, pos_over_neg: 31656.52734375 lr: 0.00031623\n",
      "Iter: 62/889, loss_train: 5.7515, loss_val: nan, pos_over_neg: 3922.24560546875 lr: 0.00031623\n",
      "Iter: 63/889, loss_train: 5.7524, loss_val: nan, pos_over_neg: -8057.77587890625 lr: 0.00031623\n",
      "Iter: 64/889, loss_train: 5.7697, loss_val: nan, pos_over_neg: 6075.76904296875 lr: 0.00031623\n",
      "Iter: 65/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 483.89727783203125 lr: 0.00031623\n",
      "Iter: 66/889, loss_train: 5.7718, loss_val: nan, pos_over_neg: 410.66424560546875 lr: 0.00031623\n",
      "Iter: 67/889, loss_train: 5.761, loss_val: nan, pos_over_neg: 444.1174011230469 lr: 0.00031623\n",
      "Iter: 68/889, loss_train: 5.7616, loss_val: nan, pos_over_neg: 1470.6282958984375 lr: 0.00031623\n",
      "Iter: 69/889, loss_train: 5.7592, loss_val: nan, pos_over_neg: 1063.909912109375 lr: 0.00031623\n",
      "Iter: 70/889, loss_train: 5.7487, loss_val: nan, pos_over_neg: 13662.65625 lr: 0.00031623\n",
      "Iter: 71/889, loss_train: 5.7644, loss_val: nan, pos_over_neg: 658.16845703125 lr: 0.00031623\n",
      "Iter: 72/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: 2463.534423828125 lr: 0.00031623\n",
      "Iter: 73/889, loss_train: 5.7692, loss_val: nan, pos_over_neg: 1993.255859375 lr: 0.00031623\n",
      "Iter: 74/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1276.925537109375 lr: 0.00031623\n",
      "Iter: 75/889, loss_train: 5.7954, loss_val: nan, pos_over_neg: 6392.48779296875 lr: 0.00031623\n",
      "Iter: 76/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 1386.2755126953125 lr: 0.00031623\n",
      "Iter: 77/889, loss_train: 5.7549, loss_val: nan, pos_over_neg: 7806.6904296875 lr: 0.00031623\n",
      "Iter: 78/889, loss_train: 5.7668, loss_val: nan, pos_over_neg: -3337.55126953125 lr: 0.00031623\n",
      "Iter: 79/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: -3075.16748046875 lr: 0.00031623\n",
      "Iter: 80/889, loss_train: 5.7542, loss_val: nan, pos_over_neg: 1700.8494873046875 lr: 0.00031623\n",
      "Iter: 81/889, loss_train: 5.765, loss_val: nan, pos_over_neg: 589.3689575195312 lr: 0.00031623\n",
      "Iter: 82/889, loss_train: 5.7583, loss_val: nan, pos_over_neg: 711.6436157226562 lr: 0.00031623\n",
      "Iter: 83/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: 2391.689697265625 lr: 0.00031623\n",
      "Iter: 84/889, loss_train: 5.7654, loss_val: nan, pos_over_neg: 672.3837890625 lr: 0.00031623\n",
      "Iter: 85/889, loss_train: 5.7521, loss_val: nan, pos_over_neg: 6630.103515625 lr: 0.00031623\n",
      "Iter: 86/889, loss_train: 5.7591, loss_val: nan, pos_over_neg: 18666.138671875 lr: 0.00031623\n",
      "Iter: 87/889, loss_train: 5.7713, loss_val: nan, pos_over_neg: 1722.971435546875 lr: 0.00031623\n",
      "Iter: 88/889, loss_train: 5.7565, loss_val: nan, pos_over_neg: 745.351806640625 lr: 0.00031623\n",
      "Iter: 89/889, loss_train: 5.7735, loss_val: nan, pos_over_neg: 711.9664916992188 lr: 0.00031623\n",
      "Iter: 90/889, loss_train: 5.7762, loss_val: nan, pos_over_neg: 1583.855712890625 lr: 0.00031623\n",
      "Iter: 91/889, loss_train: 5.7432, loss_val: nan, pos_over_neg: 3260.794921875 lr: 0.00031623\n",
      "Iter: 92/889, loss_train: 5.7614, loss_val: nan, pos_over_neg: 3943.010986328125 lr: 0.00031623\n",
      "Iter: 93/889, loss_train: 5.7902, loss_val: nan, pos_over_neg: 783.2340087890625 lr: 0.00031623\n",
      "Iter: 94/889, loss_train: 5.753, loss_val: nan, pos_over_neg: 2410.512939453125 lr: 0.00031623\n",
      "Iter: 95/889, loss_train: 5.7605, loss_val: nan, pos_over_neg: -4611.40966796875 lr: 0.00031623\n",
      "Iter: 96/889, loss_train: 5.7514, loss_val: nan, pos_over_neg: 3021.51904296875 lr: 0.00031623\n",
      "Iter: 97/889, loss_train: 5.7486, loss_val: nan, pos_over_neg: -21926.61328125 lr: 0.00031623\n",
      "Iter: 98/889, loss_train: 5.7628, loss_val: nan, pos_over_neg: 933.7967529296875 lr: 0.00031623\n",
      "Iter: 99/889, loss_train: 5.7571, loss_val: nan, pos_over_neg: 1127.1778564453125 lr: 0.00031623\n",
      "Iter: 100/889, loss_train: 5.7557, loss_val: nan, pos_over_neg: 984.7698974609375 lr: 0.00031623\n",
      "Iter: 101/889, loss_train: 5.77, loss_val: nan, pos_over_neg: 443.5978698730469 lr: 0.00031623\n",
      "Iter: 102/889, loss_train: 5.7543, loss_val: nan, pos_over_neg: 1661.1258544921875 lr: 0.00031623\n",
      "Iter: 103/889, loss_train: 5.7557, loss_val: nan, pos_over_neg: 2028.9183349609375 lr: 0.00031623\n",
      "Iter: 104/889, loss_train: 5.7689, loss_val: nan, pos_over_neg: 606.8182983398438 lr: 0.00031623\n",
      "Iter: 105/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 1250.1767578125 lr: 0.00031623\n",
      "Iter: 106/889, loss_train: 5.7853, loss_val: nan, pos_over_neg: 336.14215087890625 lr: 0.00031623\n",
      "Iter: 107/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 687.0829467773438 lr: 0.00031623\n",
      "Iter: 108/889, loss_train: 5.7598, loss_val: nan, pos_over_neg: 2870.23486328125 lr: 0.00031623\n",
      "Iter: 109/889, loss_train: 5.7765, loss_val: nan, pos_over_neg: 1175.3197021484375 lr: 0.00031623\n",
      "Iter: 110/889, loss_train: 5.7673, loss_val: nan, pos_over_neg: 955.9716796875 lr: 0.00031623\n",
      "Iter: 111/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 567.1452026367188 lr: 0.00031623\n",
      "Iter: 112/889, loss_train: 5.7535, loss_val: nan, pos_over_neg: 3966.12255859375 lr: 0.00031623\n",
      "Iter: 113/889, loss_train: 5.7564, loss_val: nan, pos_over_neg: 3734.4775390625 lr: 0.00031623\n",
      "Iter: 114/889, loss_train: 5.7709, loss_val: nan, pos_over_neg: 1034.1707763671875 lr: 0.00031623\n",
      "Iter: 115/889, loss_train: 5.7405, loss_val: nan, pos_over_neg: 1888.54541015625 lr: 0.00031623\n",
      "Iter: 116/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: 6520.9189453125 lr: 0.00031623\n",
      "Iter: 117/889, loss_train: 5.7708, loss_val: nan, pos_over_neg: 1090.8917236328125 lr: 0.00031623\n",
      "Iter: 118/889, loss_train: 5.7814, loss_val: nan, pos_over_neg: 993.9779663085938 lr: 0.00031623\n",
      "Iter: 119/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 815.0916748046875 lr: 0.00031623\n",
      "Iter: 120/889, loss_train: 5.7571, loss_val: nan, pos_over_neg: 2046.90234375 lr: 0.00031623\n",
      "Iter: 121/889, loss_train: 5.757, loss_val: nan, pos_over_neg: 1358.8089599609375 lr: 0.00031623\n",
      "Iter: 122/889, loss_train: 5.7795, loss_val: nan, pos_over_neg: 447.6628112792969 lr: 0.00031623\n",
      "Iter: 123/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 2023.5169677734375 lr: 0.00031623\n",
      "Iter: 124/889, loss_train: 5.7565, loss_val: nan, pos_over_neg: 1606.17041015625 lr: 0.00031623\n",
      "Iter: 125/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1749.6497802734375 lr: 0.00031623\n",
      "Iter: 126/889, loss_train: 5.7651, loss_val: nan, pos_over_neg: 6132.35498046875 lr: 0.00031623\n",
      "Iter: 127/889, loss_train: 5.7538, loss_val: nan, pos_over_neg: 1735.79296875 lr: 0.00031623\n",
      "Iter: 128/889, loss_train: 5.7611, loss_val: nan, pos_over_neg: 1026.9100341796875 lr: 0.00031623\n",
      "Iter: 129/889, loss_train: 5.7631, loss_val: nan, pos_over_neg: 1303.8828125 lr: 0.00031623\n",
      "Iter: 130/889, loss_train: 5.775, loss_val: nan, pos_over_neg: 901.628173828125 lr: 0.00031623\n",
      "Iter: 131/889, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1463.94580078125 lr: 0.00031623\n",
      "Iter: 132/889, loss_train: 5.7671, loss_val: nan, pos_over_neg: 550.8403930664062 lr: 0.00031623\n",
      "Iter: 133/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 1900.3839111328125 lr: 0.00031623\n",
      "Iter: 134/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 3058.12255859375 lr: 0.00031623\n",
      "Iter: 135/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1912.2501220703125 lr: 0.00031623\n",
      "Iter: 136/889, loss_train: 5.7662, loss_val: nan, pos_over_neg: 1246.39453125 lr: 0.00031623\n",
      "Iter: 137/889, loss_train: 5.7779, loss_val: nan, pos_over_neg: 383.7130432128906 lr: 0.00031623\n",
      "Iter: 138/889, loss_train: 5.7739, loss_val: nan, pos_over_neg: 836.7442626953125 lr: 0.00031623\n",
      "Iter: 139/889, loss_train: 5.7679, loss_val: nan, pos_over_neg: 800.6622314453125 lr: 0.00031623\n",
      "Iter: 140/889, loss_train: 5.7817, loss_val: nan, pos_over_neg: 823.7171630859375 lr: 0.00031623\n",
      "Iter: 141/889, loss_train: 5.7557, loss_val: nan, pos_over_neg: 3482.28955078125 lr: 0.00031623\n",
      "Iter: 142/889, loss_train: 5.7474, loss_val: nan, pos_over_neg: 17867.5 lr: 0.00031623\n",
      "Iter: 143/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 1814.210693359375 lr: 0.00031623\n",
      "Iter: 144/889, loss_train: 5.7607, loss_val: nan, pos_over_neg: -2614.760498046875 lr: 0.00031623\n",
      "Iter: 145/889, loss_train: 5.7563, loss_val: nan, pos_over_neg: -10102.3203125 lr: 0.00031623\n",
      "Iter: 146/889, loss_train: 5.7655, loss_val: nan, pos_over_neg: 1605.1143798828125 lr: 0.00031623\n",
      "Iter: 147/889, loss_train: 5.757, loss_val: nan, pos_over_neg: 4352.92138671875 lr: 0.00031623\n",
      "Iter: 148/889, loss_train: 5.7603, loss_val: nan, pos_over_neg: 713.572998046875 lr: 0.00031623\n",
      "Iter: 149/889, loss_train: 5.7576, loss_val: nan, pos_over_neg: 1229.6190185546875 lr: 0.00031623\n",
      "Iter: 150/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 1113.718994140625 lr: 0.00031623\n",
      "Iter: 151/889, loss_train: 5.7589, loss_val: nan, pos_over_neg: 1874.2110595703125 lr: 0.00031623\n",
      "Iter: 152/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 613.6465454101562 lr: 0.00031623\n",
      "Iter: 153/889, loss_train: 5.776, loss_val: nan, pos_over_neg: 694.5994262695312 lr: 0.00031623\n",
      "Iter: 154/889, loss_train: 5.7609, loss_val: nan, pos_over_neg: 353.5083312988281 lr: 0.00031623\n",
      "Iter: 155/889, loss_train: 5.7696, loss_val: nan, pos_over_neg: 1175.7542724609375 lr: 0.00031623\n",
      "Iter: 156/889, loss_train: 5.7731, loss_val: nan, pos_over_neg: 917.9271240234375 lr: 0.00031623\n",
      "Iter: 157/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1419.3231201171875 lr: 0.00031623\n",
      "Iter: 158/889, loss_train: 5.7501, loss_val: nan, pos_over_neg: 5206.1513671875 lr: 0.00031623\n",
      "Iter: 159/889, loss_train: 5.7558, loss_val: nan, pos_over_neg: 1710.81591796875 lr: 0.00031623\n",
      "Iter: 160/889, loss_train: 5.7638, loss_val: nan, pos_over_neg: 2716.01953125 lr: 0.00031623\n",
      "Iter: 161/889, loss_train: 5.7678, loss_val: nan, pos_over_neg: 3446.34716796875 lr: 0.00031623\n",
      "Iter: 162/889, loss_train: 5.768, loss_val: nan, pos_over_neg: -12560.833984375 lr: 0.00031623\n",
      "Iter: 163/889, loss_train: 5.7768, loss_val: nan, pos_over_neg: 758.62841796875 lr: 0.00031623\n",
      "Iter: 164/889, loss_train: 5.7621, loss_val: nan, pos_over_neg: 648.3519897460938 lr: 0.00031623\n",
      "Iter: 165/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 706.016357421875 lr: 0.00031623\n",
      "Iter: 166/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 577.0567016601562 lr: 0.00031623\n",
      "Iter: 167/889, loss_train: 5.7642, loss_val: nan, pos_over_neg: 959.5797729492188 lr: 0.00031623\n",
      "Iter: 168/889, loss_train: 5.7836, loss_val: nan, pos_over_neg: 722.0645141601562 lr: 0.00031623\n",
      "Iter: 169/889, loss_train: 5.774, loss_val: nan, pos_over_neg: 379.4381408691406 lr: 0.00031623\n",
      "Iter: 170/889, loss_train: 5.7581, loss_val: nan, pos_over_neg: 1162.76171875 lr: 0.00031623\n",
      "Iter: 171/889, loss_train: 5.7759, loss_val: nan, pos_over_neg: 1059.2322998046875 lr: 0.00031623\n",
      "Iter: 172/889, loss_train: 5.7677, loss_val: nan, pos_over_neg: 842.33935546875 lr: 0.00031623\n",
      "Iter: 173/889, loss_train: 5.7661, loss_val: nan, pos_over_neg: 999.5978393554688 lr: 0.00031623\n",
      "Iter: 174/889, loss_train: 5.7627, loss_val: nan, pos_over_neg: 2263.635009765625 lr: 0.00031623\n",
      "Iter: 175/889, loss_train: 5.7602, loss_val: nan, pos_over_neg: 430.2338562011719 lr: 0.00031623\n",
      "Iter: 176/889, loss_train: 5.7614, loss_val: nan, pos_over_neg: 26464.95703125 lr: 0.00031623\n",
      "Iter: 177/889, loss_train: 5.7794, loss_val: nan, pos_over_neg: 745.06005859375 lr: 0.00031623\n",
      "Iter: 178/889, loss_train: 5.7537, loss_val: nan, pos_over_neg: 1042.7313232421875 lr: 0.00031623\n",
      "Iter: 179/889, loss_train: 5.759, loss_val: nan, pos_over_neg: 2658.191162109375 lr: 0.00031623\n",
      "Iter: 180/889, loss_train: 5.7626, loss_val: nan, pos_over_neg: 515.6300048828125 lr: 0.00031623\n",
      "Iter: 181/889, loss_train: 5.7419, loss_val: nan, pos_over_neg: -19992.048828125 lr: 0.00031623\n",
      "Iter: 182/889, loss_train: 5.7474, loss_val: nan, pos_over_neg: 1318.89208984375 lr: 0.00031623\n",
      "Iter: 183/889, loss_train: 5.7553, loss_val: nan, pos_over_neg: 9437.1123046875 lr: 0.00031623\n",
      "Iter: 184/889, loss_train: 5.7723, loss_val: nan, pos_over_neg: 625.505859375 lr: 0.00031623\n",
      "Iter: 185/889, loss_train: 5.7663, loss_val: nan, pos_over_neg: 786.4564819335938 lr: 0.00031623\n",
      "Iter: 186/889, loss_train: 5.7579, loss_val: nan, pos_over_neg: 2132.359619140625 lr: 0.00031623\n",
      "Iter: 187/889, loss_train: 5.7597, loss_val: nan, pos_over_neg: 1695.4527587890625 lr: 0.00031623\n",
      "Iter: 188/889, loss_train: 5.7512, loss_val: nan, pos_over_neg: 7077.49609375 lr: 0.00031623\n",
      "Iter: 189/889, loss_train: 5.7895, loss_val: nan, pos_over_neg: 1323.089599609375 lr: 0.00031623\n",
      "Iter: 190/889, loss_train: 5.7606, loss_val: nan, pos_over_neg: 3436.812255859375 lr: 0.00031623\n",
      "Iter: 191/889, loss_train: 5.7934, loss_val: nan, pos_over_neg: 3946.84814453125 lr: 0.00031623\n",
      "Iter: 192/889, loss_train: 5.7711, loss_val: nan, pos_over_neg: 1195.3419189453125 lr: 0.00031623\n",
      "Iter: 193/889, loss_train: 5.7752, loss_val: nan, pos_over_neg: 2669.86865234375 lr: 0.00031623\n",
      "Iter: 194/889, loss_train: 5.7596, loss_val: nan, pos_over_neg: 1124.671630859375 lr: 0.00031623\n",
      "Iter: 195/889, loss_train: 5.7584, loss_val: nan, pos_over_neg: 595.416259765625 lr: 0.00031623\n",
      "Iter: 196/889, loss_train: 5.7588, loss_val: nan, pos_over_neg: 2298.48046875 lr: 0.00031623\n",
      "Iter: 197/889, loss_train: 5.7783, loss_val: nan, pos_over_neg: 497.2257080078125 lr: 0.00031623\n",
      "Iter: 198/889, loss_train: 5.7505, loss_val: nan, pos_over_neg: -2850.65185546875 lr: 0.00031623\n",
      "Iter: 199/889, loss_train: 5.7567, loss_val: nan, pos_over_neg: 1028.609130859375 lr: 0.00031623\n",
      "Iter: 200/889, loss_train: 5.7732, loss_val: nan, pos_over_neg: 2306.042724609375 lr: 0.00031623\n",
      "Iter: 201/889, loss_train: 5.7552, loss_val: nan, pos_over_neg: 4071.2744140625 lr: 0.00031623\n",
      "Iter: 202/889, loss_train: 5.7505, loss_val: nan, pos_over_neg: 2235.651123046875 lr: 0.00031623\n",
      "Iter: 203/889, loss_train: 5.7519, loss_val: nan, pos_over_neg: 6866.8125 lr: 0.00031623\n",
      "Iter: 204/889, loss_train: 5.7633, loss_val: nan, pos_over_neg: 471.1123962402344 lr: 0.00031623\n",
      "Iter: 205/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 2702.7421875 lr: 0.00031623\n",
      "Iter: 206/889, loss_train: 5.7674, loss_val: nan, pos_over_neg: 1341.3568115234375 lr: 0.00031623\n",
      "Iter: 207/889, loss_train: 5.7514, loss_val: nan, pos_over_neg: 2212.736328125 lr: 0.00031623\n",
      "Iter: 208/889, loss_train: 5.7595, loss_val: nan, pos_over_neg: 2204.45556640625 lr: 0.00031623\n",
      "Iter: 209/889, loss_train: 5.7634, loss_val: nan, pos_over_neg: 4566.548828125 lr: 0.00031623\n",
      "Iter: 210/889, loss_train: 5.7574, loss_val: nan, pos_over_neg: 3359.252685546875 lr: 0.00031623\n",
      "Iter: 211/889, loss_train: 5.7686, loss_val: nan, pos_over_neg: 750.6594848632812 lr: 0.00031623\n",
      "Iter: 212/889, loss_train: 5.7647, loss_val: nan, pos_over_neg: 1204.5045166015625 lr: 0.00031623\n",
      "Iter: 213/889, loss_train: 5.7472, loss_val: nan, pos_over_neg: 543.5588989257812 lr: 0.00031623\n",
      "Iter: 214/889, loss_train: 5.7699, loss_val: nan, pos_over_neg: 7505.08251953125 lr: 0.00031623\n",
      "Iter: 215/889, loss_train: 5.7734, loss_val: nan, pos_over_neg: 1597.905029296875 lr: 0.00031623\n",
      "Iter: 216/889, loss_train: 5.7653, loss_val: nan, pos_over_neg: -2941.937255859375 lr: 0.00031623\n"
     ]
    }
   ],
   "source": [
    "l2_alpha = 0.000\n",
    "n_epochs=300000\n",
    "\n",
    "acc_train_lst, acc_val_lst = [], []\n",
    "acc_train_SYT_lst, acc_val_SYT_lst = [], []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr, run_transform, expand_dim=False)\n",
    "    features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval, run_transform, expand_dim=False)\n",
    "    features_val = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    \n",
    "    ds_run, dl_run = get_ds_dl(input_xtr_SYT, run_transform, expand_dim=False)\n",
    "    features_train_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "    ds_run, dl_run = get_ds_dl(input_xval_SYT, run_transform, expand_dim=False)\n",
    "    features_val_SYT = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "\n",
    "\n",
    "    acc_train, acc_val = {}, {}\n",
    "    acc_train_SYT, acc_val_SYT = {}, {}\n",
    "    C_toUse = np.array([1e1,1e0,1e-1,1e-2])\n",
    "    for C in C_toUse:\n",
    "        logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg.fit(features_train, y_labeled_train)\n",
    "        acc_train_tmp = logreg.score(features_train, y_labeled_train)\n",
    "        acc_train[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg.score(features_val, y_labeled_val)\n",
    "        acc_val[C] = acc_val_tmp\n",
    "        \n",
    "        logreg_SYT = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "        logreg_SYT.fit(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_tmp = logreg_SYT.score(features_train_SYT, y_labeled_train_SYT)\n",
    "        acc_train_SYT[C] = acc_train_tmp\n",
    "        acc_val_tmp = logreg_SYT.score(features_val_SYT, y_labeled_val_SYT)\n",
    "        acc_val_SYT[C] = acc_val_tmp\n",
    "\n",
    "    acc_train_lst.append(acc_train)\n",
    "    acc_val_lst.append(acc_val)\n",
    "    acc_train_SYT_lst.append(acc_train_SYT)\n",
    "    acc_val_SYT_lst.append(acc_val_SYT)\n",
    "\n",
    "    acc_train_df, acc_val_df, acc_train_SYT_df, acc_val_SYT_df = pd.DataFrame(acc_train_lst), pd.DataFrame(acc_val_lst), pd.DataFrame(acc_train_SYT_lst), pd.DataFrame(acc_val_SYT_lst)\n",
    "    \n",
    "    acc_train_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train.csv')\n",
    "    acc_train_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_train_SYT.csv')\n",
    "    acc_val_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val.csv')\n",
    "    acc_val_SYT_df.to_csv(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/accuracies/{model_file_name}_val_SYT.csv')\n",
    "    \n",
    "    \n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion,\n",
    "                                    scheduler=scheduler,\n",
    "                                    temperature=0.5,\n",
    "                                    # l2_alpha,\n",
    "                                    mode='semi-supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    device=DEVICE, \n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')\n",
    "   \n",
    "    \n",
    "    losses_train_npy = np.array(losses_train)\n",
    "    losses_val_npy = np.array(losses_val)\n",
    "    val_accs_npy = np.array(val_accs)\n",
    "    acc_npy = np.array(acc)\n",
    "\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "    np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "af10GlccgaV4",
    "outputId": "2ec75ade-6308-4a67-89e4-4bf3f996f746"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.set(style='white', palette='bright', context='poster')\n",
    "plt.rcdefaults()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(losses_train, label='Training Loss')\n",
    "plt.plot(losses_val, label='Validation Loss')\n",
    "plt.title(f'Loss — Balanced Transfer Learning, No Data Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Step')\n",
    "plt.ylabel('Loss')\n",
    "# plt.savefig('./Training-Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "id": "Cl4TSsfc2MDy",
    "outputId": "ccc80bf3-a191-49ec-e635-dce022144cbe"
   },
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,12))\n",
    "# val_transfer_cm = get_cm(features_val, y_val)\n",
    "# plt.imshow(val_transfer_cm)\n",
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "plt.imshow(test_transfer_cm)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(test_transfer_cm.shape[0]):\n",
    "    for j in range(test_transfer_cm.shape[1]):\n",
    "        plt.annotate(np.round(test_transfer_cm[i,j], 3), (j,i), ha='center')\n",
    "plt.title(f'Test Confusion Matrix — Balanced Transfer Learning, No Augmentation, L2 Lambda = {l2_alpha}')\n",
    "plt.xlabel('True Class')\n",
    "plt.ylabel('Predicted Class')\n",
    "# plt.savefig('./Confusion-Matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "# model_file_name = 'ResNet18_simCLR_model_202112078_temp=1.0'\n",
    "model_file_name = 'ResNet18_simCLR_model_202112078_EOD_transfmod=efficient'\n",
    "\n",
    "# torch.save(model.state_dict(), '/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_3.pth')\n",
    "torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/Home_Linux_partition/github_repos/GCaMP_ROI_classifier/new_stuff/models/ResNet18_simCLR_model_20211205_2.pth'))\n",
    "model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/{model_file_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_train_npy = np.array(losses_train)\n",
    "losses_val_npy = np.array(losses_val)\n",
    "val_accs_npy = np.array(val_accs)\n",
    "acc_npy = np.array(acc)\n",
    "\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_train.npy', losses_train_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_losses_val.npy', losses_val_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_val_accs.npy', val_accs_npy)\n",
    "np.save(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/model_performance/{model_file_name}_tr_accs.npy', acc_npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEqA0gLPl3-6"
   },
   "source": [
    "## Train classifier using classifier layers of model (or do supervised learning)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "fmMkNykeVHbn"
   },
   "source": [
    "test_transfer_cm = get_cm(features_test, y_test)\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.npy',\n",
    "        test_transfer_cm)\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-Transfer—L2Lambda={l2_alpha}.pth')\n",
    "\n",
    "np.save(f'/content/drive/MyDrive/00 - ROI/GCaMP_ROI_classifier/new_stuff/npy-figures/TestingCM-{\"Un\" if not balanced else \"\"}Balanced-SKLearn-Solver={solver}—C={C_reg}.npy',\n",
    "        logistic_pred_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "zo42G3CeWozY"
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_cm(pred_cm, y_cm, plot=False):\n",
    "  ### NOTE — RETURNS A MATRIX WITH PREDICTION NUM ASSOCIATED WITH ROW NUM\n",
    "  ### AND COLUMN NUM ASSOCIATED WITH TRUE VALUE. (TRANSPOSE OF SKLEARN OUTPUT.)\n",
    "\n",
    "  cm = confusion_matrix(y_cm, np.argmax(pred_cm, -1))\n",
    "  cm = cm / np.where(cm.sum(1, keepdims=True)==0, np.ones_like(cm.sum(1, keepdims=True)), cm.sum(1, keepdims=True))\n",
    "  \n",
    "  # cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "  # print(cm)\n",
    "  \n",
    "  if plot:\n",
    "    plt.figure()\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "  \n",
    "  return cm.T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWk_NgpNd2Ia",
    "outputId": "2959f230-bd91-46cd-e898-d270aade7e54"
   },
   "source": [
    "num_tr_ex = X_val.shape[0]\n",
    "\n",
    "\n",
    "# solver = 'lbfgs'\n",
    "solver = 'liblinear'\n",
    "# solver = 'newton-cg'\n",
    "C_reg = 0.01\n",
    "# C_reg = 0.0001\n",
    "\n",
    "\n",
    "# logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', )\n",
    "# logreg = LogisticRegression(solver=solver, penalty='none', max_iter=4000)\n",
    "# logreg = LogisticRegression(solver=solver)\n",
    "logreg = LogisticRegression(solver=solver, C=C_reg)\n",
    "# logreg = LogisticRegression(solver='lbfgs', penalty='none', max_iter=4000)\n",
    "\n",
    "# base_features_train = base_model_frozen(x_feed_through_tr).detach().cpu()\n",
    "base_features_train = cpu_tr.cpu().detach().numpy()\n",
    "logreg.fit(base_features_train, y_train)\n",
    "\n",
    "# base_features_val = base_model_frozen(x_feed_through_val).detach().cpu()\n",
    "base_features_val = cpu_val.cpu().detach().numpy()\n",
    "\n",
    "base_features_te = cpu_te.cpu().detach().numpy()\n",
    "\n",
    "# base_model_frozen.to('cpu')\n",
    "# X_labeled_train.to('cpu')\n",
    "\n",
    "logistic_pred_train = get_cm(logreg.predict_proba(base_features_train), y_train)\n",
    "logistic_pred_val = get_cm(logreg.predict_proba(base_features_val), y_val)\n",
    "logistic_pred_test = get_cm(logreg.predict_proba(base_features_te), y_test)\n",
    "\n",
    "\n",
    "x_feed_through_tr.to(DEVICE)\n",
    "x_feed_through_val.to(DEVICE)\n",
    "x_feed_through_te.to(DEVICE)\n",
    "\n",
    "print(x_feed_through_tr.shape, x_feed_through_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLH9o3jLl4G_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNJk6Qwl4O3"
   },
   "source": [
    "Freeze pre-head layers, unfreeze classification layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zq4toNxdl4jb"
   },
   "source": [
    "Define labeled dataset to use"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MGvBSux9l4pn"
   },
   "source": [
    "X_labeled_train, X_labeled_val, y_labeled_train, y_labeled_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS_mTd7cl4vI"
   },
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "# criterion = CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-2)\n",
    "optimizer = Adam(model.parameters(), lr=10**(-4.5))\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=1-0.0000,\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "criterion = [_.to(DEVICE) for _ in criterion]\n",
    "losses_train, losses_val, val_accs, acc = [], [np.nan], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_null(var):\n",
    "    return(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reinit_classifier()\n",
    "model.train()\n",
    "model.prep_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(masks_cat, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_validation = torch.utils.data.DataLoader( dataset_validation,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True,\n",
    "                                                drop_last=True,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=32,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4WvU5xxl41A"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "# model.to(DEVICE)\n",
    "\n",
    "l2_alpha = 0.000\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "    loss_rolling_train = training_simCLR.epoch_step(dataloader_validation, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    L2_alpha=0.04,\n",
    "                                    mode='supervised',\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=1,\n",
    "                                   \n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAcpUsTJl46l"
   },
   "source": [
    "Evalculate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss_rolling_train)\n",
    "\n",
    "data_in = torch.as_tensor(X_labeled_val, dtype=torch.float32, device=DEVICE)\n",
    "# data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_val)\n",
    "# cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHaYL5XjaBfP"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "data_in = torch.as_tensor(X_labeled_train, dtype=torch.float32, device=DEVICE)\n",
    "data_in = util.tile_channels(data_in[:,None,...], dim=1)\n",
    "proba = torch.nn.functional.softmax(model.forward_classifier(data_in), dim=1)\n",
    "cm = classification.confusion_matrix(proba.detach().cpu().numpy(), y_labeled_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNlRDjrVaCD-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misc.estimate_size_of_float_array(input_shape=(80000,3,224,224), bitsize=32)/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape, features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train_SYT)\n",
    "    # logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "    # acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "    # acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_train, acc_val = [], []\n",
    "# # C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# # C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "# C_toUse = np.array([10000])\n",
    "# for C in tqdm(C_toUse):\n",
    "# #     print(f'C = {C}')\n",
    "#     logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "# #     tic = time.time()\n",
    "#     logreg.fit(features_train, y_labeled_train_SYT)\n",
    "# #     print(f'time: {time.time() - tic}')\n",
    "#     acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "#     acc_train.append(acc)\n",
    "# #     print(f'acc_train: {acc}')\n",
    "#     acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "#     acc_val.append(acc)\n",
    "# #     print(f'acc_val: {acc}')\n",
    "# #     print('')\n",
    "    \n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(C_toUse, acc_train)\n",
    "# plt.plot(C_toUse, acc_val)\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel('C')\n",
    "# plt.ylabel('acc')\n",
    "# plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(1)).fit(features_train, y_labeled_train_SYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "# proba = logreg.predict_proba(features_train)\n",
    "\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('train');\n",
    "\n",
    "# proba = logreg.predict_proba(features_val)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "# plt.figure()\n",
    "# sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "# plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CchY4kGDB00"
   },
   "source": [
    "## Check embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();\n",
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcrdLrYtDB00"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_cat[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_cat[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_labe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "                                    torch.as_tensor(images_dup[:], device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(torch.zeros(images_dup[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "#                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=False,\n",
    "                                                drop_last=False,\n",
    "                                                pin_memory=True,\n",
    "                                                num_workers=16,\n",
    "                                                persistent_workers=True,\n",
    "                                                # prefetch_factor=0\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_unlabeled_noAug = util.dataset_simCLR(\n",
    "#                                     torch.as_tensor(masks_SYT[:], device='cpu', dtype=torch.float32), \n",
    "#                                     torch.as_tensor(torch.zeros(masks_SYT[:].shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "#                                     n_transforms=1,\n",
    "#                                     class_weights=np.array([1]),\n",
    "#                                     # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "#                                     transform=scripted_transforms_validation,\n",
    "#                                     # DEVICE='cpu',\n",
    "#                                     DEVICE='cpu',\n",
    "#                                     dtype_X=torch.float32,\n",
    "#                                     dtype_y=torch.int64,\n",
    "                                    \n",
    "#                                     temp_uncertainty=16\n",
    "#                                     )\n",
    "# dataloader_unlabeled_noAug = torch.utils.data.DataLoader( dataset_unlabeled_noAug,\n",
    "# #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "#                                                 batch_size=1024,\n",
    "#                                                 shuffle=False,\n",
    "#                                                 drop_last=False,\n",
    "#                                                 pin_memory=True,\n",
    "#                                                 num_workers=32,\n",
    "#                                                 persistent_workers=True,\n",
    "#                                                 # prefetch_factor=0\n",
    "#                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: run unlabeled data through model\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_unlabeled_noAug], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPyKFRdq28d3"
   },
   "outputs": [],
   "source": [
    "### REMOVE\n",
    "\n",
    "DEVICE='cuda'\n",
    "# DEVICE='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fpQXf0o28d3"
   },
   "outputs": [],
   "source": [
    "# model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gwucuZXDB00"
   },
   "outputs": [],
   "source": [
    "_, features_embedded, _, evr = decomposition.torch_pca(features_train, device=DEVICE, return_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = cuml.TSNE( n_components=2,\n",
    "                  perplexity=50.0,\n",
    "                  early_exaggeration=12.0,\n",
    "#                   late_exaggeration=1.0,\n",
    "                  learning_rate=200.0,\n",
    "                  n_iter=1000,\n",
    "                  n_iter_without_progress=300,\n",
    "                  min_grad_norm=1e-07,\n",
    "                  metric='euclidean',\n",
    "                  init='random',\n",
    "                  verbose=False,\n",
    "#                   random_state=None,\n",
    "#                   method='barnes_hut',\n",
    "#                   angle=0.5,\n",
    "#                   learning_rate_method='adaptive',\n",
    "# #                   n_neighbors=90,\n",
    "#                   perplexity_max_iter=100,\n",
    "#                   exaggeration_iter=250,\n",
    "#                   pre_momentum=0.5,\n",
    "#                   post_momentum=0.8,\n",
    "# #                   square_distances=True,\n",
    "#                   handle=None,\n",
    "#                   output_type=None\n",
    "                )\n",
    "features_embedded = tsne.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = cuml.UMAP(n_neighbors=100,\n",
    "                n_components=2,\n",
    "                n_epochs=None,\n",
    "                learning_rate=1.0,\n",
    "                min_dist=0.1,\n",
    "                spread=1.0,\n",
    "                set_op_mix_ratio=1.0, \n",
    "                local_connectivity=1.0,\n",
    "                repulsion_strength=1.0, \n",
    "                negative_sample_rate=5, \n",
    "                transform_queue_size=4.0, \n",
    "                init='spectral', \n",
    "                verbose=False,\n",
    "                a=None, \n",
    "                b=None, \n",
    "                target_n_neighbors=- 1, \n",
    "#                 target_weight=0.5, \n",
    "                target_metric='categorical', \n",
    "                handle=None,                \n",
    "                hash_input=False, \n",
    "                random_state=None, \n",
    "                callback=None, \n",
    "                output_type=None\n",
    "                )\n",
    "features_embedded = umap.fit_transform(features_train.to(DEVICE)).get()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.delete_all_cuda_tensors(globals())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch_helpers.tensor_sizeOnDisk(features_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2imvF8ZoDB00"
   },
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "tsne = manifold.TSNE(n_components=2, \n",
    "                     perplexity=120.0, \n",
    "                     early_exaggeration=12.0, \n",
    "                     learning_rate=200, \n",
    "                     n_iter=1000, \n",
    "                     n_iter_without_progress=300, \n",
    "                     min_grad_norm=1e-07, \n",
    "                     metric='euclidean', \n",
    "                     init='pca', \n",
    "                     verbose=0, \n",
    "                     random_state=None, \n",
    "                     method='barnes_hut', \n",
    "                     angle=0.5, \n",
    "                     n_jobs=-1, \n",
    "#                      square_distances='legacy'\n",
    "                    )\n",
    "features_embedded = tsne.fit_transform(features_train.cpu())\n",
    "# features_embedded = tsne.fit_transform(features_embedded[:,:5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_embedded.shape, labels_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_dup, cmap=plt.get_cmap('tab10'))\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(evr[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "\n",
    "# # mpl.rcParams['image.cmap'] = 'Set1'\n",
    "# %matplotlib notebook\n",
    "# plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=10, c=labels_SYT, cmap=plt.get_cmap('tab10'))\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.001)\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# # plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# # plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# # plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgxJ8VXwDB00"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# mpl.rcParams['image.cmap'] = 'Set1'\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], s=30, c=y_labeled_train, cmap=plt.get_cmap('tab10'))\n",
    "plt.scatter(features_embedded[:,0], features_embedded[:,1], s=0.2)\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=labels[labels!=3])\n",
    "# plt.scatter(features_embedded[:,0], features_embedded[:,1], c=y_val)\n",
    "# plt.scatter(features_embedded[:,4], features_embedded[:,5], c=y_train)\n",
    "# plt.scatter(features_embedded[:,11], features[:,43].cpu(), c=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwFf2BsVDB00"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(features_train.cpu().detach(), aspect='auto', interpolation='antialiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SARf-u1K28d4"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(evr)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHXPapkDB00"
   },
   "source": [
    "## Check filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aBVd9FTDB00"
   },
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dK_-Xu9EDB01"
   },
   "outputs": [],
   "source": [
    "layer_1 = model.state_dict()['base_model.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.4.0.conv1.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.7.0.conv1.weight'].cpu()\n",
    "layer_4 = model.state_dict()['base_model.7.1.conv2.weight'].cpu()\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(layer_1.shape[1]):\n",
    "    for jj in range(layer_1.shape[0]):\n",
    "        plt.subplot2grid((layer_1.shape[1],layer_1.shape[0]),(ii,jj))\n",
    "        fig = plt.imshow(layer_1[jj,ii,:,:] , clim=(-0.2,0.2))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_2[jj,ii,:,:], clim=(-.05,.05))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_3[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "plt.figure()\n",
    "for ii in range(16):\n",
    "    for jj in range(16):\n",
    "        plt.subplot2grid((16,16),(ii,jj))\n",
    "        fig = plt.imshow(layer_4[jj, ii,:,:], clim=(-.1,.1))\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGiz2fHFDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwJQBUhpDB01"
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/media/rich/bigSSD/Net_trainedOnAug_20211025_trainingSet_mouse628_20200903and20200815_simCLR.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1grXld0IDB01"
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.load_state_dict(torch.load('test_save.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quqNFL1jDB01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f"
   },
   "outputs": [],
   "source": [
    "data_unlabeled = torch.as_tensor(masks_cat, dtype=torch.float32, device='cpu')\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# penalized_params = list(model.modules())[-1].parameters()\n",
    "# penalized_params = torch.cat([_.view(-1) for _ in penalized_params], -1)\n",
    "\n",
    "early_stopping = 50\n",
    "prv_best_val = np.inf\n",
    "early_stopping_cnt = 0\n",
    "\n",
    "l2_alpha = 0.1\n",
    "\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "#     loss_rolling_train, loss_rolling_val = training_supervised.epoch_step(dataloader_train, \n",
    "#                                     model, \n",
    "#                                     optimizer, \n",
    "#                                     criterion, \n",
    "\n",
    "#                                     penalized_params, l2_alpha,\n",
    "\n",
    "#                                     scheduler=scheduler,\n",
    "#                                     loss_rolling_train=losses_train, \n",
    "#                                     device=DEVICE, \n",
    "#                                     loss_rolling_val=losses_val,\n",
    "#                                     verbose=2,\n",
    "#                                     verbose_update_period=100,\n",
    "                                   \n",
    "#                                     do_validation=True,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "#                                    )\n",
    "    \n",
    "    loss_rolling_train, loss_rolling_val = training_simCLR.epoch_step(dataloader_train, \n",
    "                                    model, \n",
    "                                    optimizer, \n",
    "                                    criterion, \n",
    "\n",
    "                                    # penalized_params, l2_alpha,\n",
    "\n",
    "                                    scheduler=scheduler,\n",
    "                                    loss_rolling_train=losses_train, \n",
    "                                    device=DEVICE, \n",
    "                                    loss_rolling_val=losses_val,\n",
    "                                    verbose=2,\n",
    "                                    verbose_update_period=100,\n",
    "                                   \n",
    "                                    do_validation=True,\n",
    "                                    X_val=x_feed_through_val,\n",
    "                                    y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    "                                   )\n",
    "    \n",
    "    \n",
    "    if early_stopping:\n",
    "      if len(loss_rolling_val) > 0:\n",
    "        if loss_rolling_val[-1] < prv_best_val:\n",
    "          early_stopping_cnt = 0\n",
    "          prv_best_val = loss_rolling_val[-1]\n",
    "          torch.save(model.state_dict(), f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth')\n",
    "        else:\n",
    "          early_stopping_cnt += 1\n",
    "    \n",
    "      if early_stopping_cnt >= early_stopping:\n",
    "        model.load_state_dict(torch.load(f'{base_dir}/github_repos/GCaMP_ROI_classifier/new_stuff/models/checkpoints/checkpoint.pth'))\n",
    "        break\n",
    "    \n",
    "    # torch_helpers.show_all_tensors(globals())\n",
    "    \n",
    "    features_train = model(x_feed_through_tr)\n",
    "    features_train = features_train.cpu().detach().numpy()\n",
    "    features_val = model(x_feed_through_val)\n",
    "    features_val = features_val.cpu().detach().numpy()\n",
    "    # y_hat = scipy.special.softmax(features_val, axis=-1) # logreg.predict_proba(features_val)\n",
    "    \n",
    "    print('Training Confusion Matrix')\n",
    "    print(get_cm(features_train, y_train))\n",
    "    print()\n",
    "    print(logistic_pred_train)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('Val Confusion Matrix')\n",
    "    print(get_cm(features_val, y_val))\n",
    "    print()\n",
    "    print(logistic_pred_val)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "E5EeUhzUDB0v"
   },
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear', max_iter=30)\n",
    "# logreg_predict_head = LogisticRegression(solver='liblinear')\n",
    "dataset_train.classification_model = None\n",
    "\n",
    "\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "#                                                    gamma=1-0.001,\n",
    "# #                                                    gamma=1,\n",
    "#                                                   )\n",
    "n_epochs=300000\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model.prep_contrast()\n",
    "    training_simCLR.epoch_step( dataloader_train, \n",
    "                                model, \n",
    "                                optimizer, \n",
    "                                criterion,\n",
    "                                scheduler=scheduler, \n",
    "                                temperature=0.5,\n",
    "                                loss_rolling_train=losses_train, \n",
    "                                device=DEVICE, \n",
    "                                do_validation=False,\n",
    "#                                 validation_Object=val_obj,\n",
    "                                loss_rolling_val=losses_val,\n",
    "                                verbose=2,\n",
    "                                verbose_update_period=100,\n",
    "                               )\n",
    "    \n",
    "\n",
    "    model.prep_classifier()\n",
    "\n",
    "    # print(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1).shape)\n",
    "\n",
    "    features_train = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "    # features_train = model(util.tile_channels(torch.as_tensor(X_labeled_train[:,None,...], device=DEVICE, dtype=torch.float32), dim=1)).detach().cpu()\n",
    "    # features_train = model(torch.as_tensor(X_labeled_train, device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    # features = model(torch.tensor(X_train[y_train != 3], device=DEVICE, dtype=torch.float32)[:,None,...]).detach().cpu()\n",
    "    \n",
    "    tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train)\n",
    "    print(time.time() - tic)\n",
    "    acc.append(logreg.score(features_train, y_labeled_train))\n",
    "    print(f'acc: {acc[-1]}')\n",
    "    \n",
    "    dataset_train.net_model = copy.deepcopy(model).to('cpu')\n",
    "    dataset_train.classification_model = logreg\n",
    "    \n",
    "\n",
    "#     sample_id_num = np.arange(X_labeled_val.shape[0])\n",
    "#     epoch_val = epoch\n",
    "#     batch_val = -1\n",
    "#     p_tmp = logreg.predict_proba(model(torch.as_tensor(util.tile_channels(X_labeled_val), device=DEVICE, dtype=torch.float32)).detach().cpu())\n",
    "#     logits = p_tmp\n",
    "#     # logits = np.log(1/(1/p_tmp - 1))\n",
    "\n",
    "#     col_vals = [sample_id_num, epoch_val, batch_val, y_labeled_val]\n",
    "#     setup = np.empty((len(sample_id_num), len(col_vals)))\n",
    "#     for icv, col_val in enumerate(col_vals):\n",
    "#       setup[:, icv] = col_val\n",
    "#     tmp_tracking_np = np.concatenate([setup, logits], axis=1)\n",
    "\n",
    "#     tmp_tracking_df = pd.DataFrame(tmp_tracking_np, index=sample_id_num, columns=tracking_df_cols + [f'logits_{i}' for i in range(logits.shape[1])])\n",
    "#     tracking_df = tracking_df.append(tmp_tracking_df, ignore_index=True)\n",
    "#     display(tracking_df)\n",
    "\n",
    "\n",
    "    \n",
    "    features_val = model.get_head(model.base_model(util.tile_channels(torch.as_tensor(X_labeled_val[:,None,...], device=DEVICE, dtype=torch.float32), dim=1))).detach().cpu()\n",
    "\n",
    "\n",
    "    # logreg_predict_head.fit(features_train, y_labeled_train)\n",
    "    # y_hat = logreg_predict_head.predict_proba(features_val)\n",
    "\n",
    "    y_hat = logreg.predict_proba(features_val)\n",
    "    \n",
    "    cm = classification.confusion_matrix(y_hat, y_labeled_val)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(cm)\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "    # tracking_df = tracking_df.append(pd.DataFrame([np.array([100, 0, 0, 0])], index=tracking_df_cols), ignore_index=True)\n",
    "    \n",
    "    # model predict\n",
    "    # Update model in DS\n",
    "    # get item calls model for each sample\n",
    "    # output\n",
    "    # X sample weights predictions\n",
    "    \n",
    "#     classHead.fit(X_train[:, None, :, :], y_train, solver='liblinear')\n",
    "    \n",
    "#     proba = classHead.predict_proba(X_train[:, None, :, :])\n",
    "#     class_weights = proba.sum(axis=0)\n",
    "#     total_num = class_weights.sum()\n",
    "    \n",
    "#     eps = 1e-4\n",
    "    \n",
    "#     class_weights[class_weights <= 3] = total_num\n",
    "#     weightings = class_weights.sum()/class_weights\n",
    "#     final_weights = weightings / weightings.sum()\n",
    "#     final_weights = np.array([1/proba.shape[1] for _ in range(proba.shape[1])])\n",
    "    \n",
    "#     print(class_weights)\n",
    "\n",
    "#     dataset_train.set_classweights(final_weights)\n",
    "    \n",
    "#     print('dataset_train.final_weights', dataset_train.class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Precomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "no_transforms = torch.nn.Sequential()\n",
    "scale_rsz_transforms = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR))\n",
    "tile_transforms = torch.nn.Sequential(augmentation.TileChannels(dim=0, n_channels=3),)\n",
    "\n",
    "\n",
    "scripted_all_transforms = torch.jit.script(all_transforms)\n",
    "scripted_no_transforms = torch.jit.script(no_transforms)\n",
    "scripted_scale_rsz_transform = torch.jit.script(scale_rsz_transforms)\n",
    "scripted_tile_transform = torch.jit.script(tile_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_no_transforms\n",
    "run_transform = scripted_all_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'All Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_transform = scripted_all_transforms\n",
    "run_transform = scripted_no_transforms\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                                expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'No Transformations Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "setup_transform = scripted_scale_rsz_transform\n",
    "run_transform = scripted_tile_transform\n",
    "\n",
    "setup_ds = util.dataset_simCLR(\n",
    "                                torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                \n",
    "                                n_transforms=1,\n",
    "                                transform=setup_transform,\n",
    "                                class_weights=np.array([1]),\n",
    "                                DEVICE='cpu',\n",
    "                                dtype_X=torch.float32,\n",
    "                                dtype_y=torch.int64,\n",
    "\n",
    "                                temp_uncertainty=16\n",
    "                                )\n",
    "setup_dl = torch.utils.data.DataLoader( setup_ds,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "\n",
    "input_x = torch.cat([data[0][0].detach().cpu() for data in setup_dl], dim=0)\n",
    "\n",
    "print(\"Starting!\")\n",
    "tik = time.time()\n",
    "ds_run = util.dataset_simCLR(\n",
    "                            torch.as_tensor(input_x, device='cpu', dtype=torch.float32), \n",
    "                            torch.as_tensor(torch.zeros(input_x.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                            n_transforms=1,\n",
    "                            class_weights=np.array([1]),\n",
    "                            transform=run_transform,\n",
    "                            DEVICE='cpu',\n",
    "                            dtype_X=torch.float32,\n",
    "                            dtype_y=torch.int64,\n",
    "\n",
    "                            temp_uncertainty=16,\n",
    "                            expand_dim=False\n",
    "                            )\n",
    "dl_run = torch.utils.data.DataLoader( ds_run,\n",
    "                                        batch_size=1024,\n",
    "                                        shuffle=False,\n",
    "                                        drop_last=False,\n",
    "                                        # pin_memory=True,\n",
    "                                        # num_workers=16,\n",
    "                                        # persistent_workers=True,\n",
    "                                        )\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dl_run], dim=0)\n",
    "tok = time.time()\n",
    "\n",
    "print(f'Tile Transformation Alone Took: {tok-tik} s to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "943px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "501px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
