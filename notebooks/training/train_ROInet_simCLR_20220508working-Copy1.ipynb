{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# # !source activate jupyter_launcher\n",
    "# !pip3 install numba\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install scipy\n",
    "# !pip3 install torch\n",
    "# !pip3 install torchvision\n",
    "# !pip3 install sklearn\n",
    "# !pip3 install pycuda\n",
    "# !pip3 install tqdm\n",
    "# !pip3 install seaborn\n",
    "# !pip3 install h5py\n",
    "# !pip3 install hdfdict\n",
    "# !pip3 install ipywidgets\n",
    "# !pip3 install numpy==1.20\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Parse arguments\n",
    "\n",
    "# dir_save = '/media/rich/bigSSD/analysis_data/ROI_net_training/testing_dispatcher_20220504'\n",
    "dir_save = Path(r'/media/rich/bigSSD/')\n",
    "\n",
    "params = {\n",
    "    'paths': {\n",
    "        'dir_github':'/media/rich/Home_Linux_partition/github_repos',\n",
    "        'fileName_save_model':'EfficientNet_b0_7unfrozen_simCLR',\n",
    "        'path_data_training':'/media/rich/bigSSD/analysis_data/ROIs_for_training/sf_sparse_36x36_20220503.npz',\n",
    "    },\n",
    "    \n",
    "    'prefs': {\n",
    "        'saveModelIteratively':True,\n",
    "        'saveLogs':True,\n",
    "    },\n",
    "    \n",
    "    'useGPU_training': True,\n",
    "    'useGPU_dataloader': False,\n",
    "    'dataloader_kwargs':{\n",
    "        'batch_size': 1024,\n",
    "        'shuffle': True,\n",
    "        'drop_last': True,\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 36,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 2,\n",
    "    },\n",
    "\n",
    "    'pre_head_fc_sizes': [128, 128],\n",
    "    'post_head_fc_sizes': [128],\n",
    "    'block_to_unfreeze': '1.2',\n",
    "    'n_block_toInclude': 4,\n",
    "    \n",
    "    'lr': 1*10**-4,\n",
    "    'gamma': 1-0.0000,\n",
    "    'n_epochs': 9999999,\n",
    "    'temperature': 0.5,\n",
    "    \n",
    "    'augmentation': {\n",
    "        'Scale_image_sum': {'sum_val':1, 'epsilon':1e-9, 'min_sub':True},\n",
    "        'AddPoissonNoise': {'scaler_bounds':(10**(4), 10**(5)), 'prob':0.5, 'base':1000, 'scaling':'log'},\n",
    "        'Horizontal_stripe_scale': {'alpha_min_max':(0.5, 1), 'im_size':(36,36), 'prob':0.5},\n",
    "        'Horizontal_stripe_shift': {'alpha_min_max':(1  , 3), 'im_size':(36,36), 'prob':0.5},\n",
    "        'RandomHorizontalFlip': {'p':0.5},\n",
    "        'RandomAffine': {\n",
    "            'degrees':(-180,180),\n",
    "            'translate':(0.1, 0.1), #0, .3, .45 (DEFAULT)\n",
    "            'scale':(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "            'shear':(-15, 15, -15, 15),\n",
    "#             'interpolation':torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "            'interpolation':'bilinear', \n",
    "            'fill':0, \n",
    "            'fillcolor':None, \n",
    "            'resample':None,\n",
    "        },\n",
    "        'AddGaussianNoise': {'mean':0, 'std':0.0010, 'prob':0.5},\n",
    "        'ScaleDynamicRange': {'scaler_bounds':(0,1), 'epsilon':1e-9},\n",
    "        'WarpPoints': {\n",
    "            'r':[0.3, 0.6],\n",
    "            'cx':[-0.3, 0.3],\n",
    "            'cy':[-0.3, 0.3], \n",
    "            'dx':[-0.24, 0.24], \n",
    "            'dy':[-0.24, 0.24], \n",
    "            'n_warps':2,\n",
    "            'prob':0.5,\n",
    "            'img_size_in':[36, 36],\n",
    "            'img_size_out':[72,72],\n",
    "        },\n",
    "        'TileChannels': {'dim':0, 'n_channels':3},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Import personal libraries\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(params['paths']['dir_github'])\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition, path_helpers\n",
    "from GCaMP_ROI_classifier import util, models, training, augmentation, dataset\n",
    "\n",
    "def write_to_log(path_log, text, mode='a', start_on_new_line=True):\n",
    "    with open(path_log, mode=mode) as log:\n",
    "        if start_on_new_line==True:\n",
    "            log.write('\\n')\n",
    "        log.write(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Prepare paths\n",
    "\n",
    "path_saveModel = str((dir_save / params['paths']['fileName_save_model']).with_suffix('.pth'))\n",
    "path_saveLog = str(dir_save / 'log.txt')\n",
    "path_saveLoss = (dir_save / 'loss.npy')\n",
    "\n",
    "device_train = torch_helpers.set_device(use_GPU=params['useGPU_training'], verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Import unlabeled training data\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "sf_sparse = scipy.sparse.load_npz(params['paths']['path_data_training'])\n",
    "\n",
    "sf_dense = torch.as_tensor(sf_sparse.toarray().reshape(sf_sparse.shape[0], 36,36), dtype=torch.float32)\n",
    "\n",
    "##toss any NaNs\n",
    "\n",
    "# print(f'Number of masks: {sf_dense.shape}')\n",
    "ROIs_without_NaNs = ~torch.any(torch.any(torch.isnan(sf_dense), dim=1), dim=1)\n",
    "ROIs_nonAllZero = (torch.max(torch.max(sf_dense, dim=1)[0], dim=1)[0] > 0)\n",
    "ROIs_toKeep = torch.where(ROIs_without_NaNs * ROIs_nonAllZero)[0]\n",
    "masks_cat = sf_dense[ROIs_toKeep]\n",
    "\n",
    "n_masks_removed = np.sum(sf_dense.shape[0] - ROIs_toKeep.shape[0])\n",
    "# print(f'Number of masks: {masks_cat.shape}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Define augmentation pipeline\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    *[augmentation.__dict__[key](**params) for key,params in params['augmentation'].items()]\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "\n",
    "device_dataloader = torch_helpers.set_device(use_GPU=params['useGPU_dataloader'])\n",
    "\n",
    "dataset_train = dataset.dataset_simCLR(\n",
    "    torch.as_tensor(masks_cat, device=device_dataloader, dtype=torch.float32), \n",
    "    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device=device_dataloader, dtype=torch.float32),\n",
    "    n_transforms=2,\n",
    "    class_weights=np.array([1]),\n",
    "    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "    transform=scripted_transforms,\n",
    "    # DEVICE='cpu',\n",
    "    DEVICE=device_dataloader,\n",
    "    dtype_X=torch.float32,\n",
    "    dtype_y=torch.int64,\n",
    "    # temp_uncertainty=1\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    **params['dataloader_kwargs']\n",
    "\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    "    \n",
    "#     batch_size=1024,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    ")\n",
    "\n",
    "# %matplotlib notebook\n",
    "\n",
    "# idx_rand = np.random.randint(0,masks_cat2.shape[0], 10)\n",
    "# for ii in idx_rand:\n",
    "#     fig, axs = plt.subplots(1,2)\n",
    "#     # print(dataset_train[ii][0][0][0].shape)\n",
    "#     axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "#     axs[1].imshow(dataset_train[ii][0][1][0].cpu())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Define ModelTackOn\n",
    "\n",
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, data_dim=(1,3,36,36), pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "            \n",
    "            self.data_dim = data_dim\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "        \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 960\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 768\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1536\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1024\n",
    "                in_features = self.base_model(torch.rand(*(self.data_dim))).data.squeeze().shape[0]  ## RH EDIT\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Import pretrained model\n",
    "\n",
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.convnext_tiny(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_small(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_base(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_large(pretrained=True)\n",
    "\n",
    "\n",
    "# base_model_frozen = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Make combined model\n",
    "\n",
    "## Tacking on the latent layers needs to be done in a few steps.\n",
    "\n",
    "## 0. Chop the base model\n",
    "## 1. Tack on a pooling layer to reduce the size of the convlutional parameters\n",
    "## 2. Determine the size of the output (internally done in ModelTackOn)\n",
    "## 3. Tack on a linear layer of the correct size  (internally done in ModelTackOn)\n",
    "\n",
    "\n",
    "model_chopped = torch.nn.Sequential(list(base_model_frozen.children())[0][:params['n_block_toInclude']])  ## 0.\n",
    "model_chopped_pooled = torch.nn.Sequential(model_chopped, torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten())  ## 1.\n",
    "\n",
    "image_out_size = list(dataset_train[0][0][0].shape)\n",
    "data_dim = tuple([1] + list(image_out_size))\n",
    "\n",
    "## 2. , 3.\n",
    "model = ModelTackOn(\n",
    "#     model_chopped.to('cpu'),\n",
    "    model_chopped_pooled.to('cpu'),\n",
    "    base_model_frozen.to('cpu'),\n",
    "    data_dim=data_dim,\n",
    "    pre_head_fc_sizes=params['pre_head_fc_sizes'], \n",
    "    post_head_fc_sizes=params['post_head_fc_sizes'], \n",
    "    classifier_fc_sizes=None\n",
    ")\n",
    "model.train();\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### unfreeze particular blocks in model\n",
    "\n",
    "mnp = [name for name, param in model.named_parameters()]  ## 'model named parameters'\n",
    "mnp_blockNums = [name[name.find('.'):name.find('.')+8] for name in mnp]  ## pulls out the numbers just after the model name\n",
    "mnp_nums = [path_helpers.get_nums_from_string(name) for name in mnp_blockNums]  ## converts them to numbers\n",
    "block_to_freeze_nums = path_helpers.get_nums_from_string(params['block_to_unfreeze'])  ## converts the input parameter specifying the block to freeze into a number for comparison\n",
    "\n",
    "m_baseName = mnp[0][:mnp[0].find('.')]\n",
    "\n",
    "for ii, (name, param) in enumerate(model.named_parameters()):\n",
    "    if m_baseName in name:\n",
    "#         print(name)\n",
    "        if mnp_nums[ii] < block_to_freeze_nums:\n",
    "            param.requires_grad = False\n",
    "        elif mnp_nums[ii] >= block_to_freeze_nums:\n",
    "            param.requires_grad = True\n",
    "\n",
    "names_layers_requiresGrad = [( param.requires_grad , name ) for name,param in list(model.named_parameters())]\n",
    "\n",
    "# print('MODEL: (requires_grad , layer_name)')\n",
    "# [print(name) for name in params_calculated['names_layers_requiresGrad']];\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Save run outputs\n",
    "\n",
    "## The training step is written so that it can run until a job ends, so this needs to be saved before\n",
    "\n",
    "run_outputs = {\n",
    "    'dir_save': str(dir_save),    \n",
    "    'path_save_runOutputs': str(dir_save / 'run_outputs.json'),    \n",
    "    'path_saveModel': str(path_saveModel),\n",
    "    'path_saveLog': str(path_saveLog),\n",
    "    'path_saveLoss': str(path_saveLoss),\n",
    "    'device_train': device_train,\n",
    "    'masks_training_shape': list(masks_cat.shape),\n",
    "    'n_masks_removed': int(n_masks_removed),\n",
    "    'image_resized_shape': list(dataset_train[0][0][0].shape),\n",
    "    'names_layers_requiresGrad': names_layers_requiresGrad,\n",
    "}\n",
    "run_outputs;\n",
    "\n",
    "import json\n",
    "with open(run_outputs['path_save_runOutputs'], 'w') as f:\n",
    "    json.dump(run_outputs, f) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "model.to(device_train)\n",
    "model.prep_contrast()\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "    lr=params['lr'],\n",
    "#     lr=1*10**-3,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=params['gamma'],\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(device_train) for _ in criterion]\n",
    "\n",
    "losses_train, losses_val = [], [np.nan]\n",
    "for epoch in tqdm(range(params['n_epochs'])):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    losses_train = training.epoch_step(\n",
    "        dataloader_train, \n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        scheduler=scheduler,\n",
    "        temperature=params['temperature'],\n",
    "        # l2_alpha,\n",
    "        mode='semi-supervised',\n",
    "        loss_rolling_train=losses_train, \n",
    "        loss_rolling_val=losses_val,\n",
    "        device=device_train, \n",
    "        verbose=2,\n",
    "        verbose_update_period=1,\n",
    "\n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    ")\n",
    "    \n",
    "    ## save loss stuff\n",
    "    if params['prefs']['saveLogs']:\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, completed epoch: {epoch}, loss: {losses_train[-1]}, lr: {scheduler.get_last_lr()[0]}')\n",
    "        np.save(path_saveLoss, losses_train)\n",
    "    \n",
    "    ## if loss becomes NaNs, don't save the network and stop training\n",
    "    if torch.isnan(torch.as_tensor(losses_train[-1])):\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, EXITED DUE TO loss==NaN')\n",
    "        break\n",
    "        \n",
    "    ## save model\n",
    "    if params['prefs']['saveModelIteratively']:\n",
    "        torch.save(model.state_dict(), path_saveModel)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(losses_train)\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load('/media/rich/bigSSD/EfficientNet_b0_7unfrozen_simCLR.pth'))\n",
    "# model.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_stat = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/stat.npy'\n",
    "# path_labels = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh1.npy'\n",
    "\n",
    "path_stat = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse6_28 _ day20200815/stat.npy'\n",
    "path_labels = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse6_28 _ day20200815/labels_posthoc_all.npy'\n",
    "\n",
    "\n",
    "images_labeled = np.concatenate(\n",
    "    util.import_multiple_stat_files(   \n",
    "        paths_statFiles=[path_stat],\n",
    "        out_height_width=[36,36],\n",
    "        max_footprint_width=241,\n",
    "        plot_pref=True\n",
    "    )\n",
    ")\n",
    "\n",
    "labels = classification.squeeze_integers(np.concatenate([np.load(path_labels)]))\n",
    "\n",
    "assert images_labeled.shape[0] == labels.shape[0] , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_toKeep = np.where(np.logical_not(labels == 4))[0]\n",
    "\n",
    "images_labeled_clean = images_labeled[idx_toKeep]\n",
    "labels_clean = labels[idx_toKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_classifier = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    \n",
    "    torchvision.transforms.Resize(\n",
    "#         size=(224, 224),\n",
    "#         size=(180, 180),\n",
    "        size=(36, 36),        \n",
    "        interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "\n",
    "scripted_transforms_classifier = torch.jit.script(transforms_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_make_dataset(X):\n",
    "    out = dataset.dataset_simCLR(\n",
    "        X=torch.as_tensor(X, device='cpu', dtype=torch.float32),\n",
    "        y=torch.as_tensor(torch.zeros(X.shape[0]), device='cpu', dtype=torch.float32),\n",
    "        n_transforms=1,\n",
    "        class_weights=np.array([1]),\n",
    "        transform=scripted_transforms_classifier,\n",
    "        DEVICE='cpu',\n",
    "        dtype_X=torch.float32,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def helper_make_dataloader(ds):\n",
    "    out = torch.utils.data.DataLoader( \n",
    "        ds,\n",
    "        batch_size=1024,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "#         pin_memory=True,\n",
    "#         num_workers=36,\n",
    "#         persistent_workers=True,\n",
    "#         prefetch_factor=2\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_clean = helper_make_dataset(images_labeled_clean)\n",
    "    \n",
    "dataloader_labeled_clean = helper_make_dataloader(dataset_labeled_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_trainingData = helper_make_dataset(masks_cat[torch.randint(low=0, high=masks_cat.shape[0], size=(100000,1)).squeeze()])\n",
    "    \n",
    "dataloader_trainingData = helper_make_dataloader(dataset_trainingData)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_trainingData = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_trainingData], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pc_tform, _, _, _ = decomposition.torch_pca(features_trainingData, rank=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model_chopped.to(DEVICE)(data[0][0].to(DEVICE)).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pc_tform.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_tformed = features_nn @ pc_tform[:,:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_sample_weights(labels):\n",
    "    labels = np.int64(labels.copy())\n",
    "    counts, vals = np.histogram(labels, bins=np.concatenate((np.unique(labels), [labels.max()+1])))\n",
    "    vals = vals[:-1]\n",
    "\n",
    "    n_labels = len(labels)\n",
    "    weights = n_labels / counts\n",
    "    \n",
    "    sample_weights = np.array([weights[l] for l in labels])\n",
    "    \n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kymatio import Scattering2D\n",
    "\n",
    "def get_latents_swt(sfs, swt, device_model):\n",
    "    sfs = torch.as_tensor(np.ascontiguousarray(sfs[None,...]), device=device_model, dtype=torch.float32)\n",
    "    latents_swt = swt(sfs[None,...]).squeeze()\n",
    "    latents_swt = latents_swt.reshape(latents_swt.shape[0], -1)\n",
    "    return latents_swt\n",
    "\n",
    "\n",
    "device_model = torch_helpers.set_device(use_GPU=True)\n",
    "\n",
    "scattering = Scattering2D(J=2, L=8, shape=images_labeled_clean[0].shape[-2:])\n",
    "if device_model != 'cpu':\n",
    "    scattering = scattering.cuda()\n",
    "\n",
    "latents_swt = get_latents_swt(images_labeled_clean, scattering.cuda(), device_model).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp, scores_nn, SVs, EVR = decomposition.torch_pca(features_nn, rank=30)\n",
    "comp, scores_swt, SVs, EVR = decomposition.torch_pca(latents_swt, rank=30)\n",
    "comp, scores_image, SVs, EVR = decomposition.torch_pca(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1), rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanSub = sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "\n",
    "features_norm = meanSub.fit_transform(features_nn)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(latents_swt)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1))\n",
    "\n",
    "# features_norm = meanSub.fit_transform(scores)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(np.concatenate((scores_nn, scores_swt, scores_image), axis=1))\n",
    "# features_all = np.concatenate([meanSub.fit_transform(scores) / torch.var(scores).mean() for scores in [features_nn, latents_swt, torch.as_tensor(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1))]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp, features_norm, SVs, EVR = decomposition.torch_pca(features_norm, rank=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp, features_norm, SVs, EVR = decomposition.torch_pca(features_all, rank=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_norm = torch.cat([val / torch.std(val) for val in [scores_nn, scores_swt]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_swt]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_image]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn, scores_swt]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn, scores_swt, scores_image]], dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_norm = features_nn\n",
    "\n",
    "# features_norm = scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_val, labels_train, labels_val = sklearn.model_selection.train_test_split(features_norm, labels_clean, test_size=0.3)\n",
    "print(f'train / test shapes: images_labeled_train, images_labeled_val, labels_train, labels_val: {features_train.shape, features_val.shape, labels_train.shape, labels_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "C_toUse = np.array([1000, 100, 10,1,0.1,0.01,0.001])\n",
    "# C_toUse = np.array([10**-4, 10**-5, 10**-6, 10**-7])\n",
    "# C_toUse = np.array([10**2, 10**3, 10**4, 10**5])\n",
    "for C in C_toUse:\n",
    "    logreg = sklearn.linear_model.LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        max_iter=6000, \n",
    "        C=C,\n",
    "        fit_intercept=True, \n",
    "        class_weight='balanced',\n",
    "    #     n_jobs=-1\n",
    "    )\n",
    "    logreg.fit(features_train, labels_train)\n",
    "\n",
    "    acc = logreg.score(features_train, labels_train, sample_weight=get_balanced_sample_weights(labels_train))\n",
    "    acc_train.append(acc)\n",
    "\n",
    "    acc = logreg.score(features_val, labels_val, sample_weight=get_balanced_sample_weights(labels_val))\n",
    "    acc_val.append(acc)\n",
    "    \n",
    "        \n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);\n",
    "\n",
    "print(f'best val score: {max(acc_val)}')\n",
    "print(f'best C value: {C_toUse[np.argmax(acc_val)]}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "acc_train, acc_val = [], []\n",
    "C_toUse = np.array([1000, 100, 10,1,0.1,0.01,0.001])\n",
    "for C in C_toUse:\n",
    "    svm = sklearn.svm.SVC(\n",
    "        C=C, \n",
    "        kernel='rbf',\n",
    "        degree=3, \n",
    "        gamma='scale',\n",
    "        coef0=0.0, \n",
    "        shrinking=True,\n",
    "        probability=False, \n",
    "        tol=0.001, \n",
    "        cache_size=200,\n",
    "        class_weight='balanced',\n",
    "        verbose=False, \n",
    "        max_iter=- 1, \n",
    "        decision_function_shape='ovr',\n",
    "        break_ties=False,\n",
    "        random_state=None\n",
    "    )\n",
    "    svm.fit(features_train, labels_train)\n",
    "\n",
    "    acc = svm.score(features_train, labels_train, sample_weight=get_balanced_sample_weights(labels_train))\n",
    "    acc_train.append(acc)\n",
    "\n",
    "    acc = svm.score(features_val, labels_val, sample_weight=get_balanced_sample_weights(labels_val))\n",
    "    acc_val.append(acc)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    max_iter=8000, \n",
    "    C=10**(0),\n",
    "    fit_intercept=True, \n",
    "    class_weight='balanced',\n",
    "#     n_jobs=-1\n",
    ").fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svm = sklearn.svm.SVC(\n",
    "    C=10**(1), \n",
    "    kernel='rbf',\n",
    "    degree=3, \n",
    "    gamma='scale',\n",
    "    coef0=0.0, \n",
    "    shrinking=True,\n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200,\n",
    "    class_weight='balanced',\n",
    "    verbose=False, \n",
    "    max_iter=- 1, \n",
    "    decision_function_shape='ovr',\n",
    "    break_ties=False,\n",
    "    random_state=None\n",
    ")\n",
    "svm.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "\n",
    "# preds = svm.predict(features_train_norm).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_train.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "\n",
    "# preds = svm.predict(features_val_norm).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_val.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "# proba = logreg.predict_proba(features_train_norm)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "\n",
    "preds = svm.predict(features_train).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_train.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "\n",
    "\n",
    "# proba = logreg.predict_proba(features_val_norm)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "\n",
    "preds = svm.predict(features_val).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_val.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sesh1 = np.load(r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh1.npy')\n",
    "labels_sesh2 = np.load(r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(\n",
    "    classification.confusion_matrix(labels_sesh1.astype(np.int32), labels_sesh2.astype(np.int32)),\n",
    "    annot=True, \n",
    "    annot_kws={\"size\": 16}, \n",
    "    vmax=1., \n",
    "    cmap=plt.get_cmap('gray')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, params.shape) for name, params in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "layer_1 = model.state_dict()['base_model.0.0.0.weight'].cpu()\n",
    "# layer_2 = model.state_dict()['base_model.0.6.3.block.1.0.weight'].cpu()\n",
    "# layer_3 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "# layer_4 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "\n",
    "\n",
    "plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_1], dim=0), grid_shape=(15,15), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_2], dim=0), grid_shape=(6,6), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_3], dim=0), grid_shape=(4,6), kwargs_imshow={'vmax': 0.2});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 920.76378,
   "position": {
    "height": "40px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "500.99px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
