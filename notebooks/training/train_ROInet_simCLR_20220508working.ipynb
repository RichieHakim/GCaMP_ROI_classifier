{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_434411/1226147920.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "outputs": [],
   "source": [
    "# # !source activate jupyter_launcher\n",
    "# !pip3 install numba\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install scipy\n",
    "# !pip3 install torch\n",
    "# !pip3 install torchvision\n",
    "# !pip3 install sklearn\n",
    "# !pip3 install pycuda\n",
    "# !pip3 install tqdm\n",
    "# !pip3 install seaborn\n",
    "# !pip3 install h5py\n",
    "# !pip3 install hdfdict\n",
    "# !pip3 install ipywidgets\n",
    "# !pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parse arguments\n",
    "\n",
    "# import sys\n",
    "# path_script, path_params, dir_save = sys.argv\n",
    "                \n",
    "# import json\n",
    "# with open(path_params, 'r') as f:\n",
    "#     params = json.load(f)\n",
    "\n",
    "# import shutil\n",
    "# shutil.copy2(path_script, str(Path(dir_save) / Path(path_script).name));\n",
    "\n",
    "\n",
    "# dir_save = '/media/rich/bigSSD/analysis_data/ROI_net_training/testing_dispatcher_20220504'\n",
    "dir_save = Path(r'/media/rich/bigSSD/')\n",
    "\n",
    "# params_template = {\n",
    "params = {\n",
    "    'paths': {\n",
    "        'dir_github':'/media/rich/Home_Linux_partition/github_repos',\n",
    "        'fileName_save_model':'ConvNext_tiny_1.0unfrozen_simCLR',\n",
    "        'path_data_training':'/media/rich/bigSSD/analysis_data/ROIs_for_training/sf_sparse_36x36_20220503.npz',\n",
    "    },\n",
    "    \n",
    "    'prefs': {\n",
    "        'saveModelIteratively':True,\n",
    "        'saveLogs':True,\n",
    "    },\n",
    "    \n",
    "    'useGPU_training': True,\n",
    "    'useGPU_dataloader': False,\n",
    "    'dataloader_kwargs':{\n",
    "        'batch_size': 1024,\n",
    "        'shuffle': True,\n",
    "        'drop_last': True,\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 36,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 2,\n",
    "    },\n",
    "    'inner_batch_size': 256,\n",
    "\n",
    "    'torchvision_model': 'convnext_tiny',\n",
    "\n",
    "    'pre_head_fc_sizes': [256, 128],\n",
    "    'post_head_fc_sizes': [128],\n",
    "    'block_to_unfreeze': '7.0',\n",
    "    'n_block_toInclude': 9,\n",
    "    \n",
    "    'lr': 5*10**-3,\n",
    "    'penalty_orthogonality':0,\n",
    "    'weight_decay': 0.0000,\n",
    "    'gamma': 1-0.0000,\n",
    "    'n_epochs': 9999999,\n",
    "    'temperature': 0.5,\n",
    "    'l2_alpha': 0.0000,\n",
    "    \n",
    "    'augmentation': {\n",
    "        'Scale_image_sum': {'sum_val':1, 'epsilon':1e-9, 'min_sub':True},\n",
    "        'AddPoissonNoise': {'scaler_bounds':(10**(3.5), 10**(4)), 'prob':0.7, 'base':1000, 'scaling':'log'},\n",
    "        'Horizontal_stripe_scale': {'alpha_min_max':(0.5, 1), 'im_size':(36,36), 'prob':0.3},\n",
    "        'Horizontal_stripe_shift': {'alpha_min_max':(1  , 2), 'im_size':(36,36), 'prob':0.3},\n",
    "        'RandomHorizontalFlip': {'p':0.5},\n",
    "        'RandomAffine': {\n",
    "            'degrees':(-180,180),\n",
    "            'translate':(0.1, 0.1), #0, .3, .45 (DEFAULT)\n",
    "            'scale':(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "            'shear':(-8, 8, -8, 8),\n",
    "#             'interpolation':torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "            'interpolation':'bilinear', \n",
    "            'fill':0, \n",
    "            'fillcolor':None, \n",
    "            'resample':None,\n",
    "        },\n",
    "        'AddGaussianNoise': {'mean':0, 'std':0.0010, 'prob':0.5},\n",
    "        'ScaleDynamicRange': {'scaler_bounds':(0,1), 'epsilon':1e-9},\n",
    "        'WarpPoints': {\n",
    "            'r':[0.3, 0.6],\n",
    "            'cx':[-0.3, 0.3],\n",
    "            'cy':[-0.3, 0.3], \n",
    "            'dx':[-0.24, 0.24], \n",
    "            'dy':[-0.24, 0.24], \n",
    "            'n_warps':2,\n",
    "            'prob':0.5,\n",
    "            'img_size_in':[36, 36],\n",
    "#             'img_size_out':[72,72],\n",
    "            'img_size_out':[224,224],\n",
    "        },\n",
    "        'TileChannels': {'dim':0, 'n_channels':3},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import personal libraries\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(params['paths']['dir_github'])\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition, path_helpers\n",
    "from GCaMP_ROI_classifier import util, models, training, augmentation, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log(path_log, text, mode='a', start_on_new_line=True):\n",
    "    with open(path_log, mode=mode) as log:\n",
    "        if start_on_new_line==True:\n",
    "            log.write('\\n')\n",
    "        log.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare paths\n",
    "\n",
    "path_saveModel = str((dir_save / params['paths']['fileName_save_model']).with_suffix('.pth'))\n",
    "path_saveLog = str(dir_save / 'log.txt')\n",
    "path_saveLoss = (dir_save / 'loss.npy')\n",
    "\n",
    "device_train = torch_helpers.set_device(use_GPU=params['useGPU_training'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "outputs": [],
   "source": [
    "### Import unlabeled training data\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "sf_sparse = scipy.sparse.load_npz(params['paths']['path_data_training'])\n",
    "\n",
    "sf_dense = torch.as_tensor(sf_sparse.toarray().reshape(sf_sparse.shape[0], 36,36), dtype=torch.float32)\n",
    "\n",
    "##toss any NaNs\n",
    "\n",
    "# print(f'Number of masks: {sf_dense.shape}')\n",
    "ROIs_without_NaNs = ~torch.any(torch.any(torch.isnan(sf_dense), dim=1), dim=1)\n",
    "ROIs_nonAllZero = (torch.max(torch.max(sf_dense, dim=1)[0], dim=1)[0] > 0)\n",
    "ROIs_toKeep = torch.where(ROIs_without_NaNs * ROIs_nonAllZero)[0]\n",
    "masks_cat = sf_dense[ROIs_toKeep]\n",
    "\n",
    "n_masks_removed = np.sum(sf_dense.shape[0] - ROIs_toKeep.shape[0])\n",
    "# print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cpu'\n"
     ]
    }
   ],
   "source": [
    "### Define augmentation pipeline\n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    *[augmentation.__dict__[key](**params) for key,params in params['augmentation'].items()]\n",
    ")\n",
    "scripted_transforms = torch.jit.script(transforms)\n",
    "\n",
    "device_dataloader = torch_helpers.set_device(use_GPU=params['useGPU_dataloader'])\n",
    "\n",
    "dataset_train = dataset.dataset_simCLR(\n",
    "    torch.as_tensor(masks_cat, device=device_dataloader, dtype=torch.float32), \n",
    "    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device=device_dataloader, dtype=torch.float32),\n",
    "    n_transforms=2,\n",
    "    class_weights=np.array([1]),\n",
    "    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "    transform=scripted_transforms,\n",
    "    # DEVICE='cpu',\n",
    "    DEVICE=device_dataloader,\n",
    "    dtype_X=torch.float32,\n",
    "    dtype_y=torch.int64,\n",
    "    # temp_uncertainty=1\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    **params['dataloader_kwargs']\n",
    "\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    "    \n",
    "#     batch_size=1024,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib notebook\n",
    "\n",
    "# idx_rand = np.random.randint(0,masks_cat.shape[0], 10)\n",
    "# for ii in idx_rand:\n",
    "#     fig, axs = plt.subplots(1,2)\n",
    "#     # print(dataset_train[ii][0][0][0].shape)\n",
    "#     axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "#     axs[1].imshow(dataset_train[ii][0][1][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "outputs": [],
   "source": [
    "### Define ModelTackOn\n",
    "\n",
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model, \n",
    "        un_modified_model,\n",
    "        data_dim=(1,3,36,36), \n",
    "        pre_head_fc_sizes=[100], \n",
    "        post_head_fc_sizes=[100], \n",
    "        classifier_fc_sizes=None, \n",
    "        nonlinearity='relu', \n",
    "        kwargs_nonlinearity={},\n",
    "    ):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "            \n",
    "            self.data_dim = data_dim\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "                \n",
    "            self.nonlinearity = nonlinearity\n",
    "            self.kwargs_nonlinearity = kwargs_nonlinearity\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "            \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 960\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 768\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1536\n",
    "#                 in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1024\n",
    "                in_features = self.base_model(torch.rand(*(self.data_dim))).data.squeeze().shape[0]  ## RH EDIT\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "#             non_linearity = torch.nn.ReLU()\n",
    "#             non_linearity = torch.nn.GELU()\n",
    "            non_linearity = torch.nn.__dict__[self.nonlinearity](**self.kwargs_nonlinearity)\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#                 non_linearity = torch.nn.ReLU()\n",
    "#                 non_linearity = torch.nn.GELU()\n",
    "            non_linearity = torch.nn.__dict__[self.nonlinearity](**self.kwargs_nonlinearity)    \n",
    "            self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "### Import pretrained model\n",
    "\n",
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# base_model_frozen = torchvision.models.convnext_tiny(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_small(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_base(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.convnext_large(pretrained=True)\n",
    "\n",
    "\n",
    "# base_model_frozen = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.__dict__[params['torchvision_model']](pretrained=True)\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "outputs": [],
   "source": [
    "### Make combined model\n",
    "\n",
    "## Tacking on the latent layers needs to be done in a few steps.\n",
    "\n",
    "## 0. Chop the base model\n",
    "## 1. Tack on a pooling layer to reduce the size of the convlutional parameters\n",
    "## 2. Determine the size of the output (internally done in ModelTackOn)\n",
    "## 3. Tack on a linear layer of the correct size  (internally done in ModelTackOn)\n",
    "\n",
    "\n",
    "model_chopped = torch.nn.Sequential(list(base_model_frozen.children())[0][:params['n_block_toInclude']])  ## 0.\n",
    "model_chopped_pooled = torch.nn.Sequential(model_chopped, torch.nn.AdaptiveAvgPool2d(output_size=1), torch.nn.Flatten())  ## 1.\n",
    "\n",
    "image_out_size = list(dataset_train[0][0][0].shape)\n",
    "data_dim = tuple([1] + list(image_out_size))\n",
    "\n",
    "## 2. , 3.\n",
    "model = ModelTackOn(\n",
    "#     model_chopped.to('cpu'),\n",
    "    model_chopped_pooled.to('cpu'),\n",
    "    base_model_frozen.to('cpu'),\n",
    "    data_dim=data_dim,\n",
    "    pre_head_fc_sizes=params['pre_head_fc_sizes'], \n",
    "    post_head_fc_sizes=params['post_head_fc_sizes'], \n",
    "    classifier_fc_sizes=None,\n",
    "    nonlinearity='GELU',\n",
    "    kwargs_nonlinearity={},\n",
    ")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### unfreeze particular blocks in model\n",
    "\n",
    "mnp = [name for name, param in model.named_parameters()]  ## 'model named parameters'\n",
    "mnp_blockNums = [name[name.find('.'):name.find('.')+8] for name in mnp]  ## pulls out the numbers just after the model name\n",
    "mnp_nums = [path_helpers.get_nums_from_string(name) for name in mnp_blockNums]  ## converts them to numbers\n",
    "block_to_freeze_nums = path_helpers.get_nums_from_string(params['block_to_unfreeze'])  ## converts the input parameter specifying the block to freeze into a number for comparison\n",
    "\n",
    "m_baseName = mnp[0][:mnp[0].find('.')]\n",
    "\n",
    "for ii, (name, param) in enumerate(model.named_parameters()):\n",
    "    if m_baseName in name:\n",
    "#         print(name)\n",
    "        if mnp_nums[ii] < block_to_freeze_nums:\n",
    "            param.requires_grad = False\n",
    "        elif mnp_nums[ii] >= block_to_freeze_nums:\n",
    "            param.requires_grad = True\n",
    "\n",
    "names_layers_requiresGrad = [( param.requires_grad , name ) for name,param in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[name for name,val in base_model_frozen.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(False, 'base_model.0.0.0.0.weight'),\n",
       " (False, 'base_model.0.0.0.0.bias'),\n",
       " (False, 'base_model.0.0.0.1.weight'),\n",
       " (False, 'base_model.0.0.0.1.bias'),\n",
       " (False, 'base_model.0.0.1.0.layer_scale'),\n",
       " (False, 'base_model.0.0.1.0.block.0.weight'),\n",
       " (False, 'base_model.0.0.1.0.block.0.bias'),\n",
       " (False, 'base_model.0.0.1.0.block.2.weight'),\n",
       " (False, 'base_model.0.0.1.0.block.2.bias'),\n",
       " (False, 'base_model.0.0.1.0.block.3.weight'),\n",
       " (False, 'base_model.0.0.1.0.block.3.bias'),\n",
       " (False, 'base_model.0.0.1.0.block.5.weight'),\n",
       " (False, 'base_model.0.0.1.0.block.5.bias'),\n",
       " (False, 'base_model.0.0.1.1.layer_scale'),\n",
       " (False, 'base_model.0.0.1.1.block.0.weight'),\n",
       " (False, 'base_model.0.0.1.1.block.0.bias'),\n",
       " (False, 'base_model.0.0.1.1.block.2.weight'),\n",
       " (False, 'base_model.0.0.1.1.block.2.bias'),\n",
       " (False, 'base_model.0.0.1.1.block.3.weight'),\n",
       " (False, 'base_model.0.0.1.1.block.3.bias'),\n",
       " (False, 'base_model.0.0.1.1.block.5.weight'),\n",
       " (False, 'base_model.0.0.1.1.block.5.bias'),\n",
       " (False, 'base_model.0.0.1.2.layer_scale'),\n",
       " (False, 'base_model.0.0.1.2.block.0.weight'),\n",
       " (False, 'base_model.0.0.1.2.block.0.bias'),\n",
       " (False, 'base_model.0.0.1.2.block.2.weight'),\n",
       " (False, 'base_model.0.0.1.2.block.2.bias'),\n",
       " (False, 'base_model.0.0.1.2.block.3.weight'),\n",
       " (False, 'base_model.0.0.1.2.block.3.bias'),\n",
       " (False, 'base_model.0.0.1.2.block.5.weight'),\n",
       " (False, 'base_model.0.0.1.2.block.5.bias'),\n",
       " (False, 'base_model.0.0.2.0.weight'),\n",
       " (False, 'base_model.0.0.2.0.bias'),\n",
       " (False, 'base_model.0.0.2.1.weight'),\n",
       " (False, 'base_model.0.0.2.1.bias'),\n",
       " (False, 'base_model.0.0.3.0.layer_scale'),\n",
       " (False, 'base_model.0.0.3.0.block.0.weight'),\n",
       " (False, 'base_model.0.0.3.0.block.0.bias'),\n",
       " (False, 'base_model.0.0.3.0.block.2.weight'),\n",
       " (False, 'base_model.0.0.3.0.block.2.bias'),\n",
       " (False, 'base_model.0.0.3.0.block.3.weight'),\n",
       " (False, 'base_model.0.0.3.0.block.3.bias'),\n",
       " (False, 'base_model.0.0.3.0.block.5.weight'),\n",
       " (False, 'base_model.0.0.3.0.block.5.bias'),\n",
       " (False, 'base_model.0.0.3.1.layer_scale'),\n",
       " (False, 'base_model.0.0.3.1.block.0.weight'),\n",
       " (False, 'base_model.0.0.3.1.block.0.bias'),\n",
       " (False, 'base_model.0.0.3.1.block.2.weight'),\n",
       " (False, 'base_model.0.0.3.1.block.2.bias'),\n",
       " (False, 'base_model.0.0.3.1.block.3.weight'),\n",
       " (False, 'base_model.0.0.3.1.block.3.bias'),\n",
       " (False, 'base_model.0.0.3.1.block.5.weight'),\n",
       " (False, 'base_model.0.0.3.1.block.5.bias'),\n",
       " (False, 'base_model.0.0.3.2.layer_scale'),\n",
       " (False, 'base_model.0.0.3.2.block.0.weight'),\n",
       " (False, 'base_model.0.0.3.2.block.0.bias'),\n",
       " (False, 'base_model.0.0.3.2.block.2.weight'),\n",
       " (False, 'base_model.0.0.3.2.block.2.bias'),\n",
       " (False, 'base_model.0.0.3.2.block.3.weight'),\n",
       " (False, 'base_model.0.0.3.2.block.3.bias'),\n",
       " (False, 'base_model.0.0.3.2.block.5.weight'),\n",
       " (False, 'base_model.0.0.3.2.block.5.bias'),\n",
       " (False, 'base_model.0.0.4.0.weight'),\n",
       " (False, 'base_model.0.0.4.0.bias'),\n",
       " (False, 'base_model.0.0.4.1.weight'),\n",
       " (False, 'base_model.0.0.4.1.bias'),\n",
       " (False, 'base_model.0.0.5.0.layer_scale'),\n",
       " (False, 'base_model.0.0.5.0.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.0.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.0.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.0.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.0.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.0.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.0.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.0.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.1.layer_scale'),\n",
       " (False, 'base_model.0.0.5.1.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.1.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.1.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.1.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.1.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.1.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.1.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.1.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.2.layer_scale'),\n",
       " (False, 'base_model.0.0.5.2.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.2.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.2.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.2.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.2.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.2.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.2.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.2.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.3.layer_scale'),\n",
       " (False, 'base_model.0.0.5.3.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.3.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.3.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.3.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.3.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.3.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.3.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.3.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.4.layer_scale'),\n",
       " (False, 'base_model.0.0.5.4.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.4.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.4.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.4.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.4.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.4.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.4.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.4.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.5.layer_scale'),\n",
       " (False, 'base_model.0.0.5.5.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.5.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.5.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.5.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.5.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.5.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.5.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.5.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.6.layer_scale'),\n",
       " (False, 'base_model.0.0.5.6.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.6.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.6.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.6.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.6.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.6.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.6.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.6.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.7.layer_scale'),\n",
       " (False, 'base_model.0.0.5.7.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.7.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.7.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.7.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.7.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.7.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.7.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.7.block.5.bias'),\n",
       " (False, 'base_model.0.0.5.8.layer_scale'),\n",
       " (False, 'base_model.0.0.5.8.block.0.weight'),\n",
       " (False, 'base_model.0.0.5.8.block.0.bias'),\n",
       " (False, 'base_model.0.0.5.8.block.2.weight'),\n",
       " (False, 'base_model.0.0.5.8.block.2.bias'),\n",
       " (False, 'base_model.0.0.5.8.block.3.weight'),\n",
       " (False, 'base_model.0.0.5.8.block.3.bias'),\n",
       " (False, 'base_model.0.0.5.8.block.5.weight'),\n",
       " (False, 'base_model.0.0.5.8.block.5.bias'),\n",
       " (False, 'base_model.0.0.6.0.weight'),\n",
       " (False, 'base_model.0.0.6.0.bias'),\n",
       " (False, 'base_model.0.0.6.1.weight'),\n",
       " (False, 'base_model.0.0.6.1.bias'),\n",
       " (True, 'base_model.0.0.7.0.layer_scale'),\n",
       " (True, 'base_model.0.0.7.0.block.0.weight'),\n",
       " (True, 'base_model.0.0.7.0.block.0.bias'),\n",
       " (True, 'base_model.0.0.7.0.block.2.weight'),\n",
       " (True, 'base_model.0.0.7.0.block.2.bias'),\n",
       " (True, 'base_model.0.0.7.0.block.3.weight'),\n",
       " (True, 'base_model.0.0.7.0.block.3.bias'),\n",
       " (True, 'base_model.0.0.7.0.block.5.weight'),\n",
       " (True, 'base_model.0.0.7.0.block.5.bias'),\n",
       " (True, 'base_model.0.0.7.1.layer_scale'),\n",
       " (True, 'base_model.0.0.7.1.block.0.weight'),\n",
       " (True, 'base_model.0.0.7.1.block.0.bias'),\n",
       " (True, 'base_model.0.0.7.1.block.2.weight'),\n",
       " (True, 'base_model.0.0.7.1.block.2.bias'),\n",
       " (True, 'base_model.0.0.7.1.block.3.weight'),\n",
       " (True, 'base_model.0.0.7.1.block.3.bias'),\n",
       " (True, 'base_model.0.0.7.1.block.5.weight'),\n",
       " (True, 'base_model.0.0.7.1.block.5.bias'),\n",
       " (True, 'base_model.0.0.7.2.layer_scale'),\n",
       " (True, 'base_model.0.0.7.2.block.0.weight'),\n",
       " (True, 'base_model.0.0.7.2.block.0.bias'),\n",
       " (True, 'base_model.0.0.7.2.block.2.weight'),\n",
       " (True, 'base_model.0.0.7.2.block.2.bias'),\n",
       " (True, 'base_model.0.0.7.2.block.3.weight'),\n",
       " (True, 'base_model.0.0.7.2.block.3.bias'),\n",
       " (True, 'base_model.0.0.7.2.block.5.weight'),\n",
       " (True, 'base_model.0.0.7.2.block.5.bias'),\n",
       " (True, 'PreHead_0.weight'),\n",
       " (True, 'PreHead_0.bias'),\n",
       " (True, 'PreHead_1.weight'),\n",
       " (True, 'PreHead_1.bias'),\n",
       " (True, 'PostHead_0.weight'),\n",
       " (True, 'PostHead_0.bias')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_layers_requiresGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save run outputs\n",
    "\n",
    "## The training step is written so that it can run until a job ends, so this needs to be saved before\n",
    "\n",
    "run_outputs = {\n",
    "    'dir_save': str(dir_save),    \n",
    "    'path_save_runOutputs': str(dir_save / 'run_outputs.json'),    \n",
    "    'path_saveModel': str(path_saveModel),\n",
    "    'path_saveLog': str(path_saveLog),\n",
    "    'path_saveLoss': str(path_saveLoss),\n",
    "    'device_train': device_train,\n",
    "    'masks_training_shape': list(masks_cat.shape),\n",
    "    'n_masks_removed': int(n_masks_removed),\n",
    "    'image_resized_shape': list(dataset_train[0][0][0].shape),\n",
    "    'names_layers_requiresGrad': names_layers_requiresGrad,\n",
    "}\n",
    "run_outputs;\n",
    "\n",
    "import json\n",
    "with open(run_outputs['path_save_runOutputs'], 'w') as f:\n",
    "    json.dump(run_outputs, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "outputs": [],
   "source": [
    "### Training\n",
    "\n",
    "model.to(device_train)\n",
    "model.prep_contrast()\n",
    "model.forward = model.forward_latent\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "    lr=params['lr'],\n",
    "#     lr=1*10**-2,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=params['gamma'],\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(device_train) for _ in criterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ27o1ny9Xfi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                               | 0/9999999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/3021, loss_train: 7.6234, loss_val: nan, pos_over_neg: 1.0014119148254395 lr: 0.005\n",
      "Iter: 1/3021, loss_train: 7.6995, loss_val: nan, pos_over_neg: 1.0075384378433228 lr: 0.005\n",
      "Iter: 2/3021, loss_train: 7.6254, loss_val: nan, pos_over_neg: 1.0013622045516968 lr: 0.005\n",
      "Iter: 3/3021, loss_train: 7.609, loss_val: nan, pos_over_neg: 1.0089548826217651 lr: 0.005\n",
      "Iter: 4/3021, loss_train: 7.4757, loss_val: nan, pos_over_neg: 1.339573621749878 lr: 0.005\n",
      "Iter: 5/3021, loss_train: 7.6604, loss_val: nan, pos_over_neg: 0.9991263747215271 lr: 0.005\n",
      "Iter: 6/3021, loss_train: 7.6107, loss_val: nan, pos_over_neg: 1.0099250078201294 lr: 0.005\n",
      "Iter: 7/3021, loss_train: 7.6101, loss_val: nan, pos_over_neg: 1.0086206197738647 lr: 0.005\n",
      "Iter: 8/3021, loss_train: 7.6032, loss_val: nan, pos_over_neg: 1.0129486322402954 lr: 0.005\n",
      "Iter: 9/3021, loss_train: 7.5149, loss_val: nan, pos_over_neg: 1.0658153295516968 lr: 0.005\n",
      "Iter: 10/3021, loss_train: 7.3777, loss_val: nan, pos_over_neg: 1.2259445190429688 lr: 0.005\n",
      "Iter: 11/3021, loss_train: 7.4531, loss_val: nan, pos_over_neg: 1.3706977367401123 lr: 0.005\n",
      "Iter: 12/3021, loss_train: 7.3058, loss_val: nan, pos_over_neg: 1.6497983932495117 lr: 0.005\n",
      "Iter: 13/3021, loss_train: 7.2386, loss_val: nan, pos_over_neg: 1.6614289283752441 lr: 0.005\n",
      "Iter: 14/3021, loss_train: 7.2767, loss_val: nan, pos_over_neg: 1.6812878847122192 lr: 0.005\n",
      "Iter: 15/3021, loss_train: 7.1571, loss_val: nan, pos_over_neg: 2.4553029537200928 lr: 0.005\n",
      "Iter: 16/3021, loss_train: 7.2127, loss_val: nan, pos_over_neg: 3.3078877925872803 lr: 0.005\n",
      "Iter: 17/3021, loss_train: 7.1436, loss_val: nan, pos_over_neg: 3.249767780303955 lr: 0.005\n",
      "Iter: 18/3021, loss_train: 7.1705, loss_val: nan, pos_over_neg: 2.599964141845703 lr: 0.005\n",
      "Iter: 19/3021, loss_train: 7.0769, loss_val: nan, pos_over_neg: 4.5148138999938965 lr: 0.005\n",
      "Iter: 20/3021, loss_train: 7.0927, loss_val: nan, pos_over_neg: 8.318370819091797 lr: 0.005\n",
      "Iter: 21/3021, loss_train: 7.0749, loss_val: nan, pos_over_neg: 8.9671630859375 lr: 0.005\n",
      "Iter: 22/3021, loss_train: 7.0529, loss_val: nan, pos_over_neg: 6.2268195152282715 lr: 0.005\n",
      "Iter: 23/3021, loss_train: 7.0266, loss_val: nan, pos_over_neg: 3.927631378173828 lr: 0.005\n",
      "Iter: 24/3021, loss_train: 7.0751, loss_val: nan, pos_over_neg: 3.892305850982666 lr: 0.005\n",
      "Iter: 25/3021, loss_train: 7.0134, loss_val: nan, pos_over_neg: 5.270881175994873 lr: 0.005\n",
      "Iter: 26/3021, loss_train: 6.9999, loss_val: nan, pos_over_neg: 8.187849998474121 lr: 0.005\n",
      "Iter: 27/3021, loss_train: 6.9745, loss_val: nan, pos_over_neg: 6.779975414276123 lr: 0.005\n",
      "Iter: 28/3021, loss_train: 6.9723, loss_val: nan, pos_over_neg: 5.095625400543213 lr: 0.005\n",
      "Iter: 29/3021, loss_train: 6.9585, loss_val: nan, pos_over_neg: 6.254696846008301 lr: 0.005\n",
      "Iter: 30/3021, loss_train: 6.9493, loss_val: nan, pos_over_neg: 9.145079612731934 lr: 0.005\n",
      "Iter: 31/3021, loss_train: 6.951, loss_val: nan, pos_over_neg: 10.853986740112305 lr: 0.005\n",
      "Iter: 32/3021, loss_train: 6.9257, loss_val: nan, pos_over_neg: 7.8181304931640625 lr: 0.005\n",
      "Iter: 33/3021, loss_train: 6.9093, loss_val: nan, pos_over_neg: 8.117621421813965 lr: 0.005\n",
      "Iter: 34/3021, loss_train: 6.9026, loss_val: nan, pos_over_neg: 24.7810115814209 lr: 0.005\n",
      "Iter: 35/3021, loss_train: 6.9206, loss_val: nan, pos_over_neg: 32.657291412353516 lr: 0.005\n",
      "Iter: 36/3021, loss_train: 6.8609, loss_val: nan, pos_over_neg: 25.207796096801758 lr: 0.005\n",
      "Iter: 37/3021, loss_train: 6.9137, loss_val: nan, pos_over_neg: 9.788631439208984 lr: 0.005\n",
      "Iter: 38/3021, loss_train: 6.8944, loss_val: nan, pos_over_neg: 16.544706344604492 lr: 0.005\n",
      "Iter: 39/3021, loss_train: 6.9292, loss_val: nan, pos_over_neg: 89.22954559326172 lr: 0.005\n",
      "Iter: 40/3021, loss_train: 6.8909, loss_val: nan, pos_over_neg: 25.735076904296875 lr: 0.005\n",
      "Iter: 41/3021, loss_train: 6.9251, loss_val: nan, pos_over_neg: 5.701947212219238 lr: 0.005\n",
      "Iter: 42/3021, loss_train: 6.8899, loss_val: nan, pos_over_neg: 6.915005207061768 lr: 0.005\n",
      "Iter: 43/3021, loss_train: 6.8717, loss_val: nan, pos_over_neg: 24.796607971191406 lr: 0.005\n",
      "Iter: 44/3021, loss_train: 6.8498, loss_val: nan, pos_over_neg: 126.80731964111328 lr: 0.005\n",
      "Iter: 45/3021, loss_train: 6.8424, loss_val: nan, pos_over_neg: 62.1352424621582 lr: 0.005\n",
      "Iter: 46/3021, loss_train: 6.8738, loss_val: nan, pos_over_neg: 19.28673553466797 lr: 0.005\n",
      "Iter: 47/3021, loss_train: 6.8527, loss_val: nan, pos_over_neg: 20.127843856811523 lr: 0.005\n",
      "Iter: 48/3021, loss_train: 6.8764, loss_val: nan, pos_over_neg: 30.219013214111328 lr: 0.005\n",
      "Iter: 49/3021, loss_train: 6.8703, loss_val: nan, pos_over_neg: 19.714284896850586 lr: 0.005\n",
      "Iter: 50/3021, loss_train: 6.8152, loss_val: nan, pos_over_neg: 17.91923713684082 lr: 0.005\n",
      "Iter: 51/3021, loss_train: 6.8176, loss_val: nan, pos_over_neg: 15.561943054199219 lr: 0.005\n",
      "Iter: 52/3021, loss_train: 6.8356, loss_val: nan, pos_over_neg: 34.927921295166016 lr: 0.005\n",
      "Iter: 53/3021, loss_train: 6.839, loss_val: nan, pos_over_neg: 33.21874237060547 lr: 0.005\n",
      "Iter: 54/3021, loss_train: 6.8406, loss_val: nan, pos_over_neg: 30.989665985107422 lr: 0.005\n",
      "Iter: 55/3021, loss_train: 6.8369, loss_val: nan, pos_over_neg: 90.63933563232422 lr: 0.005\n",
      "Iter: 56/3021, loss_train: 6.8429, loss_val: nan, pos_over_neg: 9.967479705810547 lr: 0.005\n",
      "Iter: 57/3021, loss_train: 6.8209, loss_val: nan, pos_over_neg: 13.801436424255371 lr: 0.005\n",
      "Iter: 58/3021, loss_train: 6.8252, loss_val: nan, pos_over_neg: 32.48649978637695 lr: 0.005\n",
      "Iter: 59/3021, loss_train: 6.8206, loss_val: nan, pos_over_neg: 124.46537780761719 lr: 0.005\n",
      "Iter: 60/3021, loss_train: 6.7949, loss_val: nan, pos_over_neg: 24.276348114013672 lr: 0.005\n",
      "Iter: 61/3021, loss_train: 6.7991, loss_val: nan, pos_over_neg: 31.800085067749023 lr: 0.005\n",
      "Iter: 62/3021, loss_train: 6.8196, loss_val: nan, pos_over_neg: 85.4085922241211 lr: 0.005\n",
      "Iter: 63/3021, loss_train: 6.842, loss_val: nan, pos_over_neg: 73.47928619384766 lr: 0.005\n",
      "Iter: 64/3021, loss_train: 6.8031, loss_val: nan, pos_over_neg: 30.237590789794922 lr: 0.005\n",
      "Iter: 65/3021, loss_train: 6.8121, loss_val: nan, pos_over_neg: 20.57511329650879 lr: 0.005\n",
      "Iter: 66/3021, loss_train: 6.8112, loss_val: nan, pos_over_neg: 20.547077178955078 lr: 0.005\n",
      "Iter: 67/3021, loss_train: 6.796, loss_val: nan, pos_over_neg: 20.5573787689209 lr: 0.005\n",
      "Iter: 68/3021, loss_train: 6.8084, loss_val: nan, pos_over_neg: 21.393512725830078 lr: 0.005\n",
      "Iter: 69/3021, loss_train: 6.8335, loss_val: nan, pos_over_neg: 26.09358024597168 lr: 0.005\n",
      "Iter: 70/3021, loss_train: 6.8214, loss_val: nan, pos_over_neg: 65.40471649169922 lr: 0.005\n",
      "Iter: 71/3021, loss_train: 6.8271, loss_val: nan, pos_over_neg: 70.29733276367188 lr: 0.005\n",
      "Iter: 72/3021, loss_train: 6.7838, loss_val: nan, pos_over_neg: 18.03687286376953 lr: 0.005\n",
      "Iter: 73/3021, loss_train: 6.7749, loss_val: nan, pos_over_neg: 17.211275100708008 lr: 0.005\n",
      "Iter: 74/3021, loss_train: 6.7984, loss_val: nan, pos_over_neg: 83.89199829101562 lr: 0.005\n",
      "Iter: 75/3021, loss_train: 6.8329, loss_val: nan, pos_over_neg: 112.98320770263672 lr: 0.005\n",
      "Iter: 76/3021, loss_train: 6.7869, loss_val: nan, pos_over_neg: 33.518619537353516 lr: 0.005\n",
      "Iter: 77/3021, loss_train: 6.7771, loss_val: nan, pos_over_neg: 15.21240234375 lr: 0.005\n",
      "Iter: 78/3021, loss_train: 6.7978, loss_val: nan, pos_over_neg: 17.129840850830078 lr: 0.005\n",
      "Iter: 79/3021, loss_train: 6.8329, loss_val: nan, pos_over_neg: 36.258235931396484 lr: 0.005\n",
      "Iter: 80/3021, loss_train: 6.8, loss_val: nan, pos_over_neg: 269.3336486816406 lr: 0.005\n",
      "Iter: 81/3021, loss_train: 6.8022, loss_val: nan, pos_over_neg: 52.20427703857422 lr: 0.005\n",
      "Iter: 82/3021, loss_train: 6.771, loss_val: nan, pos_over_neg: 13.886643409729004 lr: 0.005\n",
      "Iter: 83/3021, loss_train: 6.7547, loss_val: nan, pos_over_neg: 23.19707489013672 lr: 0.005\n",
      "Iter: 84/3021, loss_train: 6.7846, loss_val: nan, pos_over_neg: 27.443403244018555 lr: 0.005\n",
      "Iter: 85/3021, loss_train: 6.7923, loss_val: nan, pos_over_neg: 72.976318359375 lr: 0.005\n",
      "Iter: 86/3021, loss_train: 6.7989, loss_val: nan, pos_over_neg: 65.98827362060547 lr: 0.005\n",
      "Iter: 87/3021, loss_train: 6.7991, loss_val: nan, pos_over_neg: 38.55025863647461 lr: 0.005\n",
      "Iter: 88/3021, loss_train: 6.7837, loss_val: nan, pos_over_neg: 74.69367980957031 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 89/3021, loss_train: 6.7717, loss_val: nan, pos_over_neg: 32.65091323852539 lr: 0.005\n",
      "Iter: 90/3021, loss_train: 6.7887, loss_val: nan, pos_over_neg: 114.59394073486328 lr: 0.005\n",
      "Iter: 91/3021, loss_train: 6.7763, loss_val: nan, pos_over_neg: 39.343387603759766 lr: 0.005\n",
      "Iter: 92/3021, loss_train: 6.7788, loss_val: nan, pos_over_neg: 8.511529922485352 lr: 0.005\n",
      "Iter: 93/3021, loss_train: 6.7341, loss_val: nan, pos_over_neg: 36.150291442871094 lr: 0.005\n",
      "Iter: 94/3021, loss_train: 6.7606, loss_val: nan, pos_over_neg: 60.49471664428711 lr: 0.005\n",
      "Iter: 95/3021, loss_train: 6.764, loss_val: nan, pos_over_neg: 180.05670166015625 lr: 0.005\n",
      "Iter: 96/3021, loss_train: 6.7597, loss_val: nan, pos_over_neg: 114.0985336303711 lr: 0.005\n",
      "Iter: 97/3021, loss_train: 6.7565, loss_val: nan, pos_over_neg: 20.483449935913086 lr: 0.005\n",
      "Iter: 98/3021, loss_train: 6.7774, loss_val: nan, pos_over_neg: 25.387393951416016 lr: 0.005\n",
      "Iter: 99/3021, loss_train: 6.7692, loss_val: nan, pos_over_neg: 65.76619720458984 lr: 0.005\n",
      "Iter: 100/3021, loss_train: 6.8153, loss_val: nan, pos_over_neg: 136.5393524169922 lr: 0.005\n",
      "Iter: 101/3021, loss_train: 6.7912, loss_val: nan, pos_over_neg: 30.716506958007812 lr: 0.005\n",
      "Iter: 102/3021, loss_train: 6.7836, loss_val: nan, pos_over_neg: 18.197465896606445 lr: 0.005\n",
      "Iter: 103/3021, loss_train: 6.7683, loss_val: nan, pos_over_neg: 15.311384201049805 lr: 0.005\n",
      "Iter: 104/3021, loss_train: 6.7967, loss_val: nan, pos_over_neg: 25.824687957763672 lr: 0.005\n",
      "Iter: 105/3021, loss_train: 6.79, loss_val: nan, pos_over_neg: 48.51321792602539 lr: 0.005\n",
      "Iter: 106/3021, loss_train: 6.7158, loss_val: nan, pos_over_neg: 30.825742721557617 lr: 0.005\n",
      "Iter: 107/3021, loss_train: 6.7449, loss_val: nan, pos_over_neg: 44.544525146484375 lr: 0.005\n",
      "Iter: 108/3021, loss_train: 6.7501, loss_val: nan, pos_over_neg: 75.79341888427734 lr: 0.005\n",
      "Iter: 109/3021, loss_train: 6.7716, loss_val: nan, pos_over_neg: 72.02241516113281 lr: 0.005\n",
      "Iter: 110/3021, loss_train: 6.7543, loss_val: nan, pos_over_neg: 42.6937370300293 lr: 0.005\n",
      "Iter: 111/3021, loss_train: 6.7315, loss_val: nan, pos_over_neg: 24.213483810424805 lr: 0.005\n",
      "Iter: 112/3021, loss_train: 6.7381, loss_val: nan, pos_over_neg: 21.297155380249023 lr: 0.005\n",
      "Iter: 113/3021, loss_train: 6.7553, loss_val: nan, pos_over_neg: 31.350780487060547 lr: 0.005\n",
      "Iter: 114/3021, loss_train: 6.7562, loss_val: nan, pos_over_neg: 58.242881774902344 lr: 0.005\n",
      "Iter: 115/3021, loss_train: 6.7253, loss_val: nan, pos_over_neg: 127.05570220947266 lr: 0.005\n",
      "Iter: 116/3021, loss_train: 6.757, loss_val: nan, pos_over_neg: 35.72759246826172 lr: 0.005\n",
      "Iter: 117/3021, loss_train: 6.7363, loss_val: nan, pos_over_neg: 55.03295135498047 lr: 0.005\n",
      "Iter: 118/3021, loss_train: 6.7541, loss_val: nan, pos_over_neg: 91.03873443603516 lr: 0.005\n",
      "Iter: 119/3021, loss_train: 6.7539, loss_val: nan, pos_over_neg: 67.98992156982422 lr: 0.005\n",
      "Iter: 120/3021, loss_train: 6.7708, loss_val: nan, pos_over_neg: 26.809934616088867 lr: 0.005\n",
      "Iter: 121/3021, loss_train: 6.7337, loss_val: nan, pos_over_neg: 45.5324592590332 lr: 0.005\n",
      "Iter: 122/3021, loss_train: 6.7299, loss_val: nan, pos_over_neg: 38.420467376708984 lr: 0.005\n",
      "Iter: 123/3021, loss_train: 6.7373, loss_val: nan, pos_over_neg: 45.391231536865234 lr: 0.005\n",
      "Iter: 124/3021, loss_train: 6.7768, loss_val: nan, pos_over_neg: 32.629188537597656 lr: 0.005\n",
      "Iter: 125/3021, loss_train: 6.7685, loss_val: nan, pos_over_neg: 81.96945190429688 lr: 0.005\n",
      "Iter: 126/3021, loss_train: 6.7316, loss_val: nan, pos_over_neg: 107.99268341064453 lr: 0.005\n",
      "Iter: 127/3021, loss_train: 6.7233, loss_val: nan, pos_over_neg: 108.318115234375 lr: 0.005\n",
      "Iter: 128/3021, loss_train: 6.7078, loss_val: nan, pos_over_neg: 71.98869323730469 lr: 0.005\n",
      "Iter: 129/3021, loss_train: 6.7341, loss_val: nan, pos_over_neg: 34.41007995605469 lr: 0.005\n",
      "Iter: 130/3021, loss_train: 6.7113, loss_val: nan, pos_over_neg: 31.499282836914062 lr: 0.005\n",
      "Iter: 131/3021, loss_train: 6.6894, loss_val: nan, pos_over_neg: 23.833269119262695 lr: 0.005\n",
      "Iter: 132/3021, loss_train: 6.6447, loss_val: nan, pos_over_neg: 46.03585433959961 lr: 0.005\n",
      "Iter: 133/3021, loss_train: 6.6698, loss_val: nan, pos_over_neg: 36.74613952636719 lr: 0.005\n",
      "Iter: 134/3021, loss_train: 6.6681, loss_val: nan, pos_over_neg: 58.413177490234375 lr: 0.005\n",
      "Iter: 135/3021, loss_train: 6.6663, loss_val: nan, pos_over_neg: 48.02354049682617 lr: 0.005\n",
      "Iter: 136/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 84.27426147460938 lr: 0.005\n",
      "Iter: 137/3021, loss_train: 6.6475, loss_val: nan, pos_over_neg: 28.8056640625 lr: 0.005\n",
      "Iter: 138/3021, loss_train: 6.6099, loss_val: nan, pos_over_neg: 72.98983764648438 lr: 0.005\n",
      "Iter: 139/3021, loss_train: 6.657, loss_val: nan, pos_over_neg: 49.394195556640625 lr: 0.005\n",
      "Iter: 140/3021, loss_train: 6.6336, loss_val: nan, pos_over_neg: 30.38931655883789 lr: 0.005\n",
      "Iter: 141/3021, loss_train: 6.6142, loss_val: nan, pos_over_neg: 34.698360443115234 lr: 0.005\n",
      "Iter: 142/3021, loss_train: 6.608, loss_val: nan, pos_over_neg: 160.76161193847656 lr: 0.005\n",
      "Iter: 143/3021, loss_train: 6.5954, loss_val: nan, pos_over_neg: 357.77294921875 lr: 0.005\n",
      "Iter: 144/3021, loss_train: 6.5925, loss_val: nan, pos_over_neg: 26.541597366333008 lr: 0.005\n",
      "Iter: 145/3021, loss_train: 6.571, loss_val: nan, pos_over_neg: 86.21200561523438 lr: 0.005\n",
      "Iter: 146/3021, loss_train: 6.6149, loss_val: nan, pos_over_neg: 55.6665153503418 lr: 0.005\n",
      "Iter: 147/3021, loss_train: 6.6091, loss_val: nan, pos_over_neg: 84.31470489501953 lr: 0.005\n",
      "Iter: 148/3021, loss_train: 6.623, loss_val: nan, pos_over_neg: 32.52756118774414 lr: 0.005\n",
      "Iter: 149/3021, loss_train: 6.5673, loss_val: nan, pos_over_neg: 92.69930267333984 lr: 0.005\n",
      "Iter: 150/3021, loss_train: 6.5746, loss_val: nan, pos_over_neg: 216.7814178466797 lr: 0.005\n",
      "Iter: 151/3021, loss_train: 6.5901, loss_val: nan, pos_over_neg: 58.03712844848633 lr: 0.005\n",
      "Iter: 152/3021, loss_train: 6.5412, loss_val: nan, pos_over_neg: 48.814239501953125 lr: 0.005\n",
      "Iter: 153/3021, loss_train: 6.5626, loss_val: nan, pos_over_neg: 30.18010902404785 lr: 0.005\n",
      "Iter: 154/3021, loss_train: 6.5706, loss_val: nan, pos_over_neg: 97.10284423828125 lr: 0.005\n",
      "Iter: 155/3021, loss_train: 6.5489, loss_val: nan, pos_over_neg: 315.3736267089844 lr: 0.005\n",
      "Iter: 156/3021, loss_train: 6.5548, loss_val: nan, pos_over_neg: 114.53845977783203 lr: 0.005\n",
      "Iter: 157/3021, loss_train: 6.5431, loss_val: nan, pos_over_neg: 70.58580780029297 lr: 0.005\n",
      "Iter: 158/3021, loss_train: 6.5361, loss_val: nan, pos_over_neg: 67.2261734008789 lr: 0.005\n",
      "Iter: 159/3021, loss_train: 6.5466, loss_val: nan, pos_over_neg: 121.2074966430664 lr: 0.005\n",
      "Iter: 160/3021, loss_train: 6.5521, loss_val: nan, pos_over_neg: 223.47743225097656 lr: 0.005\n",
      "Iter: 161/3021, loss_train: 6.5539, loss_val: nan, pos_over_neg: 35.817848205566406 lr: 0.005\n",
      "Iter: 162/3021, loss_train: 6.5328, loss_val: nan, pos_over_neg: 50.720550537109375 lr: 0.005\n",
      "Iter: 163/3021, loss_train: 6.5258, loss_val: nan, pos_over_neg: 44.01578903198242 lr: 0.005\n",
      "Iter: 164/3021, loss_train: 6.5236, loss_val: nan, pos_over_neg: 66.93896484375 lr: 0.005\n",
      "Iter: 165/3021, loss_train: 6.5494, loss_val: nan, pos_over_neg: 74.00028228759766 lr: 0.005\n",
      "Iter: 166/3021, loss_train: 6.5253, loss_val: nan, pos_over_neg: 77.31857299804688 lr: 0.005\n",
      "Iter: 167/3021, loss_train: 6.5315, loss_val: nan, pos_over_neg: 42.6760139465332 lr: 0.005\n",
      "Iter: 168/3021, loss_train: 6.5267, loss_val: nan, pos_over_neg: 35.77903366088867 lr: 0.005\n",
      "Iter: 169/3021, loss_train: 6.5279, loss_val: nan, pos_over_neg: 67.8132553100586 lr: 0.005\n",
      "Iter: 170/3021, loss_train: 6.5367, loss_val: nan, pos_over_neg: 67.56440734863281 lr: 0.005\n",
      "Iter: 171/3021, loss_train: 6.5259, loss_val: nan, pos_over_neg: 43.695335388183594 lr: 0.005\n",
      "Iter: 172/3021, loss_train: 6.5467, loss_val: nan, pos_over_neg: 31.6124210357666 lr: 0.005\n",
      "Iter: 173/3021, loss_train: 6.5327, loss_val: nan, pos_over_neg: 47.398902893066406 lr: 0.005\n",
      "Iter: 174/3021, loss_train: 6.5289, loss_val: nan, pos_over_neg: 176.30296325683594 lr: 0.005\n",
      "Iter: 175/3021, loss_train: 6.5055, loss_val: nan, pos_over_neg: 393.93157958984375 lr: 0.005\n",
      "Iter: 176/3021, loss_train: 6.5039, loss_val: nan, pos_over_neg: 54.52951431274414 lr: 0.005\n",
      "Iter: 177/3021, loss_train: 6.5094, loss_val: nan, pos_over_neg: 41.33363342285156 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 178/3021, loss_train: 6.5131, loss_val: nan, pos_over_neg: 63.90556335449219 lr: 0.005\n",
      "Iter: 179/3021, loss_train: 6.4769, loss_val: nan, pos_over_neg: 386.8766784667969 lr: 0.005\n",
      "Iter: 180/3021, loss_train: 6.505, loss_val: nan, pos_over_neg: 98.10549926757812 lr: 0.005\n",
      "Iter: 181/3021, loss_train: 6.4876, loss_val: nan, pos_over_neg: 971.7208862304688 lr: 0.005\n",
      "Iter: 182/3021, loss_train: 6.4815, loss_val: nan, pos_over_neg: 53.3283805847168 lr: 0.005\n",
      "Iter: 183/3021, loss_train: 6.504, loss_val: nan, pos_over_neg: 124.21345520019531 lr: 0.005\n",
      "Iter: 184/3021, loss_train: 6.4999, loss_val: nan, pos_over_neg: 51.35084533691406 lr: 0.005\n",
      "Iter: 185/3021, loss_train: 6.4833, loss_val: nan, pos_over_neg: 84.83181762695312 lr: 0.005\n",
      "Iter: 186/3021, loss_train: 6.5286, loss_val: nan, pos_over_neg: 62.0367317199707 lr: 0.005\n",
      "Iter: 187/3021, loss_train: 6.479, loss_val: nan, pos_over_neg: 105.40966033935547 lr: 0.005\n",
      "Iter: 188/3021, loss_train: 6.4856, loss_val: nan, pos_over_neg: 182.22979736328125 lr: 0.005\n",
      "Iter: 189/3021, loss_train: 6.4665, loss_val: nan, pos_over_neg: 367.8060302734375 lr: 0.005\n",
      "Iter: 190/3021, loss_train: 6.469, loss_val: nan, pos_over_neg: 1269.3614501953125 lr: 0.005\n",
      "Iter: 191/3021, loss_train: 6.4897, loss_val: nan, pos_over_neg: 346.54803466796875 lr: 0.005\n",
      "Iter: 192/3021, loss_train: 6.4818, loss_val: nan, pos_over_neg: 61.552371978759766 lr: 0.005\n",
      "Iter: 193/3021, loss_train: 6.4862, loss_val: nan, pos_over_neg: 154.7575225830078 lr: 0.005\n",
      "Iter: 194/3021, loss_train: 6.4882, loss_val: nan, pos_over_neg: 372.89678955078125 lr: 0.005\n",
      "Iter: 195/3021, loss_train: 6.4743, loss_val: nan, pos_over_neg: 364.4648132324219 lr: 0.005\n",
      "Iter: 196/3021, loss_train: 6.4805, loss_val: nan, pos_over_neg: 144.09266662597656 lr: 0.005\n",
      "Iter: 197/3021, loss_train: 6.4632, loss_val: nan, pos_over_neg: 477.3169250488281 lr: 0.005\n",
      "Iter: 198/3021, loss_train: 6.4678, loss_val: nan, pos_over_neg: 124.76688385009766 lr: 0.005\n",
      "Iter: 199/3021, loss_train: 6.4664, loss_val: nan, pos_over_neg: 421.8525695800781 lr: 0.005\n",
      "Iter: 200/3021, loss_train: 6.4556, loss_val: nan, pos_over_neg: 283.2348327636719 lr: 0.005\n",
      "Iter: 201/3021, loss_train: 6.4466, loss_val: nan, pos_over_neg: 90.36929321289062 lr: 0.005\n",
      "Iter: 202/3021, loss_train: 6.4873, loss_val: nan, pos_over_neg: 171.08543395996094 lr: 0.005\n",
      "Iter: 203/3021, loss_train: 6.4613, loss_val: nan, pos_over_neg: 91.62147521972656 lr: 0.005\n",
      "Iter: 204/3021, loss_train: 6.4493, loss_val: nan, pos_over_neg: 83.64224243164062 lr: 0.005\n",
      "Iter: 205/3021, loss_train: 6.4751, loss_val: nan, pos_over_neg: 50.30927658081055 lr: 0.005\n",
      "Iter: 206/3021, loss_train: 6.4568, loss_val: nan, pos_over_neg: 52.626434326171875 lr: 0.005\n",
      "Iter: 207/3021, loss_train: 6.4792, loss_val: nan, pos_over_neg: 3924.414794921875 lr: 0.005\n",
      "Iter: 208/3021, loss_train: 6.5167, loss_val: nan, pos_over_neg: 41.08775329589844 lr: 0.005\n",
      "Iter: 209/3021, loss_train: 6.4637, loss_val: nan, pos_over_neg: 47.905303955078125 lr: 0.005\n",
      "Iter: 210/3021, loss_train: 6.4584, loss_val: nan, pos_over_neg: 45.899314880371094 lr: 0.005\n",
      "Iter: 211/3021, loss_train: 6.499, loss_val: nan, pos_over_neg: 64.91993713378906 lr: 0.005\n",
      "Iter: 212/3021, loss_train: 6.4934, loss_val: nan, pos_over_neg: 293.37261962890625 lr: 0.005\n",
      "Iter: 213/3021, loss_train: 6.4658, loss_val: nan, pos_over_neg: 51.65974426269531 lr: 0.005\n",
      "Iter: 214/3021, loss_train: 6.463, loss_val: nan, pos_over_neg: 38.9481315612793 lr: 0.005\n",
      "Iter: 215/3021, loss_train: 6.4896, loss_val: nan, pos_over_neg: 54.889591217041016 lr: 0.005\n",
      "Iter: 216/3021, loss_train: 6.4726, loss_val: nan, pos_over_neg: 41.02629089355469 lr: 0.005\n",
      "Iter: 217/3021, loss_train: 6.4674, loss_val: nan, pos_over_neg: 114.05326843261719 lr: 0.005\n",
      "Iter: 218/3021, loss_train: 6.4404, loss_val: nan, pos_over_neg: 417.7835998535156 lr: 0.005\n",
      "Iter: 219/3021, loss_train: 6.4471, loss_val: nan, pos_over_neg: 70.21625518798828 lr: 0.005\n",
      "Iter: 220/3021, loss_train: 6.4753, loss_val: nan, pos_over_neg: 41.80958938598633 lr: 0.005\n",
      "Iter: 221/3021, loss_train: 6.4773, loss_val: nan, pos_over_neg: 100.1406478881836 lr: 0.005\n",
      "Iter: 222/3021, loss_train: 6.4776, loss_val: nan, pos_over_neg: 208.62171936035156 lr: 0.005\n",
      "Iter: 223/3021, loss_train: 6.4511, loss_val: nan, pos_over_neg: 431.39947509765625 lr: 0.005\n",
      "Iter: 224/3021, loss_train: 6.4373, loss_val: nan, pos_over_neg: 90.88873291015625 lr: 0.005\n",
      "Iter: 225/3021, loss_train: 6.443, loss_val: nan, pos_over_neg: 29.231719970703125 lr: 0.005\n",
      "Iter: 226/3021, loss_train: 6.4602, loss_val: nan, pos_over_neg: 57.67707443237305 lr: 0.005\n",
      "Iter: 227/3021, loss_train: 6.4284, loss_val: nan, pos_over_neg: 192.6178741455078 lr: 0.005\n",
      "Iter: 228/3021, loss_train: 6.4587, loss_val: nan, pos_over_neg: 106.04666900634766 lr: 0.005\n",
      "Iter: 229/3021, loss_train: 6.4612, loss_val: nan, pos_over_neg: 113.21623229980469 lr: 0.005\n",
      "Iter: 230/3021, loss_train: 6.4427, loss_val: nan, pos_over_neg: 120.68062591552734 lr: 0.005\n",
      "Iter: 231/3021, loss_train: 6.4422, loss_val: nan, pos_over_neg: 69.2668685913086 lr: 0.005\n",
      "Iter: 232/3021, loss_train: 6.4181, loss_val: nan, pos_over_neg: 340.46173095703125 lr: 0.005\n",
      "Iter: 233/3021, loss_train: 6.4173, loss_val: nan, pos_over_neg: 138.19093322753906 lr: 0.005\n",
      "Iter: 234/3021, loss_train: 6.4266, loss_val: nan, pos_over_neg: 81.01753234863281 lr: 0.005\n",
      "Iter: 235/3021, loss_train: 6.4451, loss_val: nan, pos_over_neg: 79.88286590576172 lr: 0.005\n",
      "Iter: 236/3021, loss_train: 6.432, loss_val: nan, pos_over_neg: 89.30613708496094 lr: 0.005\n",
      "Iter: 237/3021, loss_train: 6.4466, loss_val: nan, pos_over_neg: 40.3118896484375 lr: 0.005\n",
      "Iter: 238/3021, loss_train: 6.4349, loss_val: nan, pos_over_neg: 101.2473373413086 lr: 0.005\n",
      "Iter: 239/3021, loss_train: 6.4519, loss_val: nan, pos_over_neg: 80.70484161376953 lr: 0.005\n",
      "Iter: 240/3021, loss_train: 6.4427, loss_val: nan, pos_over_neg: 56.66227340698242 lr: 0.005\n",
      "Iter: 241/3021, loss_train: 6.4238, loss_val: nan, pos_over_neg: 113.26256561279297 lr: 0.005\n",
      "Iter: 242/3021, loss_train: 6.4225, loss_val: nan, pos_over_neg: 153.98060607910156 lr: 0.005\n",
      "Iter: 243/3021, loss_train: 6.441, loss_val: nan, pos_over_neg: 82.83104705810547 lr: 0.005\n",
      "Iter: 244/3021, loss_train: 6.4349, loss_val: nan, pos_over_neg: 215.8414306640625 lr: 0.005\n",
      "Iter: 245/3021, loss_train: 6.4446, loss_val: nan, pos_over_neg: 71.26343536376953 lr: 0.005\n",
      "Iter: 246/3021, loss_train: 6.4384, loss_val: nan, pos_over_neg: 46.834808349609375 lr: 0.005\n",
      "Iter: 247/3021, loss_train: 6.4267, loss_val: nan, pos_over_neg: 104.53186798095703 lr: 0.005\n",
      "Iter: 248/3021, loss_train: 6.4342, loss_val: nan, pos_over_neg: 170.2896728515625 lr: 0.005\n",
      "Iter: 249/3021, loss_train: 6.4275, loss_val: nan, pos_over_neg: 188.7281494140625 lr: 0.005\n",
      "Iter: 250/3021, loss_train: 6.4449, loss_val: nan, pos_over_neg: 120.1809310913086 lr: 0.005\n",
      "Iter: 251/3021, loss_train: 6.4454, loss_val: nan, pos_over_neg: 145.3831787109375 lr: 0.005\n",
      "Iter: 252/3021, loss_train: 6.43, loss_val: nan, pos_over_neg: 248.58523559570312 lr: 0.005\n",
      "Iter: 253/3021, loss_train: 6.4358, loss_val: nan, pos_over_neg: 65.47396850585938 lr: 0.005\n",
      "Iter: 254/3021, loss_train: 6.4358, loss_val: nan, pos_over_neg: 39.388160705566406 lr: 0.005\n",
      "Iter: 255/3021, loss_train: 6.3937, loss_val: nan, pos_over_neg: 50.32878875732422 lr: 0.005\n",
      "Iter: 256/3021, loss_train: 6.423, loss_val: nan, pos_over_neg: 688.6966552734375 lr: 0.005\n",
      "Iter: 257/3021, loss_train: 6.4314, loss_val: nan, pos_over_neg: 170.39891052246094 lr: 0.005\n",
      "Iter: 258/3021, loss_train: 6.4309, loss_val: nan, pos_over_neg: 294.8248596191406 lr: 0.005\n",
      "Iter: 259/3021, loss_train: 6.4021, loss_val: nan, pos_over_neg: 125.1771469116211 lr: 0.005\n",
      "Iter: 260/3021, loss_train: 6.4127, loss_val: nan, pos_over_neg: 76.78219604492188 lr: 0.005\n",
      "Iter: 261/3021, loss_train: 6.4326, loss_val: nan, pos_over_neg: 79.77378845214844 lr: 0.005\n",
      "Iter: 262/3021, loss_train: 6.4351, loss_val: nan, pos_over_neg: 90.1539535522461 lr: 0.005\n",
      "Iter: 263/3021, loss_train: 6.3934, loss_val: nan, pos_over_neg: 363.6450500488281 lr: 0.005\n",
      "Iter: 264/3021, loss_train: 6.4023, loss_val: nan, pos_over_neg: 244.45875549316406 lr: 0.005\n",
      "Iter: 265/3021, loss_train: 6.4311, loss_val: nan, pos_over_neg: 66.88378143310547 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 266/3021, loss_train: 6.4233, loss_val: nan, pos_over_neg: 79.15790557861328 lr: 0.005\n",
      "Iter: 267/3021, loss_train: 6.4266, loss_val: nan, pos_over_neg: 115.12944793701172 lr: 0.005\n",
      "Iter: 268/3021, loss_train: 6.4255, loss_val: nan, pos_over_neg: 346.2503967285156 lr: 0.005\n",
      "Iter: 269/3021, loss_train: 6.4117, loss_val: nan, pos_over_neg: 95.34675598144531 lr: 0.005\n",
      "Iter: 270/3021, loss_train: 6.4208, loss_val: nan, pos_over_neg: 53.95091247558594 lr: 0.005\n",
      "Iter: 271/3021, loss_train: 6.4006, loss_val: nan, pos_over_neg: 206.87330627441406 lr: 0.005\n",
      "Iter: 272/3021, loss_train: 6.3857, loss_val: nan, pos_over_neg: -1386.2283935546875 lr: 0.005\n",
      "Iter: 273/3021, loss_train: 6.4192, loss_val: nan, pos_over_neg: 159.6041717529297 lr: 0.005\n",
      "Iter: 274/3021, loss_train: 6.4022, loss_val: nan, pos_over_neg: 89.94294738769531 lr: 0.005\n",
      "Iter: 275/3021, loss_train: 6.4233, loss_val: nan, pos_over_neg: 56.7406120300293 lr: 0.005\n",
      "Iter: 276/3021, loss_train: 6.4085, loss_val: nan, pos_over_neg: 38.78227615356445 lr: 0.005\n",
      "Iter: 277/3021, loss_train: 6.417, loss_val: nan, pos_over_neg: 225.50294494628906 lr: 0.005\n",
      "Iter: 278/3021, loss_train: 6.3998, loss_val: nan, pos_over_neg: 94.68910217285156 lr: 0.005\n",
      "Iter: 279/3021, loss_train: 6.3915, loss_val: nan, pos_over_neg: 220.93438720703125 lr: 0.005\n",
      "Iter: 280/3021, loss_train: 6.4163, loss_val: nan, pos_over_neg: 77.19951629638672 lr: 0.005\n",
      "Iter: 281/3021, loss_train: 6.4321, loss_val: nan, pos_over_neg: 31.697893142700195 lr: 0.005\n",
      "Iter: 282/3021, loss_train: 6.3986, loss_val: nan, pos_over_neg: 44.481014251708984 lr: 0.005\n",
      "Iter: 283/3021, loss_train: 6.4451, loss_val: nan, pos_over_neg: 71.84119415283203 lr: 0.005\n",
      "Iter: 284/3021, loss_train: 6.4141, loss_val: nan, pos_over_neg: 88.49095153808594 lr: 0.005\n",
      "Iter: 285/3021, loss_train: 6.4245, loss_val: nan, pos_over_neg: 52.58517837524414 lr: 0.005\n",
      "Iter: 286/3021, loss_train: 6.4639, loss_val: nan, pos_over_neg: 26.08051872253418 lr: 0.005\n",
      "Iter: 287/3021, loss_train: 6.3992, loss_val: nan, pos_over_neg: 41.139854431152344 lr: 0.005\n",
      "Iter: 288/3021, loss_train: 6.3949, loss_val: nan, pos_over_neg: 82.51679992675781 lr: 0.005\n",
      "Iter: 289/3021, loss_train: 6.441, loss_val: nan, pos_over_neg: 58.283119201660156 lr: 0.005\n",
      "Iter: 290/3021, loss_train: 6.4399, loss_val: nan, pos_over_neg: 139.95391845703125 lr: 0.005\n",
      "Iter: 291/3021, loss_train: 6.3969, loss_val: nan, pos_over_neg: 243.4108123779297 lr: 0.005\n",
      "Iter: 292/3021, loss_train: 6.3859, loss_val: nan, pos_over_neg: 91.30982208251953 lr: 0.005\n",
      "Iter: 293/3021, loss_train: 6.4384, loss_val: nan, pos_over_neg: 30.766983032226562 lr: 0.005\n",
      "Iter: 294/3021, loss_train: 6.4177, loss_val: nan, pos_over_neg: 43.192256927490234 lr: 0.005\n",
      "Iter: 295/3021, loss_train: 6.4217, loss_val: nan, pos_over_neg: 84.63044738769531 lr: 0.005\n",
      "Iter: 296/3021, loss_train: 6.4102, loss_val: nan, pos_over_neg: 147.2158203125 lr: 0.005\n",
      "Iter: 297/3021, loss_train: 6.4142, loss_val: nan, pos_over_neg: 129.6334991455078 lr: 0.005\n",
      "Iter: 298/3021, loss_train: 6.4104, loss_val: nan, pos_over_neg: 148.573974609375 lr: 0.005\n",
      "Iter: 299/3021, loss_train: 6.4053, loss_val: nan, pos_over_neg: 62.020145416259766 lr: 0.005\n",
      "Iter: 300/3021, loss_train: 6.415, loss_val: nan, pos_over_neg: 64.24992370605469 lr: 0.005\n",
      "Iter: 301/3021, loss_train: 6.4321, loss_val: nan, pos_over_neg: 43.258541107177734 lr: 0.005\n",
      "Iter: 302/3021, loss_train: 6.4071, loss_val: nan, pos_over_neg: 118.34446716308594 lr: 0.005\n",
      "Iter: 303/3021, loss_train: 6.4032, loss_val: nan, pos_over_neg: 101.64563751220703 lr: 0.005\n",
      "Iter: 304/3021, loss_train: 6.4079, loss_val: nan, pos_over_neg: 62.840213775634766 lr: 0.005\n",
      "Iter: 305/3021, loss_train: 6.4235, loss_val: nan, pos_over_neg: 123.51563262939453 lr: 0.005\n",
      "Iter: 306/3021, loss_train: 6.3898, loss_val: nan, pos_over_neg: 178.09312438964844 lr: 0.005\n",
      "Iter: 307/3021, loss_train: 6.4068, loss_val: nan, pos_over_neg: 82.49334716796875 lr: 0.005\n",
      "Iter: 308/3021, loss_train: 6.3862, loss_val: nan, pos_over_neg: 73.8804931640625 lr: 0.005\n",
      "Iter: 309/3021, loss_train: 6.3967, loss_val: nan, pos_over_neg: 45.93568801879883 lr: 0.005\n",
      "Iter: 310/3021, loss_train: 6.4025, loss_val: nan, pos_over_neg: 67.59880828857422 lr: 0.005\n",
      "Iter: 311/3021, loss_train: 6.3691, loss_val: nan, pos_over_neg: 482.232421875 lr: 0.005\n",
      "Iter: 312/3021, loss_train: 6.3943, loss_val: nan, pos_over_neg: 123.61210632324219 lr: 0.005\n",
      "Iter: 313/3021, loss_train: 6.4111, loss_val: nan, pos_over_neg: 82.41867065429688 lr: 0.005\n",
      "Iter: 314/3021, loss_train: 6.4008, loss_val: nan, pos_over_neg: 73.69729614257812 lr: 0.005\n",
      "Iter: 315/3021, loss_train: 6.3878, loss_val: nan, pos_over_neg: 174.988037109375 lr: 0.005\n",
      "Iter: 316/3021, loss_train: 6.3874, loss_val: nan, pos_over_neg: 159.30874633789062 lr: 0.005\n",
      "Iter: 317/3021, loss_train: 6.3966, loss_val: nan, pos_over_neg: 96.84009552001953 lr: 0.005\n",
      "Iter: 318/3021, loss_train: 6.3773, loss_val: nan, pos_over_neg: 809.6714477539062 lr: 0.005\n",
      "Iter: 319/3021, loss_train: 6.36, loss_val: nan, pos_over_neg: 322.52197265625 lr: 0.005\n",
      "Iter: 320/3021, loss_train: 6.3782, loss_val: nan, pos_over_neg: 297.4371337890625 lr: 0.005\n",
      "Iter: 321/3021, loss_train: 6.3756, loss_val: nan, pos_over_neg: 86.23104858398438 lr: 0.005\n",
      "Iter: 322/3021, loss_train: 6.3945, loss_val: nan, pos_over_neg: 61.399993896484375 lr: 0.005\n",
      "Iter: 323/3021, loss_train: 6.4189, loss_val: nan, pos_over_neg: 62.3300666809082 lr: 0.005\n",
      "Iter: 324/3021, loss_train: 6.3981, loss_val: nan, pos_over_neg: 68.25440216064453 lr: 0.005\n",
      "Iter: 325/3021, loss_train: 6.3938, loss_val: nan, pos_over_neg: 141.1295623779297 lr: 0.005\n",
      "Iter: 326/3021, loss_train: 6.4103, loss_val: nan, pos_over_neg: 209.9698944091797 lr: 0.005\n",
      "Iter: 327/3021, loss_train: 6.3984, loss_val: nan, pos_over_neg: 87.40886688232422 lr: 0.005\n",
      "Iter: 328/3021, loss_train: 6.3716, loss_val: nan, pos_over_neg: 695.446044921875 lr: 0.005\n",
      "Iter: 329/3021, loss_train: 6.3871, loss_val: nan, pos_over_neg: 64.2538070678711 lr: 0.005\n",
      "Iter: 330/3021, loss_train: 6.374, loss_val: nan, pos_over_neg: 459.2799377441406 lr: 0.005\n",
      "Iter: 331/3021, loss_train: 6.3881, loss_val: nan, pos_over_neg: 122.6441421508789 lr: 0.005\n",
      "Iter: 332/3021, loss_train: 6.3823, loss_val: nan, pos_over_neg: 196.4600372314453 lr: 0.005\n",
      "Iter: 333/3021, loss_train: 6.3772, loss_val: nan, pos_over_neg: 81.46273040771484 lr: 0.005\n",
      "Iter: 334/3021, loss_train: 6.3782, loss_val: nan, pos_over_neg: 85.99775695800781 lr: 0.005\n",
      "Iter: 335/3021, loss_train: 6.3802, loss_val: nan, pos_over_neg: 146.61590576171875 lr: 0.005\n",
      "Iter: 336/3021, loss_train: 6.3676, loss_val: nan, pos_over_neg: 168.40956115722656 lr: 0.005\n",
      "Iter: 337/3021, loss_train: 6.377, loss_val: nan, pos_over_neg: 117.50054931640625 lr: 0.005\n",
      "Iter: 338/3021, loss_train: 6.3831, loss_val: nan, pos_over_neg: 49.33854293823242 lr: 0.005\n",
      "Iter: 339/3021, loss_train: 6.392, loss_val: nan, pos_over_neg: 32.690425872802734 lr: 0.005\n",
      "Iter: 340/3021, loss_train: 6.3735, loss_val: nan, pos_over_neg: 75.78902435302734 lr: 0.005\n",
      "Iter: 341/3021, loss_train: 6.3846, loss_val: nan, pos_over_neg: 47.54232406616211 lr: 0.005\n",
      "Iter: 342/3021, loss_train: 6.383, loss_val: nan, pos_over_neg: 236.529296875 lr: 0.005\n",
      "Iter: 343/3021, loss_train: 6.3743, loss_val: nan, pos_over_neg: 50.0485954284668 lr: 0.005\n",
      "Iter: 344/3021, loss_train: 6.37, loss_val: nan, pos_over_neg: 46.92353057861328 lr: 0.005\n",
      "Iter: 345/3021, loss_train: 6.3873, loss_val: nan, pos_over_neg: 90.70333862304688 lr: 0.005\n",
      "Iter: 346/3021, loss_train: 6.3838, loss_val: nan, pos_over_neg: 113.42289733886719 lr: 0.005\n",
      "Iter: 347/3021, loss_train: 6.3635, loss_val: nan, pos_over_neg: 69.67958068847656 lr: 0.005\n",
      "Iter: 348/3021, loss_train: 6.3508, loss_val: nan, pos_over_neg: 147.67991638183594 lr: 0.005\n",
      "Iter: 349/3021, loss_train: 6.3809, loss_val: nan, pos_over_neg: 54.86189651489258 lr: 0.005\n",
      "Iter: 350/3021, loss_train: 6.3674, loss_val: nan, pos_over_neg: 62.38333511352539 lr: 0.005\n",
      "Iter: 351/3021, loss_train: 6.3681, loss_val: nan, pos_over_neg: 81.38465118408203 lr: 0.005\n",
      "Iter: 352/3021, loss_train: 6.3765, loss_val: nan, pos_over_neg: 102.9979248046875 lr: 0.005\n",
      "Iter: 353/3021, loss_train: 6.3752, loss_val: nan, pos_over_neg: 138.49090576171875 lr: 0.005\n",
      "Iter: 354/3021, loss_train: 6.3812, loss_val: nan, pos_over_neg: 93.31983947753906 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 355/3021, loss_train: 6.3442, loss_val: nan, pos_over_neg: 89.98515319824219 lr: 0.005\n",
      "Iter: 356/3021, loss_train: 6.3671, loss_val: nan, pos_over_neg: 72.8331298828125 lr: 0.005\n",
      "Iter: 357/3021, loss_train: 6.3808, loss_val: nan, pos_over_neg: 49.53177261352539 lr: 0.005\n",
      "Iter: 358/3021, loss_train: 6.3899, loss_val: nan, pos_over_neg: 116.06730651855469 lr: 0.005\n",
      "Iter: 359/3021, loss_train: 6.3935, loss_val: nan, pos_over_neg: 107.86182403564453 lr: 0.005\n",
      "Iter: 360/3021, loss_train: 6.3969, loss_val: nan, pos_over_neg: 99.0728988647461 lr: 0.005\n",
      "Iter: 361/3021, loss_train: 6.377, loss_val: nan, pos_over_neg: 179.530029296875 lr: 0.005\n",
      "Iter: 362/3021, loss_train: 6.3456, loss_val: nan, pos_over_neg: 89.26646423339844 lr: 0.005\n",
      "Iter: 363/3021, loss_train: 6.3951, loss_val: nan, pos_over_neg: 69.73420715332031 lr: 0.005\n",
      "Iter: 364/3021, loss_train: 6.3477, loss_val: nan, pos_over_neg: 462.7091369628906 lr: 0.005\n",
      "Iter: 365/3021, loss_train: 6.3776, loss_val: nan, pos_over_neg: 63.883522033691406 lr: 0.005\n",
      "Iter: 366/3021, loss_train: 6.3831, loss_val: nan, pos_over_neg: 91.11078643798828 lr: 0.005\n",
      "Iter: 367/3021, loss_train: 6.36, loss_val: nan, pos_over_neg: 435.7890319824219 lr: 0.005\n",
      "Iter: 368/3021, loss_train: 6.3846, loss_val: nan, pos_over_neg: 159.2725067138672 lr: 0.005\n",
      "Iter: 369/3021, loss_train: 6.3694, loss_val: nan, pos_over_neg: 123.51686096191406 lr: 0.005\n",
      "Iter: 370/3021, loss_train: 6.3607, loss_val: nan, pos_over_neg: 47.200557708740234 lr: 0.005\n",
      "Iter: 371/3021, loss_train: 6.3693, loss_val: nan, pos_over_neg: 56.15275573730469 lr: 0.005\n",
      "Iter: 372/3021, loss_train: 6.3629, loss_val: nan, pos_over_neg: 95.21931457519531 lr: 0.005\n",
      "Iter: 373/3021, loss_train: 6.3653, loss_val: nan, pos_over_neg: 123.28900146484375 lr: 0.005\n",
      "Iter: 374/3021, loss_train: 6.3808, loss_val: nan, pos_over_neg: 31.82652473449707 lr: 0.005\n",
      "Iter: 375/3021, loss_train: 6.3663, loss_val: nan, pos_over_neg: 222.4011993408203 lr: 0.005\n",
      "Iter: 376/3021, loss_train: 6.3544, loss_val: nan, pos_over_neg: 277.3431701660156 lr: 0.005\n",
      "Iter: 377/3021, loss_train: 6.3579, loss_val: nan, pos_over_neg: 197.3880157470703 lr: 0.005\n",
      "Iter: 378/3021, loss_train: 6.3419, loss_val: nan, pos_over_neg: 65.0362319946289 lr: 0.005\n",
      "Iter: 379/3021, loss_train: 6.3472, loss_val: nan, pos_over_neg: 56.24142837524414 lr: 0.005\n",
      "Iter: 380/3021, loss_train: 6.3506, loss_val: nan, pos_over_neg: 267.1556091308594 lr: 0.005\n",
      "Iter: 381/3021, loss_train: 6.3538, loss_val: nan, pos_over_neg: 351.96221923828125 lr: 0.005\n",
      "Iter: 382/3021, loss_train: 6.3569, loss_val: nan, pos_over_neg: 94.15203094482422 lr: 0.005\n",
      "Iter: 383/3021, loss_train: 6.3616, loss_val: nan, pos_over_neg: 61.34379196166992 lr: 0.005\n",
      "Iter: 384/3021, loss_train: 6.3566, loss_val: nan, pos_over_neg: 64.72518157958984 lr: 0.005\n",
      "Iter: 385/3021, loss_train: 6.3699, loss_val: nan, pos_over_neg: 100.39380645751953 lr: 0.005\n",
      "Iter: 386/3021, loss_train: 6.3601, loss_val: nan, pos_over_neg: 259.87835693359375 lr: 0.005\n",
      "Iter: 387/3021, loss_train: 6.3502, loss_val: nan, pos_over_neg: 146.4817352294922 lr: 0.005\n",
      "Iter: 388/3021, loss_train: 6.3707, loss_val: nan, pos_over_neg: 64.07682800292969 lr: 0.005\n",
      "Iter: 389/3021, loss_train: 6.3476, loss_val: nan, pos_over_neg: 168.883544921875 lr: 0.005\n",
      "Iter: 390/3021, loss_train: 6.3837, loss_val: nan, pos_over_neg: 86.4381103515625 lr: 0.005\n",
      "Iter: 391/3021, loss_train: 6.3623, loss_val: nan, pos_over_neg: 66.18429565429688 lr: 0.005\n",
      "Iter: 392/3021, loss_train: 6.3706, loss_val: nan, pos_over_neg: 160.2082061767578 lr: 0.005\n",
      "Iter: 393/3021, loss_train: 6.3687, loss_val: nan, pos_over_neg: 365.902587890625 lr: 0.005\n",
      "Iter: 394/3021, loss_train: 6.3433, loss_val: nan, pos_over_neg: 133.84368896484375 lr: 0.005\n",
      "Iter: 395/3021, loss_train: 6.3304, loss_val: nan, pos_over_neg: 303.9984436035156 lr: 0.005\n",
      "Iter: 396/3021, loss_train: 6.3553, loss_val: nan, pos_over_neg: 76.81147003173828 lr: 0.005\n",
      "Iter: 397/3021, loss_train: 6.3805, loss_val: nan, pos_over_neg: 87.7232894897461 lr: 0.005\n",
      "Iter: 398/3021, loss_train: 6.3243, loss_val: nan, pos_over_neg: 94.63546752929688 lr: 0.005\n",
      "Iter: 399/3021, loss_train: 6.3723, loss_val: nan, pos_over_neg: 321.026123046875 lr: 0.005\n",
      "Iter: 400/3021, loss_train: 6.3538, loss_val: nan, pos_over_neg: 91.83246612548828 lr: 0.005\n",
      "Iter: 401/3021, loss_train: 6.3457, loss_val: nan, pos_over_neg: 111.2275390625 lr: 0.005\n",
      "Iter: 402/3021, loss_train: 6.3852, loss_val: nan, pos_over_neg: 32.22391128540039 lr: 0.005\n",
      "Iter: 403/3021, loss_train: 6.3464, loss_val: nan, pos_over_neg: 64.14498901367188 lr: 0.005\n",
      "Iter: 404/3021, loss_train: 6.3438, loss_val: nan, pos_over_neg: 190.3336944580078 lr: 0.005\n",
      "Iter: 405/3021, loss_train: 6.3601, loss_val: nan, pos_over_neg: 131.8747100830078 lr: 0.005\n",
      "Iter: 406/3021, loss_train: 6.3765, loss_val: nan, pos_over_neg: 88.41987609863281 lr: 0.005\n",
      "Iter: 407/3021, loss_train: 6.3366, loss_val: nan, pos_over_neg: 558.1915893554688 lr: 0.005\n",
      "Iter: 408/3021, loss_train: 6.3714, loss_val: nan, pos_over_neg: 144.1543731689453 lr: 0.005\n",
      "Iter: 409/3021, loss_train: 6.3721, loss_val: nan, pos_over_neg: 51.27216720581055 lr: 0.005\n",
      "Iter: 410/3021, loss_train: 6.3388, loss_val: nan, pos_over_neg: 179.30856323242188 lr: 0.005\n",
      "Iter: 411/3021, loss_train: 6.3446, loss_val: nan, pos_over_neg: 316.7696533203125 lr: 0.005\n",
      "Iter: 412/3021, loss_train: 6.347, loss_val: nan, pos_over_neg: 145.04331970214844 lr: 0.005\n",
      "Iter: 413/3021, loss_train: 6.349, loss_val: nan, pos_over_neg: 105.12700653076172 lr: 0.005\n",
      "Iter: 414/3021, loss_train: 6.3379, loss_val: nan, pos_over_neg: 54.31664276123047 lr: 0.005\n",
      "Iter: 415/3021, loss_train: 6.3561, loss_val: nan, pos_over_neg: 124.92786407470703 lr: 0.005\n",
      "Iter: 416/3021, loss_train: 6.3442, loss_val: nan, pos_over_neg: 100.9557876586914 lr: 0.005\n",
      "Iter: 417/3021, loss_train: 6.3242, loss_val: nan, pos_over_neg: 201.70379638671875 lr: 0.005\n",
      "Iter: 418/3021, loss_train: 6.3453, loss_val: nan, pos_over_neg: 147.5831756591797 lr: 0.005\n",
      "Iter: 419/3021, loss_train: 6.3596, loss_val: nan, pos_over_neg: 97.98603057861328 lr: 0.005\n",
      "Iter: 420/3021, loss_train: 6.3555, loss_val: nan, pos_over_neg: 82.31266784667969 lr: 0.005\n",
      "Iter: 421/3021, loss_train: 6.3296, loss_val: nan, pos_over_neg: 166.07301330566406 lr: 0.005\n",
      "Iter: 422/3021, loss_train: 6.3301, loss_val: nan, pos_over_neg: 79.99774932861328 lr: 0.005\n",
      "Iter: 423/3021, loss_train: 6.3544, loss_val: nan, pos_over_neg: 107.11965942382812 lr: 0.005\n",
      "Iter: 424/3021, loss_train: 6.3391, loss_val: nan, pos_over_neg: 162.73507690429688 lr: 0.005\n",
      "Iter: 425/3021, loss_train: 6.3545, loss_val: nan, pos_over_neg: 138.06312561035156 lr: 0.005\n",
      "Iter: 426/3021, loss_train: 6.3622, loss_val: nan, pos_over_neg: 78.19526672363281 lr: 0.005\n",
      "Iter: 427/3021, loss_train: 6.3684, loss_val: nan, pos_over_neg: 48.30350875854492 lr: 0.005\n",
      "Iter: 428/3021, loss_train: 6.3625, loss_val: nan, pos_over_neg: 51.98767852783203 lr: 0.005\n",
      "Iter: 429/3021, loss_train: 6.356, loss_val: nan, pos_over_neg: 193.13963317871094 lr: 0.005\n",
      "Iter: 430/3021, loss_train: 6.3384, loss_val: nan, pos_over_neg: 297.04754638671875 lr: 0.005\n",
      "Iter: 431/3021, loss_train: 6.3578, loss_val: nan, pos_over_neg: 43.539947509765625 lr: 0.005\n",
      "Iter: 432/3021, loss_train: 6.3569, loss_val: nan, pos_over_neg: 137.90733337402344 lr: 0.005\n",
      "Iter: 433/3021, loss_train: 6.3773, loss_val: nan, pos_over_neg: 165.1762237548828 lr: 0.005\n",
      "Iter: 434/3021, loss_train: 6.3384, loss_val: nan, pos_over_neg: 261.70068359375 lr: 0.005\n",
      "Iter: 435/3021, loss_train: 6.3701, loss_val: nan, pos_over_neg: 53.4660530090332 lr: 0.005\n",
      "Iter: 436/3021, loss_train: 6.3428, loss_val: nan, pos_over_neg: 94.3339614868164 lr: 0.005\n",
      "Iter: 437/3021, loss_train: 6.3459, loss_val: nan, pos_over_neg: 39.631446838378906 lr: 0.005\n",
      "Iter: 438/3021, loss_train: 6.33, loss_val: nan, pos_over_neg: 143.07809448242188 lr: 0.005\n",
      "Iter: 439/3021, loss_train: 6.3641, loss_val: nan, pos_over_neg: 55.55384826660156 lr: 0.005\n",
      "Iter: 440/3021, loss_train: 6.3451, loss_val: nan, pos_over_neg: 78.70028686523438 lr: 0.005\n",
      "Iter: 441/3021, loss_train: 6.3342, loss_val: nan, pos_over_neg: 86.27140808105469 lr: 0.005\n",
      "Iter: 442/3021, loss_train: 6.3384, loss_val: nan, pos_over_neg: 110.3511962890625 lr: 0.005\n",
      "Iter: 443/3021, loss_train: 6.3499, loss_val: nan, pos_over_neg: 57.4686279296875 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 444/3021, loss_train: 6.3496, loss_val: nan, pos_over_neg: 67.2026138305664 lr: 0.005\n",
      "Iter: 445/3021, loss_train: 6.3572, loss_val: nan, pos_over_neg: 91.3141860961914 lr: 0.005\n",
      "Iter: 446/3021, loss_train: 6.3493, loss_val: nan, pos_over_neg: 89.88750457763672 lr: 0.005\n",
      "Iter: 447/3021, loss_train: 6.3251, loss_val: nan, pos_over_neg: 119.20319366455078 lr: 0.005\n",
      "Iter: 448/3021, loss_train: 6.3714, loss_val: nan, pos_over_neg: 99.38732147216797 lr: 0.005\n",
      "Iter: 449/3021, loss_train: 6.3542, loss_val: nan, pos_over_neg: 59.13113021850586 lr: 0.005\n",
      "Iter: 450/3021, loss_train: 6.3409, loss_val: nan, pos_over_neg: 80.0784912109375 lr: 0.005\n",
      "Iter: 451/3021, loss_train: 6.3354, loss_val: nan, pos_over_neg: 219.1938934326172 lr: 0.005\n",
      "Iter: 452/3021, loss_train: 6.3362, loss_val: nan, pos_over_neg: 155.7281494140625 lr: 0.005\n",
      "Iter: 453/3021, loss_train: 6.3629, loss_val: nan, pos_over_neg: 101.44275665283203 lr: 0.005\n",
      "Iter: 454/3021, loss_train: 6.3438, loss_val: nan, pos_over_neg: 63.24946594238281 lr: 0.005\n",
      "Iter: 455/3021, loss_train: 6.3119, loss_val: nan, pos_over_neg: 111.56437683105469 lr: 0.005\n",
      "Iter: 456/3021, loss_train: 6.3269, loss_val: nan, pos_over_neg: 127.52944946289062 lr: 0.005\n",
      "Iter: 457/3021, loss_train: 6.3373, loss_val: nan, pos_over_neg: 202.37657165527344 lr: 0.005\n",
      "Iter: 458/3021, loss_train: 6.3325, loss_val: nan, pos_over_neg: 105.70809936523438 lr: 0.005\n",
      "Iter: 459/3021, loss_train: 6.3237, loss_val: nan, pos_over_neg: 200.46705627441406 lr: 0.005\n",
      "Iter: 460/3021, loss_train: 6.3334, loss_val: nan, pos_over_neg: 100.25112915039062 lr: 0.005\n",
      "Iter: 461/3021, loss_train: 6.3274, loss_val: nan, pos_over_neg: 95.53539276123047 lr: 0.005\n",
      "Iter: 462/3021, loss_train: 6.3642, loss_val: nan, pos_over_neg: 53.043766021728516 lr: 0.005\n",
      "Iter: 463/3021, loss_train: 6.3233, loss_val: nan, pos_over_neg: 84.91983795166016 lr: 0.005\n",
      "Iter: 464/3021, loss_train: 6.3395, loss_val: nan, pos_over_neg: 166.00465393066406 lr: 0.005\n",
      "Iter: 465/3021, loss_train: 6.3491, loss_val: nan, pos_over_neg: 47.36357498168945 lr: 0.005\n",
      "Iter: 466/3021, loss_train: 6.3211, loss_val: nan, pos_over_neg: 177.3608856201172 lr: 0.005\n",
      "Iter: 467/3021, loss_train: 6.3173, loss_val: nan, pos_over_neg: 198.82815551757812 lr: 0.005\n",
      "Iter: 468/3021, loss_train: 6.3415, loss_val: nan, pos_over_neg: 82.07584381103516 lr: 0.005\n",
      "Iter: 469/3021, loss_train: 6.3388, loss_val: nan, pos_over_neg: 68.42608642578125 lr: 0.005\n",
      "Iter: 470/3021, loss_train: 6.337, loss_val: nan, pos_over_neg: 84.42801666259766 lr: 0.005\n",
      "Iter: 471/3021, loss_train: 6.3184, loss_val: nan, pos_over_neg: 67.41251373291016 lr: 0.005\n",
      "Iter: 472/3021, loss_train: 6.3264, loss_val: nan, pos_over_neg: 97.81867980957031 lr: 0.005\n",
      "Iter: 473/3021, loss_train: 6.3314, loss_val: nan, pos_over_neg: -7056.8330078125 lr: 0.005\n",
      "Iter: 474/3021, loss_train: 6.3386, loss_val: nan, pos_over_neg: 125.03312683105469 lr: 0.005\n",
      "Iter: 475/3021, loss_train: 6.3372, loss_val: nan, pos_over_neg: 113.13121032714844 lr: 0.005\n",
      "Iter: 476/3021, loss_train: 6.3364, loss_val: nan, pos_over_neg: 70.87903594970703 lr: 0.005\n",
      "Iter: 477/3021, loss_train: 6.32, loss_val: nan, pos_over_neg: 94.22341918945312 lr: 0.005\n",
      "Iter: 478/3021, loss_train: 6.3467, loss_val: nan, pos_over_neg: 41.12929916381836 lr: 0.005\n",
      "Iter: 479/3021, loss_train: 6.3265, loss_val: nan, pos_over_neg: 89.56366729736328 lr: 0.005\n",
      "Iter: 480/3021, loss_train: 6.3446, loss_val: nan, pos_over_neg: 54.91522216796875 lr: 0.005\n",
      "Iter: 481/3021, loss_train: 6.3326, loss_val: nan, pos_over_neg: 116.28331756591797 lr: 0.005\n",
      "Iter: 482/3021, loss_train: 6.3541, loss_val: nan, pos_over_neg: 261.1139831542969 lr: 0.005\n",
      "Iter: 483/3021, loss_train: 6.3538, loss_val: nan, pos_over_neg: 122.61441802978516 lr: 0.005\n",
      "Iter: 484/3021, loss_train: 6.3434, loss_val: nan, pos_over_neg: 59.868736267089844 lr: 0.005\n",
      "Iter: 485/3021, loss_train: 6.3326, loss_val: nan, pos_over_neg: 71.0146713256836 lr: 0.005\n",
      "Iter: 486/3021, loss_train: 6.3185, loss_val: nan, pos_over_neg: 82.08428192138672 lr: 0.005\n",
      "Iter: 487/3021, loss_train: 6.3421, loss_val: nan, pos_over_neg: 65.53433227539062 lr: 0.005\n",
      "Iter: 488/3021, loss_train: 6.3488, loss_val: nan, pos_over_neg: 142.75094604492188 lr: 0.005\n",
      "Iter: 489/3021, loss_train: 6.3274, loss_val: nan, pos_over_neg: 83.60598754882812 lr: 0.005\n",
      "Iter: 490/3021, loss_train: 6.3399, loss_val: nan, pos_over_neg: 80.80873107910156 lr: 0.005\n",
      "Iter: 491/3021, loss_train: 6.3469, loss_val: nan, pos_over_neg: 66.54251098632812 lr: 0.005\n",
      "Iter: 492/3021, loss_train: 6.3217, loss_val: nan, pos_over_neg: 94.2271957397461 lr: 0.005\n",
      "Iter: 493/3021, loss_train: 6.3502, loss_val: nan, pos_over_neg: 66.8260726928711 lr: 0.005\n",
      "Iter: 494/3021, loss_train: 6.327, loss_val: nan, pos_over_neg: 93.42362213134766 lr: 0.005\n",
      "Iter: 495/3021, loss_train: 6.328, loss_val: nan, pos_over_neg: 90.23360443115234 lr: 0.005\n",
      "Iter: 496/3021, loss_train: 6.315, loss_val: nan, pos_over_neg: 85.91983032226562 lr: 0.005\n",
      "Iter: 497/3021, loss_train: 6.3356, loss_val: nan, pos_over_neg: 138.63352966308594 lr: 0.005\n",
      "Iter: 498/3021, loss_train: 6.3407, loss_val: nan, pos_over_neg: 149.409423828125 lr: 0.005\n",
      "Iter: 499/3021, loss_train: 6.3288, loss_val: nan, pos_over_neg: 100.63758087158203 lr: 0.005\n",
      "Iter: 500/3021, loss_train: 6.3278, loss_val: nan, pos_over_neg: 87.41348266601562 lr: 0.005\n",
      "Iter: 501/3021, loss_train: 6.3285, loss_val: nan, pos_over_neg: 78.01581573486328 lr: 0.005\n",
      "Iter: 502/3021, loss_train: 6.3404, loss_val: nan, pos_over_neg: 135.33555603027344 lr: 0.005\n",
      "Iter: 503/3021, loss_train: 6.3231, loss_val: nan, pos_over_neg: 929.4950561523438 lr: 0.005\n",
      "Iter: 504/3021, loss_train: 6.3248, loss_val: nan, pos_over_neg: 198.6018524169922 lr: 0.005\n",
      "Iter: 505/3021, loss_train: 6.3522, loss_val: nan, pos_over_neg: 58.61754608154297 lr: 0.005\n",
      "Iter: 506/3021, loss_train: 6.3344, loss_val: nan, pos_over_neg: 60.14085006713867 lr: 0.005\n",
      "Iter: 507/3021, loss_train: 6.3465, loss_val: nan, pos_over_neg: 99.56758880615234 lr: 0.005\n",
      "Iter: 508/3021, loss_train: 6.3261, loss_val: nan, pos_over_neg: 112.53706359863281 lr: 0.005\n",
      "Iter: 509/3021, loss_train: 6.3472, loss_val: nan, pos_over_neg: 87.0759048461914 lr: 0.005\n",
      "Iter: 510/3021, loss_train: 6.3465, loss_val: nan, pos_over_neg: 44.013057708740234 lr: 0.005\n",
      "Iter: 511/3021, loss_train: 6.3297, loss_val: nan, pos_over_neg: 144.30027770996094 lr: 0.005\n",
      "Iter: 512/3021, loss_train: 6.3302, loss_val: nan, pos_over_neg: 58.42557907104492 lr: 0.005\n",
      "Iter: 513/3021, loss_train: 6.3162, loss_val: nan, pos_over_neg: 89.50080871582031 lr: 0.005\n",
      "Iter: 514/3021, loss_train: 6.3302, loss_val: nan, pos_over_neg: 273.7525329589844 lr: 0.005\n",
      "Iter: 515/3021, loss_train: 6.3294, loss_val: nan, pos_over_neg: 117.79649353027344 lr: 0.005\n",
      "Iter: 516/3021, loss_train: 6.323, loss_val: nan, pos_over_neg: 136.52891540527344 lr: 0.005\n",
      "Iter: 517/3021, loss_train: 6.328, loss_val: nan, pos_over_neg: 111.97332000732422 lr: 0.005\n",
      "Iter: 518/3021, loss_train: 6.3087, loss_val: nan, pos_over_neg: 100.38502502441406 lr: 0.005\n",
      "Iter: 519/3021, loss_train: 6.3217, loss_val: nan, pos_over_neg: 84.88494110107422 lr: 0.005\n",
      "Iter: 520/3021, loss_train: 6.3268, loss_val: nan, pos_over_neg: 77.62992095947266 lr: 0.005\n",
      "Iter: 521/3021, loss_train: 6.3122, loss_val: nan, pos_over_neg: 75.15901947021484 lr: 0.005\n",
      "Iter: 522/3021, loss_train: 6.3394, loss_val: nan, pos_over_neg: 113.77815246582031 lr: 0.005\n",
      "Iter: 523/3021, loss_train: 6.3296, loss_val: nan, pos_over_neg: 64.9805679321289 lr: 0.005\n",
      "Iter: 524/3021, loss_train: 6.3286, loss_val: nan, pos_over_neg: 104.78559875488281 lr: 0.005\n",
      "Iter: 525/3021, loss_train: 6.3259, loss_val: nan, pos_over_neg: 67.17711639404297 lr: 0.005\n",
      "Iter: 526/3021, loss_train: 6.3319, loss_val: nan, pos_over_neg: 71.1685791015625 lr: 0.005\n",
      "Iter: 527/3021, loss_train: 6.3285, loss_val: nan, pos_over_neg: 169.26748657226562 lr: 0.005\n",
      "Iter: 528/3021, loss_train: 6.3017, loss_val: nan, pos_over_neg: 143.13246154785156 lr: 0.005\n",
      "Iter: 529/3021, loss_train: 6.311, loss_val: nan, pos_over_neg: 110.57434844970703 lr: 0.005\n",
      "Iter: 530/3021, loss_train: 6.3341, loss_val: nan, pos_over_neg: 73.7203140258789 lr: 0.005\n",
      "Iter: 531/3021, loss_train: 6.3112, loss_val: nan, pos_over_neg: 190.8585968017578 lr: 0.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 532/3021, loss_train: 6.3461, loss_val: nan, pos_over_neg: 62.94941329956055 lr: 0.005\n",
      "Iter: 533/3021, loss_train: 6.3331, loss_val: nan, pos_over_neg: 88.73111724853516 lr: 0.005\n",
      "Iter: 534/3021, loss_train: 6.3258, loss_val: nan, pos_over_neg: 70.66184997558594 lr: 0.005\n",
      "Iter: 535/3021, loss_train: 6.3294, loss_val: nan, pos_over_neg: 97.19295501708984 lr: 0.005\n",
      "Iter: 536/3021, loss_train: 6.3126, loss_val: nan, pos_over_neg: 120.19790649414062 lr: 0.005\n",
      "Iter: 537/3021, loss_train: 6.3431, loss_val: nan, pos_over_neg: 102.668212890625 lr: 0.005\n",
      "Iter: 538/3021, loss_train: 6.3082, loss_val: nan, pos_over_neg: 133.54542541503906 lr: 0.005\n",
      "Iter: 539/3021, loss_train: 6.3269, loss_val: nan, pos_over_neg: 198.21746826171875 lr: 0.005\n",
      "Iter: 540/3021, loss_train: 6.3325, loss_val: nan, pos_over_neg: 91.00950622558594 lr: 0.005\n",
      "Iter: 541/3021, loss_train: 6.3281, loss_val: nan, pos_over_neg: 167.3179168701172 lr: 0.005\n",
      "Iter: 542/3021, loss_train: 6.3103, loss_val: nan, pos_over_neg: 69.05978393554688 lr: 0.005\n",
      "Iter: 543/3021, loss_train: 6.3348, loss_val: nan, pos_over_neg: 60.812747955322266 lr: 0.005\n",
      "Iter: 544/3021, loss_train: 6.3173, loss_val: nan, pos_over_neg: 79.27808380126953 lr: 0.005\n",
      "Iter: 545/3021, loss_train: 6.3263, loss_val: nan, pos_over_neg: 77.1061782836914 lr: 0.005\n",
      "Iter: 546/3021, loss_train: 6.3224, loss_val: nan, pos_over_neg: 132.28530883789062 lr: 0.005\n",
      "Iter: 547/3021, loss_train: 6.3165, loss_val: nan, pos_over_neg: 83.1778793334961 lr: 0.005\n",
      "Iter: 548/3021, loss_train: 6.3333, loss_val: nan, pos_over_neg: 65.44567108154297 lr: 0.005\n",
      "Iter: 549/3021, loss_train: 6.3516, loss_val: nan, pos_over_neg: 27.832700729370117 lr: 0.005\n",
      "Iter: 550/3021, loss_train: 6.3272, loss_val: nan, pos_over_neg: 115.98902130126953 lr: 0.005\n",
      "Iter: 551/3021, loss_train: 6.3142, loss_val: nan, pos_over_neg: 250.88955688476562 lr: 0.005\n",
      "Iter: 552/3021, loss_train: 6.3296, loss_val: nan, pos_over_neg: 109.93378448486328 lr: 0.005\n",
      "Iter: 553/3021, loss_train: 6.3333, loss_val: nan, pos_over_neg: 42.211891174316406 lr: 0.005\n",
      "Iter: 554/3021, loss_train: 6.3333, loss_val: nan, pos_over_neg: 40.87472152709961 lr: 0.005\n",
      "Iter: 555/3021, loss_train: 6.3348, loss_val: nan, pos_over_neg: 62.7681999206543 lr: 0.005\n",
      "Iter: 556/3021, loss_train: 6.3292, loss_val: nan, pos_over_neg: 107.25732421875 lr: 0.005\n",
      "Iter: 557/3021, loss_train: 6.3378, loss_val: nan, pos_over_neg: 165.6004638671875 lr: 0.005\n",
      "Iter: 558/3021, loss_train: 6.3202, loss_val: nan, pos_over_neg: 312.2404479980469 lr: 0.005\n",
      "Iter: 559/3021, loss_train: 6.3422, loss_val: nan, pos_over_neg: 94.5500717163086 lr: 0.005\n",
      "Iter: 560/3021, loss_train: 6.322, loss_val: nan, pos_over_neg: 135.6957244873047 lr: 0.005\n",
      "Iter: 561/3021, loss_train: 6.3352, loss_val: nan, pos_over_neg: 270.9175109863281 lr: 0.005\n",
      "Iter: 562/3021, loss_train: 6.3114, loss_val: nan, pos_over_neg: 126.48018646240234 lr: 0.005\n",
      "Iter: 563/3021, loss_train: 6.349, loss_val: nan, pos_over_neg: 56.338584899902344 lr: 0.005\n",
      "Iter: 564/3021, loss_train: 6.3261, loss_val: nan, pos_over_neg: 51.43009948730469 lr: 0.005\n",
      "Iter: 565/3021, loss_train: 6.319, loss_val: nan, pos_over_neg: 101.56241607666016 lr: 0.005\n",
      "Iter: 566/3021, loss_train: 6.3152, loss_val: nan, pos_over_neg: 83.17562103271484 lr: 0.005\n",
      "Iter: 567/3021, loss_train: 6.3199, loss_val: nan, pos_over_neg: 272.3957824707031 lr: 0.005\n",
      "Iter: 568/3021, loss_train: 6.325, loss_val: nan, pos_over_neg: 255.17755126953125 lr: 0.005\n",
      "Iter: 569/3021, loss_train: 6.313, loss_val: nan, pos_over_neg: 167.39129638671875 lr: 0.005\n",
      "Iter: 570/3021, loss_train: 6.3235, loss_val: nan, pos_over_neg: 443.1306457519531 lr: 0.005\n",
      "Iter: 571/3021, loss_train: 6.3162, loss_val: nan, pos_over_neg: 89.38021087646484 lr: 0.005\n",
      "Iter: 572/3021, loss_train: 6.3036, loss_val: nan, pos_over_neg: 76.52566528320312 lr: 0.005\n",
      "Iter: 573/3021, loss_train: 6.3229, loss_val: nan, pos_over_neg: 73.30119323730469 lr: 0.005\n",
      "Iter: 574/3021, loss_train: 6.3149, loss_val: nan, pos_over_neg: 126.60419464111328 lr: 0.005\n",
      "Iter: 575/3021, loss_train: 6.3189, loss_val: nan, pos_over_neg: 162.27220153808594 lr: 0.005\n",
      "Iter: 576/3021, loss_train: 6.3368, loss_val: nan, pos_over_neg: 41.6950569152832 lr: 0.005\n",
      "Iter: 577/3021, loss_train: 6.3303, loss_val: nan, pos_over_neg: 67.74408721923828 lr: 0.005\n",
      "Iter: 578/3021, loss_train: 6.2924, loss_val: nan, pos_over_neg: 140.1285400390625 lr: 0.005\n",
      "Iter: 579/3021, loss_train: 6.3151, loss_val: nan, pos_over_neg: 140.0999298095703 lr: 0.005\n",
      "Iter: 580/3021, loss_train: 6.3214, loss_val: nan, pos_over_neg: 75.28890991210938 lr: 0.005\n",
      "Iter: 581/3021, loss_train: 6.2947, loss_val: nan, pos_over_neg: 99.66754150390625 lr: 0.005\n",
      "Iter: 582/3021, loss_train: 6.315, loss_val: nan, pos_over_neg: 208.52691650390625 lr: 0.005\n",
      "Iter: 583/3021, loss_train: 6.3193, loss_val: nan, pos_over_neg: 113.2491683959961 lr: 0.005\n",
      "Iter: 584/3021, loss_train: 6.3434, loss_val: nan, pos_over_neg: 63.53986358642578 lr: 0.005\n",
      "Iter: 585/3021, loss_train: 6.3051, loss_val: nan, pos_over_neg: 606.4425659179688 lr: 0.005\n",
      "Iter: 586/3021, loss_train: 6.2986, loss_val: nan, pos_over_neg: 242.40603637695312 lr: 0.005\n",
      "Iter: 587/3021, loss_train: 6.3016, loss_val: nan, pos_over_neg: 136.6075897216797 lr: 0.005\n",
      "Iter: 588/3021, loss_train: 6.3266, loss_val: nan, pos_over_neg: 94.39311218261719 lr: 0.005\n",
      "Iter: 589/3021, loss_train: 6.3108, loss_val: nan, pos_over_neg: 197.2371063232422 lr: 0.005\n",
      "Iter: 590/3021, loss_train: 6.318, loss_val: nan, pos_over_neg: 114.74589538574219 lr: 0.005\n",
      "Iter: 591/3021, loss_train: 6.3093, loss_val: nan, pos_over_neg: 188.60887145996094 lr: 0.005\n",
      "Iter: 592/3021, loss_train: 6.3016, loss_val: nan, pos_over_neg: 123.4720458984375 lr: 0.005\n",
      "Iter: 593/3021, loss_train: 6.2987, loss_val: nan, pos_over_neg: 86.27879333496094 lr: 0.005\n",
      "Iter: 594/3021, loss_train: 6.3132, loss_val: nan, pos_over_neg: 132.639892578125 lr: 0.005\n",
      "Iter: 595/3021, loss_train: 6.3213, loss_val: nan, pos_over_neg: 88.5307846069336 lr: 0.005\n",
      "Iter: 596/3021, loss_train: 6.321, loss_val: nan, pos_over_neg: 64.74251556396484 lr: 0.005\n",
      "Iter: 597/3021, loss_train: 6.2936, loss_val: nan, pos_over_neg: 165.99700927734375 lr: 0.005\n",
      "Iter: 598/3021, loss_train: 6.3106, loss_val: nan, pos_over_neg: 131.93106079101562 lr: 0.005\n",
      "Iter: 599/3021, loss_train: 6.2978, loss_val: nan, pos_over_neg: 79.60699462890625 lr: 0.005\n",
      "Iter: 600/3021, loss_train: 6.3462, loss_val: nan, pos_over_neg: 130.8814697265625 lr: 0.005\n",
      "Iter: 601/3021, loss_train: 6.3225, loss_val: nan, pos_over_neg: 139.30503845214844 lr: 0.005\n",
      "Iter: 602/3021, loss_train: 6.3068, loss_val: nan, pos_over_neg: 83.44217681884766 lr: 0.005\n",
      "Iter: 603/3021, loss_train: 6.3098, loss_val: nan, pos_over_neg: 107.3470687866211 lr: 0.005\n",
      "Iter: 604/3021, loss_train: 6.3325, loss_val: nan, pos_over_neg: 105.21058654785156 lr: 0.005\n",
      "Iter: 605/3021, loss_train: 6.337, loss_val: nan, pos_over_neg: 207.06944274902344 lr: 0.005\n",
      "Iter: 606/3021, loss_train: 6.2929, loss_val: nan, pos_over_neg: 206.39730834960938 lr: 0.005\n",
      "Iter: 607/3021, loss_train: 6.3236, loss_val: nan, pos_over_neg: 93.43087768554688 lr: 0.005\n",
      "Iter: 608/3021, loss_train: 6.3012, loss_val: nan, pos_over_neg: 62.59809494018555 lr: 0.005\n",
      "Iter: 609/3021, loss_train: 6.3315, loss_val: nan, pos_over_neg: 90.25732421875 lr: 0.005\n",
      "Iter: 610/3021, loss_train: 6.3175, loss_val: nan, pos_over_neg: 110.80400085449219 lr: 0.005\n",
      "Iter: 611/3021, loss_train: 6.3397, loss_val: nan, pos_over_neg: 105.81893920898438 lr: 0.005\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = [], [np.nan]\n",
    "for epoch in tqdm(range(params['n_epochs'])):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    losses_train = training.epoch_step(\n",
    "        dataloader_train, \n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        scheduler=scheduler,\n",
    "        temperature=params['temperature'],\n",
    "        # l2_alpha,\n",
    "        penalty_orthogonality=params['penalty_orthogonality'],\n",
    "        mode='semi-supervised',\n",
    "        loss_rolling_train=losses_train, \n",
    "        loss_rolling_val=losses_val,\n",
    "        device=device_train, \n",
    "        inner_batch_size=params['inner_batch_size'],\n",
    "        verbose=2,\n",
    "        verbose_update_period=1,\n",
    "\n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    ")\n",
    "    \n",
    "    ## save loss stuff\n",
    "    if params['prefs']['saveLogs']:\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, completed epoch: {epoch}, loss: {losses_train[-1]}, lr: {scheduler.get_last_lr()[0]}')\n",
    "        np.save(path_saveLoss, losses_train)\n",
    "    \n",
    "    ## if loss becomes NaNs, don't save the network and stop training\n",
    "    if torch.isnan(torch.as_tensor(losses_train[-1])):\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, EXITED DUE TO loss==NaN')\n",
    "        break\n",
    "        \n",
    "    ## save model\n",
    "    if params['prefs']['saveModelIteratively']:\n",
    "        torch.save(model.state_dict(), path_saveModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.plot(losses_train)\n",
    "# plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('/media/rich/bigSSD/EfficientNet_b0_7unfrozen_simCLR.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stat = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/stat.npy'\n",
    "path_labels = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh1.npy'\n",
    "\n",
    "# path_stat = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse6_28 _ day20200815/stat.npy'\n",
    "# path_labels = r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse6_28 _ day20200815/labels_posthoc_all.npy'\n",
    "\n",
    "\n",
    "images_labeled = np.concatenate(\n",
    "    util.import_multiple_stat_files(   \n",
    "        paths_statFiles=[path_stat],\n",
    "        out_height_width=[36,36],\n",
    "        max_footprint_width=241,\n",
    "        plot_pref=True\n",
    "    )\n",
    ")\n",
    "\n",
    "labels = classification.squeeze_integers(np.concatenate([np.load(path_labels)]))\n",
    "\n",
    "assert images_labeled.shape[0] == labels.shape[0] , 'num images in stat files does not correspond to num labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_toKeep = np.where(np.logical_not(labels == 4))[0]\n",
    "\n",
    "images_labeled_clean = images_labeled[idx_toKeep]\n",
    "labels_clean = labels[idx_toKeep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_classifier = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    \n",
    "    torchvision.transforms.Resize(\n",
    "        size=(224, 224),\n",
    "#         size=(180, 180),\n",
    "#         size=(72, 72),        \n",
    "        interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    ")\n",
    "\n",
    "scripted_transforms_classifier = torch.jit.script(transforms_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_make_dataset(X):\n",
    "    out = dataset.dataset_simCLR(\n",
    "        X=torch.as_tensor(X, device='cpu', dtype=torch.float32),\n",
    "        y=torch.as_tensor(torch.zeros(X.shape[0]), device='cpu', dtype=torch.float32),\n",
    "        n_transforms=1,\n",
    "        class_weights=np.array([1]),\n",
    "        transform=scripted_transforms_classifier,\n",
    "        DEVICE='cpu',\n",
    "        dtype_X=torch.float32,\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def helper_make_dataloader(ds):\n",
    "    out = torch.utils.data.DataLoader( \n",
    "        ds,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "#         pin_memory=True,\n",
    "#         num_workers=36,\n",
    "#         persistent_workers=True,\n",
    "#         prefetch_factor=2\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_clean = helper_make_dataset(images_labeled_clean)\n",
    "    \n",
    "dataloader_labeled_clean = helper_make_dataloader(dataset_labeled_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_labeled_clean = helper_make_dataset(masks_cat[:])\n",
    "    \n",
    "dataloader_labeled_clean = helper_make_dataloader(dataset_labeled_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset_trainingData = helper_make_dataset(masks_cat[torch.randint(low=0, high=masks_cat.shape[0], size=(100000,1)).squeeze()])\n",
    "    \n",
    "dataloader_trainingData = helper_make_dataloader(dataset_trainingData)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_trainingData = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_trainingData], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pc_tform, _, _, _ = decomposition.torch_pca(features_trainingData, rank=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model.base_model(data[0][0].to(DEVICE)).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([base_model_frozen.to(DEVICE)(data[0][0].to(DEVICE)).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model(data[0][0].to(DEVICE)).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_nn = torch.cat([model_chopped.to(DEVICE)(data[0][0].to(DEVICE)).detach().cpu() for data in dataloader_labeled_clean], dim=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pc_tform.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_tformed = features_nn @ pc_tform[:,:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balanced_sample_weights(labels):\n",
    "    labels = np.int64(labels.copy())\n",
    "    counts, vals = np.histogram(labels, bins=np.concatenate((np.unique(labels), [labels.max()+1])))\n",
    "    vals = vals[:-1]\n",
    "\n",
    "    n_labels = len(labels)\n",
    "    weights = n_labels / counts\n",
    "    \n",
    "    sample_weights = np.array([weights[l] for l in labels])\n",
    "    \n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kymatio import Scattering2D\n",
    "\n",
    "def get_latents_swt(sfs, swt, device_model):\n",
    "    sfs = torch.as_tensor(np.ascontiguousarray(sfs[None,...]), device=device_model, dtype=torch.float32)\n",
    "    latents_swt = swt(sfs[None,...]).squeeze()\n",
    "    latents_swt = latents_swt.reshape(latents_swt.shape[0], -1)\n",
    "    return latents_swt\n",
    "\n",
    "\n",
    "device_model = torch_helpers.set_device(use_GPU=True)\n",
    "\n",
    "scattering = Scattering2D(J=2, L=8, shape=images_labeled_clean[0].shape[-2:])\n",
    "if device_model != 'cpu':\n",
    "    scattering = scattering.cuda()\n",
    "\n",
    "latents_swt = get_latents_swt(images_labeled_clean, scattering.cuda(), device_model).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.corrcoef(features_nn.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.imshow(features_nn, aspect='auto',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.figure()\n",
    "plt.imshow(scipy.stats.zscore(features_nn, axis=0), aspect='auto', vmin=-0.8, vmax=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn_z = scipy.stats.zscore(features_nn.numpy(), axis=0)\n",
    "features_nn_z = features_nn_z[:, ~np.isnan(features_nn_z[0,:])]\n",
    "features_nn_z = torch.as_tensor(features_nn_z, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_nn, scores_nn, SVs, EVR_nn = decomposition.torch_pca(features_nn_z, rank=50, zscore=True)\n",
    "comp_nn, scores_nn, SVs, EVR_nn = decomposition.torch_pca(features_nn, rank=60, zscore=True)\n",
    "comp, scores_swt, SVs, EVR_swt = decomposition.torch_pca(latents_swt, rank=30)\n",
    "comp, scores_image, SVs, EVR_image = decomposition.torch_pca(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1), rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(EVR_nn)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(torch.corrcoef(features_nn.T))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "meanSub = sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=False)\n",
    "\n",
    "features_norm = meanSub.fit_transform(features_nn)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(latents_swt)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1))\n",
    "\n",
    "# features_norm = meanSub.fit_transform(scores)\n",
    "\n",
    "# features_norm = meanSub.fit_transform(np.concatenate((scores_nn, scores_swt, scores_image), axis=1))\n",
    "# features_all = np.concatenate([meanSub.fit_transform(scores) / torch.var(scores).mean() for scores in [features_nn, latents_swt, torch.as_tensor(images_labeled_clean.reshape(images_labeled_clean.shape[0], -1))]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp, features_norm, SVs, EVR = decomposition.torch_pca(features_norm, rank=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp, features_norm, SVs, EVR = decomposition.torch_pca(features_all, rank=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_norm = torch.cat([val / torch.std(val) for val in [scores_nn, scores_swt]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_swt]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_image]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn, scores_swt]], dim=1)\n",
    "# features_norm = torch.cat([val / torch.std(val, dim=0).mean() for val in [scores_nn, scores_swt, scores_image]], dim=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features_norm = features_nn\n",
    "# features_norm = features_nn_z\n",
    "\n",
    "# features_norm = scores_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "features_train, features_val, labels_train, labels_val = sklearn.model_selection.train_test_split(features_norm, labels_clean, test_size=0.3)\n",
    "print(f'train / test shapes: images_labeled_train, images_labeled_val, labels_train, labels_val: {features_train.shape, features_val.shape, labels_train.shape, labels_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "C_toUse = np.array([1000, 100, 10,1,0.1,0.01,0.001])\n",
    "# C_toUse = np.array([10**-4, 10**-5, 10**-6, 10**-7])\n",
    "# C_toUse = np.array([10**2, 10**3, 10**4, 10**5])\n",
    "for C in C_toUse:\n",
    "    logreg = sklearn.linear_model.LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "#         solve='sag'\n",
    "#         solve='saga'\n",
    "#         solver='newton-cg',\n",
    "#         solver='liblinear',\n",
    "        max_iter=6000, \n",
    "        C=C,\n",
    "        fit_intercept=True, \n",
    "        class_weight='balanced',\n",
    "    #     n_jobs=-1\n",
    "    )\n",
    "    logreg.fit(features_train, labels_train)\n",
    "\n",
    "    acc = logreg.score(features_train, labels_train, sample_weight=get_balanced_sample_weights(labels_train))\n",
    "    acc_train.append(acc)\n",
    "\n",
    "    acc = logreg.score(features_val, labels_val, sample_weight=get_balanced_sample_weights(labels_val))\n",
    "    acc_val.append(acc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);\n",
    "\n",
    "print(f'best val score: {max(acc_val)}')\n",
    "print(f'best C value: {C_toUse[np.argmax(acc_val)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "acc_train, acc_val = [], []\n",
    "C_toUse = np.array([1000, 100, 10,1,0.1,0.01,0.001])\n",
    "for C in C_toUse:\n",
    "    svm = sklearn.svm.SVC(\n",
    "        C=C, \n",
    "        kernel='rbf',\n",
    "        degree=3, \n",
    "        gamma='scale',\n",
    "        coef0=0.0, \n",
    "        shrinking=True,\n",
    "        probability=False, \n",
    "        tol=0.001, \n",
    "        cache_size=200,\n",
    "        class_weight='balanced',\n",
    "        verbose=False, \n",
    "        max_iter=- 1, \n",
    "        decision_function_shape='ovr',\n",
    "        break_ties=False,\n",
    "        random_state=None\n",
    "    )\n",
    "    svm.fit(features_train, labels_train)\n",
    "\n",
    "    acc = svm.score(features_train, labels_train, sample_weight=get_balanced_sample_weights(labels_train))\n",
    "    acc_train.append(acc)\n",
    "\n",
    "    acc = svm.score(features_val, labels_val, sample_weight=get_balanced_sample_weights(labels_val))\n",
    "    acc_val.append(acc)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "#         solver='sag',\n",
    "#         solver='saga',\n",
    "#         solver='newton-cg',\n",
    "#         solver='liblinear',\n",
    "    max_iter=8000, \n",
    "    C=10**(-1),\n",
    "    fit_intercept=True, \n",
    "    class_weight='balanced',\n",
    "#     n_jobs=-1\n",
    ").fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "svm = sklearn.svm.SVC(\n",
    "    C=10**(1), \n",
    "    kernel='rbf',\n",
    "    degree=3, \n",
    "    gamma='scale',\n",
    "    coef0=0.0, \n",
    "    shrinking=True,\n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200,\n",
    "    class_weight='balanced',\n",
    "    verbose=False, \n",
    "    max_iter=- 1, \n",
    "    decision_function_shape='ovr',\n",
    "    break_ties=False,\n",
    "    random_state=None\n",
    ")\n",
    "svm.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "proba = logreg.predict_proba(features_train)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "\n",
    "# preds = svm.predict(features_train_norm).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_train.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "\n",
    "# preds = svm.predict(features_val_norm).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_val.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "# proba = logreg.predict_proba(features_train_norm)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "\n",
    "preds = svm.predict(features_train).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_train.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "\n",
    "\n",
    "# proba = logreg.predict_proba(features_val_norm)\n",
    "# preds = np.argmax(proba, axis=1)\n",
    "\n",
    "preds = svm.predict(features_val).astype(np.int32)\n",
    "\n",
    "cm = classification.confusion_matrix(preds, labels_val.astype(np.int32))\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sesh1 = np.load(r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh1.npy')\n",
    "labels_sesh2 = np.load(r'/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(\n",
    "    classification.confusion_matrix(labels_sesh1.astype(np.int32), labels_sesh2.astype(np.int32)),\n",
    "    annot=True, \n",
    "    annot_kws={\"size\": 16}, \n",
    "    vmax=1., \n",
    "    cmap=plt.get_cmap('gray')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(name, params.shape) for name, params in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "layer_1 = model.state_dict()['base_model.0.0.0.weight'].cpu()\n",
    "# layer_2 = model.state_dict()['base_model.0.6.3.block.1.0.weight'].cpu()\n",
    "# layer_3 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "# layer_4 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "\n",
    "\n",
    "plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_1], dim=0), grid_shape=(15,15), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_2], dim=0), grid_shape=(6,6), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_3], dim=0), grid_shape=(4,6), kwargs_imshow={'vmax': 0.2});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap = UMAP(\n",
    "    n_neighbors=30,\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    metric_kwds=None,\n",
    "    output_metric='euclidean',\n",
    "    output_metric_kwds=None,\n",
    "    n_epochs=None,\n",
    "    learning_rate=1.0,\n",
    "    init='spectral',\n",
    "    min_dist=0.1,\n",
    "    spread=1.0,\n",
    "    low_memory=True,\n",
    "    n_jobs=-1,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    repulsion_strength=1.0,\n",
    "    negative_sample_rate=5,\n",
    "    transform_queue_size=4.0,\n",
    "    a=None,\n",
    "    b=None,\n",
    "    random_state=None,\n",
    "    angular_rp_forest=False,\n",
    "    target_n_neighbors=-1,\n",
    "    target_metric='categorical',\n",
    "    target_metric_kwds=None,\n",
    "    target_weight=0.5,\n",
    "    transform_seed=42,\n",
    "    transform_mode='embedding',\n",
    "    force_approximation_algorithm=False,\n",
    "    verbose=False,\n",
    "    tqdm_kwds=None,\n",
    "    unique=False,\n",
    "    densmap=False,\n",
    "    dens_lambda=2.0,\n",
    "    dens_frac=0.3,\n",
    "    dens_var_shift=0.1,\n",
    "    output_dens=False,\n",
    "    disconnection_distance=None,\n",
    "    precomputed_knn=(None, None, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = umap.fit_transform(features_nn)\n",
    "# embeddings = umap.fit_transform(scores_nn)\n",
    "# embeddings = umap.fit_transform(latents_swt)\n",
    "# embeddings = umap.fit_transform(features_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(embeddings[:,0], embeddings[:,1], s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(embeddings[:,0], embeddings[:,1], s=5, c=labels_clean, cmap='gist_rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(scores_nn[:,1], scores_nn[:,0], c=labels_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 920.76378,
   "position": {
    "height": "40px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "500.99px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
