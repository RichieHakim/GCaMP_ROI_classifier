{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2870663/1226147920.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = '/media/rich/bigSSD/analysis_data/ROI_net_training/testing_dispatcher_20220504'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_github = '/media/rich/Home_Linux_partition/github_repos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_training = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/masks_20211202_balanced.h5'\n",
    "\n",
    "path_dataClassification_stat = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/stat.npy'\n",
    "path_dataClassification_labels = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(dir_github)\n",
    "# dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition\n",
    "from GCaMP_ROI_classifier import util, models, training, augmentation, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log(path_log, text, mode='a', start_on_new_line=True):\n",
    "    with open(path_log, mode=mode) as log:\n",
    "        if start_on_new_line==True:\n",
    "            log.write('\\n')\n",
    "        log.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'useGPU_training': True,\n",
    "    'pre_head_fc_sizes': [1024, 512],\n",
    "    'post_head_fc_sizes': [64],\n",
    "    'block_to_freeze': 7,\n",
    "    'useGPU_dataloader': False,\n",
    "    'dataloader_kwargs':{\n",
    "        'batch_size': 1024,\n",
    "        'shuffle': True,\n",
    "        'drop_last': True,\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 36,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 3,\n",
    "    },\n",
    "    'lr': 10**(-3.5),\n",
    "    'gamma': 1-0.0000,\n",
    "    'n_epochs': 9999999,\n",
    "    'temperature': 0.5,\n",
    "}\n",
    "\n",
    "dir_save = Path(r'/media/rich/bigSSD/')\n",
    "fileName_model = 'EfficientNet_b0_7unfrozen_simCLR'\n",
    "path_saveModel = f'{dir_save / fileName_model}.pth'\n",
    "path_saveLog = dir_save / 'log.txt'\n",
    "path_saveLoss = dir_save / 'loss.npy'\n",
    "\n",
    "pref_saveModelIteratively = True\n",
    "pref_saveLogs = True\n",
    "\n",
    "device_train = torch_helpers.set_device(use_GPU=params['useGPU_training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "sf_sparse = scipy.sparse.load_npz('/media/rich/bigSSD/analysis_data/ROIs_for_training/sf_sparse_36x36_20220503.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dense = torch.as_tensor(sf_sparse.toarray().reshape(sf_sparse.shape[0], 36,36), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([3094029, 36, 36])\n",
      "Number of masks: torch.Size([3094025, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {sf_dense.shape}')\n",
    "\n",
    "ROIs_without_NaNs = ~torch.any(torch.any(torch.isnan(sf_dense), dim=1), dim=1)\n",
    "ROIs_nonAllZero = (torch.max(torch.max(sf_dense, dim=1)[0], dim=1)[0] > 0)\n",
    "ROIs_toKeep = torch.where(ROIs_without_NaNs * ROIs_nonAllZero)\n",
    "masks_cat = sf_dense[ROIs_toKeep]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(\n",
    "    model_chopped,\n",
    "    base_model_frozen,\n",
    "    pre_head_fc_sizes=params['pre_head_fc_sizes'], \n",
    "    post_head_fc_sizes=params['post_head_fc_sizes'], \n",
    "    classifier_fc_sizes=None\n",
    ")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.0.7.0.block.0.0.weight\n",
      "base_model.0.7.0.block.0.1.weight\n",
      "base_model.0.7.0.block.0.1.bias\n",
      "base_model.0.7.0.block.1.0.weight\n",
      "base_model.0.7.0.block.1.1.weight\n",
      "base_model.0.7.0.block.1.1.bias\n",
      "base_model.0.7.0.block.2.fc1.weight\n",
      "base_model.0.7.0.block.2.fc1.bias\n",
      "base_model.0.7.0.block.2.fc2.weight\n",
      "base_model.0.7.0.block.2.fc2.bias\n",
      "base_model.0.7.0.block.3.0.weight\n",
      "base_model.0.7.0.block.3.1.weight\n",
      "base_model.0.7.0.block.3.1.bias\n",
      "base_model.0.8.0.weight\n",
      "base_model.0.8.1.weight\n",
      "base_model.0.8.1.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in CNN model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    # print(name)\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[13]) < params['block_to_freeze']:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[13]) >= params['block_to_freeze']:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    augmentation.Scale_image_sum(sum_val=1, epsilon=1e-9, min_sub=True),\n",
    "    \n",
    "    augmentation.AddPoissonNoise(  \n",
    "        #scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "        scaler_bounds=(10**(4), 10**(5)),\n",
    "        prob=0.5,\n",
    "        base=1000,\n",
    "        scaling='log'),\n",
    "    \n",
    "    augmentation.Horizontal_stripe_scale(alpha_min_max=(0.5, 1), im_size=(36,36), prob=0.5),\n",
    "    augmentation.Horizontal_stripe_shift(alpha_min_max=(1,3), im_size=(36,36), prob=0.5),\n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    torchvision.transforms.RandomAffine(\n",
    "        degrees=(-180,180),\n",
    "        translate=(0.1, 0.1), #0, .3, .45 (DEFAULT)\n",
    "        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "        shear=(-15, 15, -15, 15),\n",
    "        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "        fill=0, \n",
    "        fillcolor=None, \n",
    "        resample=None),\n",
    "\n",
    "    augmentation.AddGaussianNoise(  \n",
    "        mean=0, \n",
    "        std=0.0010,\n",
    "        prob=0.5),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1), epsilon=1e-9), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    augmentation.WarpPoints(\n",
    "        r=[0.3, 0.6],\n",
    "        cx=[-0.3, 0.3],\n",
    "        cy=[-0.3, 0.3], \n",
    "        dx=[-0.24, 0.24], \n",
    "        dy=[-0.24, 0.24], \n",
    "        n_warps=2,\n",
    "        prob=0.5,\n",
    "        img_size_in=[36, 36],\n",
    "#         img_size_out=[224, 224],\n",
    "        img_size_out=[180, 180],\n",
    "#         img_size_out=[36, 36],\n",
    "                           ),\n",
    "    \n",
    "#     augmentation.Check_NaN(),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "# #     torchvision.transforms.Resize(size=(180,180), \n",
    "# #                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    ")\n",
    "    \n",
    "\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cpu'\n"
     ]
    }
   ],
   "source": [
    "device_dataloader = torch_helpers.set_device(use_GPU=params['useGPU_dataloader'])\n",
    "\n",
    "dataset_train = dataset.dataset_simCLR(\n",
    "    torch.as_tensor(masks_cat, device=device_dataloader, dtype=torch.float32), \n",
    "    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device=device_dataloader, dtype=torch.float32),\n",
    "\n",
    "    n_transforms=2,\n",
    "    class_weights=np.array([1]),\n",
    "    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "    transform=scripted_transforms,\n",
    "    # DEVICE='cpu',\n",
    "    DEVICE=device_dataloader,\n",
    "    dtype_X=torch.float32,\n",
    "    dtype_y=torch.int64,\n",
    "    # temp_uncertainty=1\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    **params['dataloader_kwargs']\n",
    "\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    "    \n",
    "#     batch_size=1024,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# idx_rand = np.random.randint(0,masks_cat2.shape[0], 10)\n",
    "# for ii in idx_rand:\n",
    "#     fig, axs = plt.subplots(1,2)\n",
    "#     # print(dataset_train[ii][0][0][0].shape)\n",
    "#     axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "#     axs[1].imshow(dataset_train[ii][0][1][0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device_train)\n",
    "model.prep_contrast()\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "#     lr=params['lr'],\n",
    "    lr=1*10**-4,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=params['gamma'],\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(device_train) for _ in criterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                               | 0/9999999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/3021, loss_train: 7.6008, loss_val: nan, pos_over_neg: 1.021756887435913 lr: 0.0001\n",
      "Iter: 1/3021, loss_train: 7.565, loss_val: nan, pos_over_neg: 1.0537643432617188 lr: 0.0001\n",
      "Iter: 2/3021, loss_train: 7.4886, loss_val: nan, pos_over_neg: 1.1408154964447021 lr: 0.0001\n",
      "Iter: 3/3021, loss_train: 7.4124, loss_val: nan, pos_over_neg: 1.2915561199188232 lr: 0.0001\n",
      "Iter: 4/3021, loss_train: 7.3437, loss_val: nan, pos_over_neg: 1.5708016157150269 lr: 0.0001\n",
      "Iter: 5/3021, loss_train: 7.2783, loss_val: nan, pos_over_neg: 2.1402266025543213 lr: 0.0001\n",
      "Iter: 6/3021, loss_train: 7.2274, loss_val: nan, pos_over_neg: 2.954249382019043 lr: 0.0001\n",
      "Iter: 7/3021, loss_train: 7.1915, loss_val: nan, pos_over_neg: 3.8929965496063232 lr: 0.0001\n",
      "Iter: 8/3021, loss_train: 7.249, loss_val: nan, pos_over_neg: 4.432919502258301 lr: 0.0001\n",
      "Iter: 9/3021, loss_train: 7.2081, loss_val: nan, pos_over_neg: 5.018045902252197 lr: 0.0001\n",
      "Iter: 10/3021, loss_train: 7.1698, loss_val: nan, pos_over_neg: 4.2932820320129395 lr: 0.0001\n",
      "Iter: 11/3021, loss_train: 7.1868, loss_val: nan, pos_over_neg: 3.6771819591522217 lr: 0.0001\n",
      "Iter: 12/3021, loss_train: 7.1661, loss_val: nan, pos_over_neg: 3.214053153991699 lr: 0.0001\n",
      "Iter: 13/3021, loss_train: 7.1439, loss_val: nan, pos_over_neg: 3.1278457641601562 lr: 0.0001\n",
      "Iter: 14/3021, loss_train: 7.1156, loss_val: nan, pos_over_neg: 3.1150903701782227 lr: 0.0001\n",
      "Iter: 15/3021, loss_train: 7.1178, loss_val: nan, pos_over_neg: 2.998558282852173 lr: 0.0001\n",
      "Iter: 16/3021, loss_train: 7.1234, loss_val: nan, pos_over_neg: 3.1541924476623535 lr: 0.0001\n",
      "Iter: 17/3021, loss_train: 7.0741, loss_val: nan, pos_over_neg: 3.748960494995117 lr: 0.0001\n",
      "Iter: 18/3021, loss_train: 7.0609, loss_val: nan, pos_over_neg: 4.192848205566406 lr: 0.0001\n",
      "Iter: 19/3021, loss_train: 7.075, loss_val: nan, pos_over_neg: 4.65902042388916 lr: 0.0001\n",
      "Iter: 20/3021, loss_train: 7.0543, loss_val: nan, pos_over_neg: 5.976966381072998 lr: 0.0001\n",
      "Iter: 21/3021, loss_train: 7.0814, loss_val: nan, pos_over_neg: 6.216888427734375 lr: 0.0001\n",
      "Iter: 22/3021, loss_train: 7.0679, loss_val: nan, pos_over_neg: 7.95610237121582 lr: 0.0001\n",
      "Iter: 23/3021, loss_train: 7.0249, loss_val: nan, pos_over_neg: 8.682245254516602 lr: 0.0001\n",
      "Iter: 24/3021, loss_train: 7.0302, loss_val: nan, pos_over_neg: 9.755510330200195 lr: 0.0001\n",
      "Iter: 25/3021, loss_train: 7.031, loss_val: nan, pos_over_neg: 11.113452911376953 lr: 0.0001\n",
      "Iter: 26/3021, loss_train: 7.0397, loss_val: nan, pos_over_neg: 12.110620498657227 lr: 0.0001\n",
      "Iter: 27/3021, loss_train: 7.006, loss_val: nan, pos_over_neg: 10.989513397216797 lr: 0.0001\n",
      "Iter: 28/3021, loss_train: 6.9822, loss_val: nan, pos_over_neg: 12.371830940246582 lr: 0.0001\n",
      "Iter: 29/3021, loss_train: 6.9554, loss_val: nan, pos_over_neg: 14.386801719665527 lr: 0.0001\n",
      "Iter: 30/3021, loss_train: 6.9806, loss_val: nan, pos_over_neg: 15.291502952575684 lr: 0.0001\n",
      "Iter: 31/3021, loss_train: 6.9545, loss_val: nan, pos_over_neg: 15.260523796081543 lr: 0.0001\n",
      "Iter: 32/3021, loss_train: 6.9425, loss_val: nan, pos_over_neg: 17.28152847290039 lr: 0.0001\n",
      "Iter: 33/3021, loss_train: 6.948, loss_val: nan, pos_over_neg: 18.27750015258789 lr: 0.0001\n",
      "Iter: 34/3021, loss_train: 6.9315, loss_val: nan, pos_over_neg: 20.670175552368164 lr: 0.0001\n",
      "Iter: 35/3021, loss_train: 6.9145, loss_val: nan, pos_over_neg: 24.23166275024414 lr: 0.0001\n",
      "Iter: 36/3021, loss_train: 6.8928, loss_val: nan, pos_over_neg: 27.056079864501953 lr: 0.0001\n",
      "Iter: 37/3021, loss_train: 6.9347, loss_val: nan, pos_over_neg: 25.446367263793945 lr: 0.0001\n",
      "Iter: 38/3021, loss_train: 6.9336, loss_val: nan, pos_over_neg: 27.891082763671875 lr: 0.0001\n",
      "Iter: 39/3021, loss_train: 6.9063, loss_val: nan, pos_over_neg: 29.28904151916504 lr: 0.0001\n",
      "Iter: 40/3021, loss_train: 6.9097, loss_val: nan, pos_over_neg: 31.596050262451172 lr: 0.0001\n",
      "Iter: 41/3021, loss_train: 6.8976, loss_val: nan, pos_over_neg: 29.547401428222656 lr: 0.0001\n",
      "Iter: 42/3021, loss_train: 6.8835, loss_val: nan, pos_over_neg: 33.081722259521484 lr: 0.0001\n",
      "Iter: 43/3021, loss_train: 6.9073, loss_val: nan, pos_over_neg: 26.153966903686523 lr: 0.0001\n",
      "Iter: 44/3021, loss_train: 6.8774, loss_val: nan, pos_over_neg: 27.260713577270508 lr: 0.0001\n",
      "Iter: 45/3021, loss_train: 6.9119, loss_val: nan, pos_over_neg: 22.017616271972656 lr: 0.0001\n",
      "Iter: 46/3021, loss_train: 6.8926, loss_val: nan, pos_over_neg: 20.79217529296875 lr: 0.0001\n",
      "Iter: 47/3021, loss_train: 6.9065, loss_val: nan, pos_over_neg: 20.43366241455078 lr: 0.0001\n",
      "Iter: 48/3021, loss_train: 6.8717, loss_val: nan, pos_over_neg: 20.010391235351562 lr: 0.0001\n",
      "Iter: 49/3021, loss_train: 6.8684, loss_val: nan, pos_over_neg: 21.61119270324707 lr: 0.0001\n",
      "Iter: 50/3021, loss_train: 6.8484, loss_val: nan, pos_over_neg: 24.392173767089844 lr: 0.0001\n",
      "Iter: 51/3021, loss_train: 6.8829, loss_val: nan, pos_over_neg: 23.389657974243164 lr: 0.0001\n",
      "Iter: 52/3021, loss_train: 6.8575, loss_val: nan, pos_over_neg: 23.74220085144043 lr: 0.0001\n",
      "Iter: 53/3021, loss_train: 6.8709, loss_val: nan, pos_over_neg: 26.572397232055664 lr: 0.0001\n",
      "Iter: 54/3021, loss_train: 6.8572, loss_val: nan, pos_over_neg: 29.96137809753418 lr: 0.0001\n",
      "Iter: 55/3021, loss_train: 6.8644, loss_val: nan, pos_over_neg: 31.21910285949707 lr: 0.0001\n",
      "Iter: 56/3021, loss_train: 6.8478, loss_val: nan, pos_over_neg: 36.02167510986328 lr: 0.0001\n",
      "Iter: 57/3021, loss_train: 6.8434, loss_val: nan, pos_over_neg: 42.471534729003906 lr: 0.0001\n",
      "Iter: 58/3021, loss_train: 6.8523, loss_val: nan, pos_over_neg: 41.84825897216797 lr: 0.0001\n",
      "Iter: 59/3021, loss_train: 6.8694, loss_val: nan, pos_over_neg: 42.423824310302734 lr: 0.0001\n",
      "Iter: 60/3021, loss_train: 6.8426, loss_val: nan, pos_over_neg: 48.987857818603516 lr: 0.0001\n",
      "Iter: 61/3021, loss_train: 6.8453, loss_val: nan, pos_over_neg: 50.81122970581055 lr: 0.0001\n",
      "Iter: 62/3021, loss_train: 6.8528, loss_val: nan, pos_over_neg: 40.297523498535156 lr: 0.0001\n",
      "Iter: 63/3021, loss_train: 6.832, loss_val: nan, pos_over_neg: 40.93576431274414 lr: 0.0001\n",
      "Iter: 64/3021, loss_train: 6.8349, loss_val: nan, pos_over_neg: 40.05012893676758 lr: 0.0001\n",
      "Iter: 65/3021, loss_train: 6.8376, loss_val: nan, pos_over_neg: 47.91281509399414 lr: 0.0001\n",
      "Iter: 66/3021, loss_train: 6.8127, loss_val: nan, pos_over_neg: 40.26491928100586 lr: 0.0001\n",
      "Iter: 67/3021, loss_train: 6.8471, loss_val: nan, pos_over_neg: 39.93717956542969 lr: 0.0001\n",
      "Iter: 68/3021, loss_train: 6.8177, loss_val: nan, pos_over_neg: 44.224464416503906 lr: 0.0001\n",
      "Iter: 69/3021, loss_train: 6.8087, loss_val: nan, pos_over_neg: 51.01115417480469 lr: 0.0001\n",
      "Iter: 70/3021, loss_train: 6.8153, loss_val: nan, pos_over_neg: 44.6114501953125 lr: 0.0001\n",
      "Iter: 71/3021, loss_train: 6.8398, loss_val: nan, pos_over_neg: 49.084659576416016 lr: 0.0001\n",
      "Iter: 72/3021, loss_train: 6.8445, loss_val: nan, pos_over_neg: 39.60350036621094 lr: 0.0001\n",
      "Iter: 73/3021, loss_train: 6.8089, loss_val: nan, pos_over_neg: 44.32228469848633 lr: 0.0001\n",
      "Iter: 74/3021, loss_train: 6.8039, loss_val: nan, pos_over_neg: 40.68450164794922 lr: 0.0001\n",
      "Iter: 75/3021, loss_train: 6.8097, loss_val: nan, pos_over_neg: 49.02886962890625 lr: 0.0001\n",
      "Iter: 76/3021, loss_train: 6.7989, loss_val: nan, pos_over_neg: 49.3508186340332 lr: 0.0001\n",
      "Iter: 77/3021, loss_train: 6.7768, loss_val: nan, pos_over_neg: 38.846290588378906 lr: 0.0001\n",
      "Iter: 78/3021, loss_train: 6.8279, loss_val: nan, pos_over_neg: 37.92509841918945 lr: 0.0001\n",
      "Iter: 79/3021, loss_train: 6.8123, loss_val: nan, pos_over_neg: 39.061737060546875 lr: 0.0001\n",
      "Iter: 80/3021, loss_train: 6.8308, loss_val: nan, pos_over_neg: 37.18299865722656 lr: 0.0001\n",
      "Iter: 81/3021, loss_train: 6.7957, loss_val: nan, pos_over_neg: 43.91147232055664 lr: 0.0001\n",
      "Iter: 82/3021, loss_train: 6.8417, loss_val: nan, pos_over_neg: 50.396034240722656 lr: 0.0001\n",
      "Iter: 83/3021, loss_train: 6.8093, loss_val: nan, pos_over_neg: 47.06821060180664 lr: 0.0001\n",
      "Iter: 84/3021, loss_train: 6.833, loss_val: nan, pos_over_neg: 49.034217834472656 lr: 0.0001\n",
      "Iter: 85/3021, loss_train: 6.8018, loss_val: nan, pos_over_neg: 51.317142486572266 lr: 0.0001\n",
      "Iter: 86/3021, loss_train: 6.7627, loss_val: nan, pos_over_neg: 61.83523178100586 lr: 0.0001\n",
      "Iter: 87/3021, loss_train: 6.7674, loss_val: nan, pos_over_neg: 57.407447814941406 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 88/3021, loss_train: 6.8354, loss_val: nan, pos_over_neg: 43.760162353515625 lr: 0.0001\n",
      "Iter: 89/3021, loss_train: 6.8146, loss_val: nan, pos_over_neg: 48.851261138916016 lr: 0.0001\n",
      "Iter: 90/3021, loss_train: 6.8093, loss_val: nan, pos_over_neg: 41.814632415771484 lr: 0.0001\n",
      "Iter: 91/3021, loss_train: 6.8204, loss_val: nan, pos_over_neg: 43.28696060180664 lr: 0.0001\n",
      "Iter: 92/3021, loss_train: 6.7745, loss_val: nan, pos_over_neg: 56.589866638183594 lr: 0.0001\n",
      "Iter: 93/3021, loss_train: 6.7896, loss_val: nan, pos_over_neg: 40.6494026184082 lr: 0.0001\n",
      "Iter: 94/3021, loss_train: 6.7897, loss_val: nan, pos_over_neg: 43.2087287902832 lr: 0.0001\n",
      "Iter: 95/3021, loss_train: 6.771, loss_val: nan, pos_over_neg: 48.807743072509766 lr: 0.0001\n",
      "Iter: 96/3021, loss_train: 6.78, loss_val: nan, pos_over_neg: 53.1165657043457 lr: 0.0001\n",
      "Iter: 97/3021, loss_train: 6.8065, loss_val: nan, pos_over_neg: 51.976200103759766 lr: 0.0001\n",
      "Iter: 98/3021, loss_train: 6.7815, loss_val: nan, pos_over_neg: 71.77888488769531 lr: 0.0001\n",
      "Iter: 99/3021, loss_train: 6.81, loss_val: nan, pos_over_neg: 51.64118576049805 lr: 0.0001\n",
      "Iter: 100/3021, loss_train: 6.7576, loss_val: nan, pos_over_neg: 79.85929107666016 lr: 0.0001\n",
      "Iter: 101/3021, loss_train: 6.8098, loss_val: nan, pos_over_neg: 58.3011360168457 lr: 0.0001\n",
      "Iter: 102/3021, loss_train: 6.7837, loss_val: nan, pos_over_neg: 83.85948181152344 lr: 0.0001\n",
      "Iter: 103/3021, loss_train: 6.802, loss_val: nan, pos_over_neg: 57.09276580810547 lr: 0.0001\n",
      "Iter: 104/3021, loss_train: 6.8107, loss_val: nan, pos_over_neg: 58.292327880859375 lr: 0.0001\n",
      "Iter: 105/3021, loss_train: 6.7629, loss_val: nan, pos_over_neg: 55.25848388671875 lr: 0.0001\n",
      "Iter: 106/3021, loss_train: 6.7731, loss_val: nan, pos_over_neg: 54.20040512084961 lr: 0.0001\n",
      "Iter: 107/3021, loss_train: 6.7596, loss_val: nan, pos_over_neg: 54.654518127441406 lr: 0.0001\n",
      "Iter: 108/3021, loss_train: 6.7953, loss_val: nan, pos_over_neg: 43.745975494384766 lr: 0.0001\n",
      "Iter: 109/3021, loss_train: 6.805, loss_val: nan, pos_over_neg: 56.064483642578125 lr: 0.0001\n",
      "Iter: 110/3021, loss_train: 6.7511, loss_val: nan, pos_over_neg: 61.22787094116211 lr: 0.0001\n",
      "Iter: 111/3021, loss_train: 6.7705, loss_val: nan, pos_over_neg: 64.91835021972656 lr: 0.0001\n",
      "Iter: 112/3021, loss_train: 6.7687, loss_val: nan, pos_over_neg: 60.14411544799805 lr: 0.0001\n",
      "Iter: 113/3021, loss_train: 6.7847, loss_val: nan, pos_over_neg: 59.15860366821289 lr: 0.0001\n",
      "Iter: 114/3021, loss_train: 6.7692, loss_val: nan, pos_over_neg: 51.28641891479492 lr: 0.0001\n",
      "Iter: 115/3021, loss_train: 6.766, loss_val: nan, pos_over_neg: 77.294189453125 lr: 0.0001\n",
      "Iter: 116/3021, loss_train: 6.7518, loss_val: nan, pos_over_neg: 69.04939270019531 lr: 0.0001\n",
      "Iter: 117/3021, loss_train: 6.75, loss_val: nan, pos_over_neg: 77.16857147216797 lr: 0.0001\n",
      "Iter: 118/3021, loss_train: 6.7585, loss_val: nan, pos_over_neg: 84.59793090820312 lr: 0.0001\n",
      "Iter: 119/3021, loss_train: 6.7813, loss_val: nan, pos_over_neg: 69.23056030273438 lr: 0.0001\n",
      "Iter: 120/3021, loss_train: 6.751, loss_val: nan, pos_over_neg: 76.94059753417969 lr: 0.0001\n",
      "Iter: 121/3021, loss_train: 6.7714, loss_val: nan, pos_over_neg: 73.8790054321289 lr: 0.0001\n",
      "Iter: 122/3021, loss_train: 6.7418, loss_val: nan, pos_over_neg: 65.19808959960938 lr: 0.0001\n",
      "Iter: 123/3021, loss_train: 6.7783, loss_val: nan, pos_over_neg: 55.35531997680664 lr: 0.0001\n",
      "Iter: 124/3021, loss_train: 6.7609, loss_val: nan, pos_over_neg: 59.742027282714844 lr: 0.0001\n",
      "Iter: 125/3021, loss_train: 6.7651, loss_val: nan, pos_over_neg: 59.78522491455078 lr: 0.0001\n",
      "Iter: 126/3021, loss_train: 6.76, loss_val: nan, pos_over_neg: 61.0769157409668 lr: 0.0001\n",
      "Iter: 127/3021, loss_train: 6.7503, loss_val: nan, pos_over_neg: 67.15830993652344 lr: 0.0001\n",
      "Iter: 128/3021, loss_train: 6.7475, loss_val: nan, pos_over_neg: 78.33700561523438 lr: 0.0001\n",
      "Iter: 129/3021, loss_train: 6.7193, loss_val: nan, pos_over_neg: 67.59737396240234 lr: 0.0001\n",
      "Iter: 130/3021, loss_train: 6.7497, loss_val: nan, pos_over_neg: 50.54961395263672 lr: 0.0001\n",
      "Iter: 131/3021, loss_train: 6.7862, loss_val: nan, pos_over_neg: 65.02569580078125 lr: 0.0001\n",
      "Iter: 132/3021, loss_train: 6.7597, loss_val: nan, pos_over_neg: 64.42876434326172 lr: 0.0001\n",
      "Iter: 133/3021, loss_train: 6.7492, loss_val: nan, pos_over_neg: 67.41619110107422 lr: 0.0001\n",
      "Iter: 134/3021, loss_train: 6.7089, loss_val: nan, pos_over_neg: 58.450687408447266 lr: 0.0001\n",
      "Iter: 135/3021, loss_train: 6.7534, loss_val: nan, pos_over_neg: 73.15497589111328 lr: 0.0001\n",
      "Iter: 136/3021, loss_train: 6.7432, loss_val: nan, pos_over_neg: 54.85002899169922 lr: 0.0001\n",
      "Iter: 137/3021, loss_train: 6.7388, loss_val: nan, pos_over_neg: 60.87345504760742 lr: 0.0001\n",
      "Iter: 138/3021, loss_train: 6.7484, loss_val: nan, pos_over_neg: 72.12862396240234 lr: 0.0001\n",
      "Iter: 139/3021, loss_train: 6.7538, loss_val: nan, pos_over_neg: 71.77840423583984 lr: 0.0001\n",
      "Iter: 140/3021, loss_train: 6.7737, loss_val: nan, pos_over_neg: 92.67657470703125 lr: 0.0001\n",
      "Iter: 141/3021, loss_train: 6.7472, loss_val: nan, pos_over_neg: 117.83413696289062 lr: 0.0001\n",
      "Iter: 142/3021, loss_train: 6.7392, loss_val: nan, pos_over_neg: 96.83900451660156 lr: 0.0001\n",
      "Iter: 143/3021, loss_train: 6.7331, loss_val: nan, pos_over_neg: 113.37006378173828 lr: 0.0001\n",
      "Iter: 144/3021, loss_train: 6.749, loss_val: nan, pos_over_neg: 86.45429229736328 lr: 0.0001\n",
      "Iter: 145/3021, loss_train: 6.7307, loss_val: nan, pos_over_neg: 87.6976089477539 lr: 0.0001\n",
      "Iter: 146/3021, loss_train: 6.7304, loss_val: nan, pos_over_neg: 91.76646423339844 lr: 0.0001\n",
      "Iter: 147/3021, loss_train: 6.7277, loss_val: nan, pos_over_neg: 62.095584869384766 lr: 0.0001\n",
      "Iter: 148/3021, loss_train: 6.7065, loss_val: nan, pos_over_neg: 69.68242645263672 lr: 0.0001\n",
      "Iter: 149/3021, loss_train: 6.7207, loss_val: nan, pos_over_neg: 61.25969314575195 lr: 0.0001\n",
      "Iter: 150/3021, loss_train: 6.7187, loss_val: nan, pos_over_neg: 66.39551544189453 lr: 0.0001\n",
      "Iter: 151/3021, loss_train: 6.7208, loss_val: nan, pos_over_neg: 78.6484603881836 lr: 0.0001\n",
      "Iter: 152/3021, loss_train: 6.7337, loss_val: nan, pos_over_neg: 61.86404800415039 lr: 0.0001\n",
      "Iter: 153/3021, loss_train: 6.7498, loss_val: nan, pos_over_neg: 67.4151611328125 lr: 0.0001\n",
      "Iter: 154/3021, loss_train: 6.7459, loss_val: nan, pos_over_neg: 90.77141571044922 lr: 0.0001\n",
      "Iter: 155/3021, loss_train: 6.7277, loss_val: nan, pos_over_neg: 85.33511352539062 lr: 0.0001\n",
      "Iter: 156/3021, loss_train: 6.7447, loss_val: nan, pos_over_neg: 67.94538879394531 lr: 0.0001\n",
      "Iter: 157/3021, loss_train: 6.7234, loss_val: nan, pos_over_neg: 93.521484375 lr: 0.0001\n",
      "Iter: 158/3021, loss_train: 6.7293, loss_val: nan, pos_over_neg: 87.49002838134766 lr: 0.0001\n",
      "Iter: 159/3021, loss_train: 6.74, loss_val: nan, pos_over_neg: 66.27784729003906 lr: 0.0001\n",
      "Iter: 160/3021, loss_train: 6.7662, loss_val: nan, pos_over_neg: 70.15357208251953 lr: 0.0001\n",
      "Iter: 161/3021, loss_train: 6.6985, loss_val: nan, pos_over_neg: 81.46954345703125 lr: 0.0001\n",
      "Iter: 162/3021, loss_train: 6.7113, loss_val: nan, pos_over_neg: 80.83041381835938 lr: 0.0001\n",
      "Iter: 163/3021, loss_train: 6.7073, loss_val: nan, pos_over_neg: 81.69828033447266 lr: 0.0001\n",
      "Iter: 164/3021, loss_train: 6.725, loss_val: nan, pos_over_neg: 101.64787292480469 lr: 0.0001\n",
      "Iter: 165/3021, loss_train: 6.726, loss_val: nan, pos_over_neg: 97.93206787109375 lr: 0.0001\n",
      "Iter: 166/3021, loss_train: 6.7272, loss_val: nan, pos_over_neg: 74.03150177001953 lr: 0.0001\n",
      "Iter: 167/3021, loss_train: 6.7259, loss_val: nan, pos_over_neg: 92.36832427978516 lr: 0.0001\n",
      "Iter: 168/3021, loss_train: 6.7084, loss_val: nan, pos_over_neg: 125.54150390625 lr: 0.0001\n",
      "Iter: 169/3021, loss_train: 6.7082, loss_val: nan, pos_over_neg: 125.2975082397461 lr: 0.0001\n",
      "Iter: 170/3021, loss_train: 6.702, loss_val: nan, pos_over_neg: 78.06637573242188 lr: 0.0001\n",
      "Iter: 171/3021, loss_train: 6.7291, loss_val: nan, pos_over_neg: 77.8360595703125 lr: 0.0001\n",
      "Iter: 172/3021, loss_train: 6.7206, loss_val: nan, pos_over_neg: 120.80178833007812 lr: 0.0001\n",
      "Iter: 173/3021, loss_train: 6.7268, loss_val: nan, pos_over_neg: 101.1009750366211 lr: 0.0001\n",
      "Iter: 174/3021, loss_train: 6.7354, loss_val: nan, pos_over_neg: 90.73112487792969 lr: 0.0001\n",
      "Iter: 175/3021, loss_train: 6.719, loss_val: nan, pos_over_neg: 94.68084716796875 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 176/3021, loss_train: 6.7071, loss_val: nan, pos_over_neg: 84.76749420166016 lr: 0.0001\n",
      "Iter: 177/3021, loss_train: 6.7194, loss_val: nan, pos_over_neg: 70.1619644165039 lr: 0.0001\n",
      "Iter: 178/3021, loss_train: 6.7313, loss_val: nan, pos_over_neg: 50.301334381103516 lr: 0.0001\n",
      "Iter: 179/3021, loss_train: 6.715, loss_val: nan, pos_over_neg: 59.340797424316406 lr: 0.0001\n",
      "Iter: 180/3021, loss_train: 6.7239, loss_val: nan, pos_over_neg: 47.43552017211914 lr: 0.0001\n",
      "Iter: 181/3021, loss_train: 6.7145, loss_val: nan, pos_over_neg: 53.7491340637207 lr: 0.0001\n",
      "Iter: 182/3021, loss_train: 6.7351, loss_val: nan, pos_over_neg: 57.259178161621094 lr: 0.0001\n",
      "Iter: 183/3021, loss_train: 6.7138, loss_val: nan, pos_over_neg: 57.80210876464844 lr: 0.0001\n",
      "Iter: 184/3021, loss_train: 6.6973, loss_val: nan, pos_over_neg: 86.44732666015625 lr: 0.0001\n",
      "Iter: 185/3021, loss_train: 6.7185, loss_val: nan, pos_over_neg: 71.84893035888672 lr: 0.0001\n",
      "Iter: 186/3021, loss_train: 6.7022, loss_val: nan, pos_over_neg: 73.95223236083984 lr: 0.0001\n",
      "Iter: 187/3021, loss_train: 6.7373, loss_val: nan, pos_over_neg: 84.15961456298828 lr: 0.0001\n",
      "Iter: 188/3021, loss_train: 6.724, loss_val: nan, pos_over_neg: 97.02572631835938 lr: 0.0001\n",
      "Iter: 189/3021, loss_train: 6.6999, loss_val: nan, pos_over_neg: 105.72530364990234 lr: 0.0001\n",
      "Iter: 190/3021, loss_train: 6.7169, loss_val: nan, pos_over_neg: 101.1483383178711 lr: 0.0001\n",
      "Iter: 191/3021, loss_train: 6.7108, loss_val: nan, pos_over_neg: 103.40109252929688 lr: 0.0001\n",
      "Iter: 192/3021, loss_train: 6.7197, loss_val: nan, pos_over_neg: 125.79534149169922 lr: 0.0001\n",
      "Iter: 193/3021, loss_train: 6.6937, loss_val: nan, pos_over_neg: 113.58345794677734 lr: 0.0001\n",
      "Iter: 194/3021, loss_train: 6.7209, loss_val: nan, pos_over_neg: 101.97232055664062 lr: 0.0001\n",
      "Iter: 195/3021, loss_train: 6.6906, loss_val: nan, pos_over_neg: 133.75140380859375 lr: 0.0001\n",
      "Iter: 196/3021, loss_train: 6.6794, loss_val: nan, pos_over_neg: 101.63848876953125 lr: 0.0001\n",
      "Iter: 197/3021, loss_train: 6.7153, loss_val: nan, pos_over_neg: 66.66683959960938 lr: 0.0001\n",
      "Iter: 198/3021, loss_train: 6.6969, loss_val: nan, pos_over_neg: 68.86665344238281 lr: 0.0001\n",
      "Iter: 199/3021, loss_train: 6.7088, loss_val: nan, pos_over_neg: 68.2568588256836 lr: 0.0001\n",
      "Iter: 200/3021, loss_train: 6.685, loss_val: nan, pos_over_neg: 80.69832611083984 lr: 0.0001\n",
      "Iter: 201/3021, loss_train: 6.7106, loss_val: nan, pos_over_neg: 66.3100357055664 lr: 0.0001\n",
      "Iter: 202/3021, loss_train: 6.6827, loss_val: nan, pos_over_neg: 76.94499206542969 lr: 0.0001\n",
      "Iter: 203/3021, loss_train: 6.7002, loss_val: nan, pos_over_neg: 80.6025619506836 lr: 0.0001\n",
      "Iter: 204/3021, loss_train: 6.722, loss_val: nan, pos_over_neg: 77.79830932617188 lr: 0.0001\n",
      "Iter: 205/3021, loss_train: 6.6806, loss_val: nan, pos_over_neg: 77.56159973144531 lr: 0.0001\n",
      "Iter: 206/3021, loss_train: 6.7293, loss_val: nan, pos_over_neg: 84.55481719970703 lr: 0.0001\n",
      "Iter: 207/3021, loss_train: 6.7009, loss_val: nan, pos_over_neg: 92.25067901611328 lr: 0.0001\n",
      "Iter: 208/3021, loss_train: 6.6996, loss_val: nan, pos_over_neg: 142.18026733398438 lr: 0.0001\n",
      "Iter: 209/3021, loss_train: 6.6923, loss_val: nan, pos_over_neg: 155.98448181152344 lr: 0.0001\n",
      "Iter: 210/3021, loss_train: 6.6875, loss_val: nan, pos_over_neg: 142.7229461669922 lr: 0.0001\n",
      "Iter: 211/3021, loss_train: 6.7137, loss_val: nan, pos_over_neg: 133.74649047851562 lr: 0.0001\n",
      "Iter: 212/3021, loss_train: 6.7089, loss_val: nan, pos_over_neg: 101.69738006591797 lr: 0.0001\n",
      "Iter: 213/3021, loss_train: 6.7024, loss_val: nan, pos_over_neg: 100.99839782714844 lr: 0.0001\n",
      "Iter: 214/3021, loss_train: 6.712, loss_val: nan, pos_over_neg: 113.57456970214844 lr: 0.0001\n",
      "Iter: 215/3021, loss_train: 6.6684, loss_val: nan, pos_over_neg: 83.93827056884766 lr: 0.0001\n",
      "Iter: 216/3021, loss_train: 6.6694, loss_val: nan, pos_over_neg: 106.70840454101562 lr: 0.0001\n",
      "Iter: 217/3021, loss_train: 6.6889, loss_val: nan, pos_over_neg: 82.01596069335938 lr: 0.0001\n",
      "Iter: 218/3021, loss_train: 6.7039, loss_val: nan, pos_over_neg: 74.54161071777344 lr: 0.0001\n",
      "Iter: 219/3021, loss_train: 6.6857, loss_val: nan, pos_over_neg: 68.9515380859375 lr: 0.0001\n",
      "Iter: 220/3021, loss_train: 6.7314, loss_val: nan, pos_over_neg: 58.126834869384766 lr: 0.0001\n",
      "Iter: 221/3021, loss_train: 6.701, loss_val: nan, pos_over_neg: 81.55027770996094 lr: 0.0001\n",
      "Iter: 222/3021, loss_train: 6.7232, loss_val: nan, pos_over_neg: 91.59626770019531 lr: 0.0001\n",
      "Iter: 223/3021, loss_train: 6.6894, loss_val: nan, pos_over_neg: 116.49406433105469 lr: 0.0001\n",
      "Iter: 224/3021, loss_train: 6.6857, loss_val: nan, pos_over_neg: 102.48033142089844 lr: 0.0001\n",
      "Iter: 225/3021, loss_train: 6.6961, loss_val: nan, pos_over_neg: 114.23649597167969 lr: 0.0001\n",
      "Iter: 226/3021, loss_train: 6.6875, loss_val: nan, pos_over_neg: 97.00934600830078 lr: 0.0001\n",
      "Iter: 227/3021, loss_train: 6.6983, loss_val: nan, pos_over_neg: 78.96463775634766 lr: 0.0001\n",
      "Iter: 228/3021, loss_train: 6.7192, loss_val: nan, pos_over_neg: 64.34586334228516 lr: 0.0001\n",
      "Iter: 229/3021, loss_train: 6.6826, loss_val: nan, pos_over_neg: 93.03681182861328 lr: 0.0001\n",
      "Iter: 230/3021, loss_train: 6.703, loss_val: nan, pos_over_neg: 68.14656066894531 lr: 0.0001\n",
      "Iter: 231/3021, loss_train: 6.6776, loss_val: nan, pos_over_neg: 81.82646179199219 lr: 0.0001\n",
      "Iter: 232/3021, loss_train: 6.6925, loss_val: nan, pos_over_neg: 81.14112854003906 lr: 0.0001\n",
      "Iter: 233/3021, loss_train: 6.6841, loss_val: nan, pos_over_neg: 94.3615951538086 lr: 0.0001\n",
      "Iter: 234/3021, loss_train: 6.686, loss_val: nan, pos_over_neg: 99.1222152709961 lr: 0.0001\n",
      "Iter: 235/3021, loss_train: 6.7019, loss_val: nan, pos_over_neg: 124.48519134521484 lr: 0.0001\n",
      "Iter: 236/3021, loss_train: 6.7045, loss_val: nan, pos_over_neg: 133.365966796875 lr: 0.0001\n",
      "Iter: 237/3021, loss_train: 6.6739, loss_val: nan, pos_over_neg: 209.5004425048828 lr: 0.0001\n",
      "Iter: 238/3021, loss_train: 6.7043, loss_val: nan, pos_over_neg: 123.2796630859375 lr: 0.0001\n",
      "Iter: 239/3021, loss_train: 6.6911, loss_val: nan, pos_over_neg: 148.38894653320312 lr: 0.0001\n",
      "Iter: 240/3021, loss_train: 6.691, loss_val: nan, pos_over_neg: 97.31390380859375 lr: 0.0001\n",
      "Iter: 241/3021, loss_train: 6.6691, loss_val: nan, pos_over_neg: 82.40741729736328 lr: 0.0001\n",
      "Iter: 242/3021, loss_train: 6.6531, loss_val: nan, pos_over_neg: 65.0707015991211 lr: 0.0001\n",
      "Iter: 243/3021, loss_train: 6.6906, loss_val: nan, pos_over_neg: 49.05887222290039 lr: 0.0001\n",
      "Iter: 244/3021, loss_train: 6.6778, loss_val: nan, pos_over_neg: 51.740936279296875 lr: 0.0001\n",
      "Iter: 245/3021, loss_train: 6.7057, loss_val: nan, pos_over_neg: 49.9678955078125 lr: 0.0001\n",
      "Iter: 246/3021, loss_train: 6.6721, loss_val: nan, pos_over_neg: 63.70652770996094 lr: 0.0001\n",
      "Iter: 247/3021, loss_train: 6.6596, loss_val: nan, pos_over_neg: 74.46138000488281 lr: 0.0001\n",
      "Iter: 248/3021, loss_train: 6.695, loss_val: nan, pos_over_neg: 78.19060516357422 lr: 0.0001\n",
      "Iter: 249/3021, loss_train: 6.6695, loss_val: nan, pos_over_neg: 89.50886535644531 lr: 0.0001\n",
      "Iter: 250/3021, loss_train: 6.6872, loss_val: nan, pos_over_neg: 84.96765899658203 lr: 0.0001\n",
      "Iter: 251/3021, loss_train: 6.6531, loss_val: nan, pos_over_neg: 126.42985534667969 lr: 0.0001\n",
      "Iter: 252/3021, loss_train: 6.6978, loss_val: nan, pos_over_neg: 150.4954071044922 lr: 0.0001\n",
      "Iter: 253/3021, loss_train: 6.7268, loss_val: nan, pos_over_neg: 159.4720916748047 lr: 0.0001\n",
      "Iter: 254/3021, loss_train: 6.6526, loss_val: nan, pos_over_neg: 323.16998291015625 lr: 0.0001\n",
      "Iter: 255/3021, loss_train: 6.6958, loss_val: nan, pos_over_neg: 133.1451873779297 lr: 0.0001\n",
      "Iter: 256/3021, loss_train: 6.6475, loss_val: nan, pos_over_neg: 151.48208618164062 lr: 0.0001\n",
      "Iter: 257/3021, loss_train: 6.6544, loss_val: nan, pos_over_neg: 126.54412841796875 lr: 0.0001\n",
      "Iter: 258/3021, loss_train: 6.6853, loss_val: nan, pos_over_neg: 78.16667175292969 lr: 0.0001\n",
      "Iter: 259/3021, loss_train: 6.7041, loss_val: nan, pos_over_neg: 65.16626739501953 lr: 0.0001\n",
      "Iter: 260/3021, loss_train: 6.6725, loss_val: nan, pos_over_neg: 64.7787094116211 lr: 0.0001\n",
      "Iter: 261/3021, loss_train: 6.6757, loss_val: nan, pos_over_neg: 59.26289367675781 lr: 0.0001\n",
      "Iter: 262/3021, loss_train: 6.6678, loss_val: nan, pos_over_neg: 57.83658981323242 lr: 0.0001\n",
      "Iter: 263/3021, loss_train: 6.6999, loss_val: nan, pos_over_neg: 66.29720306396484 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 264/3021, loss_train: 6.7164, loss_val: nan, pos_over_neg: 56.449798583984375 lr: 0.0001\n",
      "Iter: 265/3021, loss_train: 6.71, loss_val: nan, pos_over_neg: 69.50334167480469 lr: 0.0001\n",
      "Iter: 266/3021, loss_train: 6.6901, loss_val: nan, pos_over_neg: 108.88590240478516 lr: 0.0001\n",
      "Iter: 267/3021, loss_train: 6.675, loss_val: nan, pos_over_neg: 110.29940795898438 lr: 0.0001\n",
      "Iter: 268/3021, loss_train: 6.6815, loss_val: nan, pos_over_neg: 136.855712890625 lr: 0.0001\n",
      "Iter: 269/3021, loss_train: 6.6864, loss_val: nan, pos_over_neg: 126.91331481933594 lr: 0.0001\n",
      "Iter: 270/3021, loss_train: 6.6948, loss_val: nan, pos_over_neg: 102.88433837890625 lr: 0.0001\n",
      "Iter: 271/3021, loss_train: 6.682, loss_val: nan, pos_over_neg: 117.58699035644531 lr: 0.0001\n",
      "Iter: 272/3021, loss_train: 6.6672, loss_val: nan, pos_over_neg: 123.74059295654297 lr: 0.0001\n",
      "Iter: 273/3021, loss_train: 6.6968, loss_val: nan, pos_over_neg: 97.98333740234375 lr: 0.0001\n",
      "Iter: 274/3021, loss_train: 6.6962, loss_val: nan, pos_over_neg: 79.65541076660156 lr: 0.0001\n",
      "Iter: 275/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 92.03646087646484 lr: 0.0001\n",
      "Iter: 276/3021, loss_train: 6.6816, loss_val: nan, pos_over_neg: 104.34444427490234 lr: 0.0001\n",
      "Iter: 277/3021, loss_train: 6.6631, loss_val: nan, pos_over_neg: 90.28307342529297 lr: 0.0001\n",
      "Iter: 278/3021, loss_train: 6.6663, loss_val: nan, pos_over_neg: 107.57144927978516 lr: 0.0001\n",
      "Iter: 279/3021, loss_train: 6.669, loss_val: nan, pos_over_neg: 82.69637298583984 lr: 0.0001\n",
      "Iter: 280/3021, loss_train: 6.6711, loss_val: nan, pos_over_neg: 116.74626159667969 lr: 0.0001\n",
      "Iter: 281/3021, loss_train: 6.6424, loss_val: nan, pos_over_neg: 94.11897277832031 lr: 0.0001\n",
      "Iter: 282/3021, loss_train: 6.6815, loss_val: nan, pos_over_neg: 64.04624938964844 lr: 0.0001\n",
      "Iter: 283/3021, loss_train: 6.6788, loss_val: nan, pos_over_neg: 76.94560241699219 lr: 0.0001\n",
      "Iter: 284/3021, loss_train: 6.6913, loss_val: nan, pos_over_neg: 73.29693603515625 lr: 0.0001\n",
      "Iter: 285/3021, loss_train: 6.6532, loss_val: nan, pos_over_neg: 77.46342468261719 lr: 0.0001\n",
      "Iter: 286/3021, loss_train: 6.6587, loss_val: nan, pos_over_neg: 82.83592224121094 lr: 0.0001\n",
      "Iter: 287/3021, loss_train: 6.661, loss_val: nan, pos_over_neg: 86.6082534790039 lr: 0.0001\n",
      "Iter: 288/3021, loss_train: 6.687, loss_val: nan, pos_over_neg: 81.53919219970703 lr: 0.0001\n",
      "Iter: 289/3021, loss_train: 6.6788, loss_val: nan, pos_over_neg: 112.6345443725586 lr: 0.0001\n",
      "Iter: 290/3021, loss_train: 6.6391, loss_val: nan, pos_over_neg: 194.77415466308594 lr: 0.0001\n",
      "Iter: 291/3021, loss_train: 6.6728, loss_val: nan, pos_over_neg: 108.34639739990234 lr: 0.0001\n",
      "Iter: 292/3021, loss_train: 6.6334, loss_val: nan, pos_over_neg: 144.98458862304688 lr: 0.0001\n",
      "Iter: 293/3021, loss_train: 6.7033, loss_val: nan, pos_over_neg: 97.97551727294922 lr: 0.0001\n",
      "Iter: 294/3021, loss_train: 6.6582, loss_val: nan, pos_over_neg: 88.14102935791016 lr: 0.0001\n",
      "Iter: 295/3021, loss_train: 6.6662, loss_val: nan, pos_over_neg: 78.91661834716797 lr: 0.0001\n",
      "Iter: 296/3021, loss_train: 6.6797, loss_val: nan, pos_over_neg: 80.546630859375 lr: 0.0001\n",
      "Iter: 297/3021, loss_train: 6.6595, loss_val: nan, pos_over_neg: 66.97296905517578 lr: 0.0001\n",
      "Iter: 298/3021, loss_train: 6.6602, loss_val: nan, pos_over_neg: 73.58814239501953 lr: 0.0001\n",
      "Iter: 299/3021, loss_train: 6.6811, loss_val: nan, pos_over_neg: 66.23306274414062 lr: 0.0001\n",
      "Iter: 300/3021, loss_train: 6.6745, loss_val: nan, pos_over_neg: 68.51049041748047 lr: 0.0001\n",
      "Iter: 301/3021, loss_train: 6.6926, loss_val: nan, pos_over_neg: 65.71810913085938 lr: 0.0001\n",
      "Iter: 302/3021, loss_train: 6.6903, loss_val: nan, pos_over_neg: 73.8968505859375 lr: 0.0001\n",
      "Iter: 303/3021, loss_train: 6.6567, loss_val: nan, pos_over_neg: 110.89866638183594 lr: 0.0001\n",
      "Iter: 304/3021, loss_train: 6.6688, loss_val: nan, pos_over_neg: 95.83787536621094 lr: 0.0001\n",
      "Iter: 305/3021, loss_train: 6.6639, loss_val: nan, pos_over_neg: 109.94551849365234 lr: 0.0001\n",
      "Iter: 306/3021, loss_train: 6.6885, loss_val: nan, pos_over_neg: 102.9513931274414 lr: 0.0001\n",
      "Iter: 307/3021, loss_train: 6.6559, loss_val: nan, pos_over_neg: 137.40386962890625 lr: 0.0001\n",
      "Iter: 308/3021, loss_train: 6.6692, loss_val: nan, pos_over_neg: 96.13809204101562 lr: 0.0001\n",
      "Iter: 309/3021, loss_train: 6.668, loss_val: nan, pos_over_neg: 86.10679626464844 lr: 0.0001\n",
      "Iter: 310/3021, loss_train: 6.6716, loss_val: nan, pos_over_neg: 87.78389739990234 lr: 0.0001\n",
      "Iter: 311/3021, loss_train: 6.6377, loss_val: nan, pos_over_neg: 109.6025619506836 lr: 0.0001\n",
      "Iter: 312/3021, loss_train: 6.6476, loss_val: nan, pos_over_neg: 100.9728012084961 lr: 0.0001\n",
      "Iter: 313/3021, loss_train: 6.6627, loss_val: nan, pos_over_neg: 98.6878890991211 lr: 0.0001\n",
      "Iter: 314/3021, loss_train: 6.6809, loss_val: nan, pos_over_neg: 110.98111724853516 lr: 0.0001\n",
      "Iter: 315/3021, loss_train: 6.6773, loss_val: nan, pos_over_neg: 92.09634399414062 lr: 0.0001\n",
      "Iter: 316/3021, loss_train: 6.6595, loss_val: nan, pos_over_neg: 86.76773071289062 lr: 0.0001\n",
      "Iter: 317/3021, loss_train: 6.6708, loss_val: nan, pos_over_neg: 68.5373764038086 lr: 0.0001\n",
      "Iter: 318/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 115.41385650634766 lr: 0.0001\n",
      "Iter: 319/3021, loss_train: 6.6508, loss_val: nan, pos_over_neg: 83.01995086669922 lr: 0.0001\n",
      "Iter: 320/3021, loss_train: 6.6735, loss_val: nan, pos_over_neg: 92.32771301269531 lr: 0.0001\n",
      "Iter: 321/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 113.03224182128906 lr: 0.0001\n",
      "Iter: 322/3021, loss_train: 6.6786, loss_val: nan, pos_over_neg: 93.7867431640625 lr: 0.0001\n",
      "Iter: 323/3021, loss_train: 6.6378, loss_val: nan, pos_over_neg: 170.68226623535156 lr: 0.0001\n",
      "Iter: 324/3021, loss_train: 6.6619, loss_val: nan, pos_over_neg: 107.05011749267578 lr: 0.0001\n",
      "Iter: 325/3021, loss_train: 6.6658, loss_val: nan, pos_over_neg: 105.67250061035156 lr: 0.0001\n",
      "Iter: 326/3021, loss_train: 6.6543, loss_val: nan, pos_over_neg: 111.35372161865234 lr: 0.0001\n",
      "Iter: 327/3021, loss_train: 6.6669, loss_val: nan, pos_over_neg: 92.36734771728516 lr: 0.0001\n",
      "Iter: 328/3021, loss_train: 6.6641, loss_val: nan, pos_over_neg: 81.20365142822266 lr: 0.0001\n",
      "Iter: 329/3021, loss_train: 6.6731, loss_val: nan, pos_over_neg: 84.63821411132812 lr: 0.0001\n",
      "Iter: 330/3021, loss_train: 6.6639, loss_val: nan, pos_over_neg: 75.58375549316406 lr: 0.0001\n",
      "Iter: 331/3021, loss_train: 6.6625, loss_val: nan, pos_over_neg: 69.33367156982422 lr: 0.0001\n",
      "Iter: 332/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 69.64566802978516 lr: 0.0001\n",
      "Iter: 333/3021, loss_train: 6.6474, loss_val: nan, pos_over_neg: 71.13440704345703 lr: 0.0001\n",
      "Iter: 334/3021, loss_train: 6.6234, loss_val: nan, pos_over_neg: 87.0166244506836 lr: 0.0001\n",
      "Iter: 335/3021, loss_train: 6.6333, loss_val: nan, pos_over_neg: 101.45155334472656 lr: 0.0001\n",
      "Iter: 336/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 80.42835998535156 lr: 0.0001\n",
      "Iter: 337/3021, loss_train: 6.6694, loss_val: nan, pos_over_neg: 96.0687026977539 lr: 0.0001\n",
      "Iter: 338/3021, loss_train: 6.6633, loss_val: nan, pos_over_neg: 113.58574676513672 lr: 0.0001\n",
      "Iter: 339/3021, loss_train: 6.6473, loss_val: nan, pos_over_neg: 134.1546630859375 lr: 0.0001\n",
      "Iter: 340/3021, loss_train: 6.6706, loss_val: nan, pos_over_neg: 133.03097534179688 lr: 0.0001\n",
      "Iter: 341/3021, loss_train: 6.6416, loss_val: nan, pos_over_neg: 110.16153717041016 lr: 0.0001\n",
      "Iter: 342/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 105.60099792480469 lr: 0.0001\n",
      "Iter: 343/3021, loss_train: 6.6853, loss_val: nan, pos_over_neg: 59.07761001586914 lr: 0.0001\n",
      "Iter: 344/3021, loss_train: 6.6673, loss_val: nan, pos_over_neg: 66.02745056152344 lr: 0.0001\n",
      "Iter: 345/3021, loss_train: 6.6591, loss_val: nan, pos_over_neg: 60.90229034423828 lr: 0.0001\n",
      "Iter: 346/3021, loss_train: 6.6335, loss_val: nan, pos_over_neg: 91.63915252685547 lr: 0.0001\n",
      "Iter: 347/3021, loss_train: 6.6363, loss_val: nan, pos_over_neg: 86.37950897216797 lr: 0.0001\n",
      "Iter: 348/3021, loss_train: 6.6512, loss_val: nan, pos_over_neg: 84.38880157470703 lr: 0.0001\n",
      "Iter: 349/3021, loss_train: 6.6348, loss_val: nan, pos_over_neg: 103.31441497802734 lr: 0.0001\n",
      "Iter: 350/3021, loss_train: 6.6434, loss_val: nan, pos_over_neg: 104.93865966796875 lr: 0.0001\n",
      "Iter: 351/3021, loss_train: 6.6734, loss_val: nan, pos_over_neg: 95.90497589111328 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 352/3021, loss_train: 6.6439, loss_val: nan, pos_over_neg: 125.76329803466797 lr: 0.0001\n",
      "Iter: 353/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 113.29776763916016 lr: 0.0001\n",
      "Iter: 354/3021, loss_train: 6.6567, loss_val: nan, pos_over_neg: 89.5253677368164 lr: 0.0001\n",
      "Iter: 355/3021, loss_train: 6.6386, loss_val: nan, pos_over_neg: 118.8377456665039 lr: 0.0001\n",
      "Iter: 356/3021, loss_train: 6.6223, loss_val: nan, pos_over_neg: 176.91030883789062 lr: 0.0001\n",
      "Iter: 357/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 96.75997924804688 lr: 0.0001\n",
      "Iter: 358/3021, loss_train: 6.6538, loss_val: nan, pos_over_neg: 98.95654296875 lr: 0.0001\n",
      "Iter: 359/3021, loss_train: 6.6364, loss_val: nan, pos_over_neg: 105.08909606933594 lr: 0.0001\n",
      "Iter: 360/3021, loss_train: 6.6822, loss_val: nan, pos_over_neg: 66.63834381103516 lr: 0.0001\n",
      "Iter: 361/3021, loss_train: 6.6514, loss_val: nan, pos_over_neg: 81.10625457763672 lr: 0.0001\n",
      "Iter: 362/3021, loss_train: 6.5959, loss_val: nan, pos_over_neg: 97.53778839111328 lr: 0.0001\n",
      "Iter: 363/3021, loss_train: 6.662, loss_val: nan, pos_over_neg: 80.00199127197266 lr: 0.0001\n",
      "Iter: 364/3021, loss_train: 6.6433, loss_val: nan, pos_over_neg: 78.23381805419922 lr: 0.0001\n",
      "Iter: 365/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 85.30908966064453 lr: 0.0001\n",
      "Iter: 366/3021, loss_train: 6.6554, loss_val: nan, pos_over_neg: 80.56475067138672 lr: 0.0001\n",
      "Iter: 367/3021, loss_train: 6.602, loss_val: nan, pos_over_neg: 102.73753356933594 lr: 0.0001\n",
      "Iter: 368/3021, loss_train: 6.6623, loss_val: nan, pos_over_neg: 107.22395324707031 lr: 0.0001\n",
      "Iter: 369/3021, loss_train: 6.6452, loss_val: nan, pos_over_neg: 116.6164321899414 lr: 0.0001\n",
      "Iter: 370/3021, loss_train: 6.6657, loss_val: nan, pos_over_neg: 176.58331298828125 lr: 0.0001\n",
      "Iter: 371/3021, loss_train: 6.6688, loss_val: nan, pos_over_neg: 138.6767120361328 lr: 0.0001\n",
      "Iter: 372/3021, loss_train: 6.645, loss_val: nan, pos_over_neg: 121.26986694335938 lr: 0.0001\n",
      "Iter: 373/3021, loss_train: 6.6288, loss_val: nan, pos_over_neg: 117.08798217773438 lr: 0.0001\n",
      "Iter: 374/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 97.62513732910156 lr: 0.0001\n",
      "Iter: 375/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 86.76309204101562 lr: 0.0001\n",
      "Iter: 376/3021, loss_train: 6.6801, loss_val: nan, pos_over_neg: 90.28624725341797 lr: 0.0001\n",
      "Iter: 377/3021, loss_train: 6.6614, loss_val: nan, pos_over_neg: 77.59906005859375 lr: 0.0001\n",
      "Iter: 378/3021, loss_train: 6.6626, loss_val: nan, pos_over_neg: 68.15480041503906 lr: 0.0001\n",
      "Iter: 379/3021, loss_train: 6.6523, loss_val: nan, pos_over_neg: 89.90132904052734 lr: 0.0001\n",
      "Iter: 380/3021, loss_train: 6.6508, loss_val: nan, pos_over_neg: 120.88763427734375 lr: 0.0001\n",
      "Iter: 381/3021, loss_train: 6.669, loss_val: nan, pos_over_neg: 96.80363464355469 lr: 0.0001\n",
      "Iter: 382/3021, loss_train: 6.6603, loss_val: nan, pos_over_neg: 106.65241241455078 lr: 0.0001\n",
      "Iter: 383/3021, loss_train: 6.6578, loss_val: nan, pos_over_neg: 105.54267883300781 lr: 0.0001\n",
      "Iter: 384/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 119.28977966308594 lr: 0.0001\n",
      "Iter: 385/3021, loss_train: 6.6363, loss_val: nan, pos_over_neg: 191.4713592529297 lr: 0.0001\n",
      "Iter: 386/3021, loss_train: 6.6279, loss_val: nan, pos_over_neg: 127.69873809814453 lr: 0.0001\n",
      "Iter: 387/3021, loss_train: 6.6239, loss_val: nan, pos_over_neg: 106.16243743896484 lr: 0.0001\n",
      "Iter: 388/3021, loss_train: 6.6402, loss_val: nan, pos_over_neg: 140.34542846679688 lr: 0.0001\n",
      "Iter: 389/3021, loss_train: 6.6375, loss_val: nan, pos_over_neg: 104.75823211669922 lr: 0.0001\n",
      "Iter: 390/3021, loss_train: 6.6226, loss_val: nan, pos_over_neg: 92.54187774658203 lr: 0.0001\n",
      "Iter: 391/3021, loss_train: 6.6514, loss_val: nan, pos_over_neg: 76.32308959960938 lr: 0.0001\n",
      "Iter: 392/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 61.9380989074707 lr: 0.0001\n",
      "Iter: 393/3021, loss_train: 6.6714, loss_val: nan, pos_over_neg: 63.10841751098633 lr: 0.0001\n",
      "Iter: 394/3021, loss_train: 6.6267, loss_val: nan, pos_over_neg: 59.48258972167969 lr: 0.0001\n",
      "Iter: 395/3021, loss_train: 6.6465, loss_val: nan, pos_over_neg: 64.98129272460938 lr: 0.0001\n",
      "Iter: 396/3021, loss_train: 6.6745, loss_val: nan, pos_over_neg: 69.62779998779297 lr: 0.0001\n",
      "Iter: 397/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 96.06591033935547 lr: 0.0001\n",
      "Iter: 398/3021, loss_train: 6.6227, loss_val: nan, pos_over_neg: 188.2084197998047 lr: 0.0001\n",
      "Iter: 399/3021, loss_train: 6.6284, loss_val: nan, pos_over_neg: 209.0440673828125 lr: 0.0001\n",
      "Iter: 400/3021, loss_train: 6.6414, loss_val: nan, pos_over_neg: 185.0422821044922 lr: 0.0001\n",
      "Iter: 401/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 121.77044677734375 lr: 0.0001\n",
      "Iter: 402/3021, loss_train: 6.6359, loss_val: nan, pos_over_neg: 146.06585693359375 lr: 0.0001\n",
      "Iter: 403/3021, loss_train: 6.6368, loss_val: nan, pos_over_neg: 107.43843841552734 lr: 0.0001\n",
      "Iter: 404/3021, loss_train: 6.6719, loss_val: nan, pos_over_neg: 136.11122131347656 lr: 0.0001\n",
      "Iter: 405/3021, loss_train: 6.6554, loss_val: nan, pos_over_neg: 95.84779357910156 lr: 0.0001\n",
      "Iter: 406/3021, loss_train: 6.656, loss_val: nan, pos_over_neg: 85.85224914550781 lr: 0.0001\n",
      "Iter: 407/3021, loss_train: 6.6559, loss_val: nan, pos_over_neg: 69.48261260986328 lr: 0.0001\n",
      "Iter: 408/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 67.01167297363281 lr: 0.0001\n",
      "Iter: 409/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 64.1944351196289 lr: 0.0001\n",
      "Iter: 410/3021, loss_train: 6.6413, loss_val: nan, pos_over_neg: 55.452640533447266 lr: 0.0001\n",
      "Iter: 411/3021, loss_train: 6.6148, loss_val: nan, pos_over_neg: 61.286834716796875 lr: 0.0001\n",
      "Iter: 412/3021, loss_train: 6.6614, loss_val: nan, pos_over_neg: 61.9512939453125 lr: 0.0001\n",
      "Iter: 413/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 69.24641418457031 lr: 0.0001\n",
      "Iter: 414/3021, loss_train: 6.6049, loss_val: nan, pos_over_neg: 102.68211364746094 lr: 0.0001\n",
      "Iter: 415/3021, loss_train: 6.6421, loss_val: nan, pos_over_neg: 115.35651397705078 lr: 0.0001\n",
      "Iter: 416/3021, loss_train: 6.6106, loss_val: nan, pos_over_neg: 129.8797607421875 lr: 0.0001\n",
      "Iter: 417/3021, loss_train: 6.6216, loss_val: nan, pos_over_neg: 104.5445556640625 lr: 0.0001\n",
      "Iter: 418/3021, loss_train: 6.6427, loss_val: nan, pos_over_neg: 123.41105651855469 lr: 0.0001\n",
      "Iter: 419/3021, loss_train: 6.6729, loss_val: nan, pos_over_neg: 108.22498321533203 lr: 0.0001\n",
      "Iter: 420/3021, loss_train: 6.6236, loss_val: nan, pos_over_neg: 107.05804443359375 lr: 0.0001\n",
      "Iter: 421/3021, loss_train: 6.6162, loss_val: nan, pos_over_neg: 100.08367156982422 lr: 0.0001\n",
      "Iter: 422/3021, loss_train: 6.662, loss_val: nan, pos_over_neg: 121.63780975341797 lr: 0.0001\n",
      "Iter: 423/3021, loss_train: 6.6265, loss_val: nan, pos_over_neg: 85.68254089355469 lr: 0.0001\n",
      "Iter: 424/3021, loss_train: 6.6665, loss_val: nan, pos_over_neg: 54.07424545288086 lr: 0.0001\n",
      "Iter: 425/3021, loss_train: 6.6338, loss_val: nan, pos_over_neg: 73.62643432617188 lr: 0.0001\n",
      "Iter: 426/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 114.16816711425781 lr: 0.0001\n",
      "Iter: 427/3021, loss_train: 6.6268, loss_val: nan, pos_over_neg: 110.62196350097656 lr: 0.0001\n",
      "Iter: 428/3021, loss_train: 6.6236, loss_val: nan, pos_over_neg: 130.805908203125 lr: 0.0001\n",
      "Iter: 429/3021, loss_train: 6.6133, loss_val: nan, pos_over_neg: 167.09495544433594 lr: 0.0001\n",
      "Iter: 430/3021, loss_train: 6.6283, loss_val: nan, pos_over_neg: 145.8020782470703 lr: 0.0001\n",
      "Iter: 431/3021, loss_train: 6.6437, loss_val: nan, pos_over_neg: 124.7811279296875 lr: 0.0001\n",
      "Iter: 432/3021, loss_train: 6.6272, loss_val: nan, pos_over_neg: 103.73365020751953 lr: 0.0001\n",
      "Iter: 433/3021, loss_train: 6.623, loss_val: nan, pos_over_neg: 137.7026824951172 lr: 0.0001\n",
      "Iter: 434/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 108.7846450805664 lr: 0.0001\n",
      "Iter: 435/3021, loss_train: 6.6342, loss_val: nan, pos_over_neg: 123.85726928710938 lr: 0.0001\n",
      "Iter: 436/3021, loss_train: 6.6591, loss_val: nan, pos_over_neg: 71.30896759033203 lr: 0.0001\n",
      "Iter: 437/3021, loss_train: 6.6415, loss_val: nan, pos_over_neg: 75.47917938232422 lr: 0.0001\n",
      "Iter: 438/3021, loss_train: 6.6451, loss_val: nan, pos_over_neg: 63.713321685791016 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 439/3021, loss_train: 6.601, loss_val: nan, pos_over_neg: 75.08138275146484 lr: 0.0001\n",
      "Iter: 440/3021, loss_train: 6.6258, loss_val: nan, pos_over_neg: 74.86217498779297 lr: 0.0001\n",
      "Iter: 441/3021, loss_train: 6.63, loss_val: nan, pos_over_neg: 83.85758972167969 lr: 0.0001\n",
      "Iter: 442/3021, loss_train: 6.6587, loss_val: nan, pos_over_neg: 91.13420867919922 lr: 0.0001\n",
      "Iter: 443/3021, loss_train: 6.6252, loss_val: nan, pos_over_neg: 178.44361877441406 lr: 0.0001\n",
      "Iter: 444/3021, loss_train: 6.6242, loss_val: nan, pos_over_neg: 131.99794006347656 lr: 0.0001\n",
      "Iter: 445/3021, loss_train: 6.6468, loss_val: nan, pos_over_neg: 139.25357055664062 lr: 0.0001\n",
      "Iter: 446/3021, loss_train: 6.6464, loss_val: nan, pos_over_neg: 164.56912231445312 lr: 0.0001\n",
      "Iter: 447/3021, loss_train: 6.647, loss_val: nan, pos_over_neg: 156.39125061035156 lr: 0.0001\n",
      "Iter: 448/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 164.92901611328125 lr: 0.0001\n",
      "Iter: 449/3021, loss_train: 6.6349, loss_val: nan, pos_over_neg: 120.19703674316406 lr: 0.0001\n",
      "Iter: 450/3021, loss_train: 6.643, loss_val: nan, pos_over_neg: 137.4410858154297 lr: 0.0001\n",
      "Iter: 451/3021, loss_train: 6.6342, loss_val: nan, pos_over_neg: 102.31673431396484 lr: 0.0001\n",
      "Iter: 452/3021, loss_train: 6.6321, loss_val: nan, pos_over_neg: 79.9969711303711 lr: 0.0001\n",
      "Iter: 453/3021, loss_train: 6.6467, loss_val: nan, pos_over_neg: 100.12159729003906 lr: 0.0001\n",
      "Iter: 454/3021, loss_train: 6.6351, loss_val: nan, pos_over_neg: 83.82473754882812 lr: 0.0001\n",
      "Iter: 455/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 88.2153549194336 lr: 0.0001\n",
      "Iter: 456/3021, loss_train: 6.6286, loss_val: nan, pos_over_neg: 73.30821228027344 lr: 0.0001\n",
      "Iter: 457/3021, loss_train: 6.6161, loss_val: nan, pos_over_neg: 68.89139556884766 lr: 0.0001\n",
      "Iter: 458/3021, loss_train: 6.6285, loss_val: nan, pos_over_neg: 76.8030014038086 lr: 0.0001\n",
      "Iter: 459/3021, loss_train: 6.6313, loss_val: nan, pos_over_neg: 74.96929931640625 lr: 0.0001\n",
      "Iter: 460/3021, loss_train: 6.6158, loss_val: nan, pos_over_neg: 78.78678131103516 lr: 0.0001\n",
      "Iter: 461/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 66.93316650390625 lr: 0.0001\n",
      "Iter: 462/3021, loss_train: 6.613, loss_val: nan, pos_over_neg: 91.77286529541016 lr: 0.0001\n",
      "Iter: 463/3021, loss_train: 6.6435, loss_val: nan, pos_over_neg: 122.16020965576172 lr: 0.0001\n",
      "Iter: 464/3021, loss_train: 6.6138, loss_val: nan, pos_over_neg: 121.81124114990234 lr: 0.0001\n",
      "Iter: 465/3021, loss_train: 6.6316, loss_val: nan, pos_over_neg: 123.59111785888672 lr: 0.0001\n",
      "Iter: 466/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 184.98440551757812 lr: 0.0001\n",
      "Iter: 467/3021, loss_train: 6.6125, loss_val: nan, pos_over_neg: 211.94020080566406 lr: 0.0001\n",
      "Iter: 468/3021, loss_train: 6.6524, loss_val: nan, pos_over_neg: 173.12298583984375 lr: 0.0001\n",
      "Iter: 469/3021, loss_train: 6.615, loss_val: nan, pos_over_neg: 157.1043243408203 lr: 0.0001\n",
      "Iter: 470/3021, loss_train: 6.6089, loss_val: nan, pos_over_neg: 130.60169982910156 lr: 0.0001\n",
      "Iter: 471/3021, loss_train: 6.6346, loss_val: nan, pos_over_neg: 94.4458999633789 lr: 0.0001\n",
      "Iter: 472/3021, loss_train: 6.5993, loss_val: nan, pos_over_neg: 91.9759750366211 lr: 0.0001\n",
      "Iter: 473/3021, loss_train: 6.6689, loss_val: nan, pos_over_neg: 73.30775451660156 lr: 0.0001\n",
      "Iter: 474/3021, loss_train: 6.6121, loss_val: nan, pos_over_neg: 73.33035278320312 lr: 0.0001\n",
      "Iter: 475/3021, loss_train: 6.634, loss_val: nan, pos_over_neg: 57.9543342590332 lr: 0.0001\n",
      "Iter: 476/3021, loss_train: 6.6221, loss_val: nan, pos_over_neg: 61.18105697631836 lr: 0.0001\n",
      "Iter: 477/3021, loss_train: 6.6434, loss_val: nan, pos_over_neg: 66.54243469238281 lr: 0.0001\n",
      "Iter: 478/3021, loss_train: 6.6391, loss_val: nan, pos_over_neg: 60.52981185913086 lr: 0.0001\n",
      "Iter: 479/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 73.69983673095703 lr: 0.0001\n",
      "Iter: 480/3021, loss_train: 6.621, loss_val: nan, pos_over_neg: 93.47068786621094 lr: 0.0001\n",
      "Iter: 481/3021, loss_train: 6.6092, loss_val: nan, pos_over_neg: 127.85000610351562 lr: 0.0001\n",
      "Iter: 482/3021, loss_train: 6.6229, loss_val: nan, pos_over_neg: 143.13327026367188 lr: 0.0001\n",
      "Iter: 483/3021, loss_train: 6.5977, loss_val: nan, pos_over_neg: 147.68682861328125 lr: 0.0001\n",
      "Iter: 484/3021, loss_train: 6.6395, loss_val: nan, pos_over_neg: 124.00170135498047 lr: 0.0001\n",
      "Iter: 485/3021, loss_train: 6.6146, loss_val: nan, pos_over_neg: 105.80421447753906 lr: 0.0001\n",
      "Iter: 486/3021, loss_train: 6.6392, loss_val: nan, pos_over_neg: 117.52613830566406 lr: 0.0001\n",
      "Iter: 487/3021, loss_train: 6.622, loss_val: nan, pos_over_neg: 135.64407348632812 lr: 0.0001\n",
      "Iter: 488/3021, loss_train: 6.6209, loss_val: nan, pos_over_neg: 142.2001953125 lr: 0.0001\n",
      "Iter: 489/3021, loss_train: 6.6478, loss_val: nan, pos_over_neg: 68.16737365722656 lr: 0.0001\n",
      "Iter: 490/3021, loss_train: 6.6405, loss_val: nan, pos_over_neg: 93.32061767578125 lr: 0.0001\n",
      "Iter: 491/3021, loss_train: 6.6193, loss_val: nan, pos_over_neg: 74.38084411621094 lr: 0.0001\n",
      "Iter: 492/3021, loss_train: 6.6015, loss_val: nan, pos_over_neg: 108.37134552001953 lr: 0.0001\n",
      "Iter: 493/3021, loss_train: 6.6359, loss_val: nan, pos_over_neg: 90.31268310546875 lr: 0.0001\n",
      "Iter: 494/3021, loss_train: 6.6439, loss_val: nan, pos_over_neg: 69.11817169189453 lr: 0.0001\n",
      "Iter: 495/3021, loss_train: 6.6155, loss_val: nan, pos_over_neg: 123.36785125732422 lr: 0.0001\n",
      "Iter: 496/3021, loss_train: 6.5973, loss_val: nan, pos_over_neg: 117.52815246582031 lr: 0.0001\n",
      "Iter: 497/3021, loss_train: 6.6206, loss_val: nan, pos_over_neg: 109.12016296386719 lr: 0.0001\n",
      "Iter: 498/3021, loss_train: 6.6088, loss_val: nan, pos_over_neg: 201.2156982421875 lr: 0.0001\n",
      "Iter: 499/3021, loss_train: 6.6373, loss_val: nan, pos_over_neg: 108.41218566894531 lr: 0.0001\n",
      "Iter: 500/3021, loss_train: 6.5954, loss_val: nan, pos_over_neg: 166.2609100341797 lr: 0.0001\n",
      "Iter: 501/3021, loss_train: 6.629, loss_val: nan, pos_over_neg: 90.65927124023438 lr: 0.0001\n",
      "Iter: 502/3021, loss_train: 6.6215, loss_val: nan, pos_over_neg: 135.4724884033203 lr: 0.0001\n",
      "Iter: 503/3021, loss_train: 6.6122, loss_val: nan, pos_over_neg: 109.04310607910156 lr: 0.0001\n",
      "Iter: 504/3021, loss_train: 6.6222, loss_val: nan, pos_over_neg: 98.29468536376953 lr: 0.0001\n",
      "Iter: 505/3021, loss_train: 6.6402, loss_val: nan, pos_over_neg: 121.7578353881836 lr: 0.0001\n",
      "Iter: 506/3021, loss_train: 6.6095, loss_val: nan, pos_over_neg: 94.19684600830078 lr: 0.0001\n",
      "Iter: 507/3021, loss_train: 6.6069, loss_val: nan, pos_over_neg: 126.89798736572266 lr: 0.0001\n",
      "Iter: 508/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 91.40262603759766 lr: 0.0001\n",
      "Iter: 509/3021, loss_train: 6.6004, loss_val: nan, pos_over_neg: 107.54779052734375 lr: 0.0001\n",
      "Iter: 510/3021, loss_train: 6.5858, loss_val: nan, pos_over_neg: 148.98587036132812 lr: 0.0001\n",
      "Iter: 511/3021, loss_train: 6.6128, loss_val: nan, pos_over_neg: 98.84847259521484 lr: 0.0001\n",
      "Iter: 512/3021, loss_train: 6.6436, loss_val: nan, pos_over_neg: 75.47498321533203 lr: 0.0001\n",
      "Iter: 513/3021, loss_train: 6.6182, loss_val: nan, pos_over_neg: 120.55462646484375 lr: 0.0001\n",
      "Iter: 514/3021, loss_train: 6.6168, loss_val: nan, pos_over_neg: 94.66842651367188 lr: 0.0001\n",
      "Iter: 515/3021, loss_train: 6.6115, loss_val: nan, pos_over_neg: 105.19046783447266 lr: 0.0001\n",
      "Iter: 516/3021, loss_train: 6.6355, loss_val: nan, pos_over_neg: 69.94659423828125 lr: 0.0001\n",
      "Iter: 517/3021, loss_train: 6.6272, loss_val: nan, pos_over_neg: 72.86248016357422 lr: 0.0001\n",
      "Iter: 518/3021, loss_train: 6.6172, loss_val: nan, pos_over_neg: 87.09858703613281 lr: 0.0001\n",
      "Iter: 519/3021, loss_train: 6.6315, loss_val: nan, pos_over_neg: 128.0094757080078 lr: 0.0001\n",
      "Iter: 520/3021, loss_train: 6.627, loss_val: nan, pos_over_neg: 81.42594146728516 lr: 0.0001\n",
      "Iter: 521/3021, loss_train: 6.6311, loss_val: nan, pos_over_neg: 147.58831787109375 lr: 0.0001\n",
      "Iter: 522/3021, loss_train: 6.6057, loss_val: nan, pos_over_neg: 115.65524291992188 lr: 0.0001\n",
      "Iter: 523/3021, loss_train: 6.6345, loss_val: nan, pos_over_neg: 105.91828155517578 lr: 0.0001\n",
      "Iter: 524/3021, loss_train: 6.5955, loss_val: nan, pos_over_neg: 111.79486083984375 lr: 0.0001\n",
      "Iter: 525/3021, loss_train: 6.6264, loss_val: nan, pos_over_neg: 90.07721710205078 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 526/3021, loss_train: 6.6163, loss_val: nan, pos_over_neg: 109.8876953125 lr: 0.0001\n",
      "Iter: 527/3021, loss_train: 6.6183, loss_val: nan, pos_over_neg: 84.85918426513672 lr: 0.0001\n",
      "Iter: 528/3021, loss_train: 6.6447, loss_val: nan, pos_over_neg: 80.35286712646484 lr: 0.0001\n",
      "Iter: 529/3021, loss_train: 6.6627, loss_val: nan, pos_over_neg: 108.20806884765625 lr: 0.0001\n",
      "Iter: 530/3021, loss_train: 6.6197, loss_val: nan, pos_over_neg: 131.59176635742188 lr: 0.0001\n",
      "Iter: 531/3021, loss_train: 6.6214, loss_val: nan, pos_over_neg: 115.59613037109375 lr: 0.0001\n",
      "Iter: 532/3021, loss_train: 6.6401, loss_val: nan, pos_over_neg: 161.75308227539062 lr: 0.0001\n",
      "Iter: 533/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 108.1818618774414 lr: 0.0001\n",
      "Iter: 534/3021, loss_train: 6.6113, loss_val: nan, pos_over_neg: 123.26493835449219 lr: 0.0001\n",
      "Iter: 535/3021, loss_train: 6.6095, loss_val: nan, pos_over_neg: 141.04315185546875 lr: 0.0001\n",
      "Iter: 536/3021, loss_train: 6.6357, loss_val: nan, pos_over_neg: 91.94810485839844 lr: 0.0001\n",
      "Iter: 537/3021, loss_train: 6.6368, loss_val: nan, pos_over_neg: 83.49420928955078 lr: 0.0001\n",
      "Iter: 538/3021, loss_train: 6.6143, loss_val: nan, pos_over_neg: 85.59711456298828 lr: 0.0001\n",
      "Iter: 539/3021, loss_train: 6.6331, loss_val: nan, pos_over_neg: 68.79597473144531 lr: 0.0001\n",
      "Iter: 540/3021, loss_train: 6.6208, loss_val: nan, pos_over_neg: 65.30167388916016 lr: 0.0001\n",
      "Iter: 541/3021, loss_train: 6.6101, loss_val: nan, pos_over_neg: 70.23233032226562 lr: 0.0001\n",
      "Iter: 542/3021, loss_train: 6.6357, loss_val: nan, pos_over_neg: 62.553253173828125 lr: 0.0001\n",
      "Iter: 543/3021, loss_train: 6.6078, loss_val: nan, pos_over_neg: 76.76283264160156 lr: 0.0001\n",
      "Iter: 544/3021, loss_train: 6.6269, loss_val: nan, pos_over_neg: 99.25552368164062 lr: 0.0001\n",
      "Iter: 545/3021, loss_train: 6.6315, loss_val: nan, pos_over_neg: 108.50141143798828 lr: 0.0001\n",
      "Iter: 546/3021, loss_train: 6.6104, loss_val: nan, pos_over_neg: 162.83193969726562 lr: 0.0001\n",
      "Iter: 547/3021, loss_train: 6.6254, loss_val: nan, pos_over_neg: 306.4184265136719 lr: 0.0001\n",
      "Iter: 548/3021, loss_train: 6.5756, loss_val: nan, pos_over_neg: 404.193115234375 lr: 0.0001\n",
      "Iter: 549/3021, loss_train: 6.5931, loss_val: nan, pos_over_neg: 174.80967712402344 lr: 0.0001\n",
      "Iter: 550/3021, loss_train: 6.6384, loss_val: nan, pos_over_neg: 141.4352264404297 lr: 0.0001\n",
      "Iter: 551/3021, loss_train: 6.6029, loss_val: nan, pos_over_neg: 156.9000244140625 lr: 0.0001\n",
      "Iter: 552/3021, loss_train: 6.6414, loss_val: nan, pos_over_neg: 89.87215423583984 lr: 0.0001\n",
      "Iter: 553/3021, loss_train: 6.6114, loss_val: nan, pos_over_neg: 77.33897399902344 lr: 0.0001\n",
      "Iter: 554/3021, loss_train: 6.6179, loss_val: nan, pos_over_neg: 63.84626770019531 lr: 0.0001\n",
      "Iter: 555/3021, loss_train: 6.6251, loss_val: nan, pos_over_neg: 75.59040069580078 lr: 0.0001\n",
      "Iter: 556/3021, loss_train: 6.621, loss_val: nan, pos_over_neg: 60.59468460083008 lr: 0.0001\n",
      "Iter: 557/3021, loss_train: 6.601, loss_val: nan, pos_over_neg: 63.21246337890625 lr: 0.0001\n",
      "Iter: 558/3021, loss_train: 6.6146, loss_val: nan, pos_over_neg: 77.74459838867188 lr: 0.0001\n",
      "Iter: 559/3021, loss_train: 6.6116, loss_val: nan, pos_over_neg: 122.55570220947266 lr: 0.0001\n",
      "Iter: 560/3021, loss_train: 6.6105, loss_val: nan, pos_over_neg: 131.24295043945312 lr: 0.0001\n",
      "Iter: 561/3021, loss_train: 6.6028, loss_val: nan, pos_over_neg: 175.1300811767578 lr: 0.0001\n",
      "Iter: 562/3021, loss_train: 6.5849, loss_val: nan, pos_over_neg: 146.05496215820312 lr: 0.0001\n",
      "Iter: 563/3021, loss_train: 6.6264, loss_val: nan, pos_over_neg: 180.6123809814453 lr: 0.0001\n",
      "Iter: 564/3021, loss_train: 6.6205, loss_val: nan, pos_over_neg: 118.85259246826172 lr: 0.0001\n",
      "Iter: 565/3021, loss_train: 6.6056, loss_val: nan, pos_over_neg: 147.16189575195312 lr: 0.0001\n",
      "Iter: 566/3021, loss_train: 6.5864, loss_val: nan, pos_over_neg: 85.67736053466797 lr: 0.0001\n",
      "Iter: 567/3021, loss_train: 6.6168, loss_val: nan, pos_over_neg: 83.41423034667969 lr: 0.0001\n",
      "Iter: 568/3021, loss_train: 6.6371, loss_val: nan, pos_over_neg: 61.29851150512695 lr: 0.0001\n",
      "Iter: 569/3021, loss_train: 6.6254, loss_val: nan, pos_over_neg: 77.30464935302734 lr: 0.0001\n",
      "Iter: 570/3021, loss_train: 6.609, loss_val: nan, pos_over_neg: 68.9383544921875 lr: 0.0001\n",
      "Iter: 571/3021, loss_train: 6.6133, loss_val: nan, pos_over_neg: 69.23075866699219 lr: 0.0001\n",
      "Iter: 572/3021, loss_train: 6.59, loss_val: nan, pos_over_neg: 119.70875549316406 lr: 0.0001\n",
      "Iter: 573/3021, loss_train: 6.6079, loss_val: nan, pos_over_neg: 121.50920104980469 lr: 0.0001\n",
      "Iter: 574/3021, loss_train: 6.5961, loss_val: nan, pos_over_neg: 125.33728790283203 lr: 0.0001\n",
      "Iter: 575/3021, loss_train: 6.6047, loss_val: nan, pos_over_neg: 190.490234375 lr: 0.0001\n",
      "Iter: 576/3021, loss_train: 6.627, loss_val: nan, pos_over_neg: 89.34983825683594 lr: 0.0001\n",
      "Iter: 577/3021, loss_train: 6.5786, loss_val: nan, pos_over_neg: 147.47406005859375 lr: 0.0001\n",
      "Iter: 578/3021, loss_train: 6.5972, loss_val: nan, pos_over_neg: 139.32620239257812 lr: 0.0001\n",
      "Iter: 579/3021, loss_train: 6.6033, loss_val: nan, pos_over_neg: 152.81028747558594 lr: 0.0001\n",
      "Iter: 580/3021, loss_train: 6.5976, loss_val: nan, pos_over_neg: 106.20579528808594 lr: 0.0001\n",
      "Iter: 581/3021, loss_train: 6.5739, loss_val: nan, pos_over_neg: 113.97660064697266 lr: 0.0001\n",
      "Iter: 582/3021, loss_train: 6.6406, loss_val: nan, pos_over_neg: 86.01107025146484 lr: 0.0001\n",
      "Iter: 583/3021, loss_train: 6.6205, loss_val: nan, pos_over_neg: 88.5457763671875 lr: 0.0001\n",
      "Iter: 584/3021, loss_train: 6.6018, loss_val: nan, pos_over_neg: 99.84664154052734 lr: 0.0001\n",
      "Iter: 585/3021, loss_train: 6.6389, loss_val: nan, pos_over_neg: 89.63492584228516 lr: 0.0001\n",
      "Iter: 586/3021, loss_train: 6.5848, loss_val: nan, pos_over_neg: 102.46236419677734 lr: 0.0001\n",
      "Iter: 587/3021, loss_train: 6.6163, loss_val: nan, pos_over_neg: 122.13184356689453 lr: 0.0001\n",
      "Iter: 588/3021, loss_train: 6.616, loss_val: nan, pos_over_neg: 112.73178100585938 lr: 0.0001\n",
      "Iter: 589/3021, loss_train: 6.5886, loss_val: nan, pos_over_neg: 165.5727996826172 lr: 0.0001\n",
      "Iter: 590/3021, loss_train: 6.6151, loss_val: nan, pos_over_neg: 238.1915283203125 lr: 0.0001\n",
      "Iter: 591/3021, loss_train: 6.5896, loss_val: nan, pos_over_neg: 122.59256744384766 lr: 0.0001\n",
      "Iter: 592/3021, loss_train: 6.6121, loss_val: nan, pos_over_neg: 148.16482543945312 lr: 0.0001\n",
      "Iter: 593/3021, loss_train: 6.584, loss_val: nan, pos_over_neg: 141.76255798339844 lr: 0.0001\n",
      "Iter: 594/3021, loss_train: 6.6352, loss_val: nan, pos_over_neg: 85.5504150390625 lr: 0.0001\n",
      "Iter: 595/3021, loss_train: 6.5984, loss_val: nan, pos_over_neg: 87.7799301147461 lr: 0.0001\n",
      "Iter: 596/3021, loss_train: 6.6266, loss_val: nan, pos_over_neg: 86.03079986572266 lr: 0.0001\n",
      "Iter: 597/3021, loss_train: 6.5959, loss_val: nan, pos_over_neg: 100.08769989013672 lr: 0.0001\n",
      "Iter: 598/3021, loss_train: 6.5976, loss_val: nan, pos_over_neg: 105.68513488769531 lr: 0.0001\n",
      "Iter: 599/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 121.44738006591797 lr: 0.0001\n",
      "Iter: 600/3021, loss_train: 6.609, loss_val: nan, pos_over_neg: 92.46785736083984 lr: 0.0001\n",
      "Iter: 601/3021, loss_train: 6.5948, loss_val: nan, pos_over_neg: 108.15098571777344 lr: 0.0001\n",
      "Iter: 602/3021, loss_train: 6.6063, loss_val: nan, pos_over_neg: 113.37541961669922 lr: 0.0001\n",
      "Iter: 603/3021, loss_train: 6.6145, loss_val: nan, pos_over_neg: 97.8232192993164 lr: 0.0001\n",
      "Iter: 604/3021, loss_train: 6.6039, loss_val: nan, pos_over_neg: 95.51597595214844 lr: 0.0001\n",
      "Iter: 605/3021, loss_train: 6.6098, loss_val: nan, pos_over_neg: 118.64756774902344 lr: 0.0001\n",
      "Iter: 606/3021, loss_train: 6.5867, loss_val: nan, pos_over_neg: 128.98312377929688 lr: 0.0001\n",
      "Iter: 607/3021, loss_train: 6.5887, loss_val: nan, pos_over_neg: 104.25284576416016 lr: 0.0001\n",
      "Iter: 608/3021, loss_train: 6.6271, loss_val: nan, pos_over_neg: 105.27714538574219 lr: 0.0001\n",
      "Iter: 609/3021, loss_train: 6.6005, loss_val: nan, pos_over_neg: 149.07559204101562 lr: 0.0001\n",
      "Iter: 610/3021, loss_train: 6.5994, loss_val: nan, pos_over_neg: 153.98248291015625 lr: 0.0001\n",
      "Iter: 611/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 97.0685043334961 lr: 0.0001\n",
      "Iter: 612/3021, loss_train: 6.6145, loss_val: nan, pos_over_neg: 149.8909149169922 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 613/3021, loss_train: 6.5952, loss_val: nan, pos_over_neg: 128.69012451171875 lr: 0.0001\n",
      "Iter: 614/3021, loss_train: 6.6181, loss_val: nan, pos_over_neg: 100.28562927246094 lr: 0.0001\n",
      "Iter: 615/3021, loss_train: 6.6068, loss_val: nan, pos_over_neg: 101.59013366699219 lr: 0.0001\n",
      "Iter: 616/3021, loss_train: 6.6296, loss_val: nan, pos_over_neg: 107.67310333251953 lr: 0.0001\n",
      "Iter: 617/3021, loss_train: 6.5962, loss_val: nan, pos_over_neg: 149.38587951660156 lr: 0.0001\n",
      "Iter: 618/3021, loss_train: 6.608, loss_val: nan, pos_over_neg: 100.8143081665039 lr: 0.0001\n",
      "Iter: 619/3021, loss_train: 6.5924, loss_val: nan, pos_over_neg: 115.50370788574219 lr: 0.0001\n",
      "Iter: 620/3021, loss_train: 6.6081, loss_val: nan, pos_over_neg: 100.75720977783203 lr: 0.0001\n",
      "Iter: 621/3021, loss_train: 6.6117, loss_val: nan, pos_over_neg: 106.71442413330078 lr: 0.0001\n",
      "Iter: 622/3021, loss_train: 6.5844, loss_val: nan, pos_over_neg: 114.02190399169922 lr: 0.0001\n",
      "Iter: 623/3021, loss_train: 6.5807, loss_val: nan, pos_over_neg: 90.38426208496094 lr: 0.0001\n",
      "Iter: 624/3021, loss_train: 6.5886, loss_val: nan, pos_over_neg: 147.60665893554688 lr: 0.0001\n",
      "Iter: 625/3021, loss_train: 6.5818, loss_val: nan, pos_over_neg: 118.66007995605469 lr: 0.0001\n",
      "Iter: 626/3021, loss_train: 6.594, loss_val: nan, pos_over_neg: 94.60941314697266 lr: 0.0001\n",
      "Iter: 627/3021, loss_train: 6.6193, loss_val: nan, pos_over_neg: 90.62916564941406 lr: 0.0001\n",
      "Iter: 628/3021, loss_train: 6.5857, loss_val: nan, pos_over_neg: 148.68092346191406 lr: 0.0001\n",
      "Iter: 629/3021, loss_train: 6.6198, loss_val: nan, pos_over_neg: 117.320068359375 lr: 0.0001\n",
      "Iter: 630/3021, loss_train: 6.6109, loss_val: nan, pos_over_neg: 184.8887939453125 lr: 0.0001\n",
      "Iter: 631/3021, loss_train: 6.6091, loss_val: nan, pos_over_neg: 126.28932189941406 lr: 0.0001\n",
      "Iter: 632/3021, loss_train: 6.611, loss_val: nan, pos_over_neg: 174.2012481689453 lr: 0.0001\n",
      "Iter: 633/3021, loss_train: 6.5862, loss_val: nan, pos_over_neg: 99.67992401123047 lr: 0.0001\n",
      "Iter: 634/3021, loss_train: 6.581, loss_val: nan, pos_over_neg: 96.00635528564453 lr: 0.0001\n",
      "Iter: 635/3021, loss_train: 6.5949, loss_val: nan, pos_over_neg: 116.11966705322266 lr: 0.0001\n",
      "Iter: 636/3021, loss_train: 6.6266, loss_val: nan, pos_over_neg: 82.5527114868164 lr: 0.0001\n",
      "Iter: 637/3021, loss_train: 6.5872, loss_val: nan, pos_over_neg: 102.69898223876953 lr: 0.0001\n",
      "Iter: 638/3021, loss_train: 6.6071, loss_val: nan, pos_over_neg: 75.60655975341797 lr: 0.0001\n",
      "Iter: 639/3021, loss_train: 6.6122, loss_val: nan, pos_over_neg: 92.618408203125 lr: 0.0001\n",
      "Iter: 640/3021, loss_train: 6.5891, loss_val: nan, pos_over_neg: 77.74364471435547 lr: 0.0001\n",
      "Iter: 641/3021, loss_train: 6.614, loss_val: nan, pos_over_neg: 90.3978271484375 lr: 0.0001\n",
      "Iter: 642/3021, loss_train: 6.616, loss_val: nan, pos_over_neg: 93.53338623046875 lr: 0.0001\n",
      "Iter: 643/3021, loss_train: 6.5945, loss_val: nan, pos_over_neg: 111.01829528808594 lr: 0.0001\n",
      "Iter: 644/3021, loss_train: 6.582, loss_val: nan, pos_over_neg: 166.74801635742188 lr: 0.0001\n",
      "Iter: 645/3021, loss_train: 6.5981, loss_val: nan, pos_over_neg: 167.72279357910156 lr: 0.0001\n",
      "Iter: 646/3021, loss_train: 6.5991, loss_val: nan, pos_over_neg: 119.41260528564453 lr: 0.0001\n",
      "Iter: 647/3021, loss_train: 6.6103, loss_val: nan, pos_over_neg: 110.62936401367188 lr: 0.0001\n",
      "Iter: 648/3021, loss_train: 6.5876, loss_val: nan, pos_over_neg: 135.94125366210938 lr: 0.0001\n",
      "Iter: 649/3021, loss_train: 6.6088, loss_val: nan, pos_over_neg: 89.50482940673828 lr: 0.0001\n",
      "Iter: 650/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 90.18988037109375 lr: 0.0001\n",
      "Iter: 651/3021, loss_train: 6.602, loss_val: nan, pos_over_neg: 76.83455657958984 lr: 0.0001\n",
      "Iter: 652/3021, loss_train: 6.5879, loss_val: nan, pos_over_neg: 76.3498764038086 lr: 0.0001\n",
      "Iter: 653/3021, loss_train: 6.6076, loss_val: nan, pos_over_neg: 90.66461944580078 lr: 0.0001\n",
      "Iter: 654/3021, loss_train: 6.588, loss_val: nan, pos_over_neg: 118.17411804199219 lr: 0.0001\n",
      "Iter: 655/3021, loss_train: 6.5836, loss_val: nan, pos_over_neg: 125.78502655029297 lr: 0.0001\n",
      "Iter: 656/3021, loss_train: 6.5967, loss_val: nan, pos_over_neg: 98.24745178222656 lr: 0.0001\n",
      "Iter: 657/3021, loss_train: 6.6134, loss_val: nan, pos_over_neg: 111.24110412597656 lr: 0.0001\n",
      "Iter: 658/3021, loss_train: 6.5838, loss_val: nan, pos_over_neg: 135.8141632080078 lr: 0.0001\n",
      "Iter: 659/3021, loss_train: 6.6422, loss_val: nan, pos_over_neg: 82.63817596435547 lr: 0.0001\n",
      "Iter: 660/3021, loss_train: 6.603, loss_val: nan, pos_over_neg: 118.46806335449219 lr: 0.0001\n",
      "Iter: 661/3021, loss_train: 6.5744, loss_val: nan, pos_over_neg: 99.66175842285156 lr: 0.0001\n",
      "Iter: 662/3021, loss_train: 6.5898, loss_val: nan, pos_over_neg: 82.22264862060547 lr: 0.0001\n",
      "Iter: 663/3021, loss_train: 6.5966, loss_val: nan, pos_over_neg: 112.99810791015625 lr: 0.0001\n",
      "Iter: 664/3021, loss_train: 6.636, loss_val: nan, pos_over_neg: 94.01675415039062 lr: 0.0001\n",
      "Iter: 665/3021, loss_train: 6.5998, loss_val: nan, pos_over_neg: 103.90379333496094 lr: 0.0001\n",
      "Iter: 666/3021, loss_train: 6.5821, loss_val: nan, pos_over_neg: 216.28103637695312 lr: 0.0001\n",
      "Iter: 667/3021, loss_train: 6.6171, loss_val: nan, pos_over_neg: 176.133056640625 lr: 0.0001\n",
      "Iter: 668/3021, loss_train: 6.587, loss_val: nan, pos_over_neg: 188.52003479003906 lr: 0.0001\n",
      "Iter: 669/3021, loss_train: 6.5814, loss_val: nan, pos_over_neg: 163.70730590820312 lr: 0.0001\n",
      "Iter: 670/3021, loss_train: 6.6147, loss_val: nan, pos_over_neg: 86.39402770996094 lr: 0.0001\n",
      "Iter: 671/3021, loss_train: 6.5808, loss_val: nan, pos_over_neg: 137.89599609375 lr: 0.0001\n",
      "Iter: 672/3021, loss_train: 6.5943, loss_val: nan, pos_over_neg: 83.94868469238281 lr: 0.0001\n",
      "Iter: 673/3021, loss_train: 6.6182, loss_val: nan, pos_over_neg: 84.34393310546875 lr: 0.0001\n",
      "Iter: 674/3021, loss_train: 6.5829, loss_val: nan, pos_over_neg: 78.46054077148438 lr: 0.0001\n",
      "Iter: 675/3021, loss_train: 6.5767, loss_val: nan, pos_over_neg: 109.20298767089844 lr: 0.0001\n",
      "Iter: 676/3021, loss_train: 6.6033, loss_val: nan, pos_over_neg: 94.33056640625 lr: 0.0001\n",
      "Iter: 677/3021, loss_train: 6.5889, loss_val: nan, pos_over_neg: 74.11611938476562 lr: 0.0001\n",
      "Iter: 678/3021, loss_train: 6.6018, loss_val: nan, pos_over_neg: 108.98560333251953 lr: 0.0001\n",
      "Iter: 679/3021, loss_train: 6.6186, loss_val: nan, pos_over_neg: 105.86277770996094 lr: 0.0001\n",
      "Iter: 680/3021, loss_train: 6.567, loss_val: nan, pos_over_neg: 147.7794952392578 lr: 0.0001\n",
      "Iter: 681/3021, loss_train: 6.5648, loss_val: nan, pos_over_neg: 163.72720336914062 lr: 0.0001\n",
      "Iter: 682/3021, loss_train: 6.6141, loss_val: nan, pos_over_neg: 115.72303771972656 lr: 0.0001\n",
      "Iter: 683/3021, loss_train: 6.6015, loss_val: nan, pos_over_neg: 97.99879455566406 lr: 0.0001\n",
      "Iter: 684/3021, loss_train: 6.5822, loss_val: nan, pos_over_neg: 89.02739715576172 lr: 0.0001\n",
      "Iter: 685/3021, loss_train: 6.5842, loss_val: nan, pos_over_neg: 90.65805053710938 lr: 0.0001\n",
      "Iter: 686/3021, loss_train: 6.589, loss_val: nan, pos_over_neg: 71.03034210205078 lr: 0.0001\n",
      "Iter: 687/3021, loss_train: 6.5765, loss_val: nan, pos_over_neg: 100.04374694824219 lr: 0.0001\n",
      "Iter: 688/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 94.22358703613281 lr: 0.0001\n",
      "Iter: 689/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 92.12555694580078 lr: 0.0001\n",
      "Iter: 690/3021, loss_train: 6.5956, loss_val: nan, pos_over_neg: 93.27652740478516 lr: 0.0001\n",
      "Iter: 691/3021, loss_train: 6.5947, loss_val: nan, pos_over_neg: 143.22329711914062 lr: 0.0001\n",
      "Iter: 692/3021, loss_train: 6.6153, loss_val: nan, pos_over_neg: 141.1318817138672 lr: 0.0001\n",
      "Iter: 693/3021, loss_train: 6.5453, loss_val: nan, pos_over_neg: 127.14126586914062 lr: 0.0001\n",
      "Iter: 694/3021, loss_train: 6.5728, loss_val: nan, pos_over_neg: 123.66120910644531 lr: 0.0001\n",
      "Iter: 695/3021, loss_train: 6.5915, loss_val: nan, pos_over_neg: 151.01333618164062 lr: 0.0001\n",
      "Iter: 696/3021, loss_train: 6.5712, loss_val: nan, pos_over_neg: 165.84442138671875 lr: 0.0001\n",
      "Iter: 697/3021, loss_train: 6.5933, loss_val: nan, pos_over_neg: 120.06879425048828 lr: 0.0001\n",
      "Iter: 698/3021, loss_train: 6.5876, loss_val: nan, pos_over_neg: 98.9247817993164 lr: 0.0001\n",
      "Iter: 699/3021, loss_train: 6.6013, loss_val: nan, pos_over_neg: 90.98076629638672 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 700/3021, loss_train: 6.572, loss_val: nan, pos_over_neg: 94.82418060302734 lr: 0.0001\n",
      "Iter: 701/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 100.7938461303711 lr: 0.0001\n",
      "Iter: 702/3021, loss_train: 6.5829, loss_val: nan, pos_over_neg: 88.68292999267578 lr: 0.0001\n",
      "Iter: 703/3021, loss_train: 6.5951, loss_val: nan, pos_over_neg: 101.81523132324219 lr: 0.0001\n",
      "Iter: 704/3021, loss_train: 6.5961, loss_val: nan, pos_over_neg: 171.6141815185547 lr: 0.0001\n",
      "Iter: 705/3021, loss_train: 6.6094, loss_val: nan, pos_over_neg: 114.46845245361328 lr: 0.0001\n",
      "Iter: 706/3021, loss_train: 6.5904, loss_val: nan, pos_over_neg: 116.8626937866211 lr: 0.0001\n",
      "Iter: 707/3021, loss_train: 6.6149, loss_val: nan, pos_over_neg: 82.73287963867188 lr: 0.0001\n",
      "Iter: 708/3021, loss_train: 6.6279, loss_val: nan, pos_over_neg: 93.81600189208984 lr: 0.0001\n",
      "Iter: 709/3021, loss_train: 6.6053, loss_val: nan, pos_over_neg: 103.77784729003906 lr: 0.0001\n",
      "Iter: 710/3021, loss_train: 6.5989, loss_val: nan, pos_over_neg: 114.22103881835938 lr: 0.0001\n",
      "Iter: 711/3021, loss_train: 6.589, loss_val: nan, pos_over_neg: 109.76660919189453 lr: 0.0001\n",
      "Iter: 712/3021, loss_train: 6.5825, loss_val: nan, pos_over_neg: 73.29486083984375 lr: 0.0001\n",
      "Iter: 713/3021, loss_train: 6.6098, loss_val: nan, pos_over_neg: 73.3893051147461 lr: 0.0001\n",
      "Iter: 714/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 66.46199035644531 lr: 0.0001\n",
      "Iter: 715/3021, loss_train: 6.5871, loss_val: nan, pos_over_neg: 76.87630462646484 lr: 0.0001\n",
      "Iter: 716/3021, loss_train: 6.5833, loss_val: nan, pos_over_neg: 69.87895965576172 lr: 0.0001\n",
      "Iter: 717/3021, loss_train: 6.6025, loss_val: nan, pos_over_neg: 99.14900970458984 lr: 0.0001\n",
      "Iter: 718/3021, loss_train: 6.5908, loss_val: nan, pos_over_neg: 131.8505859375 lr: 0.0001\n",
      "Iter: 719/3021, loss_train: 6.5788, loss_val: nan, pos_over_neg: 157.80743408203125 lr: 0.0001\n",
      "Iter: 720/3021, loss_train: 6.6086, loss_val: nan, pos_over_neg: 111.38347625732422 lr: 0.0001\n",
      "Iter: 721/3021, loss_train: 6.6021, loss_val: nan, pos_over_neg: 108.7108154296875 lr: 0.0001\n",
      "Iter: 722/3021, loss_train: 6.5849, loss_val: nan, pos_over_neg: 132.10877990722656 lr: 0.0001\n",
      "Iter: 723/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 102.30925750732422 lr: 0.0001\n",
      "Iter: 724/3021, loss_train: 6.6221, loss_val: nan, pos_over_neg: 98.22590637207031 lr: 0.0001\n",
      "Iter: 725/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 139.20152282714844 lr: 0.0001\n",
      "Iter: 726/3021, loss_train: 6.5815, loss_val: nan, pos_over_neg: 152.375732421875 lr: 0.0001\n",
      "Iter: 727/3021, loss_train: 6.6042, loss_val: nan, pos_over_neg: 153.21017456054688 lr: 0.0001\n",
      "Iter: 728/3021, loss_train: 6.5909, loss_val: nan, pos_over_neg: 138.31646728515625 lr: 0.0001\n",
      "Iter: 729/3021, loss_train: 6.6204, loss_val: nan, pos_over_neg: 87.23120880126953 lr: 0.0001\n",
      "Iter: 730/3021, loss_train: 6.5699, loss_val: nan, pos_over_neg: 103.18000793457031 lr: 0.0001\n",
      "Iter: 731/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 76.64822387695312 lr: 0.0001\n",
      "Iter: 732/3021, loss_train: 6.6012, loss_val: nan, pos_over_neg: 77.05658721923828 lr: 0.0001\n",
      "Iter: 733/3021, loss_train: 6.6112, loss_val: nan, pos_over_neg: 61.744014739990234 lr: 0.0001\n",
      "Iter: 734/3021, loss_train: 6.5985, loss_val: nan, pos_over_neg: 67.07396697998047 lr: 0.0001\n",
      "Iter: 735/3021, loss_train: 6.591, loss_val: nan, pos_over_neg: 75.07990264892578 lr: 0.0001\n",
      "Iter: 736/3021, loss_train: 6.5565, loss_val: nan, pos_over_neg: 79.77616119384766 lr: 0.0001\n",
      "Iter: 737/3021, loss_train: 6.5732, loss_val: nan, pos_over_neg: 94.3510971069336 lr: 0.0001\n",
      "Iter: 738/3021, loss_train: 6.6243, loss_val: nan, pos_over_neg: 89.19917297363281 lr: 0.0001\n",
      "Iter: 739/3021, loss_train: 6.5891, loss_val: nan, pos_over_neg: 144.8668975830078 lr: 0.0001\n",
      "Iter: 740/3021, loss_train: 6.5989, loss_val: nan, pos_over_neg: 128.92776489257812 lr: 0.0001\n",
      "Iter: 741/3021, loss_train: 6.6202, loss_val: nan, pos_over_neg: 126.48570251464844 lr: 0.0001\n",
      "Iter: 742/3021, loss_train: 6.559, loss_val: nan, pos_over_neg: 152.12013244628906 lr: 0.0001\n",
      "Iter: 743/3021, loss_train: 6.5795, loss_val: nan, pos_over_neg: 87.68231201171875 lr: 0.0001\n",
      "Iter: 744/3021, loss_train: 6.6107, loss_val: nan, pos_over_neg: 108.50746154785156 lr: 0.0001\n",
      "Iter: 745/3021, loss_train: 6.5808, loss_val: nan, pos_over_neg: 104.51139068603516 lr: 0.0001\n",
      "Iter: 746/3021, loss_train: 6.5915, loss_val: nan, pos_over_neg: 98.31632995605469 lr: 0.0001\n",
      "Iter: 747/3021, loss_train: 6.5945, loss_val: nan, pos_over_neg: 104.99259185791016 lr: 0.0001\n",
      "Iter: 748/3021, loss_train: 6.5936, loss_val: nan, pos_over_neg: 172.98275756835938 lr: 0.0001\n",
      "Iter: 749/3021, loss_train: 6.5721, loss_val: nan, pos_over_neg: 142.9506072998047 lr: 0.0001\n",
      "Iter: 750/3021, loss_train: 6.576, loss_val: nan, pos_over_neg: 129.3081512451172 lr: 0.0001\n",
      "Iter: 751/3021, loss_train: 6.6131, loss_val: nan, pos_over_neg: 84.6493911743164 lr: 0.0001\n",
      "Iter: 752/3021, loss_train: 6.5836, loss_val: nan, pos_over_neg: 127.84735107421875 lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = [], [np.nan]\n",
    "for epoch in tqdm(range(params['n_epochs'])):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    losses_train = training.epoch_step(\n",
    "        dataloader_train, \n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        scheduler=scheduler,\n",
    "        temperature=params['temperature'],\n",
    "        # l2_alpha,\n",
    "        mode='semi-supervised',\n",
    "        loss_rolling_train=losses_train, \n",
    "        loss_rolling_val=losses_val,\n",
    "        device=device_train, \n",
    "        verbose=2,\n",
    "        verbose_update_period=1,\n",
    "\n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    ")\n",
    "    \n",
    "    ## save loss stuff\n",
    "    if pref_saveLogs:\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, completed epoch: {epoch}, loss: {losses_train[-1]}, lr: {scheduler.get_last_lr()[0]}')\n",
    "        np.save(path_saveLoss, losses_train)\n",
    "    \n",
    "    ## if loss becomes NaNs, don't save the network and stop training\n",
    "    if torch.isnan(losses_train[-1]):\n",
    "        break\n",
    "        \n",
    "    ## save model\n",
    "    if pref_saveModelIteratively:\n",
    "        torch.save(model.state_dict(), path_saveModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape, features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train_SYT)\n",
    "    # logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "    # acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "    # acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(1)).fit(features_train, y_labeled_train_SYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(key, '    ', layer.shape) for key, layer in model.state_dict().items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(model.state_dict()['PostHead_0.weight'].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "layer_1 = model.state_dict()['base_model.0.0.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.0.6.3.block.1.0.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "# layer_4 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "\n",
    "\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_1], dim=0), grid_shape=(5,6), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_2], dim=0), grid_shape=(6,6), kwargs_imshow={'vmax': 0.2});\n",
    "plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_3], dim=0), grid_shape=(4,6), kwargs_imshow={'vmax': 0.2});\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 920.76378,
   "position": {
    "height": "40px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "500.99px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
