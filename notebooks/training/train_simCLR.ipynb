{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "S3q3I42jDB0f",
    "outputId": "3ad88a07-0e8b-474f-b0d8-9fb6c2a99f0c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2870663/1226147920.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PUUWS0VmwD7-"
   },
   "source": [
    "# !source activate jupyter_launcher\n",
    "!pip3 install numba\n",
    "!pip3 install matplotlib\n",
    "!pip3 install scipy\n",
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install sklearn\n",
    "!pip3 install pycuda\n",
    "!pip3 install tqdm\n",
    "!pip3 install seaborn\n",
    "!pip3 install h5py\n",
    "!pip3 install hdfdict\n",
    "!pip3 install ipywidgets\n",
    "!pip3 install numpy==1.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eTIgCGQsDB0i"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "# from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# import cuml\n",
    "\n",
    "# for creating validation set\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "# %matplotlib inline\n",
    "# %matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GExNkvATEBtG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "MZ9Hq6SVvves"
   },
   "source": [
    "base_dir = '/n/data1/hms/neurobio/sabatini/josh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = '/media/rich/bigSSD/analysis_data/ROI_net_training/testing_dispatcher_20220504'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_github = '/media/rich/Home_Linux_partition/github_repos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data_training = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/masks_20211202_balanced.h5'\n",
    "\n",
    "path_dataClassification_stat = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/stat.npy'\n",
    "path_dataClassification_labels = '/media/rich/bigSSD/for_Josh/SimCLR-Label-Data/mouse2_6__20210409/labels_round2_sesh2.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9w3t_mtdDB0j"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(dir_github)\n",
    "# dir_folders = f'{base_dir}/label_data'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from basic_neural_processing_modules import torch_helpers, math_functions, classification, h5_handling, plotting_helpers, indexing, misc, decomposition\n",
    "from GCaMP_ROI_classifier import util, models, training, augmentation, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log(path_log, text, mode='a', start_on_new_line=True):\n",
    "    with open(path_log, mode=mode) as log:\n",
    "        if start_on_new_line==True:\n",
    "            log.write('\\n')\n",
    "        log.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cuda'\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'useGPU_training': True,\n",
    "    'pre_head_fc_sizes': [1024, 512],\n",
    "    'post_head_fc_sizes': [64],\n",
    "    'block_to_freeze': 7,\n",
    "    'useGPU_dataloader': False,\n",
    "    'dataloader_kwargs':{\n",
    "        'batch_size': 1024,\n",
    "        'shuffle': True,\n",
    "        'drop_last': True,\n",
    "        'pin_memory': True,\n",
    "        'num_workers': 36,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 3,\n",
    "    },\n",
    "    'lr': 1*10**-4,\n",
    "    'gamma': 1-0.0000,\n",
    "    'n_epochs': 9999999,\n",
    "    'temperature': 0.5,\n",
    "}\n",
    "\n",
    "dir_save = Path(r'/media/rich/bigSSD/')\n",
    "fileName_model = 'EfficientNet_b0_7unfrozen_simCLR'\n",
    "path_saveModel = f'{dir_save / fileName_model}.pth'\n",
    "path_saveLog = dir_save / 'log.txt'\n",
    "path_saveLoss = dir_save / 'loss.npy'\n",
    "\n",
    "pref_saveModelIteratively = True\n",
    "pref_saveLogs = True\n",
    "\n",
    "device_train = torch_helpers.set_device(use_GPU=params['useGPU_training'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTqZzmpJDB0j"
   },
   "source": [
    "## Import unlabeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "sf_sparse = scipy.sparse.load_npz('/media/rich/bigSSD/analysis_data/ROIs_for_training/sf_sparse_36x36_20220503.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_dense = torch.as_tensor(sf_sparse.toarray().reshape(sf_sparse.shape[0], 36,36), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toss any NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: torch.Size([3094029, 36, 36])\n",
      "Number of masks: torch.Size([3094025, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of masks: {sf_dense.shape}')\n",
    "\n",
    "ROIs_without_NaNs = ~torch.any(torch.any(torch.isnan(sf_dense), dim=1), dim=1)\n",
    "ROIs_nonAllZero = (torch.max(torch.max(sf_dense, dim=1)[0], dim=1)[0] > 0)\n",
    "ROIs_toKeep = torch.where(ROIs_without_NaNs * ROIs_nonAllZero)\n",
    "masks_cat = sf_dense[ROIs_toKeep]\n",
    "\n",
    "print(f'Number of masks: {masks_cat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aA1-hY4DB0v"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define New model = model + pre-head + latent layer OR classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gt4xpqbHBjyL"
   },
   "outputs": [],
   "source": [
    "class ModelTackOn(torch.nn.Module):\n",
    "    def __init__(self, base_model, un_modified_model, pre_head_fc_sizes=[100], post_head_fc_sizes=[100], classifier_fc_sizes=None):\n",
    "            super(ModelTackOn, self).__init__()\n",
    "            self.base_model = base_model\n",
    "            final_base_layer = list(un_modified_model.children())[-1]\n",
    "            # final_base_layer = list(list(model.children())[-1].children())[-1]\n",
    "            # print(final_base_layer)\n",
    "\n",
    "            self.pre_head_fc_lst = []\n",
    "            self.post_head_fc_lst = []\n",
    "            self.classifier_fc_lst = []\n",
    "\n",
    "            self.init_prehead(final_base_layer, pre_head_fc_sizes)\n",
    "            self.init_posthead(pre_head_fc_sizes[-1], post_head_fc_sizes)\n",
    "            if classifier_fc_sizes is not None:\n",
    "                self.init_classifier(pre_head_fc_sizes[-1], classifier_fc_sizes)\n",
    "    \n",
    "    def init_prehead(self, prv_layer, pre_head_fc_sizes):\n",
    "        for i, pre_head_fc in enumerate(pre_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_layer.in_features if hasattr(prv_layer,'in_features') else 1280\n",
    "            else:\n",
    "                in_features = pre_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=pre_head_fc)\n",
    "            self.add_module(f'PreHead_{i}', fc_layer)\n",
    "            self.pre_head_fc_lst.append(fc_layer)\n",
    "\n",
    "#             if i < len(pre_head_fc_sizes) - 1:\n",
    "            non_linearity = torch.nn.ReLU()\n",
    "            self.add_module(f'PreHead_{i}_NonLinearity', non_linearity)\n",
    "            self.pre_head_fc_lst.append(non_linearity)\n",
    "\n",
    "    def init_posthead(self, prv_size, post_head_fc_sizes):\n",
    "        for i, post_head_fc in enumerate(post_head_fc_sizes):\n",
    "            if i == 0:\n",
    "                in_features = prv_size\n",
    "            else:\n",
    "                in_features = post_head_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=post_head_fc)\n",
    "            self.add_module(f'PostHead_{i}', fc_layer)\n",
    "            self.post_head_fc_lst.append(fc_layer)\n",
    "\n",
    "            if i < len(post_head_fc_sizes) - 1:\n",
    "                non_linearity = torch.nn.ReLU()\n",
    "                self.add_module(f'PostHead_{i}_NonLinearity', non_linearity)\n",
    "                self.pre_head_fc_lst.append(non_linearity)\n",
    "    \n",
    "    def init_classifier(self, prv_size, classifier_fc_sizes):\n",
    "            for i, classifier_fc in enumerate(classifier_fc_sizes):\n",
    "                if i == 0:\n",
    "                    in_features = prv_size\n",
    "                else:\n",
    "                    in_features = classifier_fc_sizes[i - 1]\n",
    "            fc_layer = torch.nn.Linear(in_features=in_features, out_features=classifier_fc)\n",
    "            self.add_module(f'Classifier_{i}', fc_layer)\n",
    "            self.classifier_fc_lst.append(fc_layer)\n",
    "\n",
    "    def reinit_classifier(self):\n",
    "        for i_layer, layer in enumerate(self.classifier_fc_lst):\n",
    "            layer.reset_parameters()\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         interim = self.base_model(X)\n",
    "#         interim = self.get_head(interim)\n",
    "#         interim = self.get_latent(interim)\n",
    "#         return interim\n",
    "\n",
    "    def forward_classifier(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.classify(interim)\n",
    "        return interim\n",
    "\n",
    "    def forward_latent(self, X):\n",
    "        interim = self.base_model(X)\n",
    "        interim = self.get_head(interim)\n",
    "        interim = self.get_latent(interim)\n",
    "        return interim\n",
    "\n",
    "\n",
    "    def get_head(self, base_out):\n",
    "        # print('base_out', base_out.shape)\n",
    "        head = base_out\n",
    "        for pre_head_layer in self.pre_head_fc_lst:\n",
    "          # print('pre_head_layer', pre_head_layer.in_features)\n",
    "          head = pre_head_layer(head)\n",
    "          # print('head', head.shape)\n",
    "        return head\n",
    "\n",
    "    def get_latent(self, head):\n",
    "        latent = head\n",
    "        for post_head_layer in self.post_head_fc_lst:\n",
    "            latent = post_head_layer(latent)\n",
    "        return latent\n",
    "\n",
    "    def classify(self, head):\n",
    "        logit = head\n",
    "        for classifier_layer in self.classifier_fc_lst:\n",
    "            logit = classifier_layer(logit)\n",
    "        return logit\n",
    "\n",
    "    def set_pre_head_grad(self, requires_grad=True):\n",
    "        for layer in self.pre_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "                \n",
    "    def set_post_head_grad(self, requires_grad=True):\n",
    "        for layer in self.post_head_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def set_classifier_grad(self, requires_grad=True):\n",
    "        for layer in self.classifier_fc_lst:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = requires_grad\n",
    "\n",
    "    def prep_contrast(self):\n",
    "        self.set_pre_head_grad(requires_grad=True)\n",
    "        self.set_post_head_grad(requires_grad=True)\n",
    "        self.set_classifier_grad(requires_grad=False)\n",
    "\n",
    "    def prep_classifier(self):\n",
    "        self.set_pre_head_grad(requires_grad=False)\n",
    "        self.set_post_head_grad(requires_grad=False)\n",
    "        self.set_classifier_grad(requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MIix9BdUCkqf"
   },
   "outputs": [],
   "source": [
    "# import torchvision.models\n",
    "\n",
    "# # base_model = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# retrain = list(base_model.children())[-1:]\n",
    "# for layer in retrain:\n",
    "#     params = layer.parameters()\n",
    "#     for param in params:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oyjLftj_cEGW"
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "# base_model_frozen = torchvision.models.resnet101(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet18(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "# base_model_frozen = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "base_model_frozen = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "\n",
    "for param in base_model_frozen.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start with a pretrained resnet model, and chop off the final layer. This will be used as the base on which we add the pre-head layers (for expressivity), latent layers (for simCLR), or classification layers (for post-hoc logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aWnb7WWri9qK"
   },
   "outputs": [],
   "source": [
    "model_chopped = torch.nn.Sequential(*(list(base_model_frozen.children())[:-1] + [torch.nn.Flatten()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E18ZEzpClNd"
   },
   "source": [
    "### Make combined model\n",
    "'model' has two forward methods. One for generating latents (for simCLR) and one for classifying labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6Qx-1NGJNY3",
    "outputId": "f7cb3ded-3b48-439e-bf57-a526fb48bac7"
   },
   "outputs": [],
   "source": [
    "model = ModelTackOn(\n",
    "    model_chopped,\n",
    "    base_model_frozen,\n",
    "    pre_head_fc_sizes=params['pre_head_fc_sizes'], \n",
    "    post_head_fc_sizes=params['post_head_fc_sizes'], \n",
    "    classifier_fc_sizes=None\n",
    ")\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.0.7.0.block.0.0.weight\n",
      "base_model.0.7.0.block.0.1.weight\n",
      "base_model.0.7.0.block.0.1.bias\n",
      "base_model.0.7.0.block.1.0.weight\n",
      "base_model.0.7.0.block.1.1.weight\n",
      "base_model.0.7.0.block.1.1.bias\n",
      "base_model.0.7.0.block.2.fc1.weight\n",
      "base_model.0.7.0.block.2.fc1.bias\n",
      "base_model.0.7.0.block.2.fc2.weight\n",
      "base_model.0.7.0.block.2.fc2.bias\n",
      "base_model.0.7.0.block.3.0.weight\n",
      "base_model.0.7.0.block.3.1.weight\n",
      "base_model.0.7.0.block.3.1.bias\n",
      "base_model.0.8.0.weight\n",
      "base_model.0.8.1.weight\n",
      "base_model.0.8.1.bias\n",
      "PreHead_0.weight\n",
      "PreHead_0.bias\n",
      "PreHead_1.weight\n",
      "PreHead_1.bias\n",
      "PostHead_0.weight\n",
      "PostHead_0.bias\n"
     ]
    }
   ],
   "source": [
    "# unfreeze particular blocks in CNN model\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    # print(name)\n",
    "    if name[:10] == 'base_model':\n",
    "        if int(name[13]) < params['block_to_freeze']:\n",
    "            param.requires_grad = False\n",
    "        elif int(name[13]) >= params['block_to_freeze']:\n",
    "            param.requires_grad = True\n",
    "\n",
    "for name, param in list(model.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2ARByXvDB0s"
   },
   "source": [
    "## Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms    \n",
    "\n",
    "transforms = torch.nn.Sequential(\n",
    "    augmentation.Scale_image_sum(sum_val=1, epsilon=1e-9, min_sub=True),\n",
    "    \n",
    "    augmentation.AddPoissonNoise(  \n",
    "        #scaler_bounds=(10**(4.5), 10**(6.)),\n",
    "        scaler_bounds=(10**(4), 10**(5)),\n",
    "        prob=0.5,\n",
    "        base=1000,\n",
    "        scaling='log'),\n",
    "    \n",
    "    augmentation.Horizontal_stripe_scale(alpha_min_max=(0.5, 1), im_size=(36,36), prob=0.5),\n",
    "    augmentation.Horizontal_stripe_shift(alpha_min_max=(1,3), im_size=(36,36), prob=0.5),\n",
    "\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    torchvision.transforms.RandomAffine(\n",
    "        degrees=(-180,180),\n",
    "        translate=(0.1, 0.1), #0, .3, .45 (DEFAULT)\n",
    "        scale=(0.6, 1.2), # no scale (1,1), (0.4, 1.5)\n",
    "        shear=(-15, 15, -15, 15),\n",
    "        interpolation=torchvision.transforms.InterpolationMode.BILINEAR, \n",
    "        fill=0, \n",
    "        fillcolor=None, \n",
    "        resample=None),\n",
    "\n",
    "    augmentation.AddGaussianNoise(  \n",
    "        mean=0, \n",
    "        std=0.0010,\n",
    "        prob=0.5),\n",
    "    \n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1), epsilon=1e-9), # just clamping, both this and clamping = normalizing (DEFAULT)\n",
    "    \n",
    "    augmentation.WarpPoints(\n",
    "        r=[0.3, 0.6],\n",
    "        cx=[-0.3, 0.3],\n",
    "        cy=[-0.3, 0.3], \n",
    "        dx=[-0.24, 0.24], \n",
    "        dy=[-0.24, 0.24], \n",
    "        n_warps=2,\n",
    "        prob=0.5,\n",
    "        img_size_in=[36, 36],\n",
    "#         img_size_out=[224, 224],\n",
    "        img_size_out=[180, 180],\n",
    "#         img_size_out=[36, 36],\n",
    "                           ),\n",
    "    \n",
    "#     augmentation.Check_NaN(),\n",
    "    \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "# #     torchvision.transforms.Resize(size=(180,180), \n",
    "# #                                   interpolation=torchvision.transforms.InterpolationMode.BILINEAR), # To do or not to do (DEFAULT)\n",
    "    \n",
    ")\n",
    "    \n",
    "\n",
    "scripted_transforms = torch.jit.script(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PmM4nnV1nCVd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: 'cpu'\n"
     ]
    }
   ],
   "source": [
    "device_dataloader = torch_helpers.set_device(use_GPU=params['useGPU_dataloader'])\n",
    "\n",
    "dataset_train = dataset.dataset_simCLR(\n",
    "    torch.as_tensor(masks_cat, device=device_dataloader, dtype=torch.float32), \n",
    "    torch.as_tensor(torch.zeros(masks_cat.shape[0]), device=device_dataloader, dtype=torch.float32),\n",
    "\n",
    "    n_transforms=2,\n",
    "    class_weights=np.array([1]),\n",
    "    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "    transform=scripted_transforms,\n",
    "    # DEVICE='cpu',\n",
    "    DEVICE=device_dataloader,\n",
    "    dtype_X=torch.float32,\n",
    "    dtype_y=torch.int64,\n",
    "    # temp_uncertainty=1\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    **params['dataloader_kwargs']\n",
    "\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    "    \n",
    "#     batch_size=1024,\n",
    "#     shuffle=False,\n",
    "#     drop_last=True,\n",
    "#     pin_memory=False,\n",
    "#     num_workers=36,\n",
    "#     persistent_workers=True,\n",
    "#     prefetch_factor=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# idx_rand = np.random.randint(0,masks_cat2.shape[0], 10)\n",
    "# for ii in idx_rand:\n",
    "#     fig, axs = plt.subplots(1,2)\n",
    "#     # print(dataset_train[ii][0][0][0].shape)\n",
    "#     axs[0].imshow(dataset_train[ii][0][0][0].cpu())\n",
    "#     axs[1].imshow(dataset_train[ii][0][1][0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ27o1ny9Xfi"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device_train)\n",
    "model.prep_contrast()\n",
    "model.forward = model.forward_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yDqu-bi8mnJB"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "criterion = [CrossEntropyLoss()]\n",
    "optimizer = Adam(\n",
    "    model.parameters(), \n",
    "    lr=params['lr'],\n",
    "#     lr=1*10**-4,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=params['gamma'],\n",
    "#                                                    gamma=1,\n",
    "                                                  )\n",
    "\n",
    "criterion = [_.to(device_train) for _ in criterion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvDiVxDICXEn",
    "outputId": "2c29e3cf-4515-4aae-f0b1-22e30d51fa5f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                               | 0/9999999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Iter: 0/3021, loss_train: 7.6008, loss_val: nan, pos_over_neg: 1.021756887435913 lr: 0.0001\n",
      "Iter: 1/3021, loss_train: 7.565, loss_val: nan, pos_over_neg: 1.0537643432617188 lr: 0.0001\n",
      "Iter: 2/3021, loss_train: 7.4886, loss_val: nan, pos_over_neg: 1.1408154964447021 lr: 0.0001\n",
      "Iter: 3/3021, loss_train: 7.4124, loss_val: nan, pos_over_neg: 1.2915561199188232 lr: 0.0001\n",
      "Iter: 4/3021, loss_train: 7.3437, loss_val: nan, pos_over_neg: 1.5708016157150269 lr: 0.0001\n",
      "Iter: 5/3021, loss_train: 7.2783, loss_val: nan, pos_over_neg: 2.1402266025543213 lr: 0.0001\n",
      "Iter: 6/3021, loss_train: 7.2274, loss_val: nan, pos_over_neg: 2.954249382019043 lr: 0.0001\n",
      "Iter: 7/3021, loss_train: 7.1915, loss_val: nan, pos_over_neg: 3.8929965496063232 lr: 0.0001\n",
      "Iter: 8/3021, loss_train: 7.249, loss_val: nan, pos_over_neg: 4.432919502258301 lr: 0.0001\n",
      "Iter: 9/3021, loss_train: 7.2081, loss_val: nan, pos_over_neg: 5.018045902252197 lr: 0.0001\n",
      "Iter: 10/3021, loss_train: 7.1698, loss_val: nan, pos_over_neg: 4.2932820320129395 lr: 0.0001\n",
      "Iter: 11/3021, loss_train: 7.1868, loss_val: nan, pos_over_neg: 3.6771819591522217 lr: 0.0001\n",
      "Iter: 12/3021, loss_train: 7.1661, loss_val: nan, pos_over_neg: 3.214053153991699 lr: 0.0001\n",
      "Iter: 13/3021, loss_train: 7.1439, loss_val: nan, pos_over_neg: 3.1278457641601562 lr: 0.0001\n",
      "Iter: 14/3021, loss_train: 7.1156, loss_val: nan, pos_over_neg: 3.1150903701782227 lr: 0.0001\n",
      "Iter: 15/3021, loss_train: 7.1178, loss_val: nan, pos_over_neg: 2.998558282852173 lr: 0.0001\n",
      "Iter: 16/3021, loss_train: 7.1234, loss_val: nan, pos_over_neg: 3.1541924476623535 lr: 0.0001\n",
      "Iter: 17/3021, loss_train: 7.0741, loss_val: nan, pos_over_neg: 3.748960494995117 lr: 0.0001\n",
      "Iter: 18/3021, loss_train: 7.0609, loss_val: nan, pos_over_neg: 4.192848205566406 lr: 0.0001\n",
      "Iter: 19/3021, loss_train: 7.075, loss_val: nan, pos_over_neg: 4.65902042388916 lr: 0.0001\n",
      "Iter: 20/3021, loss_train: 7.0543, loss_val: nan, pos_over_neg: 5.976966381072998 lr: 0.0001\n",
      "Iter: 21/3021, loss_train: 7.0814, loss_val: nan, pos_over_neg: 6.216888427734375 lr: 0.0001\n",
      "Iter: 22/3021, loss_train: 7.0679, loss_val: nan, pos_over_neg: 7.95610237121582 lr: 0.0001\n",
      "Iter: 23/3021, loss_train: 7.0249, loss_val: nan, pos_over_neg: 8.682245254516602 lr: 0.0001\n",
      "Iter: 24/3021, loss_train: 7.0302, loss_val: nan, pos_over_neg: 9.755510330200195 lr: 0.0001\n",
      "Iter: 25/3021, loss_train: 7.031, loss_val: nan, pos_over_neg: 11.113452911376953 lr: 0.0001\n",
      "Iter: 26/3021, loss_train: 7.0397, loss_val: nan, pos_over_neg: 12.110620498657227 lr: 0.0001\n",
      "Iter: 27/3021, loss_train: 7.006, loss_val: nan, pos_over_neg: 10.989513397216797 lr: 0.0001\n",
      "Iter: 28/3021, loss_train: 6.9822, loss_val: nan, pos_over_neg: 12.371830940246582 lr: 0.0001\n",
      "Iter: 29/3021, loss_train: 6.9554, loss_val: nan, pos_over_neg: 14.386801719665527 lr: 0.0001\n",
      "Iter: 30/3021, loss_train: 6.9806, loss_val: nan, pos_over_neg: 15.291502952575684 lr: 0.0001\n",
      "Iter: 31/3021, loss_train: 6.9545, loss_val: nan, pos_over_neg: 15.260523796081543 lr: 0.0001\n",
      "Iter: 32/3021, loss_train: 6.9425, loss_val: nan, pos_over_neg: 17.28152847290039 lr: 0.0001\n",
      "Iter: 33/3021, loss_train: 6.948, loss_val: nan, pos_over_neg: 18.27750015258789 lr: 0.0001\n",
      "Iter: 34/3021, loss_train: 6.9315, loss_val: nan, pos_over_neg: 20.670175552368164 lr: 0.0001\n",
      "Iter: 35/3021, loss_train: 6.9145, loss_val: nan, pos_over_neg: 24.23166275024414 lr: 0.0001\n",
      "Iter: 36/3021, loss_train: 6.8928, loss_val: nan, pos_over_neg: 27.056079864501953 lr: 0.0001\n",
      "Iter: 37/3021, loss_train: 6.9347, loss_val: nan, pos_over_neg: 25.446367263793945 lr: 0.0001\n",
      "Iter: 38/3021, loss_train: 6.9336, loss_val: nan, pos_over_neg: 27.891082763671875 lr: 0.0001\n",
      "Iter: 39/3021, loss_train: 6.9063, loss_val: nan, pos_over_neg: 29.28904151916504 lr: 0.0001\n",
      "Iter: 40/3021, loss_train: 6.9097, loss_val: nan, pos_over_neg: 31.596050262451172 lr: 0.0001\n",
      "Iter: 41/3021, loss_train: 6.8976, loss_val: nan, pos_over_neg: 29.547401428222656 lr: 0.0001\n",
      "Iter: 42/3021, loss_train: 6.8835, loss_val: nan, pos_over_neg: 33.081722259521484 lr: 0.0001\n",
      "Iter: 43/3021, loss_train: 6.9073, loss_val: nan, pos_over_neg: 26.153966903686523 lr: 0.0001\n",
      "Iter: 44/3021, loss_train: 6.8774, loss_val: nan, pos_over_neg: 27.260713577270508 lr: 0.0001\n",
      "Iter: 45/3021, loss_train: 6.9119, loss_val: nan, pos_over_neg: 22.017616271972656 lr: 0.0001\n",
      "Iter: 46/3021, loss_train: 6.8926, loss_val: nan, pos_over_neg: 20.79217529296875 lr: 0.0001\n",
      "Iter: 47/3021, loss_train: 6.9065, loss_val: nan, pos_over_neg: 20.43366241455078 lr: 0.0001\n",
      "Iter: 48/3021, loss_train: 6.8717, loss_val: nan, pos_over_neg: 20.010391235351562 lr: 0.0001\n",
      "Iter: 49/3021, loss_train: 6.8684, loss_val: nan, pos_over_neg: 21.61119270324707 lr: 0.0001\n",
      "Iter: 50/3021, loss_train: 6.8484, loss_val: nan, pos_over_neg: 24.392173767089844 lr: 0.0001\n",
      "Iter: 51/3021, loss_train: 6.8829, loss_val: nan, pos_over_neg: 23.389657974243164 lr: 0.0001\n",
      "Iter: 52/3021, loss_train: 6.8575, loss_val: nan, pos_over_neg: 23.74220085144043 lr: 0.0001\n",
      "Iter: 53/3021, loss_train: 6.8709, loss_val: nan, pos_over_neg: 26.572397232055664 lr: 0.0001\n",
      "Iter: 54/3021, loss_train: 6.8572, loss_val: nan, pos_over_neg: 29.96137809753418 lr: 0.0001\n",
      "Iter: 55/3021, loss_train: 6.8644, loss_val: nan, pos_over_neg: 31.21910285949707 lr: 0.0001\n",
      "Iter: 56/3021, loss_train: 6.8478, loss_val: nan, pos_over_neg: 36.02167510986328 lr: 0.0001\n",
      "Iter: 57/3021, loss_train: 6.8434, loss_val: nan, pos_over_neg: 42.471534729003906 lr: 0.0001\n",
      "Iter: 58/3021, loss_train: 6.8523, loss_val: nan, pos_over_neg: 41.84825897216797 lr: 0.0001\n",
      "Iter: 59/3021, loss_train: 6.8694, loss_val: nan, pos_over_neg: 42.423824310302734 lr: 0.0001\n",
      "Iter: 60/3021, loss_train: 6.8426, loss_val: nan, pos_over_neg: 48.987857818603516 lr: 0.0001\n",
      "Iter: 61/3021, loss_train: 6.8453, loss_val: nan, pos_over_neg: 50.81122970581055 lr: 0.0001\n",
      "Iter: 62/3021, loss_train: 6.8528, loss_val: nan, pos_over_neg: 40.297523498535156 lr: 0.0001\n",
      "Iter: 63/3021, loss_train: 6.832, loss_val: nan, pos_over_neg: 40.93576431274414 lr: 0.0001\n",
      "Iter: 64/3021, loss_train: 6.8349, loss_val: nan, pos_over_neg: 40.05012893676758 lr: 0.0001\n",
      "Iter: 65/3021, loss_train: 6.8376, loss_val: nan, pos_over_neg: 47.91281509399414 lr: 0.0001\n",
      "Iter: 66/3021, loss_train: 6.8127, loss_val: nan, pos_over_neg: 40.26491928100586 lr: 0.0001\n",
      "Iter: 67/3021, loss_train: 6.8471, loss_val: nan, pos_over_neg: 39.93717956542969 lr: 0.0001\n",
      "Iter: 68/3021, loss_train: 6.8177, loss_val: nan, pos_over_neg: 44.224464416503906 lr: 0.0001\n",
      "Iter: 69/3021, loss_train: 6.8087, loss_val: nan, pos_over_neg: 51.01115417480469 lr: 0.0001\n",
      "Iter: 70/3021, loss_train: 6.8153, loss_val: nan, pos_over_neg: 44.6114501953125 lr: 0.0001\n",
      "Iter: 71/3021, loss_train: 6.8398, loss_val: nan, pos_over_neg: 49.084659576416016 lr: 0.0001\n",
      "Iter: 72/3021, loss_train: 6.8445, loss_val: nan, pos_over_neg: 39.60350036621094 lr: 0.0001\n",
      "Iter: 73/3021, loss_train: 6.8089, loss_val: nan, pos_over_neg: 44.32228469848633 lr: 0.0001\n",
      "Iter: 74/3021, loss_train: 6.8039, loss_val: nan, pos_over_neg: 40.68450164794922 lr: 0.0001\n",
      "Iter: 75/3021, loss_train: 6.8097, loss_val: nan, pos_over_neg: 49.02886962890625 lr: 0.0001\n",
      "Iter: 76/3021, loss_train: 6.7989, loss_val: nan, pos_over_neg: 49.3508186340332 lr: 0.0001\n",
      "Iter: 77/3021, loss_train: 6.7768, loss_val: nan, pos_over_neg: 38.846290588378906 lr: 0.0001\n",
      "Iter: 78/3021, loss_train: 6.8279, loss_val: nan, pos_over_neg: 37.92509841918945 lr: 0.0001\n",
      "Iter: 79/3021, loss_train: 6.8123, loss_val: nan, pos_over_neg: 39.061737060546875 lr: 0.0001\n",
      "Iter: 80/3021, loss_train: 6.8308, loss_val: nan, pos_over_neg: 37.18299865722656 lr: 0.0001\n",
      "Iter: 81/3021, loss_train: 6.7957, loss_val: nan, pos_over_neg: 43.91147232055664 lr: 0.0001\n",
      "Iter: 82/3021, loss_train: 6.8417, loss_val: nan, pos_over_neg: 50.396034240722656 lr: 0.0001\n",
      "Iter: 83/3021, loss_train: 6.8093, loss_val: nan, pos_over_neg: 47.06821060180664 lr: 0.0001\n",
      "Iter: 84/3021, loss_train: 6.833, loss_val: nan, pos_over_neg: 49.034217834472656 lr: 0.0001\n",
      "Iter: 85/3021, loss_train: 6.8018, loss_val: nan, pos_over_neg: 51.317142486572266 lr: 0.0001\n",
      "Iter: 86/3021, loss_train: 6.7627, loss_val: nan, pos_over_neg: 61.83523178100586 lr: 0.0001\n",
      "Iter: 87/3021, loss_train: 6.7674, loss_val: nan, pos_over_neg: 57.407447814941406 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 88/3021, loss_train: 6.8354, loss_val: nan, pos_over_neg: 43.760162353515625 lr: 0.0001\n",
      "Iter: 89/3021, loss_train: 6.8146, loss_val: nan, pos_over_neg: 48.851261138916016 lr: 0.0001\n",
      "Iter: 90/3021, loss_train: 6.8093, loss_val: nan, pos_over_neg: 41.814632415771484 lr: 0.0001\n",
      "Iter: 91/3021, loss_train: 6.8204, loss_val: nan, pos_over_neg: 43.28696060180664 lr: 0.0001\n",
      "Iter: 92/3021, loss_train: 6.7745, loss_val: nan, pos_over_neg: 56.589866638183594 lr: 0.0001\n",
      "Iter: 93/3021, loss_train: 6.7896, loss_val: nan, pos_over_neg: 40.6494026184082 lr: 0.0001\n",
      "Iter: 94/3021, loss_train: 6.7897, loss_val: nan, pos_over_neg: 43.2087287902832 lr: 0.0001\n",
      "Iter: 95/3021, loss_train: 6.771, loss_val: nan, pos_over_neg: 48.807743072509766 lr: 0.0001\n",
      "Iter: 96/3021, loss_train: 6.78, loss_val: nan, pos_over_neg: 53.1165657043457 lr: 0.0001\n",
      "Iter: 97/3021, loss_train: 6.8065, loss_val: nan, pos_over_neg: 51.976200103759766 lr: 0.0001\n",
      "Iter: 98/3021, loss_train: 6.7815, loss_val: nan, pos_over_neg: 71.77888488769531 lr: 0.0001\n",
      "Iter: 99/3021, loss_train: 6.81, loss_val: nan, pos_over_neg: 51.64118576049805 lr: 0.0001\n",
      "Iter: 100/3021, loss_train: 6.7576, loss_val: nan, pos_over_neg: 79.85929107666016 lr: 0.0001\n",
      "Iter: 101/3021, loss_train: 6.8098, loss_val: nan, pos_over_neg: 58.3011360168457 lr: 0.0001\n",
      "Iter: 102/3021, loss_train: 6.7837, loss_val: nan, pos_over_neg: 83.85948181152344 lr: 0.0001\n",
      "Iter: 103/3021, loss_train: 6.802, loss_val: nan, pos_over_neg: 57.09276580810547 lr: 0.0001\n",
      "Iter: 104/3021, loss_train: 6.8107, loss_val: nan, pos_over_neg: 58.292327880859375 lr: 0.0001\n",
      "Iter: 105/3021, loss_train: 6.7629, loss_val: nan, pos_over_neg: 55.25848388671875 lr: 0.0001\n",
      "Iter: 106/3021, loss_train: 6.7731, loss_val: nan, pos_over_neg: 54.20040512084961 lr: 0.0001\n",
      "Iter: 107/3021, loss_train: 6.7596, loss_val: nan, pos_over_neg: 54.654518127441406 lr: 0.0001\n",
      "Iter: 108/3021, loss_train: 6.7953, loss_val: nan, pos_over_neg: 43.745975494384766 lr: 0.0001\n",
      "Iter: 109/3021, loss_train: 6.805, loss_val: nan, pos_over_neg: 56.064483642578125 lr: 0.0001\n",
      "Iter: 110/3021, loss_train: 6.7511, loss_val: nan, pos_over_neg: 61.22787094116211 lr: 0.0001\n",
      "Iter: 111/3021, loss_train: 6.7705, loss_val: nan, pos_over_neg: 64.91835021972656 lr: 0.0001\n",
      "Iter: 112/3021, loss_train: 6.7687, loss_val: nan, pos_over_neg: 60.14411544799805 lr: 0.0001\n",
      "Iter: 113/3021, loss_train: 6.7847, loss_val: nan, pos_over_neg: 59.15860366821289 lr: 0.0001\n",
      "Iter: 114/3021, loss_train: 6.7692, loss_val: nan, pos_over_neg: 51.28641891479492 lr: 0.0001\n",
      "Iter: 115/3021, loss_train: 6.766, loss_val: nan, pos_over_neg: 77.294189453125 lr: 0.0001\n",
      "Iter: 116/3021, loss_train: 6.7518, loss_val: nan, pos_over_neg: 69.04939270019531 lr: 0.0001\n",
      "Iter: 117/3021, loss_train: 6.75, loss_val: nan, pos_over_neg: 77.16857147216797 lr: 0.0001\n",
      "Iter: 118/3021, loss_train: 6.7585, loss_val: nan, pos_over_neg: 84.59793090820312 lr: 0.0001\n",
      "Iter: 119/3021, loss_train: 6.7813, loss_val: nan, pos_over_neg: 69.23056030273438 lr: 0.0001\n",
      "Iter: 120/3021, loss_train: 6.751, loss_val: nan, pos_over_neg: 76.94059753417969 lr: 0.0001\n",
      "Iter: 121/3021, loss_train: 6.7714, loss_val: nan, pos_over_neg: 73.8790054321289 lr: 0.0001\n",
      "Iter: 122/3021, loss_train: 6.7418, loss_val: nan, pos_over_neg: 65.19808959960938 lr: 0.0001\n",
      "Iter: 123/3021, loss_train: 6.7783, loss_val: nan, pos_over_neg: 55.35531997680664 lr: 0.0001\n",
      "Iter: 124/3021, loss_train: 6.7609, loss_val: nan, pos_over_neg: 59.742027282714844 lr: 0.0001\n",
      "Iter: 125/3021, loss_train: 6.7651, loss_val: nan, pos_over_neg: 59.78522491455078 lr: 0.0001\n",
      "Iter: 126/3021, loss_train: 6.76, loss_val: nan, pos_over_neg: 61.0769157409668 lr: 0.0001\n",
      "Iter: 127/3021, loss_train: 6.7503, loss_val: nan, pos_over_neg: 67.15830993652344 lr: 0.0001\n",
      "Iter: 128/3021, loss_train: 6.7475, loss_val: nan, pos_over_neg: 78.33700561523438 lr: 0.0001\n",
      "Iter: 129/3021, loss_train: 6.7193, loss_val: nan, pos_over_neg: 67.59737396240234 lr: 0.0001\n",
      "Iter: 130/3021, loss_train: 6.7497, loss_val: nan, pos_over_neg: 50.54961395263672 lr: 0.0001\n",
      "Iter: 131/3021, loss_train: 6.7862, loss_val: nan, pos_over_neg: 65.02569580078125 lr: 0.0001\n",
      "Iter: 132/3021, loss_train: 6.7597, loss_val: nan, pos_over_neg: 64.42876434326172 lr: 0.0001\n",
      "Iter: 133/3021, loss_train: 6.7492, loss_val: nan, pos_over_neg: 67.41619110107422 lr: 0.0001\n",
      "Iter: 134/3021, loss_train: 6.7089, loss_val: nan, pos_over_neg: 58.450687408447266 lr: 0.0001\n",
      "Iter: 135/3021, loss_train: 6.7534, loss_val: nan, pos_over_neg: 73.15497589111328 lr: 0.0001\n",
      "Iter: 136/3021, loss_train: 6.7432, loss_val: nan, pos_over_neg: 54.85002899169922 lr: 0.0001\n",
      "Iter: 137/3021, loss_train: 6.7388, loss_val: nan, pos_over_neg: 60.87345504760742 lr: 0.0001\n",
      "Iter: 138/3021, loss_train: 6.7484, loss_val: nan, pos_over_neg: 72.12862396240234 lr: 0.0001\n",
      "Iter: 139/3021, loss_train: 6.7538, loss_val: nan, pos_over_neg: 71.77840423583984 lr: 0.0001\n",
      "Iter: 140/3021, loss_train: 6.7737, loss_val: nan, pos_over_neg: 92.67657470703125 lr: 0.0001\n",
      "Iter: 141/3021, loss_train: 6.7472, loss_val: nan, pos_over_neg: 117.83413696289062 lr: 0.0001\n",
      "Iter: 142/3021, loss_train: 6.7392, loss_val: nan, pos_over_neg: 96.83900451660156 lr: 0.0001\n",
      "Iter: 143/3021, loss_train: 6.7331, loss_val: nan, pos_over_neg: 113.37006378173828 lr: 0.0001\n",
      "Iter: 144/3021, loss_train: 6.749, loss_val: nan, pos_over_neg: 86.45429229736328 lr: 0.0001\n",
      "Iter: 145/3021, loss_train: 6.7307, loss_val: nan, pos_over_neg: 87.6976089477539 lr: 0.0001\n",
      "Iter: 146/3021, loss_train: 6.7304, loss_val: nan, pos_over_neg: 91.76646423339844 lr: 0.0001\n",
      "Iter: 147/3021, loss_train: 6.7277, loss_val: nan, pos_over_neg: 62.095584869384766 lr: 0.0001\n",
      "Iter: 148/3021, loss_train: 6.7065, loss_val: nan, pos_over_neg: 69.68242645263672 lr: 0.0001\n",
      "Iter: 149/3021, loss_train: 6.7207, loss_val: nan, pos_over_neg: 61.25969314575195 lr: 0.0001\n",
      "Iter: 150/3021, loss_train: 6.7187, loss_val: nan, pos_over_neg: 66.39551544189453 lr: 0.0001\n",
      "Iter: 151/3021, loss_train: 6.7208, loss_val: nan, pos_over_neg: 78.6484603881836 lr: 0.0001\n",
      "Iter: 152/3021, loss_train: 6.7337, loss_val: nan, pos_over_neg: 61.86404800415039 lr: 0.0001\n",
      "Iter: 153/3021, loss_train: 6.7498, loss_val: nan, pos_over_neg: 67.4151611328125 lr: 0.0001\n",
      "Iter: 154/3021, loss_train: 6.7459, loss_val: nan, pos_over_neg: 90.77141571044922 lr: 0.0001\n",
      "Iter: 155/3021, loss_train: 6.7277, loss_val: nan, pos_over_neg: 85.33511352539062 lr: 0.0001\n",
      "Iter: 156/3021, loss_train: 6.7447, loss_val: nan, pos_over_neg: 67.94538879394531 lr: 0.0001\n",
      "Iter: 157/3021, loss_train: 6.7234, loss_val: nan, pos_over_neg: 93.521484375 lr: 0.0001\n",
      "Iter: 158/3021, loss_train: 6.7293, loss_val: nan, pos_over_neg: 87.49002838134766 lr: 0.0001\n",
      "Iter: 159/3021, loss_train: 6.74, loss_val: nan, pos_over_neg: 66.27784729003906 lr: 0.0001\n",
      "Iter: 160/3021, loss_train: 6.7662, loss_val: nan, pos_over_neg: 70.15357208251953 lr: 0.0001\n",
      "Iter: 161/3021, loss_train: 6.6985, loss_val: nan, pos_over_neg: 81.46954345703125 lr: 0.0001\n",
      "Iter: 162/3021, loss_train: 6.7113, loss_val: nan, pos_over_neg: 80.83041381835938 lr: 0.0001\n",
      "Iter: 163/3021, loss_train: 6.7073, loss_val: nan, pos_over_neg: 81.69828033447266 lr: 0.0001\n",
      "Iter: 164/3021, loss_train: 6.725, loss_val: nan, pos_over_neg: 101.64787292480469 lr: 0.0001\n",
      "Iter: 165/3021, loss_train: 6.726, loss_val: nan, pos_over_neg: 97.93206787109375 lr: 0.0001\n",
      "Iter: 166/3021, loss_train: 6.7272, loss_val: nan, pos_over_neg: 74.03150177001953 lr: 0.0001\n",
      "Iter: 167/3021, loss_train: 6.7259, loss_val: nan, pos_over_neg: 92.36832427978516 lr: 0.0001\n",
      "Iter: 168/3021, loss_train: 6.7084, loss_val: nan, pos_over_neg: 125.54150390625 lr: 0.0001\n",
      "Iter: 169/3021, loss_train: 6.7082, loss_val: nan, pos_over_neg: 125.2975082397461 lr: 0.0001\n",
      "Iter: 170/3021, loss_train: 6.702, loss_val: nan, pos_over_neg: 78.06637573242188 lr: 0.0001\n",
      "Iter: 171/3021, loss_train: 6.7291, loss_val: nan, pos_over_neg: 77.8360595703125 lr: 0.0001\n",
      "Iter: 172/3021, loss_train: 6.7206, loss_val: nan, pos_over_neg: 120.80178833007812 lr: 0.0001\n",
      "Iter: 173/3021, loss_train: 6.7268, loss_val: nan, pos_over_neg: 101.1009750366211 lr: 0.0001\n",
      "Iter: 174/3021, loss_train: 6.7354, loss_val: nan, pos_over_neg: 90.73112487792969 lr: 0.0001\n",
      "Iter: 175/3021, loss_train: 6.719, loss_val: nan, pos_over_neg: 94.68084716796875 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 176/3021, loss_train: 6.7071, loss_val: nan, pos_over_neg: 84.76749420166016 lr: 0.0001\n",
      "Iter: 177/3021, loss_train: 6.7194, loss_val: nan, pos_over_neg: 70.1619644165039 lr: 0.0001\n",
      "Iter: 178/3021, loss_train: 6.7313, loss_val: nan, pos_over_neg: 50.301334381103516 lr: 0.0001\n",
      "Iter: 179/3021, loss_train: 6.715, loss_val: nan, pos_over_neg: 59.340797424316406 lr: 0.0001\n",
      "Iter: 180/3021, loss_train: 6.7239, loss_val: nan, pos_over_neg: 47.43552017211914 lr: 0.0001\n",
      "Iter: 181/3021, loss_train: 6.7145, loss_val: nan, pos_over_neg: 53.7491340637207 lr: 0.0001\n",
      "Iter: 182/3021, loss_train: 6.7351, loss_val: nan, pos_over_neg: 57.259178161621094 lr: 0.0001\n",
      "Iter: 183/3021, loss_train: 6.7138, loss_val: nan, pos_over_neg: 57.80210876464844 lr: 0.0001\n",
      "Iter: 184/3021, loss_train: 6.6973, loss_val: nan, pos_over_neg: 86.44732666015625 lr: 0.0001\n",
      "Iter: 185/3021, loss_train: 6.7185, loss_val: nan, pos_over_neg: 71.84893035888672 lr: 0.0001\n",
      "Iter: 186/3021, loss_train: 6.7022, loss_val: nan, pos_over_neg: 73.95223236083984 lr: 0.0001\n",
      "Iter: 187/3021, loss_train: 6.7373, loss_val: nan, pos_over_neg: 84.15961456298828 lr: 0.0001\n",
      "Iter: 188/3021, loss_train: 6.724, loss_val: nan, pos_over_neg: 97.02572631835938 lr: 0.0001\n",
      "Iter: 189/3021, loss_train: 6.6999, loss_val: nan, pos_over_neg: 105.72530364990234 lr: 0.0001\n",
      "Iter: 190/3021, loss_train: 6.7169, loss_val: nan, pos_over_neg: 101.1483383178711 lr: 0.0001\n",
      "Iter: 191/3021, loss_train: 6.7108, loss_val: nan, pos_over_neg: 103.40109252929688 lr: 0.0001\n",
      "Iter: 192/3021, loss_train: 6.7197, loss_val: nan, pos_over_neg: 125.79534149169922 lr: 0.0001\n",
      "Iter: 193/3021, loss_train: 6.6937, loss_val: nan, pos_over_neg: 113.58345794677734 lr: 0.0001\n",
      "Iter: 194/3021, loss_train: 6.7209, loss_val: nan, pos_over_neg: 101.97232055664062 lr: 0.0001\n",
      "Iter: 195/3021, loss_train: 6.6906, loss_val: nan, pos_over_neg: 133.75140380859375 lr: 0.0001\n",
      "Iter: 196/3021, loss_train: 6.6794, loss_val: nan, pos_over_neg: 101.63848876953125 lr: 0.0001\n",
      "Iter: 197/3021, loss_train: 6.7153, loss_val: nan, pos_over_neg: 66.66683959960938 lr: 0.0001\n",
      "Iter: 198/3021, loss_train: 6.6969, loss_val: nan, pos_over_neg: 68.86665344238281 lr: 0.0001\n",
      "Iter: 199/3021, loss_train: 6.7088, loss_val: nan, pos_over_neg: 68.2568588256836 lr: 0.0001\n",
      "Iter: 200/3021, loss_train: 6.685, loss_val: nan, pos_over_neg: 80.69832611083984 lr: 0.0001\n",
      "Iter: 201/3021, loss_train: 6.7106, loss_val: nan, pos_over_neg: 66.3100357055664 lr: 0.0001\n",
      "Iter: 202/3021, loss_train: 6.6827, loss_val: nan, pos_over_neg: 76.94499206542969 lr: 0.0001\n",
      "Iter: 203/3021, loss_train: 6.7002, loss_val: nan, pos_over_neg: 80.6025619506836 lr: 0.0001\n",
      "Iter: 204/3021, loss_train: 6.722, loss_val: nan, pos_over_neg: 77.79830932617188 lr: 0.0001\n",
      "Iter: 205/3021, loss_train: 6.6806, loss_val: nan, pos_over_neg: 77.56159973144531 lr: 0.0001\n",
      "Iter: 206/3021, loss_train: 6.7293, loss_val: nan, pos_over_neg: 84.55481719970703 lr: 0.0001\n",
      "Iter: 207/3021, loss_train: 6.7009, loss_val: nan, pos_over_neg: 92.25067901611328 lr: 0.0001\n",
      "Iter: 208/3021, loss_train: 6.6996, loss_val: nan, pos_over_neg: 142.18026733398438 lr: 0.0001\n",
      "Iter: 209/3021, loss_train: 6.6923, loss_val: nan, pos_over_neg: 155.98448181152344 lr: 0.0001\n",
      "Iter: 210/3021, loss_train: 6.6875, loss_val: nan, pos_over_neg: 142.7229461669922 lr: 0.0001\n",
      "Iter: 211/3021, loss_train: 6.7137, loss_val: nan, pos_over_neg: 133.74649047851562 lr: 0.0001\n",
      "Iter: 212/3021, loss_train: 6.7089, loss_val: nan, pos_over_neg: 101.69738006591797 lr: 0.0001\n",
      "Iter: 213/3021, loss_train: 6.7024, loss_val: nan, pos_over_neg: 100.99839782714844 lr: 0.0001\n",
      "Iter: 214/3021, loss_train: 6.712, loss_val: nan, pos_over_neg: 113.57456970214844 lr: 0.0001\n",
      "Iter: 215/3021, loss_train: 6.6684, loss_val: nan, pos_over_neg: 83.93827056884766 lr: 0.0001\n",
      "Iter: 216/3021, loss_train: 6.6694, loss_val: nan, pos_over_neg: 106.70840454101562 lr: 0.0001\n",
      "Iter: 217/3021, loss_train: 6.6889, loss_val: nan, pos_over_neg: 82.01596069335938 lr: 0.0001\n",
      "Iter: 218/3021, loss_train: 6.7039, loss_val: nan, pos_over_neg: 74.54161071777344 lr: 0.0001\n",
      "Iter: 219/3021, loss_train: 6.6857, loss_val: nan, pos_over_neg: 68.9515380859375 lr: 0.0001\n",
      "Iter: 220/3021, loss_train: 6.7314, loss_val: nan, pos_over_neg: 58.126834869384766 lr: 0.0001\n",
      "Iter: 221/3021, loss_train: 6.701, loss_val: nan, pos_over_neg: 81.55027770996094 lr: 0.0001\n",
      "Iter: 222/3021, loss_train: 6.7232, loss_val: nan, pos_over_neg: 91.59626770019531 lr: 0.0001\n",
      "Iter: 223/3021, loss_train: 6.6894, loss_val: nan, pos_over_neg: 116.49406433105469 lr: 0.0001\n",
      "Iter: 224/3021, loss_train: 6.6857, loss_val: nan, pos_over_neg: 102.48033142089844 lr: 0.0001\n",
      "Iter: 225/3021, loss_train: 6.6961, loss_val: nan, pos_over_neg: 114.23649597167969 lr: 0.0001\n",
      "Iter: 226/3021, loss_train: 6.6875, loss_val: nan, pos_over_neg: 97.00934600830078 lr: 0.0001\n",
      "Iter: 227/3021, loss_train: 6.6983, loss_val: nan, pos_over_neg: 78.96463775634766 lr: 0.0001\n",
      "Iter: 228/3021, loss_train: 6.7192, loss_val: nan, pos_over_neg: 64.34586334228516 lr: 0.0001\n",
      "Iter: 229/3021, loss_train: 6.6826, loss_val: nan, pos_over_neg: 93.03681182861328 lr: 0.0001\n",
      "Iter: 230/3021, loss_train: 6.703, loss_val: nan, pos_over_neg: 68.14656066894531 lr: 0.0001\n",
      "Iter: 231/3021, loss_train: 6.6776, loss_val: nan, pos_over_neg: 81.82646179199219 lr: 0.0001\n",
      "Iter: 232/3021, loss_train: 6.6925, loss_val: nan, pos_over_neg: 81.14112854003906 lr: 0.0001\n",
      "Iter: 233/3021, loss_train: 6.6841, loss_val: nan, pos_over_neg: 94.3615951538086 lr: 0.0001\n",
      "Iter: 234/3021, loss_train: 6.686, loss_val: nan, pos_over_neg: 99.1222152709961 lr: 0.0001\n",
      "Iter: 235/3021, loss_train: 6.7019, loss_val: nan, pos_over_neg: 124.48519134521484 lr: 0.0001\n",
      "Iter: 236/3021, loss_train: 6.7045, loss_val: nan, pos_over_neg: 133.365966796875 lr: 0.0001\n",
      "Iter: 237/3021, loss_train: 6.6739, loss_val: nan, pos_over_neg: 209.5004425048828 lr: 0.0001\n",
      "Iter: 238/3021, loss_train: 6.7043, loss_val: nan, pos_over_neg: 123.2796630859375 lr: 0.0001\n",
      "Iter: 239/3021, loss_train: 6.6911, loss_val: nan, pos_over_neg: 148.38894653320312 lr: 0.0001\n",
      "Iter: 240/3021, loss_train: 6.691, loss_val: nan, pos_over_neg: 97.31390380859375 lr: 0.0001\n",
      "Iter: 241/3021, loss_train: 6.6691, loss_val: nan, pos_over_neg: 82.40741729736328 lr: 0.0001\n",
      "Iter: 242/3021, loss_train: 6.6531, loss_val: nan, pos_over_neg: 65.0707015991211 lr: 0.0001\n",
      "Iter: 243/3021, loss_train: 6.6906, loss_val: nan, pos_over_neg: 49.05887222290039 lr: 0.0001\n",
      "Iter: 244/3021, loss_train: 6.6778, loss_val: nan, pos_over_neg: 51.740936279296875 lr: 0.0001\n",
      "Iter: 245/3021, loss_train: 6.7057, loss_val: nan, pos_over_neg: 49.9678955078125 lr: 0.0001\n",
      "Iter: 246/3021, loss_train: 6.6721, loss_val: nan, pos_over_neg: 63.70652770996094 lr: 0.0001\n",
      "Iter: 247/3021, loss_train: 6.6596, loss_val: nan, pos_over_neg: 74.46138000488281 lr: 0.0001\n",
      "Iter: 248/3021, loss_train: 6.695, loss_val: nan, pos_over_neg: 78.19060516357422 lr: 0.0001\n",
      "Iter: 249/3021, loss_train: 6.6695, loss_val: nan, pos_over_neg: 89.50886535644531 lr: 0.0001\n",
      "Iter: 250/3021, loss_train: 6.6872, loss_val: nan, pos_over_neg: 84.96765899658203 lr: 0.0001\n",
      "Iter: 251/3021, loss_train: 6.6531, loss_val: nan, pos_over_neg: 126.42985534667969 lr: 0.0001\n",
      "Iter: 252/3021, loss_train: 6.6978, loss_val: nan, pos_over_neg: 150.4954071044922 lr: 0.0001\n",
      "Iter: 253/3021, loss_train: 6.7268, loss_val: nan, pos_over_neg: 159.4720916748047 lr: 0.0001\n",
      "Iter: 254/3021, loss_train: 6.6526, loss_val: nan, pos_over_neg: 323.16998291015625 lr: 0.0001\n",
      "Iter: 255/3021, loss_train: 6.6958, loss_val: nan, pos_over_neg: 133.1451873779297 lr: 0.0001\n",
      "Iter: 256/3021, loss_train: 6.6475, loss_val: nan, pos_over_neg: 151.48208618164062 lr: 0.0001\n",
      "Iter: 257/3021, loss_train: 6.6544, loss_val: nan, pos_over_neg: 126.54412841796875 lr: 0.0001\n",
      "Iter: 258/3021, loss_train: 6.6853, loss_val: nan, pos_over_neg: 78.16667175292969 lr: 0.0001\n",
      "Iter: 259/3021, loss_train: 6.7041, loss_val: nan, pos_over_neg: 65.16626739501953 lr: 0.0001\n",
      "Iter: 260/3021, loss_train: 6.6725, loss_val: nan, pos_over_neg: 64.7787094116211 lr: 0.0001\n",
      "Iter: 261/3021, loss_train: 6.6757, loss_val: nan, pos_over_neg: 59.26289367675781 lr: 0.0001\n",
      "Iter: 262/3021, loss_train: 6.6678, loss_val: nan, pos_over_neg: 57.83658981323242 lr: 0.0001\n",
      "Iter: 263/3021, loss_train: 6.6999, loss_val: nan, pos_over_neg: 66.29720306396484 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 264/3021, loss_train: 6.7164, loss_val: nan, pos_over_neg: 56.449798583984375 lr: 0.0001\n",
      "Iter: 265/3021, loss_train: 6.71, loss_val: nan, pos_over_neg: 69.50334167480469 lr: 0.0001\n",
      "Iter: 266/3021, loss_train: 6.6901, loss_val: nan, pos_over_neg: 108.88590240478516 lr: 0.0001\n",
      "Iter: 267/3021, loss_train: 6.675, loss_val: nan, pos_over_neg: 110.29940795898438 lr: 0.0001\n",
      "Iter: 268/3021, loss_train: 6.6815, loss_val: nan, pos_over_neg: 136.855712890625 lr: 0.0001\n",
      "Iter: 269/3021, loss_train: 6.6864, loss_val: nan, pos_over_neg: 126.91331481933594 lr: 0.0001\n",
      "Iter: 270/3021, loss_train: 6.6948, loss_val: nan, pos_over_neg: 102.88433837890625 lr: 0.0001\n",
      "Iter: 271/3021, loss_train: 6.682, loss_val: nan, pos_over_neg: 117.58699035644531 lr: 0.0001\n",
      "Iter: 272/3021, loss_train: 6.6672, loss_val: nan, pos_over_neg: 123.74059295654297 lr: 0.0001\n",
      "Iter: 273/3021, loss_train: 6.6968, loss_val: nan, pos_over_neg: 97.98333740234375 lr: 0.0001\n",
      "Iter: 274/3021, loss_train: 6.6962, loss_val: nan, pos_over_neg: 79.65541076660156 lr: 0.0001\n",
      "Iter: 275/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 92.03646087646484 lr: 0.0001\n",
      "Iter: 276/3021, loss_train: 6.6816, loss_val: nan, pos_over_neg: 104.34444427490234 lr: 0.0001\n",
      "Iter: 277/3021, loss_train: 6.6631, loss_val: nan, pos_over_neg: 90.28307342529297 lr: 0.0001\n",
      "Iter: 278/3021, loss_train: 6.6663, loss_val: nan, pos_over_neg: 107.57144927978516 lr: 0.0001\n",
      "Iter: 279/3021, loss_train: 6.669, loss_val: nan, pos_over_neg: 82.69637298583984 lr: 0.0001\n",
      "Iter: 280/3021, loss_train: 6.6711, loss_val: nan, pos_over_neg: 116.74626159667969 lr: 0.0001\n",
      "Iter: 281/3021, loss_train: 6.6424, loss_val: nan, pos_over_neg: 94.11897277832031 lr: 0.0001\n",
      "Iter: 282/3021, loss_train: 6.6815, loss_val: nan, pos_over_neg: 64.04624938964844 lr: 0.0001\n",
      "Iter: 283/3021, loss_train: 6.6788, loss_val: nan, pos_over_neg: 76.94560241699219 lr: 0.0001\n",
      "Iter: 284/3021, loss_train: 6.6913, loss_val: nan, pos_over_neg: 73.29693603515625 lr: 0.0001\n",
      "Iter: 285/3021, loss_train: 6.6532, loss_val: nan, pos_over_neg: 77.46342468261719 lr: 0.0001\n",
      "Iter: 286/3021, loss_train: 6.6587, loss_val: nan, pos_over_neg: 82.83592224121094 lr: 0.0001\n",
      "Iter: 287/3021, loss_train: 6.661, loss_val: nan, pos_over_neg: 86.6082534790039 lr: 0.0001\n",
      "Iter: 288/3021, loss_train: 6.687, loss_val: nan, pos_over_neg: 81.53919219970703 lr: 0.0001\n",
      "Iter: 289/3021, loss_train: 6.6788, loss_val: nan, pos_over_neg: 112.6345443725586 lr: 0.0001\n",
      "Iter: 290/3021, loss_train: 6.6391, loss_val: nan, pos_over_neg: 194.77415466308594 lr: 0.0001\n",
      "Iter: 291/3021, loss_train: 6.6728, loss_val: nan, pos_over_neg: 108.34639739990234 lr: 0.0001\n",
      "Iter: 292/3021, loss_train: 6.6334, loss_val: nan, pos_over_neg: 144.98458862304688 lr: 0.0001\n",
      "Iter: 293/3021, loss_train: 6.7033, loss_val: nan, pos_over_neg: 97.97551727294922 lr: 0.0001\n",
      "Iter: 294/3021, loss_train: 6.6582, loss_val: nan, pos_over_neg: 88.14102935791016 lr: 0.0001\n",
      "Iter: 295/3021, loss_train: 6.6662, loss_val: nan, pos_over_neg: 78.91661834716797 lr: 0.0001\n",
      "Iter: 296/3021, loss_train: 6.6797, loss_val: nan, pos_over_neg: 80.546630859375 lr: 0.0001\n",
      "Iter: 297/3021, loss_train: 6.6595, loss_val: nan, pos_over_neg: 66.97296905517578 lr: 0.0001\n",
      "Iter: 298/3021, loss_train: 6.6602, loss_val: nan, pos_over_neg: 73.58814239501953 lr: 0.0001\n",
      "Iter: 299/3021, loss_train: 6.6811, loss_val: nan, pos_over_neg: 66.23306274414062 lr: 0.0001\n",
      "Iter: 300/3021, loss_train: 6.6745, loss_val: nan, pos_over_neg: 68.51049041748047 lr: 0.0001\n",
      "Iter: 301/3021, loss_train: 6.6926, loss_val: nan, pos_over_neg: 65.71810913085938 lr: 0.0001\n",
      "Iter: 302/3021, loss_train: 6.6903, loss_val: nan, pos_over_neg: 73.8968505859375 lr: 0.0001\n",
      "Iter: 303/3021, loss_train: 6.6567, loss_val: nan, pos_over_neg: 110.89866638183594 lr: 0.0001\n",
      "Iter: 304/3021, loss_train: 6.6688, loss_val: nan, pos_over_neg: 95.83787536621094 lr: 0.0001\n",
      "Iter: 305/3021, loss_train: 6.6639, loss_val: nan, pos_over_neg: 109.94551849365234 lr: 0.0001\n",
      "Iter: 306/3021, loss_train: 6.6885, loss_val: nan, pos_over_neg: 102.9513931274414 lr: 0.0001\n",
      "Iter: 307/3021, loss_train: 6.6559, loss_val: nan, pos_over_neg: 137.40386962890625 lr: 0.0001\n",
      "Iter: 308/3021, loss_train: 6.6692, loss_val: nan, pos_over_neg: 96.13809204101562 lr: 0.0001\n",
      "Iter: 309/3021, loss_train: 6.668, loss_val: nan, pos_over_neg: 86.10679626464844 lr: 0.0001\n",
      "Iter: 310/3021, loss_train: 6.6716, loss_val: nan, pos_over_neg: 87.78389739990234 lr: 0.0001\n",
      "Iter: 311/3021, loss_train: 6.6377, loss_val: nan, pos_over_neg: 109.6025619506836 lr: 0.0001\n",
      "Iter: 312/3021, loss_train: 6.6476, loss_val: nan, pos_over_neg: 100.9728012084961 lr: 0.0001\n",
      "Iter: 313/3021, loss_train: 6.6627, loss_val: nan, pos_over_neg: 98.6878890991211 lr: 0.0001\n",
      "Iter: 314/3021, loss_train: 6.6809, loss_val: nan, pos_over_neg: 110.98111724853516 lr: 0.0001\n",
      "Iter: 315/3021, loss_train: 6.6773, loss_val: nan, pos_over_neg: 92.09634399414062 lr: 0.0001\n",
      "Iter: 316/3021, loss_train: 6.6595, loss_val: nan, pos_over_neg: 86.76773071289062 lr: 0.0001\n",
      "Iter: 317/3021, loss_train: 6.6708, loss_val: nan, pos_over_neg: 68.5373764038086 lr: 0.0001\n",
      "Iter: 318/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 115.41385650634766 lr: 0.0001\n",
      "Iter: 319/3021, loss_train: 6.6508, loss_val: nan, pos_over_neg: 83.01995086669922 lr: 0.0001\n",
      "Iter: 320/3021, loss_train: 6.6735, loss_val: nan, pos_over_neg: 92.32771301269531 lr: 0.0001\n",
      "Iter: 321/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 113.03224182128906 lr: 0.0001\n",
      "Iter: 322/3021, loss_train: 6.6786, loss_val: nan, pos_over_neg: 93.7867431640625 lr: 0.0001\n",
      "Iter: 323/3021, loss_train: 6.6378, loss_val: nan, pos_over_neg: 170.68226623535156 lr: 0.0001\n",
      "Iter: 324/3021, loss_train: 6.6619, loss_val: nan, pos_over_neg: 107.05011749267578 lr: 0.0001\n",
      "Iter: 325/3021, loss_train: 6.6658, loss_val: nan, pos_over_neg: 105.67250061035156 lr: 0.0001\n",
      "Iter: 326/3021, loss_train: 6.6543, loss_val: nan, pos_over_neg: 111.35372161865234 lr: 0.0001\n",
      "Iter: 327/3021, loss_train: 6.6669, loss_val: nan, pos_over_neg: 92.36734771728516 lr: 0.0001\n",
      "Iter: 328/3021, loss_train: 6.6641, loss_val: nan, pos_over_neg: 81.20365142822266 lr: 0.0001\n",
      "Iter: 329/3021, loss_train: 6.6731, loss_val: nan, pos_over_neg: 84.63821411132812 lr: 0.0001\n",
      "Iter: 330/3021, loss_train: 6.6639, loss_val: nan, pos_over_neg: 75.58375549316406 lr: 0.0001\n",
      "Iter: 331/3021, loss_train: 6.6625, loss_val: nan, pos_over_neg: 69.33367156982422 lr: 0.0001\n",
      "Iter: 332/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 69.64566802978516 lr: 0.0001\n",
      "Iter: 333/3021, loss_train: 6.6474, loss_val: nan, pos_over_neg: 71.13440704345703 lr: 0.0001\n",
      "Iter: 334/3021, loss_train: 6.6234, loss_val: nan, pos_over_neg: 87.0166244506836 lr: 0.0001\n",
      "Iter: 335/3021, loss_train: 6.6333, loss_val: nan, pos_over_neg: 101.45155334472656 lr: 0.0001\n",
      "Iter: 336/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 80.42835998535156 lr: 0.0001\n",
      "Iter: 337/3021, loss_train: 6.6694, loss_val: nan, pos_over_neg: 96.0687026977539 lr: 0.0001\n",
      "Iter: 338/3021, loss_train: 6.6633, loss_val: nan, pos_over_neg: 113.58574676513672 lr: 0.0001\n",
      "Iter: 339/3021, loss_train: 6.6473, loss_val: nan, pos_over_neg: 134.1546630859375 lr: 0.0001\n",
      "Iter: 340/3021, loss_train: 6.6706, loss_val: nan, pos_over_neg: 133.03097534179688 lr: 0.0001\n",
      "Iter: 341/3021, loss_train: 6.6416, loss_val: nan, pos_over_neg: 110.16153717041016 lr: 0.0001\n",
      "Iter: 342/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 105.60099792480469 lr: 0.0001\n",
      "Iter: 343/3021, loss_train: 6.6853, loss_val: nan, pos_over_neg: 59.07761001586914 lr: 0.0001\n",
      "Iter: 344/3021, loss_train: 6.6673, loss_val: nan, pos_over_neg: 66.02745056152344 lr: 0.0001\n",
      "Iter: 345/3021, loss_train: 6.6591, loss_val: nan, pos_over_neg: 60.90229034423828 lr: 0.0001\n",
      "Iter: 346/3021, loss_train: 6.6335, loss_val: nan, pos_over_neg: 91.63915252685547 lr: 0.0001\n",
      "Iter: 347/3021, loss_train: 6.6363, loss_val: nan, pos_over_neg: 86.37950897216797 lr: 0.0001\n",
      "Iter: 348/3021, loss_train: 6.6512, loss_val: nan, pos_over_neg: 84.38880157470703 lr: 0.0001\n",
      "Iter: 349/3021, loss_train: 6.6348, loss_val: nan, pos_over_neg: 103.31441497802734 lr: 0.0001\n",
      "Iter: 350/3021, loss_train: 6.6434, loss_val: nan, pos_over_neg: 104.93865966796875 lr: 0.0001\n",
      "Iter: 351/3021, loss_train: 6.6734, loss_val: nan, pos_over_neg: 95.90497589111328 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 352/3021, loss_train: 6.6439, loss_val: nan, pos_over_neg: 125.76329803466797 lr: 0.0001\n",
      "Iter: 353/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 113.29776763916016 lr: 0.0001\n",
      "Iter: 354/3021, loss_train: 6.6567, loss_val: nan, pos_over_neg: 89.5253677368164 lr: 0.0001\n",
      "Iter: 355/3021, loss_train: 6.6386, loss_val: nan, pos_over_neg: 118.8377456665039 lr: 0.0001\n",
      "Iter: 356/3021, loss_train: 6.6223, loss_val: nan, pos_over_neg: 176.91030883789062 lr: 0.0001\n",
      "Iter: 357/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 96.75997924804688 lr: 0.0001\n",
      "Iter: 358/3021, loss_train: 6.6538, loss_val: nan, pos_over_neg: 98.95654296875 lr: 0.0001\n",
      "Iter: 359/3021, loss_train: 6.6364, loss_val: nan, pos_over_neg: 105.08909606933594 lr: 0.0001\n",
      "Iter: 360/3021, loss_train: 6.6822, loss_val: nan, pos_over_neg: 66.63834381103516 lr: 0.0001\n",
      "Iter: 361/3021, loss_train: 6.6514, loss_val: nan, pos_over_neg: 81.10625457763672 lr: 0.0001\n",
      "Iter: 362/3021, loss_train: 6.5959, loss_val: nan, pos_over_neg: 97.53778839111328 lr: 0.0001\n",
      "Iter: 363/3021, loss_train: 6.662, loss_val: nan, pos_over_neg: 80.00199127197266 lr: 0.0001\n",
      "Iter: 364/3021, loss_train: 6.6433, loss_val: nan, pos_over_neg: 78.23381805419922 lr: 0.0001\n",
      "Iter: 365/3021, loss_train: 6.6588, loss_val: nan, pos_over_neg: 85.30908966064453 lr: 0.0001\n",
      "Iter: 366/3021, loss_train: 6.6554, loss_val: nan, pos_over_neg: 80.56475067138672 lr: 0.0001\n",
      "Iter: 367/3021, loss_train: 6.602, loss_val: nan, pos_over_neg: 102.73753356933594 lr: 0.0001\n",
      "Iter: 368/3021, loss_train: 6.6623, loss_val: nan, pos_over_neg: 107.22395324707031 lr: 0.0001\n",
      "Iter: 369/3021, loss_train: 6.6452, loss_val: nan, pos_over_neg: 116.6164321899414 lr: 0.0001\n",
      "Iter: 370/3021, loss_train: 6.6657, loss_val: nan, pos_over_neg: 176.58331298828125 lr: 0.0001\n",
      "Iter: 371/3021, loss_train: 6.6688, loss_val: nan, pos_over_neg: 138.6767120361328 lr: 0.0001\n",
      "Iter: 372/3021, loss_train: 6.645, loss_val: nan, pos_over_neg: 121.26986694335938 lr: 0.0001\n",
      "Iter: 373/3021, loss_train: 6.6288, loss_val: nan, pos_over_neg: 117.08798217773438 lr: 0.0001\n",
      "Iter: 374/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 97.62513732910156 lr: 0.0001\n",
      "Iter: 375/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 86.76309204101562 lr: 0.0001\n",
      "Iter: 376/3021, loss_train: 6.6801, loss_val: nan, pos_over_neg: 90.28624725341797 lr: 0.0001\n",
      "Iter: 377/3021, loss_train: 6.6614, loss_val: nan, pos_over_neg: 77.59906005859375 lr: 0.0001\n",
      "Iter: 378/3021, loss_train: 6.6626, loss_val: nan, pos_over_neg: 68.15480041503906 lr: 0.0001\n",
      "Iter: 379/3021, loss_train: 6.6523, loss_val: nan, pos_over_neg: 89.90132904052734 lr: 0.0001\n",
      "Iter: 380/3021, loss_train: 6.6508, loss_val: nan, pos_over_neg: 120.88763427734375 lr: 0.0001\n",
      "Iter: 381/3021, loss_train: 6.669, loss_val: nan, pos_over_neg: 96.80363464355469 lr: 0.0001\n",
      "Iter: 382/3021, loss_train: 6.6603, loss_val: nan, pos_over_neg: 106.65241241455078 lr: 0.0001\n",
      "Iter: 383/3021, loss_train: 6.6578, loss_val: nan, pos_over_neg: 105.54267883300781 lr: 0.0001\n",
      "Iter: 384/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 119.28977966308594 lr: 0.0001\n",
      "Iter: 385/3021, loss_train: 6.6363, loss_val: nan, pos_over_neg: 191.4713592529297 lr: 0.0001\n",
      "Iter: 386/3021, loss_train: 6.6279, loss_val: nan, pos_over_neg: 127.69873809814453 lr: 0.0001\n",
      "Iter: 387/3021, loss_train: 6.6239, loss_val: nan, pos_over_neg: 106.16243743896484 lr: 0.0001\n",
      "Iter: 388/3021, loss_train: 6.6402, loss_val: nan, pos_over_neg: 140.34542846679688 lr: 0.0001\n",
      "Iter: 389/3021, loss_train: 6.6375, loss_val: nan, pos_over_neg: 104.75823211669922 lr: 0.0001\n",
      "Iter: 390/3021, loss_train: 6.6226, loss_val: nan, pos_over_neg: 92.54187774658203 lr: 0.0001\n",
      "Iter: 391/3021, loss_train: 6.6514, loss_val: nan, pos_over_neg: 76.32308959960938 lr: 0.0001\n",
      "Iter: 392/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 61.9380989074707 lr: 0.0001\n",
      "Iter: 393/3021, loss_train: 6.6714, loss_val: nan, pos_over_neg: 63.10841751098633 lr: 0.0001\n",
      "Iter: 394/3021, loss_train: 6.6267, loss_val: nan, pos_over_neg: 59.48258972167969 lr: 0.0001\n",
      "Iter: 395/3021, loss_train: 6.6465, loss_val: nan, pos_over_neg: 64.98129272460938 lr: 0.0001\n",
      "Iter: 396/3021, loss_train: 6.6745, loss_val: nan, pos_over_neg: 69.62779998779297 lr: 0.0001\n",
      "Iter: 397/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 96.06591033935547 lr: 0.0001\n",
      "Iter: 398/3021, loss_train: 6.6227, loss_val: nan, pos_over_neg: 188.2084197998047 lr: 0.0001\n",
      "Iter: 399/3021, loss_train: 6.6284, loss_val: nan, pos_over_neg: 209.0440673828125 lr: 0.0001\n",
      "Iter: 400/3021, loss_train: 6.6414, loss_val: nan, pos_over_neg: 185.0422821044922 lr: 0.0001\n",
      "Iter: 401/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 121.77044677734375 lr: 0.0001\n",
      "Iter: 402/3021, loss_train: 6.6359, loss_val: nan, pos_over_neg: 146.06585693359375 lr: 0.0001\n",
      "Iter: 403/3021, loss_train: 6.6368, loss_val: nan, pos_over_neg: 107.43843841552734 lr: 0.0001\n",
      "Iter: 404/3021, loss_train: 6.6719, loss_val: nan, pos_over_neg: 136.11122131347656 lr: 0.0001\n",
      "Iter: 405/3021, loss_train: 6.6554, loss_val: nan, pos_over_neg: 95.84779357910156 lr: 0.0001\n",
      "Iter: 406/3021, loss_train: 6.656, loss_val: nan, pos_over_neg: 85.85224914550781 lr: 0.0001\n",
      "Iter: 407/3021, loss_train: 6.6559, loss_val: nan, pos_over_neg: 69.48261260986328 lr: 0.0001\n",
      "Iter: 408/3021, loss_train: 6.6463, loss_val: nan, pos_over_neg: 67.01167297363281 lr: 0.0001\n",
      "Iter: 409/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 64.1944351196289 lr: 0.0001\n",
      "Iter: 410/3021, loss_train: 6.6413, loss_val: nan, pos_over_neg: 55.452640533447266 lr: 0.0001\n",
      "Iter: 411/3021, loss_train: 6.6148, loss_val: nan, pos_over_neg: 61.286834716796875 lr: 0.0001\n",
      "Iter: 412/3021, loss_train: 6.6614, loss_val: nan, pos_over_neg: 61.9512939453125 lr: 0.0001\n",
      "Iter: 413/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 69.24641418457031 lr: 0.0001\n",
      "Iter: 414/3021, loss_train: 6.6049, loss_val: nan, pos_over_neg: 102.68211364746094 lr: 0.0001\n",
      "Iter: 415/3021, loss_train: 6.6421, loss_val: nan, pos_over_neg: 115.35651397705078 lr: 0.0001\n",
      "Iter: 416/3021, loss_train: 6.6106, loss_val: nan, pos_over_neg: 129.8797607421875 lr: 0.0001\n",
      "Iter: 417/3021, loss_train: 6.6216, loss_val: nan, pos_over_neg: 104.5445556640625 lr: 0.0001\n",
      "Iter: 418/3021, loss_train: 6.6427, loss_val: nan, pos_over_neg: 123.41105651855469 lr: 0.0001\n",
      "Iter: 419/3021, loss_train: 6.6729, loss_val: nan, pos_over_neg: 108.22498321533203 lr: 0.0001\n",
      "Iter: 420/3021, loss_train: 6.6236, loss_val: nan, pos_over_neg: 107.05804443359375 lr: 0.0001\n",
      "Iter: 421/3021, loss_train: 6.6162, loss_val: nan, pos_over_neg: 100.08367156982422 lr: 0.0001\n",
      "Iter: 422/3021, loss_train: 6.662, loss_val: nan, pos_over_neg: 121.63780975341797 lr: 0.0001\n",
      "Iter: 423/3021, loss_train: 6.6265, loss_val: nan, pos_over_neg: 85.68254089355469 lr: 0.0001\n",
      "Iter: 424/3021, loss_train: 6.6665, loss_val: nan, pos_over_neg: 54.07424545288086 lr: 0.0001\n",
      "Iter: 425/3021, loss_train: 6.6338, loss_val: nan, pos_over_neg: 73.62643432617188 lr: 0.0001\n",
      "Iter: 426/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 114.16816711425781 lr: 0.0001\n",
      "Iter: 427/3021, loss_train: 6.6268, loss_val: nan, pos_over_neg: 110.62196350097656 lr: 0.0001\n",
      "Iter: 428/3021, loss_train: 6.6236, loss_val: nan, pos_over_neg: 130.805908203125 lr: 0.0001\n",
      "Iter: 429/3021, loss_train: 6.6133, loss_val: nan, pos_over_neg: 167.09495544433594 lr: 0.0001\n",
      "Iter: 430/3021, loss_train: 6.6283, loss_val: nan, pos_over_neg: 145.8020782470703 lr: 0.0001\n",
      "Iter: 431/3021, loss_train: 6.6437, loss_val: nan, pos_over_neg: 124.7811279296875 lr: 0.0001\n",
      "Iter: 432/3021, loss_train: 6.6272, loss_val: nan, pos_over_neg: 103.73365020751953 lr: 0.0001\n",
      "Iter: 433/3021, loss_train: 6.623, loss_val: nan, pos_over_neg: 137.7026824951172 lr: 0.0001\n",
      "Iter: 434/3021, loss_train: 6.651, loss_val: nan, pos_over_neg: 108.7846450805664 lr: 0.0001\n",
      "Iter: 435/3021, loss_train: 6.6342, loss_val: nan, pos_over_neg: 123.85726928710938 lr: 0.0001\n",
      "Iter: 436/3021, loss_train: 6.6591, loss_val: nan, pos_over_neg: 71.30896759033203 lr: 0.0001\n",
      "Iter: 437/3021, loss_train: 6.6415, loss_val: nan, pos_over_neg: 75.47917938232422 lr: 0.0001\n",
      "Iter: 438/3021, loss_train: 6.6451, loss_val: nan, pos_over_neg: 63.713321685791016 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 439/3021, loss_train: 6.601, loss_val: nan, pos_over_neg: 75.08138275146484 lr: 0.0001\n",
      "Iter: 440/3021, loss_train: 6.6258, loss_val: nan, pos_over_neg: 74.86217498779297 lr: 0.0001\n",
      "Iter: 441/3021, loss_train: 6.63, loss_val: nan, pos_over_neg: 83.85758972167969 lr: 0.0001\n",
      "Iter: 442/3021, loss_train: 6.6587, loss_val: nan, pos_over_neg: 91.13420867919922 lr: 0.0001\n",
      "Iter: 443/3021, loss_train: 6.6252, loss_val: nan, pos_over_neg: 178.44361877441406 lr: 0.0001\n",
      "Iter: 444/3021, loss_train: 6.6242, loss_val: nan, pos_over_neg: 131.99794006347656 lr: 0.0001\n",
      "Iter: 445/3021, loss_train: 6.6468, loss_val: nan, pos_over_neg: 139.25357055664062 lr: 0.0001\n",
      "Iter: 446/3021, loss_train: 6.6464, loss_val: nan, pos_over_neg: 164.56912231445312 lr: 0.0001\n",
      "Iter: 447/3021, loss_train: 6.647, loss_val: nan, pos_over_neg: 156.39125061035156 lr: 0.0001\n",
      "Iter: 448/3021, loss_train: 6.6403, loss_val: nan, pos_over_neg: 164.92901611328125 lr: 0.0001\n",
      "Iter: 449/3021, loss_train: 6.6349, loss_val: nan, pos_over_neg: 120.19703674316406 lr: 0.0001\n",
      "Iter: 450/3021, loss_train: 6.643, loss_val: nan, pos_over_neg: 137.4410858154297 lr: 0.0001\n",
      "Iter: 451/3021, loss_train: 6.6342, loss_val: nan, pos_over_neg: 102.31673431396484 lr: 0.0001\n",
      "Iter: 452/3021, loss_train: 6.6321, loss_val: nan, pos_over_neg: 79.9969711303711 lr: 0.0001\n",
      "Iter: 453/3021, loss_train: 6.6467, loss_val: nan, pos_over_neg: 100.12159729003906 lr: 0.0001\n",
      "Iter: 454/3021, loss_train: 6.6351, loss_val: nan, pos_over_neg: 83.82473754882812 lr: 0.0001\n",
      "Iter: 455/3021, loss_train: 6.6295, loss_val: nan, pos_over_neg: 88.2153549194336 lr: 0.0001\n",
      "Iter: 456/3021, loss_train: 6.6286, loss_val: nan, pos_over_neg: 73.30821228027344 lr: 0.0001\n",
      "Iter: 457/3021, loss_train: 6.6161, loss_val: nan, pos_over_neg: 68.89139556884766 lr: 0.0001\n",
      "Iter: 458/3021, loss_train: 6.6285, loss_val: nan, pos_over_neg: 76.8030014038086 lr: 0.0001\n",
      "Iter: 459/3021, loss_train: 6.6313, loss_val: nan, pos_over_neg: 74.96929931640625 lr: 0.0001\n",
      "Iter: 460/3021, loss_train: 6.6158, loss_val: nan, pos_over_neg: 78.78678131103516 lr: 0.0001\n",
      "Iter: 461/3021, loss_train: 6.6312, loss_val: nan, pos_over_neg: 66.93316650390625 lr: 0.0001\n",
      "Iter: 462/3021, loss_train: 6.613, loss_val: nan, pos_over_neg: 91.77286529541016 lr: 0.0001\n",
      "Iter: 463/3021, loss_train: 6.6435, loss_val: nan, pos_over_neg: 122.16020965576172 lr: 0.0001\n",
      "Iter: 464/3021, loss_train: 6.6138, loss_val: nan, pos_over_neg: 121.81124114990234 lr: 0.0001\n",
      "Iter: 465/3021, loss_train: 6.6316, loss_val: nan, pos_over_neg: 123.59111785888672 lr: 0.0001\n",
      "Iter: 466/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 184.98440551757812 lr: 0.0001\n",
      "Iter: 467/3021, loss_train: 6.6125, loss_val: nan, pos_over_neg: 211.94020080566406 lr: 0.0001\n",
      "Iter: 468/3021, loss_train: 6.6524, loss_val: nan, pos_over_neg: 173.12298583984375 lr: 0.0001\n",
      "Iter: 469/3021, loss_train: 6.615, loss_val: nan, pos_over_neg: 157.1043243408203 lr: 0.0001\n",
      "Iter: 470/3021, loss_train: 6.6089, loss_val: nan, pos_over_neg: 130.60169982910156 lr: 0.0001\n",
      "Iter: 471/3021, loss_train: 6.6346, loss_val: nan, pos_over_neg: 94.4458999633789 lr: 0.0001\n",
      "Iter: 472/3021, loss_train: 6.5993, loss_val: nan, pos_over_neg: 91.9759750366211 lr: 0.0001\n",
      "Iter: 473/3021, loss_train: 6.6689, loss_val: nan, pos_over_neg: 73.30775451660156 lr: 0.0001\n",
      "Iter: 474/3021, loss_train: 6.6121, loss_val: nan, pos_over_neg: 73.33035278320312 lr: 0.0001\n",
      "Iter: 475/3021, loss_train: 6.634, loss_val: nan, pos_over_neg: 57.9543342590332 lr: 0.0001\n",
      "Iter: 476/3021, loss_train: 6.6221, loss_val: nan, pos_over_neg: 61.18105697631836 lr: 0.0001\n",
      "Iter: 477/3021, loss_train: 6.6434, loss_val: nan, pos_over_neg: 66.54243469238281 lr: 0.0001\n",
      "Iter: 478/3021, loss_train: 6.6391, loss_val: nan, pos_over_neg: 60.52981185913086 lr: 0.0001\n",
      "Iter: 479/3021, loss_train: 6.6188, loss_val: nan, pos_over_neg: 73.69983673095703 lr: 0.0001\n",
      "Iter: 480/3021, loss_train: 6.621, loss_val: nan, pos_over_neg: 93.47068786621094 lr: 0.0001\n",
      "Iter: 481/3021, loss_train: 6.6092, loss_val: nan, pos_over_neg: 127.85000610351562 lr: 0.0001\n",
      "Iter: 482/3021, loss_train: 6.6229, loss_val: nan, pos_over_neg: 143.13327026367188 lr: 0.0001\n",
      "Iter: 483/3021, loss_train: 6.5977, loss_val: nan, pos_over_neg: 147.68682861328125 lr: 0.0001\n",
      "Iter: 484/3021, loss_train: 6.6395, loss_val: nan, pos_over_neg: 124.00170135498047 lr: 0.0001\n",
      "Iter: 485/3021, loss_train: 6.6146, loss_val: nan, pos_over_neg: 105.80421447753906 lr: 0.0001\n",
      "Iter: 486/3021, loss_train: 6.6392, loss_val: nan, pos_over_neg: 117.52613830566406 lr: 0.0001\n",
      "Iter: 487/3021, loss_train: 6.622, loss_val: nan, pos_over_neg: 135.64407348632812 lr: 0.0001\n",
      "Iter: 488/3021, loss_train: 6.6209, loss_val: nan, pos_over_neg: 142.2001953125 lr: 0.0001\n",
      "Iter: 489/3021, loss_train: 6.6478, loss_val: nan, pos_over_neg: 68.16737365722656 lr: 0.0001\n",
      "Iter: 490/3021, loss_train: 6.6405, loss_val: nan, pos_over_neg: 93.32061767578125 lr: 0.0001\n",
      "Iter: 491/3021, loss_train: 6.6193, loss_val: nan, pos_over_neg: 74.38084411621094 lr: 0.0001\n",
      "Iter: 492/3021, loss_train: 6.6015, loss_val: nan, pos_over_neg: 108.37134552001953 lr: 0.0001\n",
      "Iter: 493/3021, loss_train: 6.6359, loss_val: nan, pos_over_neg: 90.31268310546875 lr: 0.0001\n",
      "Iter: 494/3021, loss_train: 6.6439, loss_val: nan, pos_over_neg: 69.11817169189453 lr: 0.0001\n",
      "Iter: 495/3021, loss_train: 6.6155, loss_val: nan, pos_over_neg: 123.36785125732422 lr: 0.0001\n",
      "Iter: 496/3021, loss_train: 6.5973, loss_val: nan, pos_over_neg: 117.52815246582031 lr: 0.0001\n",
      "Iter: 497/3021, loss_train: 6.6206, loss_val: nan, pos_over_neg: 109.12016296386719 lr: 0.0001\n",
      "Iter: 498/3021, loss_train: 6.6088, loss_val: nan, pos_over_neg: 201.2156982421875 lr: 0.0001\n",
      "Iter: 499/3021, loss_train: 6.6373, loss_val: nan, pos_over_neg: 108.41218566894531 lr: 0.0001\n",
      "Iter: 500/3021, loss_train: 6.5954, loss_val: nan, pos_over_neg: 166.2609100341797 lr: 0.0001\n",
      "Iter: 501/3021, loss_train: 6.629, loss_val: nan, pos_over_neg: 90.65927124023438 lr: 0.0001\n",
      "Iter: 502/3021, loss_train: 6.6215, loss_val: nan, pos_over_neg: 135.4724884033203 lr: 0.0001\n",
      "Iter: 503/3021, loss_train: 6.6122, loss_val: nan, pos_over_neg: 109.04310607910156 lr: 0.0001\n",
      "Iter: 504/3021, loss_train: 6.6222, loss_val: nan, pos_over_neg: 98.29468536376953 lr: 0.0001\n",
      "Iter: 505/3021, loss_train: 6.6402, loss_val: nan, pos_over_neg: 121.7578353881836 lr: 0.0001\n",
      "Iter: 506/3021, loss_train: 6.6095, loss_val: nan, pos_over_neg: 94.19684600830078 lr: 0.0001\n",
      "Iter: 507/3021, loss_train: 6.6069, loss_val: nan, pos_over_neg: 126.89798736572266 lr: 0.0001\n",
      "Iter: 508/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 91.40262603759766 lr: 0.0001\n",
      "Iter: 509/3021, loss_train: 6.6004, loss_val: nan, pos_over_neg: 107.54779052734375 lr: 0.0001\n",
      "Iter: 510/3021, loss_train: 6.5858, loss_val: nan, pos_over_neg: 148.98587036132812 lr: 0.0001\n",
      "Iter: 511/3021, loss_train: 6.6128, loss_val: nan, pos_over_neg: 98.84847259521484 lr: 0.0001\n",
      "Iter: 512/3021, loss_train: 6.6436, loss_val: nan, pos_over_neg: 75.47498321533203 lr: 0.0001\n",
      "Iter: 513/3021, loss_train: 6.6182, loss_val: nan, pos_over_neg: 120.55462646484375 lr: 0.0001\n",
      "Iter: 514/3021, loss_train: 6.6168, loss_val: nan, pos_over_neg: 94.66842651367188 lr: 0.0001\n",
      "Iter: 515/3021, loss_train: 6.6115, loss_val: nan, pos_over_neg: 105.19046783447266 lr: 0.0001\n",
      "Iter: 516/3021, loss_train: 6.6355, loss_val: nan, pos_over_neg: 69.94659423828125 lr: 0.0001\n",
      "Iter: 517/3021, loss_train: 6.6272, loss_val: nan, pos_over_neg: 72.86248016357422 lr: 0.0001\n",
      "Iter: 518/3021, loss_train: 6.6172, loss_val: nan, pos_over_neg: 87.09858703613281 lr: 0.0001\n",
      "Iter: 519/3021, loss_train: 6.6315, loss_val: nan, pos_over_neg: 128.0094757080078 lr: 0.0001\n",
      "Iter: 520/3021, loss_train: 6.627, loss_val: nan, pos_over_neg: 81.42594146728516 lr: 0.0001\n",
      "Iter: 521/3021, loss_train: 6.6311, loss_val: nan, pos_over_neg: 147.58831787109375 lr: 0.0001\n",
      "Iter: 522/3021, loss_train: 6.6057, loss_val: nan, pos_over_neg: 115.65524291992188 lr: 0.0001\n",
      "Iter: 523/3021, loss_train: 6.6345, loss_val: nan, pos_over_neg: 105.91828155517578 lr: 0.0001\n",
      "Iter: 524/3021, loss_train: 6.5955, loss_val: nan, pos_over_neg: 111.79486083984375 lr: 0.0001\n",
      "Iter: 525/3021, loss_train: 6.6264, loss_val: nan, pos_over_neg: 90.07721710205078 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 526/3021, loss_train: 6.6163, loss_val: nan, pos_over_neg: 109.8876953125 lr: 0.0001\n",
      "Iter: 527/3021, loss_train: 6.6183, loss_val: nan, pos_over_neg: 84.85918426513672 lr: 0.0001\n",
      "Iter: 528/3021, loss_train: 6.6447, loss_val: nan, pos_over_neg: 80.35286712646484 lr: 0.0001\n",
      "Iter: 529/3021, loss_train: 6.6627, loss_val: nan, pos_over_neg: 108.20806884765625 lr: 0.0001\n",
      "Iter: 530/3021, loss_train: 6.6197, loss_val: nan, pos_over_neg: 131.59176635742188 lr: 0.0001\n",
      "Iter: 531/3021, loss_train: 6.6214, loss_val: nan, pos_over_neg: 115.59613037109375 lr: 0.0001\n",
      "Iter: 532/3021, loss_train: 6.6401, loss_val: nan, pos_over_neg: 161.75308227539062 lr: 0.0001\n",
      "Iter: 533/3021, loss_train: 6.6432, loss_val: nan, pos_over_neg: 108.1818618774414 lr: 0.0001\n",
      "Iter: 534/3021, loss_train: 6.6113, loss_val: nan, pos_over_neg: 123.26493835449219 lr: 0.0001\n",
      "Iter: 535/3021, loss_train: 6.6095, loss_val: nan, pos_over_neg: 141.04315185546875 lr: 0.0001\n",
      "Iter: 536/3021, loss_train: 6.6357, loss_val: nan, pos_over_neg: 91.94810485839844 lr: 0.0001\n",
      "Iter: 537/3021, loss_train: 6.6368, loss_val: nan, pos_over_neg: 83.49420928955078 lr: 0.0001\n",
      "Iter: 538/3021, loss_train: 6.6143, loss_val: nan, pos_over_neg: 85.59711456298828 lr: 0.0001\n",
      "Iter: 539/3021, loss_train: 6.6331, loss_val: nan, pos_over_neg: 68.79597473144531 lr: 0.0001\n",
      "Iter: 540/3021, loss_train: 6.6208, loss_val: nan, pos_over_neg: 65.30167388916016 lr: 0.0001\n",
      "Iter: 541/3021, loss_train: 6.6101, loss_val: nan, pos_over_neg: 70.23233032226562 lr: 0.0001\n",
      "Iter: 542/3021, loss_train: 6.6357, loss_val: nan, pos_over_neg: 62.553253173828125 lr: 0.0001\n",
      "Iter: 543/3021, loss_train: 6.6078, loss_val: nan, pos_over_neg: 76.76283264160156 lr: 0.0001\n",
      "Iter: 544/3021, loss_train: 6.6269, loss_val: nan, pos_over_neg: 99.25552368164062 lr: 0.0001\n",
      "Iter: 545/3021, loss_train: 6.6315, loss_val: nan, pos_over_neg: 108.50141143798828 lr: 0.0001\n",
      "Iter: 546/3021, loss_train: 6.6104, loss_val: nan, pos_over_neg: 162.83193969726562 lr: 0.0001\n",
      "Iter: 547/3021, loss_train: 6.6254, loss_val: nan, pos_over_neg: 306.4184265136719 lr: 0.0001\n",
      "Iter: 548/3021, loss_train: 6.5756, loss_val: nan, pos_over_neg: 404.193115234375 lr: 0.0001\n",
      "Iter: 549/3021, loss_train: 6.5931, loss_val: nan, pos_over_neg: 174.80967712402344 lr: 0.0001\n",
      "Iter: 550/3021, loss_train: 6.6384, loss_val: nan, pos_over_neg: 141.4352264404297 lr: 0.0001\n",
      "Iter: 551/3021, loss_train: 6.6029, loss_val: nan, pos_over_neg: 156.9000244140625 lr: 0.0001\n",
      "Iter: 552/3021, loss_train: 6.6414, loss_val: nan, pos_over_neg: 89.87215423583984 lr: 0.0001\n",
      "Iter: 553/3021, loss_train: 6.6114, loss_val: nan, pos_over_neg: 77.33897399902344 lr: 0.0001\n",
      "Iter: 554/3021, loss_train: 6.6179, loss_val: nan, pos_over_neg: 63.84626770019531 lr: 0.0001\n",
      "Iter: 555/3021, loss_train: 6.6251, loss_val: nan, pos_over_neg: 75.59040069580078 lr: 0.0001\n",
      "Iter: 556/3021, loss_train: 6.621, loss_val: nan, pos_over_neg: 60.59468460083008 lr: 0.0001\n",
      "Iter: 557/3021, loss_train: 6.601, loss_val: nan, pos_over_neg: 63.21246337890625 lr: 0.0001\n",
      "Iter: 558/3021, loss_train: 6.6146, loss_val: nan, pos_over_neg: 77.74459838867188 lr: 0.0001\n",
      "Iter: 559/3021, loss_train: 6.6116, loss_val: nan, pos_over_neg: 122.55570220947266 lr: 0.0001\n",
      "Iter: 560/3021, loss_train: 6.6105, loss_val: nan, pos_over_neg: 131.24295043945312 lr: 0.0001\n",
      "Iter: 561/3021, loss_train: 6.6028, loss_val: nan, pos_over_neg: 175.1300811767578 lr: 0.0001\n",
      "Iter: 562/3021, loss_train: 6.5849, loss_val: nan, pos_over_neg: 146.05496215820312 lr: 0.0001\n",
      "Iter: 563/3021, loss_train: 6.6264, loss_val: nan, pos_over_neg: 180.6123809814453 lr: 0.0001\n",
      "Iter: 564/3021, loss_train: 6.6205, loss_val: nan, pos_over_neg: 118.85259246826172 lr: 0.0001\n",
      "Iter: 565/3021, loss_train: 6.6056, loss_val: nan, pos_over_neg: 147.16189575195312 lr: 0.0001\n",
      "Iter: 566/3021, loss_train: 6.5864, loss_val: nan, pos_over_neg: 85.67736053466797 lr: 0.0001\n",
      "Iter: 567/3021, loss_train: 6.6168, loss_val: nan, pos_over_neg: 83.41423034667969 lr: 0.0001\n",
      "Iter: 568/3021, loss_train: 6.6371, loss_val: nan, pos_over_neg: 61.29851150512695 lr: 0.0001\n",
      "Iter: 569/3021, loss_train: 6.6254, loss_val: nan, pos_over_neg: 77.30464935302734 lr: 0.0001\n",
      "Iter: 570/3021, loss_train: 6.609, loss_val: nan, pos_over_neg: 68.9383544921875 lr: 0.0001\n",
      "Iter: 571/3021, loss_train: 6.6133, loss_val: nan, pos_over_neg: 69.23075866699219 lr: 0.0001\n",
      "Iter: 572/3021, loss_train: 6.59, loss_val: nan, pos_over_neg: 119.70875549316406 lr: 0.0001\n",
      "Iter: 573/3021, loss_train: 6.6079, loss_val: nan, pos_over_neg: 121.50920104980469 lr: 0.0001\n",
      "Iter: 574/3021, loss_train: 6.5961, loss_val: nan, pos_over_neg: 125.33728790283203 lr: 0.0001\n",
      "Iter: 575/3021, loss_train: 6.6047, loss_val: nan, pos_over_neg: 190.490234375 lr: 0.0001\n",
      "Iter: 576/3021, loss_train: 6.627, loss_val: nan, pos_over_neg: 89.34983825683594 lr: 0.0001\n",
      "Iter: 577/3021, loss_train: 6.5786, loss_val: nan, pos_over_neg: 147.47406005859375 lr: 0.0001\n",
      "Iter: 578/3021, loss_train: 6.5972, loss_val: nan, pos_over_neg: 139.32620239257812 lr: 0.0001\n",
      "Iter: 579/3021, loss_train: 6.6033, loss_val: nan, pos_over_neg: 152.81028747558594 lr: 0.0001\n",
      "Iter: 580/3021, loss_train: 6.5976, loss_val: nan, pos_over_neg: 106.20579528808594 lr: 0.0001\n",
      "Iter: 581/3021, loss_train: 6.5739, loss_val: nan, pos_over_neg: 113.97660064697266 lr: 0.0001\n",
      "Iter: 582/3021, loss_train: 6.6406, loss_val: nan, pos_over_neg: 86.01107025146484 lr: 0.0001\n",
      "Iter: 583/3021, loss_train: 6.6205, loss_val: nan, pos_over_neg: 88.5457763671875 lr: 0.0001\n",
      "Iter: 584/3021, loss_train: 6.6018, loss_val: nan, pos_over_neg: 99.84664154052734 lr: 0.0001\n",
      "Iter: 585/3021, loss_train: 6.6389, loss_val: nan, pos_over_neg: 89.63492584228516 lr: 0.0001\n",
      "Iter: 586/3021, loss_train: 6.5848, loss_val: nan, pos_over_neg: 102.46236419677734 lr: 0.0001\n",
      "Iter: 587/3021, loss_train: 6.6163, loss_val: nan, pos_over_neg: 122.13184356689453 lr: 0.0001\n",
      "Iter: 588/3021, loss_train: 6.616, loss_val: nan, pos_over_neg: 112.73178100585938 lr: 0.0001\n",
      "Iter: 589/3021, loss_train: 6.5886, loss_val: nan, pos_over_neg: 165.5727996826172 lr: 0.0001\n",
      "Iter: 590/3021, loss_train: 6.6151, loss_val: nan, pos_over_neg: 238.1915283203125 lr: 0.0001\n",
      "Iter: 591/3021, loss_train: 6.5896, loss_val: nan, pos_over_neg: 122.59256744384766 lr: 0.0001\n",
      "Iter: 592/3021, loss_train: 6.6121, loss_val: nan, pos_over_neg: 148.16482543945312 lr: 0.0001\n",
      "Iter: 593/3021, loss_train: 6.584, loss_val: nan, pos_over_neg: 141.76255798339844 lr: 0.0001\n",
      "Iter: 594/3021, loss_train: 6.6352, loss_val: nan, pos_over_neg: 85.5504150390625 lr: 0.0001\n",
      "Iter: 595/3021, loss_train: 6.5984, loss_val: nan, pos_over_neg: 87.7799301147461 lr: 0.0001\n",
      "Iter: 596/3021, loss_train: 6.6266, loss_val: nan, pos_over_neg: 86.03079986572266 lr: 0.0001\n",
      "Iter: 597/3021, loss_train: 6.5959, loss_val: nan, pos_over_neg: 100.08769989013672 lr: 0.0001\n",
      "Iter: 598/3021, loss_train: 6.5976, loss_val: nan, pos_over_neg: 105.68513488769531 lr: 0.0001\n",
      "Iter: 599/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 121.44738006591797 lr: 0.0001\n",
      "Iter: 600/3021, loss_train: 6.609, loss_val: nan, pos_over_neg: 92.46785736083984 lr: 0.0001\n",
      "Iter: 601/3021, loss_train: 6.5948, loss_val: nan, pos_over_neg: 108.15098571777344 lr: 0.0001\n",
      "Iter: 602/3021, loss_train: 6.6063, loss_val: nan, pos_over_neg: 113.37541961669922 lr: 0.0001\n",
      "Iter: 603/3021, loss_train: 6.6145, loss_val: nan, pos_over_neg: 97.8232192993164 lr: 0.0001\n",
      "Iter: 604/3021, loss_train: 6.6039, loss_val: nan, pos_over_neg: 95.51597595214844 lr: 0.0001\n",
      "Iter: 605/3021, loss_train: 6.6098, loss_val: nan, pos_over_neg: 118.64756774902344 lr: 0.0001\n",
      "Iter: 606/3021, loss_train: 6.5867, loss_val: nan, pos_over_neg: 128.98312377929688 lr: 0.0001\n",
      "Iter: 607/3021, loss_train: 6.5887, loss_val: nan, pos_over_neg: 104.25284576416016 lr: 0.0001\n",
      "Iter: 608/3021, loss_train: 6.6271, loss_val: nan, pos_over_neg: 105.27714538574219 lr: 0.0001\n",
      "Iter: 609/3021, loss_train: 6.6005, loss_val: nan, pos_over_neg: 149.07559204101562 lr: 0.0001\n",
      "Iter: 610/3021, loss_train: 6.5994, loss_val: nan, pos_over_neg: 153.98248291015625 lr: 0.0001\n",
      "Iter: 611/3021, loss_train: 6.6309, loss_val: nan, pos_over_neg: 97.0685043334961 lr: 0.0001\n",
      "Iter: 612/3021, loss_train: 6.6145, loss_val: nan, pos_over_neg: 149.8909149169922 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 613/3021, loss_train: 6.5952, loss_val: nan, pos_over_neg: 128.69012451171875 lr: 0.0001\n",
      "Iter: 614/3021, loss_train: 6.6181, loss_val: nan, pos_over_neg: 100.28562927246094 lr: 0.0001\n",
      "Iter: 615/3021, loss_train: 6.6068, loss_val: nan, pos_over_neg: 101.59013366699219 lr: 0.0001\n",
      "Iter: 616/3021, loss_train: 6.6296, loss_val: nan, pos_over_neg: 107.67310333251953 lr: 0.0001\n",
      "Iter: 617/3021, loss_train: 6.5962, loss_val: nan, pos_over_neg: 149.38587951660156 lr: 0.0001\n",
      "Iter: 618/3021, loss_train: 6.608, loss_val: nan, pos_over_neg: 100.8143081665039 lr: 0.0001\n",
      "Iter: 619/3021, loss_train: 6.5924, loss_val: nan, pos_over_neg: 115.50370788574219 lr: 0.0001\n",
      "Iter: 620/3021, loss_train: 6.6081, loss_val: nan, pos_over_neg: 100.75720977783203 lr: 0.0001\n",
      "Iter: 621/3021, loss_train: 6.6117, loss_val: nan, pos_over_neg: 106.71442413330078 lr: 0.0001\n",
      "Iter: 622/3021, loss_train: 6.5844, loss_val: nan, pos_over_neg: 114.02190399169922 lr: 0.0001\n",
      "Iter: 623/3021, loss_train: 6.5807, loss_val: nan, pos_over_neg: 90.38426208496094 lr: 0.0001\n",
      "Iter: 624/3021, loss_train: 6.5886, loss_val: nan, pos_over_neg: 147.60665893554688 lr: 0.0001\n",
      "Iter: 625/3021, loss_train: 6.5818, loss_val: nan, pos_over_neg: 118.66007995605469 lr: 0.0001\n",
      "Iter: 626/3021, loss_train: 6.594, loss_val: nan, pos_over_neg: 94.60941314697266 lr: 0.0001\n",
      "Iter: 627/3021, loss_train: 6.6193, loss_val: nan, pos_over_neg: 90.62916564941406 lr: 0.0001\n",
      "Iter: 628/3021, loss_train: 6.5857, loss_val: nan, pos_over_neg: 148.68092346191406 lr: 0.0001\n",
      "Iter: 629/3021, loss_train: 6.6198, loss_val: nan, pos_over_neg: 117.320068359375 lr: 0.0001\n",
      "Iter: 630/3021, loss_train: 6.6109, loss_val: nan, pos_over_neg: 184.8887939453125 lr: 0.0001\n",
      "Iter: 631/3021, loss_train: 6.6091, loss_val: nan, pos_over_neg: 126.28932189941406 lr: 0.0001\n",
      "Iter: 632/3021, loss_train: 6.611, loss_val: nan, pos_over_neg: 174.2012481689453 lr: 0.0001\n",
      "Iter: 633/3021, loss_train: 6.5862, loss_val: nan, pos_over_neg: 99.67992401123047 lr: 0.0001\n",
      "Iter: 634/3021, loss_train: 6.581, loss_val: nan, pos_over_neg: 96.00635528564453 lr: 0.0001\n",
      "Iter: 635/3021, loss_train: 6.5949, loss_val: nan, pos_over_neg: 116.11966705322266 lr: 0.0001\n",
      "Iter: 636/3021, loss_train: 6.6266, loss_val: nan, pos_over_neg: 82.5527114868164 lr: 0.0001\n",
      "Iter: 637/3021, loss_train: 6.5872, loss_val: nan, pos_over_neg: 102.69898223876953 lr: 0.0001\n",
      "Iter: 638/3021, loss_train: 6.6071, loss_val: nan, pos_over_neg: 75.60655975341797 lr: 0.0001\n",
      "Iter: 639/3021, loss_train: 6.6122, loss_val: nan, pos_over_neg: 92.618408203125 lr: 0.0001\n",
      "Iter: 640/3021, loss_train: 6.5891, loss_val: nan, pos_over_neg: 77.74364471435547 lr: 0.0001\n",
      "Iter: 641/3021, loss_train: 6.614, loss_val: nan, pos_over_neg: 90.3978271484375 lr: 0.0001\n",
      "Iter: 642/3021, loss_train: 6.616, loss_val: nan, pos_over_neg: 93.53338623046875 lr: 0.0001\n",
      "Iter: 643/3021, loss_train: 6.5945, loss_val: nan, pos_over_neg: 111.01829528808594 lr: 0.0001\n",
      "Iter: 644/3021, loss_train: 6.582, loss_val: nan, pos_over_neg: 166.74801635742188 lr: 0.0001\n",
      "Iter: 645/3021, loss_train: 6.5981, loss_val: nan, pos_over_neg: 167.72279357910156 lr: 0.0001\n",
      "Iter: 646/3021, loss_train: 6.5991, loss_val: nan, pos_over_neg: 119.41260528564453 lr: 0.0001\n",
      "Iter: 647/3021, loss_train: 6.6103, loss_val: nan, pos_over_neg: 110.62936401367188 lr: 0.0001\n",
      "Iter: 648/3021, loss_train: 6.5876, loss_val: nan, pos_over_neg: 135.94125366210938 lr: 0.0001\n",
      "Iter: 649/3021, loss_train: 6.6088, loss_val: nan, pos_over_neg: 89.50482940673828 lr: 0.0001\n",
      "Iter: 650/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 90.18988037109375 lr: 0.0001\n",
      "Iter: 651/3021, loss_train: 6.602, loss_val: nan, pos_over_neg: 76.83455657958984 lr: 0.0001\n",
      "Iter: 652/3021, loss_train: 6.5879, loss_val: nan, pos_over_neg: 76.3498764038086 lr: 0.0001\n",
      "Iter: 653/3021, loss_train: 6.6076, loss_val: nan, pos_over_neg: 90.66461944580078 lr: 0.0001\n",
      "Iter: 654/3021, loss_train: 6.588, loss_val: nan, pos_over_neg: 118.17411804199219 lr: 0.0001\n",
      "Iter: 655/3021, loss_train: 6.5836, loss_val: nan, pos_over_neg: 125.78502655029297 lr: 0.0001\n",
      "Iter: 656/3021, loss_train: 6.5967, loss_val: nan, pos_over_neg: 98.24745178222656 lr: 0.0001\n",
      "Iter: 657/3021, loss_train: 6.6134, loss_val: nan, pos_over_neg: 111.24110412597656 lr: 0.0001\n",
      "Iter: 658/3021, loss_train: 6.5838, loss_val: nan, pos_over_neg: 135.8141632080078 lr: 0.0001\n",
      "Iter: 659/3021, loss_train: 6.6422, loss_val: nan, pos_over_neg: 82.63817596435547 lr: 0.0001\n",
      "Iter: 660/3021, loss_train: 6.603, loss_val: nan, pos_over_neg: 118.46806335449219 lr: 0.0001\n",
      "Iter: 661/3021, loss_train: 6.5744, loss_val: nan, pos_over_neg: 99.66175842285156 lr: 0.0001\n",
      "Iter: 662/3021, loss_train: 6.5898, loss_val: nan, pos_over_neg: 82.22264862060547 lr: 0.0001\n",
      "Iter: 663/3021, loss_train: 6.5966, loss_val: nan, pos_over_neg: 112.99810791015625 lr: 0.0001\n",
      "Iter: 664/3021, loss_train: 6.636, loss_val: nan, pos_over_neg: 94.01675415039062 lr: 0.0001\n",
      "Iter: 665/3021, loss_train: 6.5998, loss_val: nan, pos_over_neg: 103.90379333496094 lr: 0.0001\n",
      "Iter: 666/3021, loss_train: 6.5821, loss_val: nan, pos_over_neg: 216.28103637695312 lr: 0.0001\n",
      "Iter: 667/3021, loss_train: 6.6171, loss_val: nan, pos_over_neg: 176.133056640625 lr: 0.0001\n",
      "Iter: 668/3021, loss_train: 6.587, loss_val: nan, pos_over_neg: 188.52003479003906 lr: 0.0001\n",
      "Iter: 669/3021, loss_train: 6.5814, loss_val: nan, pos_over_neg: 163.70730590820312 lr: 0.0001\n",
      "Iter: 670/3021, loss_train: 6.6147, loss_val: nan, pos_over_neg: 86.39402770996094 lr: 0.0001\n",
      "Iter: 671/3021, loss_train: 6.5808, loss_val: nan, pos_over_neg: 137.89599609375 lr: 0.0001\n",
      "Iter: 672/3021, loss_train: 6.5943, loss_val: nan, pos_over_neg: 83.94868469238281 lr: 0.0001\n",
      "Iter: 673/3021, loss_train: 6.6182, loss_val: nan, pos_over_neg: 84.34393310546875 lr: 0.0001\n",
      "Iter: 674/3021, loss_train: 6.5829, loss_val: nan, pos_over_neg: 78.46054077148438 lr: 0.0001\n",
      "Iter: 675/3021, loss_train: 6.5767, loss_val: nan, pos_over_neg: 109.20298767089844 lr: 0.0001\n",
      "Iter: 676/3021, loss_train: 6.6033, loss_val: nan, pos_over_neg: 94.33056640625 lr: 0.0001\n",
      "Iter: 677/3021, loss_train: 6.5889, loss_val: nan, pos_over_neg: 74.11611938476562 lr: 0.0001\n",
      "Iter: 678/3021, loss_train: 6.6018, loss_val: nan, pos_over_neg: 108.98560333251953 lr: 0.0001\n",
      "Iter: 679/3021, loss_train: 6.6186, loss_val: nan, pos_over_neg: 105.86277770996094 lr: 0.0001\n",
      "Iter: 680/3021, loss_train: 6.567, loss_val: nan, pos_over_neg: 147.7794952392578 lr: 0.0001\n",
      "Iter: 681/3021, loss_train: 6.5648, loss_val: nan, pos_over_neg: 163.72720336914062 lr: 0.0001\n",
      "Iter: 682/3021, loss_train: 6.6141, loss_val: nan, pos_over_neg: 115.72303771972656 lr: 0.0001\n",
      "Iter: 683/3021, loss_train: 6.6015, loss_val: nan, pos_over_neg: 97.99879455566406 lr: 0.0001\n",
      "Iter: 684/3021, loss_train: 6.5822, loss_val: nan, pos_over_neg: 89.02739715576172 lr: 0.0001\n",
      "Iter: 685/3021, loss_train: 6.5842, loss_val: nan, pos_over_neg: 90.65805053710938 lr: 0.0001\n",
      "Iter: 686/3021, loss_train: 6.589, loss_val: nan, pos_over_neg: 71.03034210205078 lr: 0.0001\n",
      "Iter: 687/3021, loss_train: 6.5765, loss_val: nan, pos_over_neg: 100.04374694824219 lr: 0.0001\n",
      "Iter: 688/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 94.22358703613281 lr: 0.0001\n",
      "Iter: 689/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 92.12555694580078 lr: 0.0001\n",
      "Iter: 690/3021, loss_train: 6.5956, loss_val: nan, pos_over_neg: 93.27652740478516 lr: 0.0001\n",
      "Iter: 691/3021, loss_train: 6.5947, loss_val: nan, pos_over_neg: 143.22329711914062 lr: 0.0001\n",
      "Iter: 692/3021, loss_train: 6.6153, loss_val: nan, pos_over_neg: 141.1318817138672 lr: 0.0001\n",
      "Iter: 693/3021, loss_train: 6.5453, loss_val: nan, pos_over_neg: 127.14126586914062 lr: 0.0001\n",
      "Iter: 694/3021, loss_train: 6.5728, loss_val: nan, pos_over_neg: 123.66120910644531 lr: 0.0001\n",
      "Iter: 695/3021, loss_train: 6.5915, loss_val: nan, pos_over_neg: 151.01333618164062 lr: 0.0001\n",
      "Iter: 696/3021, loss_train: 6.5712, loss_val: nan, pos_over_neg: 165.84442138671875 lr: 0.0001\n",
      "Iter: 697/3021, loss_train: 6.5933, loss_val: nan, pos_over_neg: 120.06879425048828 lr: 0.0001\n",
      "Iter: 698/3021, loss_train: 6.5876, loss_val: nan, pos_over_neg: 98.9247817993164 lr: 0.0001\n",
      "Iter: 699/3021, loss_train: 6.6013, loss_val: nan, pos_over_neg: 90.98076629638672 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 700/3021, loss_train: 6.572, loss_val: nan, pos_over_neg: 94.82418060302734 lr: 0.0001\n",
      "Iter: 701/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 100.7938461303711 lr: 0.0001\n",
      "Iter: 702/3021, loss_train: 6.5829, loss_val: nan, pos_over_neg: 88.68292999267578 lr: 0.0001\n",
      "Iter: 703/3021, loss_train: 6.5951, loss_val: nan, pos_over_neg: 101.81523132324219 lr: 0.0001\n",
      "Iter: 704/3021, loss_train: 6.5961, loss_val: nan, pos_over_neg: 171.6141815185547 lr: 0.0001\n",
      "Iter: 705/3021, loss_train: 6.6094, loss_val: nan, pos_over_neg: 114.46845245361328 lr: 0.0001\n",
      "Iter: 706/3021, loss_train: 6.5904, loss_val: nan, pos_over_neg: 116.8626937866211 lr: 0.0001\n",
      "Iter: 707/3021, loss_train: 6.6149, loss_val: nan, pos_over_neg: 82.73287963867188 lr: 0.0001\n",
      "Iter: 708/3021, loss_train: 6.6279, loss_val: nan, pos_over_neg: 93.81600189208984 lr: 0.0001\n",
      "Iter: 709/3021, loss_train: 6.6053, loss_val: nan, pos_over_neg: 103.77784729003906 lr: 0.0001\n",
      "Iter: 710/3021, loss_train: 6.5989, loss_val: nan, pos_over_neg: 114.22103881835938 lr: 0.0001\n",
      "Iter: 711/3021, loss_train: 6.589, loss_val: nan, pos_over_neg: 109.76660919189453 lr: 0.0001\n",
      "Iter: 712/3021, loss_train: 6.5825, loss_val: nan, pos_over_neg: 73.29486083984375 lr: 0.0001\n",
      "Iter: 713/3021, loss_train: 6.6098, loss_val: nan, pos_over_neg: 73.3893051147461 lr: 0.0001\n",
      "Iter: 714/3021, loss_train: 6.5964, loss_val: nan, pos_over_neg: 66.46199035644531 lr: 0.0001\n",
      "Iter: 715/3021, loss_train: 6.5871, loss_val: nan, pos_over_neg: 76.87630462646484 lr: 0.0001\n",
      "Iter: 716/3021, loss_train: 6.5833, loss_val: nan, pos_over_neg: 69.87895965576172 lr: 0.0001\n",
      "Iter: 717/3021, loss_train: 6.6025, loss_val: nan, pos_over_neg: 99.14900970458984 lr: 0.0001\n",
      "Iter: 718/3021, loss_train: 6.5908, loss_val: nan, pos_over_neg: 131.8505859375 lr: 0.0001\n",
      "Iter: 719/3021, loss_train: 6.5788, loss_val: nan, pos_over_neg: 157.80743408203125 lr: 0.0001\n",
      "Iter: 720/3021, loss_train: 6.6086, loss_val: nan, pos_over_neg: 111.38347625732422 lr: 0.0001\n",
      "Iter: 721/3021, loss_train: 6.6021, loss_val: nan, pos_over_neg: 108.7108154296875 lr: 0.0001\n",
      "Iter: 722/3021, loss_train: 6.5849, loss_val: nan, pos_over_neg: 132.10877990722656 lr: 0.0001\n",
      "Iter: 723/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 102.30925750732422 lr: 0.0001\n",
      "Iter: 724/3021, loss_train: 6.6221, loss_val: nan, pos_over_neg: 98.22590637207031 lr: 0.0001\n",
      "Iter: 725/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 139.20152282714844 lr: 0.0001\n",
      "Iter: 726/3021, loss_train: 6.5815, loss_val: nan, pos_over_neg: 152.375732421875 lr: 0.0001\n",
      "Iter: 727/3021, loss_train: 6.6042, loss_val: nan, pos_over_neg: 153.21017456054688 lr: 0.0001\n",
      "Iter: 728/3021, loss_train: 6.5909, loss_val: nan, pos_over_neg: 138.31646728515625 lr: 0.0001\n",
      "Iter: 729/3021, loss_train: 6.6204, loss_val: nan, pos_over_neg: 87.23120880126953 lr: 0.0001\n",
      "Iter: 730/3021, loss_train: 6.5699, loss_val: nan, pos_over_neg: 103.18000793457031 lr: 0.0001\n",
      "Iter: 731/3021, loss_train: 6.5929, loss_val: nan, pos_over_neg: 76.64822387695312 lr: 0.0001\n",
      "Iter: 732/3021, loss_train: 6.6012, loss_val: nan, pos_over_neg: 77.05658721923828 lr: 0.0001\n",
      "Iter: 733/3021, loss_train: 6.6112, loss_val: nan, pos_over_neg: 61.744014739990234 lr: 0.0001\n",
      "Iter: 734/3021, loss_train: 6.5985, loss_val: nan, pos_over_neg: 67.07396697998047 lr: 0.0001\n",
      "Iter: 735/3021, loss_train: 6.591, loss_val: nan, pos_over_neg: 75.07990264892578 lr: 0.0001\n",
      "Iter: 736/3021, loss_train: 6.5565, loss_val: nan, pos_over_neg: 79.77616119384766 lr: 0.0001\n",
      "Iter: 737/3021, loss_train: 6.5732, loss_val: nan, pos_over_neg: 94.3510971069336 lr: 0.0001\n",
      "Iter: 738/3021, loss_train: 6.6243, loss_val: nan, pos_over_neg: 89.19917297363281 lr: 0.0001\n",
      "Iter: 739/3021, loss_train: 6.5891, loss_val: nan, pos_over_neg: 144.8668975830078 lr: 0.0001\n",
      "Iter: 740/3021, loss_train: 6.5989, loss_val: nan, pos_over_neg: 128.92776489257812 lr: 0.0001\n",
      "Iter: 741/3021, loss_train: 6.6202, loss_val: nan, pos_over_neg: 126.48570251464844 lr: 0.0001\n",
      "Iter: 742/3021, loss_train: 6.559, loss_val: nan, pos_over_neg: 152.12013244628906 lr: 0.0001\n",
      "Iter: 743/3021, loss_train: 6.5795, loss_val: nan, pos_over_neg: 87.68231201171875 lr: 0.0001\n",
      "Iter: 744/3021, loss_train: 6.6107, loss_val: nan, pos_over_neg: 108.50746154785156 lr: 0.0001\n",
      "Iter: 745/3021, loss_train: 6.5808, loss_val: nan, pos_over_neg: 104.51139068603516 lr: 0.0001\n",
      "Iter: 746/3021, loss_train: 6.5915, loss_val: nan, pos_over_neg: 98.31632995605469 lr: 0.0001\n",
      "Iter: 747/3021, loss_train: 6.5945, loss_val: nan, pos_over_neg: 104.99259185791016 lr: 0.0001\n",
      "Iter: 748/3021, loss_train: 6.5936, loss_val: nan, pos_over_neg: 172.98275756835938 lr: 0.0001\n",
      "Iter: 749/3021, loss_train: 6.5721, loss_val: nan, pos_over_neg: 142.9506072998047 lr: 0.0001\n",
      "Iter: 750/3021, loss_train: 6.576, loss_val: nan, pos_over_neg: 129.3081512451172 lr: 0.0001\n",
      "Iter: 751/3021, loss_train: 6.6131, loss_val: nan, pos_over_neg: 84.6493911743164 lr: 0.0001\n",
      "Iter: 752/3021, loss_train: 6.5836, loss_val: nan, pos_over_neg: 127.84735107421875 lr: 0.0001\n",
      "Iter: 753/3021, loss_train: 6.5978, loss_val: nan, pos_over_neg: 96.58321380615234 lr: 0.0001\n",
      "Iter: 754/3021, loss_train: 6.5917, loss_val: nan, pos_over_neg: 102.70083618164062 lr: 0.0001\n",
      "Iter: 755/3021, loss_train: 6.6024, loss_val: nan, pos_over_neg: 80.68453979492188 lr: 0.0001\n",
      "Iter: 756/3021, loss_train: 6.5821, loss_val: nan, pos_over_neg: 78.31182098388672 lr: 0.0001\n",
      "Iter: 757/3021, loss_train: 6.5884, loss_val: nan, pos_over_neg: 102.52578735351562 lr: 0.0001\n",
      "Iter: 758/3021, loss_train: 6.5885, loss_val: nan, pos_over_neg: 83.65754699707031 lr: 0.0001\n",
      "Iter: 759/3021, loss_train: 6.5939, loss_val: nan, pos_over_neg: 79.39031982421875 lr: 0.0001\n",
      "Iter: 760/3021, loss_train: 6.5773, loss_val: nan, pos_over_neg: 100.86004638671875 lr: 0.0001\n",
      "Iter: 761/3021, loss_train: 6.6043, loss_val: nan, pos_over_neg: 102.71807861328125 lr: 0.0001\n",
      "Iter: 762/3021, loss_train: 6.5711, loss_val: nan, pos_over_neg: 133.64044189453125 lr: 0.0001\n",
      "Iter: 763/3021, loss_train: 6.5872, loss_val: nan, pos_over_neg: 218.85787963867188 lr: 0.0001\n",
      "Iter: 764/3021, loss_train: 6.5853, loss_val: nan, pos_over_neg: 188.55897521972656 lr: 0.0001\n",
      "Iter: 765/3021, loss_train: 6.5987, loss_val: nan, pos_over_neg: 85.1949462890625 lr: 0.0001\n",
      "Iter: 766/3021, loss_train: 6.5966, loss_val: nan, pos_over_neg: 99.47931671142578 lr: 0.0001\n",
      "Iter: 767/3021, loss_train: 6.5559, loss_val: nan, pos_over_neg: 127.3941421508789 lr: 0.0001\n",
      "Iter: 768/3021, loss_train: 6.5883, loss_val: nan, pos_over_neg: 105.63689422607422 lr: 0.0001\n",
      "Iter: 769/3021, loss_train: 6.591, loss_val: nan, pos_over_neg: 100.22355651855469 lr: 0.0001\n",
      "Iter: 770/3021, loss_train: 6.5743, loss_val: nan, pos_over_neg: 126.24049377441406 lr: 0.0001\n",
      "Iter: 771/3021, loss_train: 6.6049, loss_val: nan, pos_over_neg: 98.09718322753906 lr: 0.0001\n",
      "Iter: 772/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 80.01924896240234 lr: 0.0001\n",
      "Iter: 773/3021, loss_train: 6.586, loss_val: nan, pos_over_neg: 108.43155670166016 lr: 0.0001\n",
      "Iter: 774/3021, loss_train: 6.5788, loss_val: nan, pos_over_neg: 96.88678741455078 lr: 0.0001\n",
      "Iter: 775/3021, loss_train: 6.6085, loss_val: nan, pos_over_neg: 79.73624420166016 lr: 0.0001\n",
      "Iter: 776/3021, loss_train: 6.5832, loss_val: nan, pos_over_neg: 114.96404266357422 lr: 0.0001\n",
      "Iter: 777/3021, loss_train: 6.5706, loss_val: nan, pos_over_neg: 100.1657485961914 lr: 0.0001\n",
      "Iter: 778/3021, loss_train: 6.5734, loss_val: nan, pos_over_neg: 107.69770812988281 lr: 0.0001\n",
      "Iter: 779/3021, loss_train: 6.5951, loss_val: nan, pos_over_neg: 124.25031280517578 lr: 0.0001\n",
      "Iter: 780/3021, loss_train: 6.6053, loss_val: nan, pos_over_neg: 95.15596008300781 lr: 0.0001\n",
      "Iter: 781/3021, loss_train: 6.5845, loss_val: nan, pos_over_neg: 114.03691864013672 lr: 0.0001\n",
      "Iter: 782/3021, loss_train: 6.5902, loss_val: nan, pos_over_neg: 104.40702819824219 lr: 0.0001\n",
      "Iter: 783/3021, loss_train: 6.5872, loss_val: nan, pos_over_neg: 83.47135925292969 lr: 0.0001\n",
      "Iter: 784/3021, loss_train: 6.5901, loss_val: nan, pos_over_neg: 90.88580322265625 lr: 0.0001\n",
      "Iter: 785/3021, loss_train: 6.5955, loss_val: nan, pos_over_neg: 83.52406311035156 lr: 0.0001\n",
      "Iter: 786/3021, loss_train: 6.5806, loss_val: nan, pos_over_neg: 101.07864379882812 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 787/3021, loss_train: 6.5755, loss_val: nan, pos_over_neg: 109.11198425292969 lr: 0.0001\n",
      "Iter: 788/3021, loss_train: 6.5839, loss_val: nan, pos_over_neg: 132.4898681640625 lr: 0.0001\n",
      "Iter: 789/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 134.54417419433594 lr: 0.0001\n",
      "Iter: 790/3021, loss_train: 6.5759, loss_val: nan, pos_over_neg: 156.8651580810547 lr: 0.0001\n",
      "Iter: 791/3021, loss_train: 6.5545, loss_val: nan, pos_over_neg: 171.5546417236328 lr: 0.0001\n",
      "Iter: 792/3021, loss_train: 6.6104, loss_val: nan, pos_over_neg: 109.55184173583984 lr: 0.0001\n",
      "Iter: 793/3021, loss_train: 6.5842, loss_val: nan, pos_over_neg: 93.66973114013672 lr: 0.0001\n",
      "Iter: 794/3021, loss_train: 6.632, loss_val: nan, pos_over_neg: 98.02056884765625 lr: 0.0001\n",
      "Iter: 795/3021, loss_train: 6.5858, loss_val: nan, pos_over_neg: 104.2520751953125 lr: 0.0001\n",
      "Iter: 796/3021, loss_train: 6.5625, loss_val: nan, pos_over_neg: 119.94738006591797 lr: 0.0001\n",
      "Iter: 797/3021, loss_train: 6.5722, loss_val: nan, pos_over_neg: 105.0882568359375 lr: 0.0001\n",
      "Iter: 798/3021, loss_train: 6.5773, loss_val: nan, pos_over_neg: 122.53003692626953 lr: 0.0001\n",
      "Iter: 799/3021, loss_train: 6.5798, loss_val: nan, pos_over_neg: 116.73748779296875 lr: 0.0001\n",
      "Iter: 800/3021, loss_train: 6.5672, loss_val: nan, pos_over_neg: 120.35350799560547 lr: 0.0001\n",
      "Iter: 801/3021, loss_train: 6.6009, loss_val: nan, pos_over_neg: 119.53120422363281 lr: 0.0001\n",
      "Iter: 802/3021, loss_train: 6.5682, loss_val: nan, pos_over_neg: 187.2893829345703 lr: 0.0001\n",
      "Iter: 803/3021, loss_train: 6.5816, loss_val: nan, pos_over_neg: 96.84266662597656 lr: 0.0001\n",
      "Iter: 804/3021, loss_train: 6.5877, loss_val: nan, pos_over_neg: 94.43434143066406 lr: 0.0001\n",
      "Iter: 805/3021, loss_train: 6.5792, loss_val: nan, pos_over_neg: 119.4881362915039 lr: 0.0001\n",
      "Iter: 806/3021, loss_train: 6.5774, loss_val: nan, pos_over_neg: 121.78392791748047 lr: 0.0001\n",
      "Iter: 807/3021, loss_train: 6.549, loss_val: nan, pos_over_neg: 101.5608901977539 lr: 0.0001\n",
      "Iter: 808/3021, loss_train: 6.6055, loss_val: nan, pos_over_neg: 74.5746078491211 lr: 0.0001\n",
      "Iter: 809/3021, loss_train: 6.5979, loss_val: nan, pos_over_neg: 76.86831665039062 lr: 0.0001\n",
      "Iter: 810/3021, loss_train: 6.5582, loss_val: nan, pos_over_neg: 204.60385131835938 lr: 0.0001\n",
      "Iter: 811/3021, loss_train: 6.5751, loss_val: nan, pos_over_neg: 114.60301208496094 lr: 0.0001\n",
      "Iter: 812/3021, loss_train: 6.601, loss_val: nan, pos_over_neg: 174.6918182373047 lr: 0.0001\n",
      "Iter: 813/3021, loss_train: 6.5796, loss_val: nan, pos_over_neg: 234.507080078125 lr: 0.0001\n",
      "Iter: 814/3021, loss_train: 6.5707, loss_val: nan, pos_over_neg: 267.89178466796875 lr: 0.0001\n",
      "Iter: 815/3021, loss_train: 6.5727, loss_val: nan, pos_over_neg: 110.06815338134766 lr: 0.0001\n",
      "Iter: 816/3021, loss_train: 6.6104, loss_val: nan, pos_over_neg: 143.34414672851562 lr: 0.0001\n",
      "Iter: 817/3021, loss_train: 6.578, loss_val: nan, pos_over_neg: 145.5452117919922 lr: 0.0001\n",
      "Iter: 818/3021, loss_train: 6.5729, loss_val: nan, pos_over_neg: 112.28546905517578 lr: 0.0001\n",
      "Iter: 819/3021, loss_train: 6.6057, loss_val: nan, pos_over_neg: 93.8596420288086 lr: 0.0001\n",
      "Iter: 820/3021, loss_train: 6.5888, loss_val: nan, pos_over_neg: 93.03485870361328 lr: 0.0001\n",
      "Iter: 821/3021, loss_train: 6.5483, loss_val: nan, pos_over_neg: 88.3165512084961 lr: 0.0001\n",
      "Iter: 822/3021, loss_train: 6.5882, loss_val: nan, pos_over_neg: 73.82594299316406 lr: 0.0001\n",
      "Iter: 823/3021, loss_train: 6.5838, loss_val: nan, pos_over_neg: 68.10240936279297 lr: 0.0001\n",
      "Iter: 824/3021, loss_train: 6.5923, loss_val: nan, pos_over_neg: 70.80805969238281 lr: 0.0001\n",
      "Iter: 825/3021, loss_train: 6.5896, loss_val: nan, pos_over_neg: 96.17666625976562 lr: 0.0001\n",
      "Iter: 826/3021, loss_train: 6.5828, loss_val: nan, pos_over_neg: 86.63882446289062 lr: 0.0001\n",
      "Iter: 827/3021, loss_train: 6.606, loss_val: nan, pos_over_neg: 98.90731048583984 lr: 0.0001\n",
      "Iter: 828/3021, loss_train: 6.6132, loss_val: nan, pos_over_neg: 98.24592590332031 lr: 0.0001\n",
      "Iter: 829/3021, loss_train: 6.5839, loss_val: nan, pos_over_neg: 137.680908203125 lr: 0.0001\n",
      "Iter: 830/3021, loss_train: 6.575, loss_val: nan, pos_over_neg: 148.0569305419922 lr: 0.0001\n",
      "Iter: 831/3021, loss_train: 6.5759, loss_val: nan, pos_over_neg: 179.43392944335938 lr: 0.0001\n",
      "Iter: 832/3021, loss_train: 6.5979, loss_val: nan, pos_over_neg: 173.71920776367188 lr: 0.0001\n",
      "Iter: 833/3021, loss_train: 6.5608, loss_val: nan, pos_over_neg: 193.14556884765625 lr: 0.0001\n",
      "Iter: 834/3021, loss_train: 6.579, loss_val: nan, pos_over_neg: 116.02136993408203 lr: 0.0001\n",
      "Iter: 835/3021, loss_train: 6.5496, loss_val: nan, pos_over_neg: 154.1082763671875 lr: 0.0001\n",
      "Iter: 836/3021, loss_train: 6.581, loss_val: nan, pos_over_neg: 134.77786254882812 lr: 0.0001\n",
      "Iter: 837/3021, loss_train: 6.5892, loss_val: nan, pos_over_neg: 123.32394409179688 lr: 0.0001\n",
      "Iter: 838/3021, loss_train: 6.5707, loss_val: nan, pos_over_neg: 84.55307006835938 lr: 0.0001\n",
      "Iter: 839/3021, loss_train: 6.5608, loss_val: nan, pos_over_neg: 84.38294219970703 lr: 0.0001\n",
      "Iter: 840/3021, loss_train: 6.5785, loss_val: nan, pos_over_neg: 74.79573822021484 lr: 0.0001\n",
      "Iter: 841/3021, loss_train: 6.6063, loss_val: nan, pos_over_neg: 74.86983489990234 lr: 0.0001\n",
      "Iter: 842/3021, loss_train: 6.568, loss_val: nan, pos_over_neg: 130.31503295898438 lr: 0.0001\n",
      "Iter: 843/3021, loss_train: 6.5489, loss_val: nan, pos_over_neg: 184.262939453125 lr: 0.0001\n",
      "Iter: 844/3021, loss_train: 6.5623, loss_val: nan, pos_over_neg: 131.70889282226562 lr: 0.0001\n",
      "Iter: 845/3021, loss_train: 6.5656, loss_val: nan, pos_over_neg: 104.441162109375 lr: 0.0001\n",
      "Iter: 846/3021, loss_train: 6.5765, loss_val: nan, pos_over_neg: 108.13817596435547 lr: 0.0001\n",
      "Iter: 847/3021, loss_train: 6.5614, loss_val: nan, pos_over_neg: 119.45594024658203 lr: 0.0001\n",
      "Iter: 848/3021, loss_train: 6.5976, loss_val: nan, pos_over_neg: 96.42980194091797 lr: 0.0001\n",
      "Iter: 849/3021, loss_train: 6.5514, loss_val: nan, pos_over_neg: 170.33901977539062 lr: 0.0001\n",
      "Iter: 850/3021, loss_train: 6.5853, loss_val: nan, pos_over_neg: 105.53504180908203 lr: 0.0001\n",
      "Iter: 851/3021, loss_train: 6.5851, loss_val: nan, pos_over_neg: 134.21749877929688 lr: 0.0001\n",
      "Iter: 852/3021, loss_train: 6.5614, loss_val: nan, pos_over_neg: 131.70223999023438 lr: 0.0001\n",
      "Iter: 853/3021, loss_train: 6.5656, loss_val: nan, pos_over_neg: 126.96797180175781 lr: 0.0001\n",
      "Iter: 854/3021, loss_train: 6.5869, loss_val: nan, pos_over_neg: 115.4102783203125 lr: 0.0001\n",
      "Iter: 855/3021, loss_train: 6.6107, loss_val: nan, pos_over_neg: 99.24248504638672 lr: 0.0001\n",
      "Iter: 856/3021, loss_train: 6.5772, loss_val: nan, pos_over_neg: 113.70159149169922 lr: 0.0001\n",
      "Iter: 857/3021, loss_train: 6.5973, loss_val: nan, pos_over_neg: 132.40118408203125 lr: 0.0001\n",
      "Iter: 858/3021, loss_train: 6.5915, loss_val: nan, pos_over_neg: 156.69459533691406 lr: 0.0001\n",
      "Iter: 859/3021, loss_train: 6.5778, loss_val: nan, pos_over_neg: 114.44495391845703 lr: 0.0001\n",
      "Iter: 860/3021, loss_train: 6.5823, loss_val: nan, pos_over_neg: 94.06466674804688 lr: 0.0001\n",
      "Iter: 861/3021, loss_train: 6.5651, loss_val: nan, pos_over_neg: 128.86962890625 lr: 0.0001\n",
      "Iter: 862/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 142.10464477539062 lr: 0.0001\n",
      "Iter: 863/3021, loss_train: 6.5708, loss_val: nan, pos_over_neg: 86.1020278930664 lr: 0.0001\n",
      "Iter: 864/3021, loss_train: 6.5533, loss_val: nan, pos_over_neg: 140.7494659423828 lr: 0.0001\n",
      "Iter: 865/3021, loss_train: 6.5628, loss_val: nan, pos_over_neg: 117.9881362915039 lr: 0.0001\n",
      "Iter: 866/3021, loss_train: 6.5729, loss_val: nan, pos_over_neg: 106.94337463378906 lr: 0.0001\n",
      "Iter: 867/3021, loss_train: 6.555, loss_val: nan, pos_over_neg: 96.3055648803711 lr: 0.0001\n",
      "Iter: 868/3021, loss_train: 6.5884, loss_val: nan, pos_over_neg: 117.15845489501953 lr: 0.0001\n",
      "Iter: 869/3021, loss_train: 6.5665, loss_val: nan, pos_over_neg: 106.46613311767578 lr: 0.0001\n",
      "Iter: 870/3021, loss_train: 6.5684, loss_val: nan, pos_over_neg: 130.9989013671875 lr: 0.0001\n",
      "Iter: 871/3021, loss_train: 6.5763, loss_val: nan, pos_over_neg: 179.65394592285156 lr: 0.0001\n",
      "Iter: 872/3021, loss_train: 6.6046, loss_val: nan, pos_over_neg: 115.00897216796875 lr: 0.0001\n",
      "Iter: 873/3021, loss_train: 6.5862, loss_val: nan, pos_over_neg: 164.241455078125 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 874/3021, loss_train: 6.5768, loss_val: nan, pos_over_neg: 154.78904724121094 lr: 0.0001\n",
      "Iter: 875/3021, loss_train: 6.5803, loss_val: nan, pos_over_neg: 129.99444580078125 lr: 0.0001\n",
      "Iter: 876/3021, loss_train: 6.5432, loss_val: nan, pos_over_neg: 152.75523376464844 lr: 0.0001\n",
      "Iter: 877/3021, loss_train: 6.5704, loss_val: nan, pos_over_neg: 131.41099548339844 lr: 0.0001\n",
      "Iter: 878/3021, loss_train: 6.5942, loss_val: nan, pos_over_neg: 88.29497528076172 lr: 0.0001\n",
      "Iter: 879/3021, loss_train: 6.5559, loss_val: nan, pos_over_neg: 117.21337890625 lr: 0.0001\n",
      "Iter: 880/3021, loss_train: 6.5694, loss_val: nan, pos_over_neg: 119.8251953125 lr: 0.0001\n",
      "Iter: 881/3021, loss_train: 6.5737, loss_val: nan, pos_over_neg: 100.10836029052734 lr: 0.0001\n",
      "Iter: 882/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 134.96728515625 lr: 0.0001\n",
      "Iter: 883/3021, loss_train: 6.57, loss_val: nan, pos_over_neg: 101.74969482421875 lr: 0.0001\n",
      "Iter: 884/3021, loss_train: 6.5843, loss_val: nan, pos_over_neg: 125.26480865478516 lr: 0.0001\n",
      "Iter: 885/3021, loss_train: 6.5745, loss_val: nan, pos_over_neg: 136.58721923828125 lr: 0.0001\n",
      "Iter: 886/3021, loss_train: 6.5474, loss_val: nan, pos_over_neg: 147.53555297851562 lr: 0.0001\n",
      "Iter: 887/3021, loss_train: 6.5861, loss_val: nan, pos_over_neg: 87.9935073852539 lr: 0.0001\n",
      "Iter: 888/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 99.44595336914062 lr: 0.0001\n",
      "Iter: 889/3021, loss_train: 6.5794, loss_val: nan, pos_over_neg: 139.5227813720703 lr: 0.0001\n",
      "Iter: 890/3021, loss_train: 6.5851, loss_val: nan, pos_over_neg: 114.21017456054688 lr: 0.0001\n",
      "Iter: 891/3021, loss_train: 6.5635, loss_val: nan, pos_over_neg: 133.23110961914062 lr: 0.0001\n",
      "Iter: 892/3021, loss_train: 6.5801, loss_val: nan, pos_over_neg: 173.90213012695312 lr: 0.0001\n",
      "Iter: 893/3021, loss_train: 6.5793, loss_val: nan, pos_over_neg: 136.06646728515625 lr: 0.0001\n",
      "Iter: 894/3021, loss_train: 6.5696, loss_val: nan, pos_over_neg: 148.89796447753906 lr: 0.0001\n",
      "Iter: 895/3021, loss_train: 6.5969, loss_val: nan, pos_over_neg: 129.4973907470703 lr: 0.0001\n",
      "Iter: 896/3021, loss_train: 6.563, loss_val: nan, pos_over_neg: 91.09149169921875 lr: 0.0001\n",
      "Iter: 897/3021, loss_train: 6.5722, loss_val: nan, pos_over_neg: 74.48014831542969 lr: 0.0001\n",
      "Iter: 898/3021, loss_train: 6.5788, loss_val: nan, pos_over_neg: 85.58679962158203 lr: 0.0001\n",
      "Iter: 899/3021, loss_train: 6.5655, loss_val: nan, pos_over_neg: 88.15765380859375 lr: 0.0001\n",
      "Iter: 900/3021, loss_train: 6.5893, loss_val: nan, pos_over_neg: 70.21942138671875 lr: 0.0001\n",
      "Iter: 901/3021, loss_train: 6.5591, loss_val: nan, pos_over_neg: 88.64764404296875 lr: 0.0001\n",
      "Iter: 902/3021, loss_train: 6.5573, loss_val: nan, pos_over_neg: 72.45358276367188 lr: 0.0001\n",
      "Iter: 903/3021, loss_train: 6.5791, loss_val: nan, pos_over_neg: 78.7385482788086 lr: 0.0001\n",
      "Iter: 904/3021, loss_train: 6.5712, loss_val: nan, pos_over_neg: 110.63396453857422 lr: 0.0001\n",
      "Iter: 905/3021, loss_train: 6.5728, loss_val: nan, pos_over_neg: 104.12796783447266 lr: 0.0001\n",
      "Iter: 906/3021, loss_train: 6.5542, loss_val: nan, pos_over_neg: 153.1318817138672 lr: 0.0001\n",
      "Iter: 907/3021, loss_train: 6.5565, loss_val: nan, pos_over_neg: 246.72149658203125 lr: 0.0001\n",
      "Iter: 908/3021, loss_train: 6.5825, loss_val: nan, pos_over_neg: 217.3009490966797 lr: 0.0001\n",
      "Iter: 909/3021, loss_train: 6.5687, loss_val: nan, pos_over_neg: 160.2449188232422 lr: 0.0001\n",
      "Iter: 910/3021, loss_train: 6.5873, loss_val: nan, pos_over_neg: 190.32943725585938 lr: 0.0001\n",
      "Iter: 911/3021, loss_train: 6.5671, loss_val: nan, pos_over_neg: 144.85560607910156 lr: 0.0001\n",
      "Iter: 912/3021, loss_train: 6.5752, loss_val: nan, pos_over_neg: 163.42495727539062 lr: 0.0001\n",
      "Iter: 913/3021, loss_train: 6.5778, loss_val: nan, pos_over_neg: 98.33039855957031 lr: 0.0001\n",
      "Iter: 914/3021, loss_train: 6.5455, loss_val: nan, pos_over_neg: 109.47969055175781 lr: 0.0001\n",
      "Iter: 915/3021, loss_train: 6.5754, loss_val: nan, pos_over_neg: 84.40025329589844 lr: 0.0001\n",
      "Iter: 916/3021, loss_train: 6.5567, loss_val: nan, pos_over_neg: 71.81239318847656 lr: 0.0001\n",
      "Iter: 917/3021, loss_train: 6.606, loss_val: nan, pos_over_neg: 85.62799835205078 lr: 0.0001\n",
      "Iter: 918/3021, loss_train: 6.5646, loss_val: nan, pos_over_neg: 150.7646026611328 lr: 0.0001\n",
      "Iter: 919/3021, loss_train: 6.5905, loss_val: nan, pos_over_neg: 92.63785552978516 lr: 0.0001\n",
      "Iter: 920/3021, loss_train: 6.6019, loss_val: nan, pos_over_neg: 143.0022735595703 lr: 0.0001\n",
      "Iter: 921/3021, loss_train: 6.5874, loss_val: nan, pos_over_neg: 99.89277648925781 lr: 0.0001\n",
      "Iter: 922/3021, loss_train: 6.5762, loss_val: nan, pos_over_neg: 249.96287536621094 lr: 0.0001\n",
      "Iter: 923/3021, loss_train: 6.5759, loss_val: nan, pos_over_neg: 182.08511352539062 lr: 0.0001\n",
      "Iter: 924/3021, loss_train: 6.5676, loss_val: nan, pos_over_neg: 133.8520050048828 lr: 0.0001\n",
      "Iter: 925/3021, loss_train: 6.5783, loss_val: nan, pos_over_neg: 103.31720733642578 lr: 0.0001\n",
      "Iter: 926/3021, loss_train: 6.5689, loss_val: nan, pos_over_neg: 78.12081146240234 lr: 0.0001\n",
      "Iter: 927/3021, loss_train: 6.5831, loss_val: nan, pos_over_neg: 77.76296997070312 lr: 0.0001\n",
      "Iter: 928/3021, loss_train: 6.5816, loss_val: nan, pos_over_neg: 85.48399353027344 lr: 0.0001\n",
      "Iter: 929/3021, loss_train: 6.5711, loss_val: nan, pos_over_neg: 83.0072250366211 lr: 0.0001\n",
      "Iter: 930/3021, loss_train: 6.5614, loss_val: nan, pos_over_neg: 104.88105010986328 lr: 0.0001\n",
      "Iter: 931/3021, loss_train: 6.5776, loss_val: nan, pos_over_neg: 98.67469024658203 lr: 0.0001\n",
      "Iter: 932/3021, loss_train: 6.569, loss_val: nan, pos_over_neg: 112.09076690673828 lr: 0.0001\n",
      "Iter: 933/3021, loss_train: 6.5594, loss_val: nan, pos_over_neg: 179.78797912597656 lr: 0.0001\n",
      "Iter: 934/3021, loss_train: 6.5491, loss_val: nan, pos_over_neg: 132.42474365234375 lr: 0.0001\n",
      "Iter: 935/3021, loss_train: 6.5666, loss_val: nan, pos_over_neg: 134.62725830078125 lr: 0.0001\n",
      "Iter: 936/3021, loss_train: 6.57, loss_val: nan, pos_over_neg: 167.8118438720703 lr: 0.0001\n",
      "Iter: 937/3021, loss_train: 6.5435, loss_val: nan, pos_over_neg: 160.1095733642578 lr: 0.0001\n",
      "Iter: 938/3021, loss_train: 6.5356, loss_val: nan, pos_over_neg: 114.2269058227539 lr: 0.0001\n",
      "Iter: 939/3021, loss_train: 6.5771, loss_val: nan, pos_over_neg: 81.26506805419922 lr: 0.0001\n",
      "Iter: 940/3021, loss_train: 6.5584, loss_val: nan, pos_over_neg: 149.90707397460938 lr: 0.0001\n",
      "Iter: 941/3021, loss_train: 6.5671, loss_val: nan, pos_over_neg: 97.2630386352539 lr: 0.0001\n",
      "Iter: 942/3021, loss_train: 6.5744, loss_val: nan, pos_over_neg: 85.1363525390625 lr: 0.0001\n",
      "Iter: 943/3021, loss_train: 6.5821, loss_val: nan, pos_over_neg: 76.89978790283203 lr: 0.0001\n",
      "Iter: 944/3021, loss_train: 6.5812, loss_val: nan, pos_over_neg: 81.42671966552734 lr: 0.0001\n",
      "Iter: 945/3021, loss_train: 6.5734, loss_val: nan, pos_over_neg: 119.42163848876953 lr: 0.0001\n",
      "Iter: 946/3021, loss_train: 6.5712, loss_val: nan, pos_over_neg: 128.67486572265625 lr: 0.0001\n",
      "Iter: 947/3021, loss_train: 6.5696, loss_val: nan, pos_over_neg: 143.73944091796875 lr: 0.0001\n",
      "Iter: 948/3021, loss_train: 6.5693, loss_val: nan, pos_over_neg: 107.31214141845703 lr: 0.0001\n",
      "Iter: 949/3021, loss_train: 6.5715, loss_val: nan, pos_over_neg: 143.2410125732422 lr: 0.0001\n",
      "Iter: 950/3021, loss_train: 6.5835, loss_val: nan, pos_over_neg: 125.20860290527344 lr: 0.0001\n",
      "Iter: 951/3021, loss_train: 6.5953, loss_val: nan, pos_over_neg: 83.9017333984375 lr: 0.0001\n",
      "Iter: 952/3021, loss_train: 6.5751, loss_val: nan, pos_over_neg: 128.59449768066406 lr: 0.0001\n",
      "Iter: 953/3021, loss_train: 6.5531, loss_val: nan, pos_over_neg: 154.37371826171875 lr: 0.0001\n",
      "Iter: 954/3021, loss_train: 6.5761, loss_val: nan, pos_over_neg: 152.8515625 lr: 0.0001\n",
      "Iter: 955/3021, loss_train: 6.5763, loss_val: nan, pos_over_neg: 122.51900482177734 lr: 0.0001\n",
      "Iter: 956/3021, loss_train: 6.5894, loss_val: nan, pos_over_neg: 104.45314025878906 lr: 0.0001\n",
      "Iter: 957/3021, loss_train: 6.5615, loss_val: nan, pos_over_neg: 108.87615966796875 lr: 0.0001\n",
      "Iter: 958/3021, loss_train: 6.5624, loss_val: nan, pos_over_neg: 133.97483825683594 lr: 0.0001\n",
      "Iter: 959/3021, loss_train: 6.5871, loss_val: nan, pos_over_neg: 72.12725830078125 lr: 0.0001\n",
      "Iter: 960/3021, loss_train: 6.551, loss_val: nan, pos_over_neg: 99.86529541015625 lr: 0.0001\n",
      "Iter: 961/3021, loss_train: 6.5653, loss_val: nan, pos_over_neg: 94.25920104980469 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 962/3021, loss_train: 6.5725, loss_val: nan, pos_over_neg: 96.02204132080078 lr: 0.0001\n",
      "Iter: 963/3021, loss_train: 6.5653, loss_val: nan, pos_over_neg: 121.8132095336914 lr: 0.0001\n",
      "Iter: 964/3021, loss_train: 6.5666, loss_val: nan, pos_over_neg: 235.52944946289062 lr: 0.0001\n",
      "Iter: 965/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 188.1890869140625 lr: 0.0001\n",
      "Iter: 966/3021, loss_train: 6.5744, loss_val: nan, pos_over_neg: 120.70113372802734 lr: 0.0001\n",
      "Iter: 967/3021, loss_train: 6.5869, loss_val: nan, pos_over_neg: 121.96207427978516 lr: 0.0001\n",
      "Iter: 968/3021, loss_train: 6.5935, loss_val: nan, pos_over_neg: 127.89557647705078 lr: 0.0001\n",
      "Iter: 969/3021, loss_train: 6.5876, loss_val: nan, pos_over_neg: 151.24960327148438 lr: 0.0001\n",
      "Iter: 970/3021, loss_train: 6.576, loss_val: nan, pos_over_neg: 108.139892578125 lr: 0.0001\n",
      "Iter: 971/3021, loss_train: 6.5705, loss_val: nan, pos_over_neg: 75.43293762207031 lr: 0.0001\n",
      "Iter: 972/3021, loss_train: 6.5634, loss_val: nan, pos_over_neg: 92.83189392089844 lr: 0.0001\n",
      "Iter: 973/3021, loss_train: 6.5697, loss_val: nan, pos_over_neg: 75.71949005126953 lr: 0.0001\n",
      "Iter: 974/3021, loss_train: 6.5565, loss_val: nan, pos_over_neg: 77.78703308105469 lr: 0.0001\n",
      "Iter: 975/3021, loss_train: 6.5887, loss_val: nan, pos_over_neg: 68.59326171875 lr: 0.0001\n",
      "Iter: 976/3021, loss_train: 6.5885, loss_val: nan, pos_over_neg: 81.51212310791016 lr: 0.0001\n",
      "Iter: 977/3021, loss_train: 6.5684, loss_val: nan, pos_over_neg: 114.84517669677734 lr: 0.0001\n",
      "Iter: 978/3021, loss_train: 6.5538, loss_val: nan, pos_over_neg: 258.4898376464844 lr: 0.0001\n",
      "Iter: 979/3021, loss_train: 6.5638, loss_val: nan, pos_over_neg: 133.83604431152344 lr: 0.0001\n",
      "Iter: 980/3021, loss_train: 6.5999, loss_val: nan, pos_over_neg: 119.8874282836914 lr: 0.0001\n",
      "Iter: 981/3021, loss_train: 6.5697, loss_val: nan, pos_over_neg: 114.26315307617188 lr: 0.0001\n",
      "Iter: 982/3021, loss_train: 6.5774, loss_val: nan, pos_over_neg: 180.44363403320312 lr: 0.0001\n",
      "Iter: 983/3021, loss_train: 6.6027, loss_val: nan, pos_over_neg: 105.96611785888672 lr: 0.0001\n",
      "Iter: 984/3021, loss_train: 6.5971, loss_val: nan, pos_over_neg: 102.49323272705078 lr: 0.0001\n",
      "Iter: 985/3021, loss_train: 6.5763, loss_val: nan, pos_over_neg: 90.43103790283203 lr: 0.0001\n",
      "Iter: 986/3021, loss_train: 6.5887, loss_val: nan, pos_over_neg: 92.32857513427734 lr: 0.0001\n",
      "Iter: 987/3021, loss_train: 6.5376, loss_val: nan, pos_over_neg: 111.41860961914062 lr: 0.0001\n",
      "Iter: 988/3021, loss_train: 6.5805, loss_val: nan, pos_over_neg: 101.45437622070312 lr: 0.0001\n",
      "Iter: 989/3021, loss_train: 6.5386, loss_val: nan, pos_over_neg: 130.3516845703125 lr: 0.0001\n",
      "Iter: 990/3021, loss_train: 6.5924, loss_val: nan, pos_over_neg: 113.487548828125 lr: 0.0001\n",
      "Iter: 991/3021, loss_train: 6.5909, loss_val: nan, pos_over_neg: 91.79997253417969 lr: 0.0001\n",
      "Iter: 992/3021, loss_train: 6.5636, loss_val: nan, pos_over_neg: 82.68806457519531 lr: 0.0001\n",
      "Iter: 993/3021, loss_train: 6.5476, loss_val: nan, pos_over_neg: 112.7024917602539 lr: 0.0001\n",
      "Iter: 994/3021, loss_train: 6.57, loss_val: nan, pos_over_neg: 91.02479553222656 lr: 0.0001\n",
      "Iter: 995/3021, loss_train: 6.5884, loss_val: nan, pos_over_neg: 70.57719421386719 lr: 0.0001\n",
      "Iter: 996/3021, loss_train: 6.5608, loss_val: nan, pos_over_neg: 113.08472442626953 lr: 0.0001\n",
      "Iter: 997/3021, loss_train: 6.5414, loss_val: nan, pos_over_neg: 103.58024597167969 lr: 0.0001\n",
      "Iter: 998/3021, loss_train: 6.5485, loss_val: nan, pos_over_neg: 94.90933227539062 lr: 0.0001\n",
      "Iter: 999/3021, loss_train: 6.5521, loss_val: nan, pos_over_neg: 123.99349975585938 lr: 0.0001\n",
      "Iter: 1000/3021, loss_train: 6.5636, loss_val: nan, pos_over_neg: 128.74893188476562 lr: 0.0001\n",
      "Iter: 1001/3021, loss_train: 6.5651, loss_val: nan, pos_over_neg: 128.76220703125 lr: 0.0001\n",
      "Iter: 1002/3021, loss_train: 6.5365, loss_val: nan, pos_over_neg: 162.79151916503906 lr: 0.0001\n",
      "Iter: 1003/3021, loss_train: 6.5624, loss_val: nan, pos_over_neg: 137.92893981933594 lr: 0.0001\n",
      "Iter: 1004/3021, loss_train: 6.5622, loss_val: nan, pos_over_neg: 109.38249206542969 lr: 0.0001\n",
      "Iter: 1005/3021, loss_train: 6.5967, loss_val: nan, pos_over_neg: 96.18250274658203 lr: 0.0001\n",
      "Iter: 1006/3021, loss_train: 6.5585, loss_val: nan, pos_over_neg: 168.0767059326172 lr: 0.0001\n",
      "Iter: 1007/3021, loss_train: 6.5694, loss_val: nan, pos_over_neg: 113.30426788330078 lr: 0.0001\n",
      "Iter: 1008/3021, loss_train: 6.5328, loss_val: nan, pos_over_neg: 134.15045166015625 lr: 0.0001\n",
      "Iter: 1009/3021, loss_train: 6.5668, loss_val: nan, pos_over_neg: 143.8456268310547 lr: 0.0001\n",
      "Iter: 1010/3021, loss_train: 6.5732, loss_val: nan, pos_over_neg: 149.912841796875 lr: 0.0001\n",
      "Iter: 1011/3021, loss_train: 6.589, loss_val: nan, pos_over_neg: 130.81472778320312 lr: 0.0001\n",
      "Iter: 1012/3021, loss_train: 6.557, loss_val: nan, pos_over_neg: 101.3158187866211 lr: 0.0001\n",
      "Iter: 1013/3021, loss_train: 6.5494, loss_val: nan, pos_over_neg: 114.39527130126953 lr: 0.0001\n",
      "Iter: 1014/3021, loss_train: 6.5843, loss_val: nan, pos_over_neg: 65.06259155273438 lr: 0.0001\n",
      "Iter: 1015/3021, loss_train: 6.5578, loss_val: nan, pos_over_neg: 88.88512420654297 lr: 0.0001\n",
      "Iter: 1016/3021, loss_train: 6.5502, loss_val: nan, pos_over_neg: 99.96611785888672 lr: 0.0001\n",
      "Iter: 1017/3021, loss_train: 6.5653, loss_val: nan, pos_over_neg: 121.93951416015625 lr: 0.0001\n",
      "Iter: 1018/3021, loss_train: 6.5709, loss_val: nan, pos_over_neg: 144.9116973876953 lr: 0.0001\n",
      "Iter: 1019/3021, loss_train: 6.5296, loss_val: nan, pos_over_neg: 171.37176513671875 lr: 0.0001\n",
      "Iter: 1020/3021, loss_train: 6.5519, loss_val: nan, pos_over_neg: 148.95033264160156 lr: 0.0001\n",
      "Iter: 1021/3021, loss_train: 6.5588, loss_val: nan, pos_over_neg: 93.57799530029297 lr: 0.0001\n",
      "Iter: 1022/3021, loss_train: 6.5682, loss_val: nan, pos_over_neg: 88.42861938476562 lr: 0.0001\n",
      "Iter: 1023/3021, loss_train: 6.5914, loss_val: nan, pos_over_neg: 112.10120391845703 lr: 0.0001\n",
      "Iter: 1024/3021, loss_train: 6.5622, loss_val: nan, pos_over_neg: 123.60809326171875 lr: 0.0001\n",
      "Iter: 1025/3021, loss_train: 6.5791, loss_val: nan, pos_over_neg: 151.73696899414062 lr: 0.0001\n",
      "Iter: 1026/3021, loss_train: 6.529, loss_val: nan, pos_over_neg: 179.13230895996094 lr: 0.0001\n",
      "Iter: 1027/3021, loss_train: 6.5545, loss_val: nan, pos_over_neg: 128.44247436523438 lr: 0.0001\n",
      "Iter: 1028/3021, loss_train: 6.5917, loss_val: nan, pos_over_neg: 85.08525085449219 lr: 0.0001\n",
      "Iter: 1029/3021, loss_train: 6.5903, loss_val: nan, pos_over_neg: 117.38317108154297 lr: 0.0001\n",
      "Iter: 1030/3021, loss_train: 6.5618, loss_val: nan, pos_over_neg: 84.99217224121094 lr: 0.0001\n",
      "Iter: 1031/3021, loss_train: 6.5515, loss_val: nan, pos_over_neg: 113.74560546875 lr: 0.0001\n",
      "Iter: 1032/3021, loss_train: 6.5423, loss_val: nan, pos_over_neg: 95.15591430664062 lr: 0.0001\n",
      "Iter: 1033/3021, loss_train: 6.5462, loss_val: nan, pos_over_neg: 90.83683776855469 lr: 0.0001\n",
      "Iter: 1034/3021, loss_train: 6.546, loss_val: nan, pos_over_neg: 115.88871002197266 lr: 0.0001\n",
      "Iter: 1035/3021, loss_train: 6.5418, loss_val: nan, pos_over_neg: 95.48934936523438 lr: 0.0001\n",
      "Iter: 1036/3021, loss_train: 6.5519, loss_val: nan, pos_over_neg: 81.65290832519531 lr: 0.0001\n",
      "Iter: 1037/3021, loss_train: 6.5375, loss_val: nan, pos_over_neg: 92.3565902709961 lr: 0.0001\n",
      "Iter: 1038/3021, loss_train: 6.5658, loss_val: nan, pos_over_neg: 94.41768646240234 lr: 0.0001\n",
      "Iter: 1039/3021, loss_train: 6.5579, loss_val: nan, pos_over_neg: 87.00884246826172 lr: 0.0001\n",
      "Iter: 1040/3021, loss_train: 6.5252, loss_val: nan, pos_over_neg: 233.80630493164062 lr: 0.0001\n",
      "Iter: 1041/3021, loss_train: 6.5486, loss_val: nan, pos_over_neg: 124.86923217773438 lr: 0.0001\n",
      "Iter: 1042/3021, loss_train: 6.5587, loss_val: nan, pos_over_neg: 152.30992126464844 lr: 0.0001\n",
      "Iter: 1043/3021, loss_train: 6.5555, loss_val: nan, pos_over_neg: 170.29432678222656 lr: 0.0001\n",
      "Iter: 1044/3021, loss_train: 6.5456, loss_val: nan, pos_over_neg: 159.6529998779297 lr: 0.0001\n",
      "Iter: 1045/3021, loss_train: 6.5535, loss_val: nan, pos_over_neg: 147.63246154785156 lr: 0.0001\n",
      "Iter: 1046/3021, loss_train: 6.5443, loss_val: nan, pos_over_neg: 171.78207397460938 lr: 0.0001\n",
      "Iter: 1047/3021, loss_train: 6.5638, loss_val: nan, pos_over_neg: 130.86581420898438 lr: 0.0001\n",
      "Iter: 1048/3021, loss_train: 6.5485, loss_val: nan, pos_over_neg: 115.09162139892578 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1049/3021, loss_train: 6.5706, loss_val: nan, pos_over_neg: 91.85337829589844 lr: 0.0001\n",
      "Iter: 1050/3021, loss_train: 6.5687, loss_val: nan, pos_over_neg: 79.33258819580078 lr: 0.0001\n",
      "Iter: 1051/3021, loss_train: 6.5701, loss_val: nan, pos_over_neg: 98.234130859375 lr: 0.0001\n",
      "Iter: 1052/3021, loss_train: 6.5457, loss_val: nan, pos_over_neg: 135.73501586914062 lr: 0.0001\n",
      "Iter: 1053/3021, loss_train: 6.6161, loss_val: nan, pos_over_neg: 97.24614715576172 lr: 0.0001\n",
      "Iter: 1054/3021, loss_train: 6.5773, loss_val: nan, pos_over_neg: 144.2130584716797 lr: 0.0001\n",
      "Iter: 1055/3021, loss_train: 6.5936, loss_val: nan, pos_over_neg: 140.12806701660156 lr: 0.0001\n",
      "Iter: 1056/3021, loss_train: 6.5473, loss_val: nan, pos_over_neg: 139.00344848632812 lr: 0.0001\n",
      "Iter: 1057/3021, loss_train: 6.5726, loss_val: nan, pos_over_neg: 176.90676879882812 lr: 0.0001\n",
      "Iter: 1058/3021, loss_train: 6.5625, loss_val: nan, pos_over_neg: 96.17406463623047 lr: 0.0001\n",
      "Iter: 1059/3021, loss_train: 6.554, loss_val: nan, pos_over_neg: 134.52003479003906 lr: 0.0001\n",
      "Iter: 1060/3021, loss_train: 6.5687, loss_val: nan, pos_over_neg: 124.674072265625 lr: 0.0001\n",
      "Iter: 1061/3021, loss_train: 6.5615, loss_val: nan, pos_over_neg: 66.01492309570312 lr: 0.0001\n",
      "Iter: 1062/3021, loss_train: 6.5723, loss_val: nan, pos_over_neg: 90.07743835449219 lr: 0.0001\n",
      "Iter: 1063/3021, loss_train: 6.5684, loss_val: nan, pos_over_neg: 90.30357360839844 lr: 0.0001\n",
      "Iter: 1064/3021, loss_train: 6.5807, loss_val: nan, pos_over_neg: 95.09054565429688 lr: 0.0001\n",
      "Iter: 1065/3021, loss_train: 6.5552, loss_val: nan, pos_over_neg: 177.9790496826172 lr: 0.0001\n",
      "Iter: 1066/3021, loss_train: 6.5639, loss_val: nan, pos_over_neg: 175.79324340820312 lr: 0.0001\n",
      "Iter: 1067/3021, loss_train: 6.5611, loss_val: nan, pos_over_neg: 125.10030364990234 lr: 0.0001\n",
      "Iter: 1068/3021, loss_train: 6.5624, loss_val: nan, pos_over_neg: 141.9289093017578 lr: 0.0001\n",
      "Iter: 1069/3021, loss_train: 6.5569, loss_val: nan, pos_over_neg: 99.77181243896484 lr: 0.0001\n",
      "Iter: 1070/3021, loss_train: 6.5313, loss_val: nan, pos_over_neg: 117.2889633178711 lr: 0.0001\n",
      "Iter: 1071/3021, loss_train: 6.5339, loss_val: nan, pos_over_neg: 127.74199676513672 lr: 0.0001\n",
      "Iter: 1072/3021, loss_train: 6.5615, loss_val: nan, pos_over_neg: 95.8836669921875 lr: 0.0001\n",
      "Iter: 1073/3021, loss_train: 6.5763, loss_val: nan, pos_over_neg: 69.35871887207031 lr: 0.0001\n",
      "Iter: 1074/3021, loss_train: 6.5642, loss_val: nan, pos_over_neg: 67.939697265625 lr: 0.0001\n",
      "Iter: 1075/3021, loss_train: 6.5749, loss_val: nan, pos_over_neg: 101.53254699707031 lr: 0.0001\n",
      "Iter: 1076/3021, loss_train: 6.5665, loss_val: nan, pos_over_neg: 86.59687042236328 lr: 0.0001\n",
      "Iter: 1077/3021, loss_train: 6.5483, loss_val: nan, pos_over_neg: 89.79962921142578 lr: 0.0001\n",
      "Iter: 1078/3021, loss_train: 6.5474, loss_val: nan, pos_over_neg: 137.18023681640625 lr: 0.0001\n",
      "Iter: 1079/3021, loss_train: 6.5678, loss_val: nan, pos_over_neg: 167.05503845214844 lr: 0.0001\n",
      "Iter: 1080/3021, loss_train: 6.5317, loss_val: nan, pos_over_neg: 170.24578857421875 lr: 0.0001\n",
      "Iter: 1081/3021, loss_train: 6.5712, loss_val: nan, pos_over_neg: 116.48511505126953 lr: 0.0001\n",
      "Iter: 1082/3021, loss_train: 6.5686, loss_val: nan, pos_over_neg: 129.07115173339844 lr: 0.0001\n",
      "Iter: 1083/3021, loss_train: 6.5574, loss_val: nan, pos_over_neg: 107.0772933959961 lr: 0.0001\n",
      "Iter: 1084/3021, loss_train: 6.5781, loss_val: nan, pos_over_neg: 129.50759887695312 lr: 0.0001\n",
      "Iter: 1085/3021, loss_train: 6.5601, loss_val: nan, pos_over_neg: 112.87629699707031 lr: 0.0001\n",
      "Iter: 1086/3021, loss_train: 6.5874, loss_val: nan, pos_over_neg: 131.88233947753906 lr: 0.0001\n",
      "Iter: 1087/3021, loss_train: 6.5654, loss_val: nan, pos_over_neg: 127.1209487915039 lr: 0.0001\n",
      "Iter: 1088/3021, loss_train: 6.5623, loss_val: nan, pos_over_neg: 95.28174591064453 lr: 0.0001\n",
      "Iter: 1089/3021, loss_train: 6.5412, loss_val: nan, pos_over_neg: 101.14495086669922 lr: 0.0001\n",
      "Iter: 1090/3021, loss_train: 6.5633, loss_val: nan, pos_over_neg: 94.36071014404297 lr: 0.0001\n",
      "Iter: 1091/3021, loss_train: 6.5525, loss_val: nan, pos_over_neg: 108.02278900146484 lr: 0.0001\n",
      "Iter: 1092/3021, loss_train: 6.5256, loss_val: nan, pos_over_neg: 99.59136199951172 lr: 0.0001\n",
      "Iter: 1093/3021, loss_train: 6.5575, loss_val: nan, pos_over_neg: 84.4683609008789 lr: 0.0001\n",
      "Iter: 1094/3021, loss_train: 6.5463, loss_val: nan, pos_over_neg: 99.27639770507812 lr: 0.0001\n",
      "Iter: 1095/3021, loss_train: 6.5591, loss_val: nan, pos_over_neg: 98.9499740600586 lr: 0.0001\n",
      "Iter: 1096/3021, loss_train: 6.5676, loss_val: nan, pos_over_neg: 104.38545227050781 lr: 0.0001\n",
      "Iter: 1097/3021, loss_train: 6.5317, loss_val: nan, pos_over_neg: 120.65325164794922 lr: 0.0001\n",
      "Iter: 1098/3021, loss_train: 6.554, loss_val: nan, pos_over_neg: 100.06576538085938 lr: 0.0001\n",
      "Iter: 1099/3021, loss_train: 6.5546, loss_val: nan, pos_over_neg: 93.54898834228516 lr: 0.0001\n",
      "Iter: 1100/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 105.59757995605469 lr: 0.0001\n",
      "Iter: 1101/3021, loss_train: 6.5466, loss_val: nan, pos_over_neg: 130.14810180664062 lr: 0.0001\n",
      "Iter: 1102/3021, loss_train: 6.5606, loss_val: nan, pos_over_neg: 167.6495361328125 lr: 0.0001\n",
      "Iter: 1103/3021, loss_train: 6.5789, loss_val: nan, pos_over_neg: 148.4233856201172 lr: 0.0001\n",
      "Iter: 1104/3021, loss_train: 6.5402, loss_val: nan, pos_over_neg: 189.18299865722656 lr: 0.0001\n",
      "Iter: 1105/3021, loss_train: 6.5418, loss_val: nan, pos_over_neg: 106.33514404296875 lr: 0.0001\n",
      "Iter: 1106/3021, loss_train: 6.5436, loss_val: nan, pos_over_neg: 111.0435562133789 lr: 0.0001\n",
      "Iter: 1107/3021, loss_train: 6.5874, loss_val: nan, pos_over_neg: 73.64689636230469 lr: 0.0001\n",
      "Iter: 1108/3021, loss_train: 6.5723, loss_val: nan, pos_over_neg: 97.56181335449219 lr: 0.0001\n",
      "Iter: 1109/3021, loss_train: 6.5272, loss_val: nan, pos_over_neg: 169.40899658203125 lr: 0.0001\n",
      "Iter: 1110/3021, loss_train: 6.5657, loss_val: nan, pos_over_neg: 89.20277404785156 lr: 0.0001\n",
      "Iter: 1111/3021, loss_train: 6.5521, loss_val: nan, pos_over_neg: 123.1502685546875 lr: 0.0001\n",
      "Iter: 1112/3021, loss_train: 6.588, loss_val: nan, pos_over_neg: 80.20752716064453 lr: 0.0001\n",
      "Iter: 1113/3021, loss_train: 6.559, loss_val: nan, pos_over_neg: 97.43718719482422 lr: 0.0001\n",
      "Iter: 1114/3021, loss_train: 6.5641, loss_val: nan, pos_over_neg: 130.28390502929688 lr: 0.0001\n",
      "Iter: 1115/3021, loss_train: 6.5602, loss_val: nan, pos_over_neg: 122.86337280273438 lr: 0.0001\n",
      "Iter: 1116/3021, loss_train: 6.581, loss_val: nan, pos_over_neg: 134.76904296875 lr: 0.0001\n",
      "Iter: 1117/3021, loss_train: 6.5472, loss_val: nan, pos_over_neg: 118.40300750732422 lr: 0.0001\n",
      "Iter: 1118/3021, loss_train: 6.5777, loss_val: nan, pos_over_neg: 85.3359375 lr: 0.0001\n",
      "Iter: 1119/3021, loss_train: 6.5696, loss_val: nan, pos_over_neg: 87.93347930908203 lr: 0.0001\n",
      "Iter: 1120/3021, loss_train: 6.5488, loss_val: nan, pos_over_neg: 150.75257873535156 lr: 0.0001\n",
      "Iter: 1121/3021, loss_train: 6.5614, loss_val: nan, pos_over_neg: 110.06259155273438 lr: 0.0001\n",
      "Iter: 1122/3021, loss_train: 6.5454, loss_val: nan, pos_over_neg: 122.97063446044922 lr: 0.0001\n",
      "Iter: 1123/3021, loss_train: 6.5762, loss_val: nan, pos_over_neg: 96.2569580078125 lr: 0.0001\n",
      "Iter: 1124/3021, loss_train: 6.569, loss_val: nan, pos_over_neg: 80.63849639892578 lr: 0.0001\n",
      "Iter: 1125/3021, loss_train: 6.5528, loss_val: nan, pos_over_neg: 113.20484161376953 lr: 0.0001\n",
      "Iter: 1126/3021, loss_train: 6.5478, loss_val: nan, pos_over_neg: 114.72673034667969 lr: 0.0001\n",
      "Iter: 1127/3021, loss_train: 6.5724, loss_val: nan, pos_over_neg: 111.93869018554688 lr: 0.0001\n",
      "Iter: 1128/3021, loss_train: 6.5798, loss_val: nan, pos_over_neg: 80.29670715332031 lr: 0.0001\n",
      "Iter: 1129/3021, loss_train: 6.5695, loss_val: nan, pos_over_neg: 96.26571655273438 lr: 0.0001\n",
      "Iter: 1130/3021, loss_train: 6.5275, loss_val: nan, pos_over_neg: 152.13348388671875 lr: 0.0001\n",
      "Iter: 1131/3021, loss_train: 6.5404, loss_val: nan, pos_over_neg: 160.3802947998047 lr: 0.0001\n",
      "Iter: 1132/3021, loss_train: 6.5711, loss_val: nan, pos_over_neg: 130.33624267578125 lr: 0.0001\n",
      "Iter: 1133/3021, loss_train: 6.5594, loss_val: nan, pos_over_neg: 125.9030532836914 lr: 0.0001\n",
      "Iter: 1134/3021, loss_train: 6.5639, loss_val: nan, pos_over_neg: 164.9668731689453 lr: 0.0001\n",
      "Iter: 1135/3021, loss_train: 6.5862, loss_val: nan, pos_over_neg: 90.15978240966797 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1136/3021, loss_train: 6.5394, loss_val: nan, pos_over_neg: 119.95569610595703 lr: 0.0001\n",
      "Iter: 1137/3021, loss_train: 6.5649, loss_val: nan, pos_over_neg: 90.5282974243164 lr: 0.0001\n",
      "Iter: 1138/3021, loss_train: 6.5355, loss_val: nan, pos_over_neg: 106.25878143310547 lr: 0.0001\n",
      "Iter: 1139/3021, loss_train: 6.576, loss_val: nan, pos_over_neg: 78.8614730834961 lr: 0.0001\n",
      "Iter: 1140/3021, loss_train: 6.5382, loss_val: nan, pos_over_neg: 123.21450805664062 lr: 0.0001\n",
      "Iter: 1141/3021, loss_train: 6.5917, loss_val: nan, pos_over_neg: 63.884925842285156 lr: 0.0001\n",
      "Iter: 1142/3021, loss_train: 6.5079, loss_val: nan, pos_over_neg: 123.59380340576172 lr: 0.0001\n",
      "Iter: 1143/3021, loss_train: 6.5641, loss_val: nan, pos_over_neg: 85.61296081542969 lr: 0.0001\n",
      "Iter: 1144/3021, loss_train: 6.5655, loss_val: nan, pos_over_neg: 88.60919189453125 lr: 0.0001\n",
      "Iter: 1145/3021, loss_train: 6.555, loss_val: nan, pos_over_neg: 111.32308197021484 lr: 0.0001\n",
      "Iter: 1146/3021, loss_train: 6.5671, loss_val: nan, pos_over_neg: 87.8520278930664 lr: 0.0001\n",
      "Iter: 1147/3021, loss_train: 6.5492, loss_val: nan, pos_over_neg: 75.39985656738281 lr: 0.0001\n",
      "Iter: 1148/3021, loss_train: 6.5545, loss_val: nan, pos_over_neg: 76.2397232055664 lr: 0.0001\n",
      "Iter: 1149/3021, loss_train: 6.5533, loss_val: nan, pos_over_neg: 89.5519027709961 lr: 0.0001\n",
      "Iter: 1150/3021, loss_train: 6.5404, loss_val: nan, pos_over_neg: 121.98696899414062 lr: 0.0001\n",
      "Iter: 1151/3021, loss_train: 6.5444, loss_val: nan, pos_over_neg: 96.13424682617188 lr: 0.0001\n",
      "Iter: 1152/3021, loss_train: 6.5473, loss_val: nan, pos_over_neg: 120.13858795166016 lr: 0.0001\n",
      "Iter: 1153/3021, loss_train: 6.5538, loss_val: nan, pos_over_neg: 139.71617126464844 lr: 0.0001\n",
      "Iter: 1154/3021, loss_train: 6.5864, loss_val: nan, pos_over_neg: 126.18536376953125 lr: 0.0001\n",
      "Iter: 1155/3021, loss_train: 6.5427, loss_val: nan, pos_over_neg: 184.7357177734375 lr: 0.0001\n",
      "Iter: 1156/3021, loss_train: 6.5492, loss_val: nan, pos_over_neg: 157.4878387451172 lr: 0.0001\n",
      "Iter: 1157/3021, loss_train: 6.5469, loss_val: nan, pos_over_neg: 158.6617889404297 lr: 0.0001\n",
      "Iter: 1158/3021, loss_train: 6.5301, loss_val: nan, pos_over_neg: 212.74378967285156 lr: 0.0001\n",
      "Iter: 1159/3021, loss_train: 6.5537, loss_val: nan, pos_over_neg: 158.07655334472656 lr: 0.0001\n",
      "Iter: 1160/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 143.67477416992188 lr: 0.0001\n",
      "Iter: 1161/3021, loss_train: 6.5534, loss_val: nan, pos_over_neg: 124.69242095947266 lr: 0.0001\n",
      "Iter: 1162/3021, loss_train: 6.5741, loss_val: nan, pos_over_neg: 111.05257415771484 lr: 0.0001\n",
      "Iter: 1163/3021, loss_train: 6.5365, loss_val: nan, pos_over_neg: 159.5880889892578 lr: 0.0001\n",
      "Iter: 1164/3021, loss_train: 6.56, loss_val: nan, pos_over_neg: 105.43751525878906 lr: 0.0001\n",
      "Iter: 1165/3021, loss_train: 6.5411, loss_val: nan, pos_over_neg: 82.78956604003906 lr: 0.0001\n",
      "Iter: 1166/3021, loss_train: 6.5496, loss_val: nan, pos_over_neg: 80.70234680175781 lr: 0.0001\n",
      "Iter: 1167/3021, loss_train: 6.5428, loss_val: nan, pos_over_neg: 132.767822265625 lr: 0.0001\n",
      "Iter: 1168/3021, loss_train: 6.5644, loss_val: nan, pos_over_neg: 120.5602035522461 lr: 0.0001\n",
      "Iter: 1169/3021, loss_train: 6.5419, loss_val: nan, pos_over_neg: 157.56260681152344 lr: 0.0001\n",
      "Iter: 1170/3021, loss_train: 6.5634, loss_val: nan, pos_over_neg: 131.17544555664062 lr: 0.0001\n",
      "Iter: 1171/3021, loss_train: 6.5475, loss_val: nan, pos_over_neg: 106.12963104248047 lr: 0.0001\n",
      "Iter: 1172/3021, loss_train: 6.5275, loss_val: nan, pos_over_neg: 239.54739379882812 lr: 0.0001\n",
      "Iter: 1173/3021, loss_train: 6.577, loss_val: nan, pos_over_neg: 118.03659057617188 lr: 0.0001\n",
      "Iter: 1174/3021, loss_train: 6.5766, loss_val: nan, pos_over_neg: 122.97052764892578 lr: 0.0001\n",
      "Iter: 1175/3021, loss_train: 6.556, loss_val: nan, pos_over_neg: 134.83544921875 lr: 0.0001\n",
      "Iter: 1176/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 150.59104919433594 lr: 0.0001\n",
      "Iter: 1177/3021, loss_train: 6.5583, loss_val: nan, pos_over_neg: 108.08056640625 lr: 0.0001\n",
      "Iter: 1178/3021, loss_train: 6.5479, loss_val: nan, pos_over_neg: 123.55958557128906 lr: 0.0001\n",
      "Iter: 1179/3021, loss_train: 6.5462, loss_val: nan, pos_over_neg: 100.70854187011719 lr: 0.0001\n",
      "Iter: 1180/3021, loss_train: 6.5529, loss_val: nan, pos_over_neg: 90.99481964111328 lr: 0.0001\n",
      "Iter: 1181/3021, loss_train: 6.558, loss_val: nan, pos_over_neg: 117.31495666503906 lr: 0.0001\n",
      "Iter: 1182/3021, loss_train: 6.5338, loss_val: nan, pos_over_neg: 109.80200958251953 lr: 0.0001\n",
      "Iter: 1183/3021, loss_train: 6.5505, loss_val: nan, pos_over_neg: 120.27717590332031 lr: 0.0001\n",
      "Iter: 1184/3021, loss_train: 6.5675, loss_val: nan, pos_over_neg: 137.64578247070312 lr: 0.0001\n",
      "Iter: 1185/3021, loss_train: 6.5485, loss_val: nan, pos_over_neg: 187.39230346679688 lr: 0.0001\n",
      "Iter: 1186/3021, loss_train: 6.5477, loss_val: nan, pos_over_neg: 89.7188720703125 lr: 0.0001\n",
      "Iter: 1187/3021, loss_train: 6.5641, loss_val: nan, pos_over_neg: 100.98941802978516 lr: 0.0001\n",
      "Iter: 1188/3021, loss_train: 6.5599, loss_val: nan, pos_over_neg: 137.4811553955078 lr: 0.0001\n",
      "Iter: 1189/3021, loss_train: 6.5578, loss_val: nan, pos_over_neg: 153.5692901611328 lr: 0.0001\n",
      "Iter: 1190/3021, loss_train: 6.5558, loss_val: nan, pos_over_neg: 136.82460021972656 lr: 0.0001\n",
      "Iter: 1191/3021, loss_train: 6.5434, loss_val: nan, pos_over_neg: 113.12967681884766 lr: 0.0001\n",
      "Iter: 1192/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 101.66890716552734 lr: 0.0001\n",
      "Iter: 1193/3021, loss_train: 6.571, loss_val: nan, pos_over_neg: 99.5158462524414 lr: 0.0001\n",
      "Iter: 1194/3021, loss_train: 6.5623, loss_val: nan, pos_over_neg: 63.66477584838867 lr: 0.0001\n",
      "Iter: 1195/3021, loss_train: 6.5512, loss_val: nan, pos_over_neg: 85.29916381835938 lr: 0.0001\n",
      "Iter: 1196/3021, loss_train: 6.5533, loss_val: nan, pos_over_neg: 74.28507232666016 lr: 0.0001\n",
      "Iter: 1197/3021, loss_train: 6.5207, loss_val: nan, pos_over_neg: 142.7708282470703 lr: 0.0001\n",
      "Iter: 1198/3021, loss_train: 6.5599, loss_val: nan, pos_over_neg: 100.34314727783203 lr: 0.0001\n",
      "Iter: 1199/3021, loss_train: 6.5583, loss_val: nan, pos_over_neg: 112.0466079711914 lr: 0.0001\n",
      "Iter: 1200/3021, loss_train: 6.5638, loss_val: nan, pos_over_neg: 157.35714721679688 lr: 0.0001\n",
      "Iter: 1201/3021, loss_train: 6.5791, loss_val: nan, pos_over_neg: 112.96791076660156 lr: 0.0001\n",
      "Iter: 1202/3021, loss_train: 6.5594, loss_val: nan, pos_over_neg: 86.17133331298828 lr: 0.0001\n",
      "Iter: 1203/3021, loss_train: 6.5435, loss_val: nan, pos_over_neg: 181.22665405273438 lr: 0.0001\n",
      "Iter: 1204/3021, loss_train: 6.5455, loss_val: nan, pos_over_neg: 143.88760375976562 lr: 0.0001\n",
      "Iter: 1205/3021, loss_train: 6.5472, loss_val: nan, pos_over_neg: 128.3787078857422 lr: 0.0001\n",
      "Iter: 1206/3021, loss_train: 6.5342, loss_val: nan, pos_over_neg: 143.53567504882812 lr: 0.0001\n",
      "Iter: 1207/3021, loss_train: 6.5399, loss_val: nan, pos_over_neg: 112.56072235107422 lr: 0.0001\n",
      "Iter: 1208/3021, loss_train: 6.5757, loss_val: nan, pos_over_neg: 80.9377212524414 lr: 0.0001\n",
      "Iter: 1209/3021, loss_train: 6.5363, loss_val: nan, pos_over_neg: 92.18399047851562 lr: 0.0001\n",
      "Iter: 1210/3021, loss_train: 6.5448, loss_val: nan, pos_over_neg: 91.5162353515625 lr: 0.0001\n",
      "Iter: 1211/3021, loss_train: 6.5591, loss_val: nan, pos_over_neg: 100.35796356201172 lr: 0.0001\n",
      "Iter: 1212/3021, loss_train: 6.5843, loss_val: nan, pos_over_neg: 93.70599365234375 lr: 0.0001\n",
      "Iter: 1213/3021, loss_train: 6.5416, loss_val: nan, pos_over_neg: 184.39768981933594 lr: 0.0001\n",
      "Iter: 1214/3021, loss_train: 6.5186, loss_val: nan, pos_over_neg: 218.15155029296875 lr: 0.0001\n",
      "Iter: 1215/3021, loss_train: 6.5635, loss_val: nan, pos_over_neg: 129.10272216796875 lr: 0.0001\n",
      "Iter: 1216/3021, loss_train: 6.5289, loss_val: nan, pos_over_neg: 147.54002380371094 lr: 0.0001\n",
      "Iter: 1217/3021, loss_train: 6.5515, loss_val: nan, pos_over_neg: 191.55245971679688 lr: 0.0001\n",
      "Iter: 1218/3021, loss_train: 6.5476, loss_val: nan, pos_over_neg: 102.46466827392578 lr: 0.0001\n",
      "Iter: 1219/3021, loss_train: 6.5665, loss_val: nan, pos_over_neg: 163.08822631835938 lr: 0.0001\n",
      "Iter: 1220/3021, loss_train: 6.5438, loss_val: nan, pos_over_neg: 166.6717071533203 lr: 0.0001\n",
      "Iter: 1221/3021, loss_train: 6.59, loss_val: nan, pos_over_neg: 96.75605773925781 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1222/3021, loss_train: 6.5461, loss_val: nan, pos_over_neg: 149.07638549804688 lr: 0.0001\n",
      "Iter: 1223/3021, loss_train: 6.5691, loss_val: nan, pos_over_neg: 113.75347137451172 lr: 0.0001\n",
      "Iter: 1224/3021, loss_train: 6.5747, loss_val: nan, pos_over_neg: 92.91566467285156 lr: 0.0001\n",
      "Iter: 1225/3021, loss_train: 6.545, loss_val: nan, pos_over_neg: 137.8286590576172 lr: 0.0001\n",
      "Iter: 1226/3021, loss_train: 6.5504, loss_val: nan, pos_over_neg: 119.91869354248047 lr: 0.0001\n",
      "Iter: 1227/3021, loss_train: 6.5187, loss_val: nan, pos_over_neg: 156.17721557617188 lr: 0.0001\n",
      "Iter: 1228/3021, loss_train: 6.5433, loss_val: nan, pos_over_neg: 119.5831298828125 lr: 0.0001\n",
      "Iter: 1229/3021, loss_train: 6.5768, loss_val: nan, pos_over_neg: 124.85914611816406 lr: 0.0001\n",
      "Iter: 1230/3021, loss_train: 6.5523, loss_val: nan, pos_over_neg: 125.78163146972656 lr: 0.0001\n",
      "Iter: 1231/3021, loss_train: 6.5676, loss_val: nan, pos_over_neg: 123.45030212402344 lr: 0.0001\n",
      "Iter: 1232/3021, loss_train: 6.5727, loss_val: nan, pos_over_neg: 93.78382873535156 lr: 0.0001\n",
      "Iter: 1233/3021, loss_train: 6.5629, loss_val: nan, pos_over_neg: 119.75103759765625 lr: 0.0001\n",
      "Iter: 1234/3021, loss_train: 6.5329, loss_val: nan, pos_over_neg: 154.4593505859375 lr: 0.0001\n",
      "Iter: 1235/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 169.43875122070312 lr: 0.0001\n",
      "Iter: 1236/3021, loss_train: 6.58, loss_val: nan, pos_over_neg: 115.04232788085938 lr: 0.0001\n",
      "Iter: 1237/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 114.85460662841797 lr: 0.0001\n",
      "Iter: 1238/3021, loss_train: 6.5528, loss_val: nan, pos_over_neg: 148.86558532714844 lr: 0.0001\n",
      "Iter: 1239/3021, loss_train: 6.5596, loss_val: nan, pos_over_neg: 155.594970703125 lr: 0.0001\n",
      "Iter: 1240/3021, loss_train: 6.5689, loss_val: nan, pos_over_neg: 130.4134979248047 lr: 0.0001\n",
      "Iter: 1241/3021, loss_train: 6.5446, loss_val: nan, pos_over_neg: 152.04212951660156 lr: 0.0001\n",
      "Iter: 1242/3021, loss_train: 6.5323, loss_val: nan, pos_over_neg: 154.4732666015625 lr: 0.0001\n",
      "Iter: 1243/3021, loss_train: 6.5502, loss_val: nan, pos_over_neg: 108.59349822998047 lr: 0.0001\n",
      "Iter: 1244/3021, loss_train: 6.5607, loss_val: nan, pos_over_neg: 104.43131256103516 lr: 0.0001\n",
      "Iter: 1245/3021, loss_train: 6.546, loss_val: nan, pos_over_neg: 128.20803833007812 lr: 0.0001\n",
      "Iter: 1246/3021, loss_train: 6.5267, loss_val: nan, pos_over_neg: 101.76539611816406 lr: 0.0001\n",
      "Iter: 1247/3021, loss_train: 6.5742, loss_val: nan, pos_over_neg: 73.83673858642578 lr: 0.0001\n",
      "Iter: 1248/3021, loss_train: 6.5678, loss_val: nan, pos_over_neg: 61.014198303222656 lr: 0.0001\n",
      "Iter: 1249/3021, loss_train: 6.5552, loss_val: nan, pos_over_neg: 115.75252532958984 lr: 0.0001\n",
      "Iter: 1250/3021, loss_train: 6.5691, loss_val: nan, pos_over_neg: 80.7657241821289 lr: 0.0001\n",
      "Iter: 1251/3021, loss_train: 6.5542, loss_val: nan, pos_over_neg: 106.46399688720703 lr: 0.0001\n",
      "Iter: 1252/3021, loss_train: 6.5364, loss_val: nan, pos_over_neg: 128.98373413085938 lr: 0.0001\n",
      "Iter: 1253/3021, loss_train: 6.5734, loss_val: nan, pos_over_neg: 124.71623229980469 lr: 0.0001\n",
      "Iter: 1254/3021, loss_train: 6.5417, loss_val: nan, pos_over_neg: 200.8120574951172 lr: 0.0001\n",
      "Iter: 1255/3021, loss_train: 6.5432, loss_val: nan, pos_over_neg: 286.035400390625 lr: 0.0001\n",
      "Iter: 1256/3021, loss_train: 6.5602, loss_val: nan, pos_over_neg: 161.84022521972656 lr: 0.0001\n",
      "Iter: 1257/3021, loss_train: 6.5686, loss_val: nan, pos_over_neg: 123.6350326538086 lr: 0.0001\n",
      "Iter: 1258/3021, loss_train: 6.5545, loss_val: nan, pos_over_neg: 106.75316619873047 lr: 0.0001\n",
      "Iter: 1259/3021, loss_train: 6.5719, loss_val: nan, pos_over_neg: 85.23088073730469 lr: 0.0001\n",
      "Iter: 1260/3021, loss_train: 6.5545, loss_val: nan, pos_over_neg: 89.63703918457031 lr: 0.0001\n",
      "Iter: 1261/3021, loss_train: 6.5605, loss_val: nan, pos_over_neg: 84.4100112915039 lr: 0.0001\n",
      "Iter: 1262/3021, loss_train: 6.5758, loss_val: nan, pos_over_neg: 68.71784973144531 lr: 0.0001\n",
      "Iter: 1263/3021, loss_train: 6.5398, loss_val: nan, pos_over_neg: 99.3289566040039 lr: 0.0001\n",
      "Iter: 1264/3021, loss_train: 6.5377, loss_val: nan, pos_over_neg: 102.140869140625 lr: 0.0001\n",
      "Iter: 1265/3021, loss_train: 6.5708, loss_val: nan, pos_over_neg: 91.67179107666016 lr: 0.0001\n",
      "Iter: 1266/3021, loss_train: 6.5453, loss_val: nan, pos_over_neg: 130.1385955810547 lr: 0.0001\n",
      "Iter: 1267/3021, loss_train: 6.5473, loss_val: nan, pos_over_neg: 177.2550506591797 lr: 0.0001\n",
      "Iter: 1268/3021, loss_train: 6.5398, loss_val: nan, pos_over_neg: 202.9252166748047 lr: 0.0001\n",
      "Iter: 1269/3021, loss_train: 6.5244, loss_val: nan, pos_over_neg: 215.8631134033203 lr: 0.0001\n",
      "Iter: 1270/3021, loss_train: 6.5659, loss_val: nan, pos_over_neg: 135.51080322265625 lr: 0.0001\n",
      "Iter: 1271/3021, loss_train: 6.5615, loss_val: nan, pos_over_neg: 139.74563598632812 lr: 0.0001\n",
      "Iter: 1272/3021, loss_train: 6.5534, loss_val: nan, pos_over_neg: 109.19945526123047 lr: 0.0001\n",
      "Iter: 1273/3021, loss_train: 6.5394, loss_val: nan, pos_over_neg: 93.68409729003906 lr: 0.0001\n",
      "Iter: 1274/3021, loss_train: 6.5468, loss_val: nan, pos_over_neg: 98.48787689208984 lr: 0.0001\n",
      "Iter: 1275/3021, loss_train: 6.5243, loss_val: nan, pos_over_neg: 171.66793823242188 lr: 0.0001\n",
      "Iter: 1276/3021, loss_train: 6.5579, loss_val: nan, pos_over_neg: 153.32147216796875 lr: 0.0001\n",
      "Iter: 1277/3021, loss_train: 6.5439, loss_val: nan, pos_over_neg: 141.6190948486328 lr: 0.0001\n",
      "Iter: 1278/3021, loss_train: 6.5599, loss_val: nan, pos_over_neg: 223.6033477783203 lr: 0.0001\n",
      "Iter: 1279/3021, loss_train: 6.5571, loss_val: nan, pos_over_neg: 120.36280822753906 lr: 0.0001\n",
      "Iter: 1280/3021, loss_train: 6.5682, loss_val: nan, pos_over_neg: 85.53028106689453 lr: 0.0001\n",
      "Iter: 1281/3021, loss_train: 6.53, loss_val: nan, pos_over_neg: 129.4519805908203 lr: 0.0001\n",
      "Iter: 1282/3021, loss_train: 6.5572, loss_val: nan, pos_over_neg: 72.59173583984375 lr: 0.0001\n",
      "Iter: 1283/3021, loss_train: 6.5477, loss_val: nan, pos_over_neg: 102.55599975585938 lr: 0.0001\n",
      "Iter: 1284/3021, loss_train: 6.5439, loss_val: nan, pos_over_neg: 100.9600601196289 lr: 0.0001\n",
      "Iter: 1285/3021, loss_train: 6.5691, loss_val: nan, pos_over_neg: 78.5577621459961 lr: 0.0001\n",
      "Iter: 1286/3021, loss_train: 6.5642, loss_val: nan, pos_over_neg: 87.4847640991211 lr: 0.0001\n",
      "Iter: 1287/3021, loss_train: 6.5455, loss_val: nan, pos_over_neg: 84.24140930175781 lr: 0.0001\n",
      "Iter: 1288/3021, loss_train: 6.5495, loss_val: nan, pos_over_neg: 98.19108581542969 lr: 0.0001\n",
      "Iter: 1289/3021, loss_train: 6.5441, loss_val: nan, pos_over_neg: 123.22044372558594 lr: 0.0001\n",
      "Iter: 1290/3021, loss_train: 6.5529, loss_val: nan, pos_over_neg: 172.4075164794922 lr: 0.0001\n",
      "Iter: 1291/3021, loss_train: 6.5315, loss_val: nan, pos_over_neg: 131.27110290527344 lr: 0.0001\n",
      "Iter: 1292/3021, loss_train: 6.5491, loss_val: nan, pos_over_neg: 204.80812072753906 lr: 0.0001\n",
      "Iter: 1293/3021, loss_train: 6.5572, loss_val: nan, pos_over_neg: 165.26226806640625 lr: 0.0001\n",
      "Iter: 1294/3021, loss_train: 6.5465, loss_val: nan, pos_over_neg: 161.7224578857422 lr: 0.0001\n",
      "Iter: 1295/3021, loss_train: 6.5683, loss_val: nan, pos_over_neg: 79.034912109375 lr: 0.0001\n",
      "Iter: 1296/3021, loss_train: 6.5533, loss_val: nan, pos_over_neg: 107.2251205444336 lr: 0.0001\n",
      "Iter: 1297/3021, loss_train: 6.5759, loss_val: nan, pos_over_neg: 80.36820220947266 lr: 0.0001\n",
      "Iter: 1298/3021, loss_train: 6.5607, loss_val: nan, pos_over_neg: 103.46563720703125 lr: 0.0001\n",
      "Iter: 1299/3021, loss_train: 6.5496, loss_val: nan, pos_over_neg: 143.42477416992188 lr: 0.0001\n",
      "Iter: 1300/3021, loss_train: 6.5749, loss_val: nan, pos_over_neg: 126.2432632446289 lr: 0.0001\n",
      "Iter: 1301/3021, loss_train: 6.5432, loss_val: nan, pos_over_neg: 186.59707641601562 lr: 0.0001\n",
      "Iter: 1302/3021, loss_train: 6.5602, loss_val: nan, pos_over_neg: 92.72148895263672 lr: 0.0001\n",
      "Iter: 1303/3021, loss_train: 6.5578, loss_val: nan, pos_over_neg: 160.1922607421875 lr: 0.0001\n",
      "Iter: 1304/3021, loss_train: 6.5622, loss_val: nan, pos_over_neg: 101.7720718383789 lr: 0.0001\n",
      "Iter: 1305/3021, loss_train: 6.5414, loss_val: nan, pos_over_neg: 81.85655212402344 lr: 0.0001\n",
      "Iter: 1306/3021, loss_train: 6.5602, loss_val: nan, pos_over_neg: 78.91256713867188 lr: 0.0001\n",
      "Iter: 1307/3021, loss_train: 6.5698, loss_val: nan, pos_over_neg: 85.36412048339844 lr: 0.0001\n",
      "Iter: 1308/3021, loss_train: 6.5404, loss_val: nan, pos_over_neg: 97.39971923828125 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1309/3021, loss_train: 6.5382, loss_val: nan, pos_over_neg: 113.45020294189453 lr: 0.0001\n",
      "Iter: 1310/3021, loss_train: 6.5559, loss_val: nan, pos_over_neg: 66.12593078613281 lr: 0.0001\n",
      "Iter: 1311/3021, loss_train: 6.572, loss_val: nan, pos_over_neg: 96.0416488647461 lr: 0.0001\n",
      "Iter: 1312/3021, loss_train: 6.5592, loss_val: nan, pos_over_neg: 83.86682891845703 lr: 0.0001\n",
      "Iter: 1313/3021, loss_train: 6.5365, loss_val: nan, pos_over_neg: 155.9613800048828 lr: 0.0001\n",
      "Iter: 1314/3021, loss_train: 6.5728, loss_val: nan, pos_over_neg: 109.65827941894531 lr: 0.0001\n",
      "Iter: 1315/3021, loss_train: 6.5414, loss_val: nan, pos_over_neg: 285.1993408203125 lr: 0.0001\n",
      "Iter: 1316/3021, loss_train: 6.5163, loss_val: nan, pos_over_neg: 296.63385009765625 lr: 0.0001\n",
      "Iter: 1317/3021, loss_train: 6.5626, loss_val: nan, pos_over_neg: 122.68668365478516 lr: 0.0001\n",
      "Iter: 1318/3021, loss_train: 6.543, loss_val: nan, pos_over_neg: 144.9898223876953 lr: 0.0001\n",
      "Iter: 1319/3021, loss_train: 6.5338, loss_val: nan, pos_over_neg: 130.55421447753906 lr: 0.0001\n",
      "Iter: 1320/3021, loss_train: 6.5456, loss_val: nan, pos_over_neg: 95.900390625 lr: 0.0001\n",
      "Iter: 1321/3021, loss_train: 6.5509, loss_val: nan, pos_over_neg: 77.35923767089844 lr: 0.0001\n",
      "Iter: 1322/3021, loss_train: 6.5504, loss_val: nan, pos_over_neg: 78.77107238769531 lr: 0.0001\n",
      "Iter: 1323/3021, loss_train: 6.5376, loss_val: nan, pos_over_neg: 94.02098083496094 lr: 0.0001\n",
      "Iter: 1324/3021, loss_train: 6.5615, loss_val: nan, pos_over_neg: 77.29368591308594 lr: 0.0001\n",
      "Iter: 1325/3021, loss_train: 6.5528, loss_val: nan, pos_over_neg: 75.07646942138672 lr: 0.0001\n",
      "Iter: 1326/3021, loss_train: 6.5463, loss_val: nan, pos_over_neg: 104.67565155029297 lr: 0.0001\n",
      "Iter: 1327/3021, loss_train: 6.5683, loss_val: nan, pos_over_neg: 92.54301452636719 lr: 0.0001\n",
      "Iter: 1328/3021, loss_train: 6.5477, loss_val: nan, pos_over_neg: 97.59892272949219 lr: 0.0001\n",
      "Iter: 1329/3021, loss_train: 6.5444, loss_val: nan, pos_over_neg: 134.708984375 lr: 0.0001\n",
      "Iter: 1330/3021, loss_train: 6.5798, loss_val: nan, pos_over_neg: 141.58334350585938 lr: 0.0001\n",
      "Iter: 1331/3021, loss_train: 6.5765, loss_val: nan, pos_over_neg: 132.93711853027344 lr: 0.0001\n",
      "Iter: 1332/3021, loss_train: 6.5571, loss_val: nan, pos_over_neg: 135.2015380859375 lr: 0.0001\n",
      "Iter: 1333/3021, loss_train: 6.5473, loss_val: nan, pos_over_neg: 110.48609924316406 lr: 0.0001\n",
      "Iter: 1334/3021, loss_train: 6.5322, loss_val: nan, pos_over_neg: 138.3247528076172 lr: 0.0001\n",
      "Iter: 1335/3021, loss_train: 6.5115, loss_val: nan, pos_over_neg: 161.68350219726562 lr: 0.0001\n",
      "Iter: 1336/3021, loss_train: 6.5158, loss_val: nan, pos_over_neg: 129.98190307617188 lr: 0.0001\n",
      "Iter: 1337/3021, loss_train: 6.5781, loss_val: nan, pos_over_neg: 80.84481048583984 lr: 0.0001\n",
      "Iter: 1338/3021, loss_train: 6.544, loss_val: nan, pos_over_neg: 102.39824676513672 lr: 0.0001\n",
      "Iter: 1339/3021, loss_train: 6.5371, loss_val: nan, pos_over_neg: 97.46017456054688 lr: 0.0001\n",
      "Iter: 1340/3021, loss_train: 6.5785, loss_val: nan, pos_over_neg: 81.8985366821289 lr: 0.0001\n",
      "Iter: 1341/3021, loss_train: 6.5263, loss_val: nan, pos_over_neg: 121.41150665283203 lr: 0.0001\n",
      "Iter: 1342/3021, loss_train: 6.5626, loss_val: nan, pos_over_neg: 76.5360107421875 lr: 0.0001\n",
      "Iter: 1343/3021, loss_train: 6.5526, loss_val: nan, pos_over_neg: 120.35672760009766 lr: 0.0001\n",
      "Iter: 1344/3021, loss_train: 6.5458, loss_val: nan, pos_over_neg: 123.24153900146484 lr: 0.0001\n",
      "Iter: 1345/3021, loss_train: 6.5252, loss_val: nan, pos_over_neg: 158.6103515625 lr: 0.0001\n",
      "Iter: 1346/3021, loss_train: 6.5298, loss_val: nan, pos_over_neg: 106.43031311035156 lr: 0.0001\n",
      "Iter: 1347/3021, loss_train: 6.5353, loss_val: nan, pos_over_neg: 175.06373596191406 lr: 0.0001\n",
      "Iter: 1348/3021, loss_train: 6.5436, loss_val: nan, pos_over_neg: 170.83831787109375 lr: 0.0001\n",
      "Iter: 1349/3021, loss_train: 6.5205, loss_val: nan, pos_over_neg: 164.5174560546875 lr: 0.0001\n",
      "Iter: 1350/3021, loss_train: 6.5497, loss_val: nan, pos_over_neg: 160.152587890625 lr: 0.0001\n",
      "Iter: 1351/3021, loss_train: 6.5404, loss_val: nan, pos_over_neg: 151.25917053222656 lr: 0.0001\n",
      "Iter: 1352/3021, loss_train: 6.5488, loss_val: nan, pos_over_neg: 135.49659729003906 lr: 0.0001\n",
      "Iter: 1353/3021, loss_train: 6.5339, loss_val: nan, pos_over_neg: 92.17732238769531 lr: 0.0001\n",
      "Iter: 1354/3021, loss_train: 6.5523, loss_val: nan, pos_over_neg: 76.283203125 lr: 0.0001\n",
      "Iter: 1355/3021, loss_train: 6.567, loss_val: nan, pos_over_neg: 80.1319580078125 lr: 0.0001\n",
      "Iter: 1356/3021, loss_train: 6.5265, loss_val: nan, pos_over_neg: 93.94835662841797 lr: 0.0001\n",
      "Iter: 1357/3021, loss_train: 6.5399, loss_val: nan, pos_over_neg: 77.91294860839844 lr: 0.0001\n",
      "Iter: 1358/3021, loss_train: 6.535, loss_val: nan, pos_over_neg: 95.24079132080078 lr: 0.0001\n",
      "Iter: 1359/3021, loss_train: 6.5414, loss_val: nan, pos_over_neg: 92.6098861694336 lr: 0.0001\n",
      "Iter: 1360/3021, loss_train: 6.5252, loss_val: nan, pos_over_neg: 131.77159118652344 lr: 0.0001\n",
      "Iter: 1361/3021, loss_train: 6.5587, loss_val: nan, pos_over_neg: 151.2421112060547 lr: 0.0001\n",
      "Iter: 1362/3021, loss_train: 6.5534, loss_val: nan, pos_over_neg: 194.6182098388672 lr: 0.0001\n",
      "Iter: 1363/3021, loss_train: 6.5321, loss_val: nan, pos_over_neg: 150.68980407714844 lr: 0.0001\n",
      "Iter: 1364/3021, loss_train: 6.5594, loss_val: nan, pos_over_neg: 125.02655029296875 lr: 0.0001\n",
      "Iter: 1365/3021, loss_train: 6.5375, loss_val: nan, pos_over_neg: 110.2142333984375 lr: 0.0001\n",
      "Iter: 1366/3021, loss_train: 6.539, loss_val: nan, pos_over_neg: 93.06954193115234 lr: 0.0001\n",
      "Iter: 1367/3021, loss_train: 6.5256, loss_val: nan, pos_over_neg: 164.77500915527344 lr: 0.0001\n",
      "Iter: 1368/3021, loss_train: 6.5525, loss_val: nan, pos_over_neg: 136.70767211914062 lr: 0.0001\n",
      "Iter: 1369/3021, loss_train: 6.5393, loss_val: nan, pos_over_neg: 71.88436126708984 lr: 0.0001\n",
      "Iter: 1370/3021, loss_train: 6.5638, loss_val: nan, pos_over_neg: 92.97628021240234 lr: 0.0001\n",
      "Iter: 1371/3021, loss_train: 6.5559, loss_val: nan, pos_over_neg: 75.17024230957031 lr: 0.0001\n",
      "Iter: 1372/3021, loss_train: 6.5393, loss_val: nan, pos_over_neg: 106.07295989990234 lr: 0.0001\n",
      "Iter: 1373/3021, loss_train: 6.5252, loss_val: nan, pos_over_neg: 146.46041870117188 lr: 0.0001\n",
      "Iter: 1374/3021, loss_train: 6.5623, loss_val: nan, pos_over_neg: 132.12831115722656 lr: 0.0001\n",
      "Iter: 1375/3021, loss_train: 6.5723, loss_val: nan, pos_over_neg: 116.16281127929688 lr: 0.0001\n",
      "Iter: 1376/3021, loss_train: 6.56, loss_val: nan, pos_over_neg: 165.25352478027344 lr: 0.0001\n",
      "Iter: 1377/3021, loss_train: 6.5422, loss_val: nan, pos_over_neg: 257.533935546875 lr: 0.0001\n",
      "Iter: 1378/3021, loss_train: 6.5187, loss_val: nan, pos_over_neg: 131.98046875 lr: 0.0001\n",
      "Iter: 1379/3021, loss_train: 6.5468, loss_val: nan, pos_over_neg: 143.9116973876953 lr: 0.0001\n",
      "Iter: 1380/3021, loss_train: 6.5406, loss_val: nan, pos_over_neg: 156.00563049316406 lr: 0.0001\n",
      "Iter: 1381/3021, loss_train: 6.5154, loss_val: nan, pos_over_neg: 148.84751892089844 lr: 0.0001\n",
      "Iter: 1382/3021, loss_train: 6.5176, loss_val: nan, pos_over_neg: 232.76513671875 lr: 0.0001\n",
      "Iter: 1383/3021, loss_train: 6.5297, loss_val: nan, pos_over_neg: 151.0709686279297 lr: 0.0001\n",
      "Iter: 1384/3021, loss_train: 6.5372, loss_val: nan, pos_over_neg: 87.79608917236328 lr: 0.0001\n",
      "Iter: 1385/3021, loss_train: 6.5291, loss_val: nan, pos_over_neg: 112.36619567871094 lr: 0.0001\n",
      "Iter: 1386/3021, loss_train: 6.5364, loss_val: nan, pos_over_neg: 93.47103881835938 lr: 0.0001\n",
      "Iter: 1387/3021, loss_train: 6.5453, loss_val: nan, pos_over_neg: 167.47203063964844 lr: 0.0001\n",
      "Iter: 1388/3021, loss_train: 6.5634, loss_val: nan, pos_over_neg: 98.35801696777344 lr: 0.0001\n",
      "Iter: 1389/3021, loss_train: 6.5204, loss_val: nan, pos_over_neg: 114.15827178955078 lr: 0.0001\n",
      "Iter: 1390/3021, loss_train: 6.5548, loss_val: nan, pos_over_neg: 105.30813598632812 lr: 0.0001\n",
      "Iter: 1391/3021, loss_train: 6.529, loss_val: nan, pos_over_neg: 146.23016357421875 lr: 0.0001\n",
      "Iter: 1392/3021, loss_train: 6.5456, loss_val: nan, pos_over_neg: 128.3343963623047 lr: 0.0001\n",
      "Iter: 1393/3021, loss_train: 6.5501, loss_val: nan, pos_over_neg: 102.40209197998047 lr: 0.0001\n",
      "Iter: 1394/3021, loss_train: 6.5834, loss_val: nan, pos_over_neg: 129.3527374267578 lr: 0.0001\n",
      "Iter: 1395/3021, loss_train: 6.5527, loss_val: nan, pos_over_neg: 117.56543731689453 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1396/3021, loss_train: 6.5374, loss_val: nan, pos_over_neg: 125.47113800048828 lr: 0.0001\n",
      "Iter: 1397/3021, loss_train: 6.5522, loss_val: nan, pos_over_neg: 86.76875305175781 lr: 0.0001\n",
      "Iter: 1398/3021, loss_train: 6.504, loss_val: nan, pos_over_neg: 84.27010345458984 lr: 0.0001\n",
      "Iter: 1399/3021, loss_train: 6.5444, loss_val: nan, pos_over_neg: 93.54882049560547 lr: 0.0001\n",
      "Iter: 1400/3021, loss_train: 6.5394, loss_val: nan, pos_over_neg: 132.93801879882812 lr: 0.0001\n",
      "Iter: 1401/3021, loss_train: 6.5526, loss_val: nan, pos_over_neg: 97.32282257080078 lr: 0.0001\n",
      "Iter: 1402/3021, loss_train: 6.5269, loss_val: nan, pos_over_neg: 146.22781372070312 lr: 0.0001\n",
      "Iter: 1403/3021, loss_train: 6.5676, loss_val: nan, pos_over_neg: 112.19435119628906 lr: 0.0001\n",
      "Iter: 1404/3021, loss_train: 6.544, loss_val: nan, pos_over_neg: 95.98627471923828 lr: 0.0001\n",
      "Iter: 1405/3021, loss_train: 6.5496, loss_val: nan, pos_over_neg: 116.02179718017578 lr: 0.0001\n",
      "Iter: 1406/3021, loss_train: 6.5585, loss_val: nan, pos_over_neg: 81.69291687011719 lr: 0.0001\n",
      "Iter: 1407/3021, loss_train: 6.5623, loss_val: nan, pos_over_neg: 88.91946411132812 lr: 0.0001\n",
      "Iter: 1408/3021, loss_train: 6.5508, loss_val: nan, pos_over_neg: 87.28654479980469 lr: 0.0001\n",
      "Iter: 1409/3021, loss_train: 6.5625, loss_val: nan, pos_over_neg: 101.44428253173828 lr: 0.0001\n",
      "Iter: 1410/3021, loss_train: 6.5669, loss_val: nan, pos_over_neg: 69.46464538574219 lr: 0.0001\n",
      "Iter: 1411/3021, loss_train: 6.5362, loss_val: nan, pos_over_neg: 88.71495819091797 lr: 0.0001\n",
      "Iter: 1412/3021, loss_train: 6.5544, loss_val: nan, pos_over_neg: 98.08829498291016 lr: 0.0001\n",
      "Iter: 1413/3021, loss_train: 6.5383, loss_val: nan, pos_over_neg: 92.35199737548828 lr: 0.0001\n",
      "Iter: 1414/3021, loss_train: 6.5314, loss_val: nan, pos_over_neg: 122.9801254272461 lr: 0.0001\n",
      "Iter: 1415/3021, loss_train: 6.5419, loss_val: nan, pos_over_neg: 98.3903579711914 lr: 0.0001\n",
      "Iter: 1416/3021, loss_train: 6.5538, loss_val: nan, pos_over_neg: 159.73504638671875 lr: 0.0001\n",
      "Iter: 1417/3021, loss_train: 6.5586, loss_val: nan, pos_over_neg: 130.712158203125 lr: 0.0001\n",
      "Iter: 1418/3021, loss_train: 6.526, loss_val: nan, pos_over_neg: 260.7752380371094 lr: 0.0001\n",
      "Iter: 1419/3021, loss_train: 6.5362, loss_val: nan, pos_over_neg: 320.3343811035156 lr: 0.0001\n",
      "Iter: 1420/3021, loss_train: 6.536, loss_val: nan, pos_over_neg: 191.2843780517578 lr: 0.0001\n",
      "Iter: 1421/3021, loss_train: 6.5419, loss_val: nan, pos_over_neg: 240.36927795410156 lr: 0.0001\n",
      "Iter: 1422/3021, loss_train: 6.5541, loss_val: nan, pos_over_neg: 181.70289611816406 lr: 0.0001\n",
      "Iter: 1423/3021, loss_train: 6.5321, loss_val: nan, pos_over_neg: 183.72166442871094 lr: 0.0001\n",
      "Iter: 1424/3021, loss_train: 6.5471, loss_val: nan, pos_over_neg: 139.54075622558594 lr: 0.0001\n",
      "Iter: 1425/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 125.24551391601562 lr: 0.0001\n",
      "Iter: 1426/3021, loss_train: 6.5487, loss_val: nan, pos_over_neg: 139.81947326660156 lr: 0.0001\n",
      "Iter: 1427/3021, loss_train: 6.5183, loss_val: nan, pos_over_neg: 102.59857177734375 lr: 0.0001\n",
      "Iter: 1428/3021, loss_train: 6.548, loss_val: nan, pos_over_neg: 73.38263702392578 lr: 0.0001\n",
      "Iter: 1429/3021, loss_train: 6.5436, loss_val: nan, pos_over_neg: 80.09285736083984 lr: 0.0001\n",
      "Iter: 1430/3021, loss_train: 6.5364, loss_val: nan, pos_over_neg: 89.8744888305664 lr: 0.0001\n",
      "Iter: 1431/3021, loss_train: 6.5484, loss_val: nan, pos_over_neg: 85.80965423583984 lr: 0.0001\n",
      "Iter: 1432/3021, loss_train: 6.5642, loss_val: nan, pos_over_neg: 94.53810119628906 lr: 0.0001\n",
      "Iter: 1433/3021, loss_train: 6.5476, loss_val: nan, pos_over_neg: 158.90013122558594 lr: 0.0001\n",
      "Iter: 1434/3021, loss_train: 6.5474, loss_val: nan, pos_over_neg: 131.40676879882812 lr: 0.0001\n",
      "Iter: 1435/3021, loss_train: 6.5242, loss_val: nan, pos_over_neg: 174.3657989501953 lr: 0.0001\n",
      "Iter: 1436/3021, loss_train: 6.5385, loss_val: nan, pos_over_neg: 133.5756378173828 lr: 0.0001\n",
      "Iter: 1437/3021, loss_train: 6.5265, loss_val: nan, pos_over_neg: 210.73785400390625 lr: 0.0001\n",
      "Iter: 1438/3021, loss_train: 6.5158, loss_val: nan, pos_over_neg: 197.37979125976562 lr: 0.0001\n",
      "Iter: 1439/3021, loss_train: 6.5466, loss_val: nan, pos_over_neg: 155.01844787597656 lr: 0.0001\n",
      "Iter: 1440/3021, loss_train: 6.5706, loss_val: nan, pos_over_neg: 99.4661865234375 lr: 0.0001\n",
      "Iter: 1441/3021, loss_train: 6.5481, loss_val: nan, pos_over_neg: 128.1034698486328 lr: 0.0001\n",
      "Iter: 1442/3021, loss_train: 6.5442, loss_val: nan, pos_over_neg: 84.78678131103516 lr: 0.0001\n",
      "Iter: 1443/3021, loss_train: 6.5386, loss_val: nan, pos_over_neg: 105.61705780029297 lr: 0.0001\n",
      "Iter: 1444/3021, loss_train: 6.5434, loss_val: nan, pos_over_neg: 114.72537231445312 lr: 0.0001\n",
      "Iter: 1445/3021, loss_train: 6.5601, loss_val: nan, pos_over_neg: 76.23422241210938 lr: 0.0001\n",
      "Iter: 1446/3021, loss_train: 6.5169, loss_val: nan, pos_over_neg: 92.53662109375 lr: 0.0001\n",
      "Iter: 1447/3021, loss_train: 6.5379, loss_val: nan, pos_over_neg: 95.81218719482422 lr: 0.0001\n",
      "Iter: 1448/3021, loss_train: 6.5495, loss_val: nan, pos_over_neg: 136.03514099121094 lr: 0.0001\n",
      "Iter: 1449/3021, loss_train: 6.5484, loss_val: nan, pos_over_neg: 156.05752563476562 lr: 0.0001\n",
      "Iter: 1450/3021, loss_train: 6.5625, loss_val: nan, pos_over_neg: 115.17961883544922 lr: 0.0001\n",
      "Iter: 1451/3021, loss_train: 6.5291, loss_val: nan, pos_over_neg: 108.63214111328125 lr: 0.0001\n",
      "Iter: 1452/3021, loss_train: 6.5306, loss_val: nan, pos_over_neg: 205.17662048339844 lr: 0.0001\n",
      "Iter: 1453/3021, loss_train: 6.569, loss_val: nan, pos_over_neg: 111.58326721191406 lr: 0.0001\n",
      "Iter: 1454/3021, loss_train: 6.5602, loss_val: nan, pos_over_neg: 126.00434112548828 lr: 0.0001\n",
      "Iter: 1455/3021, loss_train: 6.517, loss_val: nan, pos_over_neg: 113.65813446044922 lr: 0.0001\n",
      "Iter: 1456/3021, loss_train: 6.5419, loss_val: nan, pos_over_neg: 82.00617218017578 lr: 0.0001\n",
      "Iter: 1457/3021, loss_train: 6.5376, loss_val: nan, pos_over_neg: 83.69491577148438 lr: 0.0001\n",
      "Iter: 1458/3021, loss_train: 6.5288, loss_val: nan, pos_over_neg: 134.5753936767578 lr: 0.0001\n",
      "Iter: 1459/3021, loss_train: 6.5312, loss_val: nan, pos_over_neg: 115.89073181152344 lr: 0.0001\n",
      "Iter: 1460/3021, loss_train: 6.5225, loss_val: nan, pos_over_neg: 146.4495086669922 lr: 0.0001\n",
      "Iter: 1461/3021, loss_train: 6.5114, loss_val: nan, pos_over_neg: 139.55245971679688 lr: 0.0001\n",
      "Iter: 1462/3021, loss_train: 6.5565, loss_val: nan, pos_over_neg: 143.9048614501953 lr: 0.0001\n",
      "Iter: 1463/3021, loss_train: 6.5498, loss_val: nan, pos_over_neg: 117.77127838134766 lr: 0.0001\n",
      "Iter: 1464/3021, loss_train: 6.5366, loss_val: nan, pos_over_neg: 211.14617919921875 lr: 0.0001\n",
      "Iter: 1465/3021, loss_train: 6.5276, loss_val: nan, pos_over_neg: 205.70028686523438 lr: 0.0001\n",
      "Iter: 1466/3021, loss_train: 6.5654, loss_val: nan, pos_over_neg: 109.73756408691406 lr: 0.0001\n",
      "Iter: 1467/3021, loss_train: 6.5142, loss_val: nan, pos_over_neg: 111.8484878540039 lr: 0.0001\n",
      "Iter: 1468/3021, loss_train: 6.5431, loss_val: nan, pos_over_neg: 129.56834411621094 lr: 0.0001\n",
      "Iter: 1469/3021, loss_train: 6.5385, loss_val: nan, pos_over_neg: 125.76950073242188 lr: 0.0001\n",
      "Iter: 1470/3021, loss_train: 6.5435, loss_val: nan, pos_over_neg: 109.41683959960938 lr: 0.0001\n",
      "Iter: 1471/3021, loss_train: 6.5436, loss_val: nan, pos_over_neg: 112.10591125488281 lr: 0.0001\n",
      "Iter: 1472/3021, loss_train: 6.5172, loss_val: nan, pos_over_neg: 104.1684799194336 lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "losses_train, losses_val = [], [np.nan]\n",
    "for epoch in tqdm(range(params['n_epochs'])):\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    losses_train = training.epoch_step(\n",
    "        dataloader_train, \n",
    "        model, \n",
    "        optimizer, \n",
    "        criterion,\n",
    "        scheduler=scheduler,\n",
    "        temperature=params['temperature'],\n",
    "        # l2_alpha,\n",
    "        mode='semi-supervised',\n",
    "        loss_rolling_train=losses_train, \n",
    "        loss_rolling_val=losses_val,\n",
    "        device=device_train, \n",
    "        verbose=2,\n",
    "        verbose_update_period=1,\n",
    "\n",
    "#                                     do_validation=False,\n",
    "#                                     X_val=x_feed_through_val,\n",
    "#                                     y_val=torch.as_tensor(y_val, device=DEVICE)\n",
    ")\n",
    "    \n",
    "    ## save loss stuff\n",
    "    if pref_saveLogs:\n",
    "        write_to_log(path_log=path_saveLog, text=f'time:{time.ctime()}, completed epoch: {epoch}, loss: {losses_train[-1]}, lr: {scheduler.get_last_lr()[0]}')\n",
    "        np.save(path_saveLoss, losses_train)\n",
    "    \n",
    "    ## if loss becomes NaNs, don't save the network and stop training\n",
    "    if torch.isnan(losses_train[-1]):\n",
    "        break\n",
    "        \n",
    "    ## save model\n",
    "    if pref_saveModelIteratively:\n",
    "        torch.save(model.state_dict(), path_saveModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU8l0eP02TQR"
   },
   "outputs": [],
   "source": [
    "transforms_validation = torch.nn.Sequential(\n",
    "    augmentation.ScaleDynamicRange(scaler_bounds=(0,1)),\n",
    "    torchvision.transforms.Resize(size=(224,224),\n",
    "                                  interpolation=torchvision.transforms.InterpolationMode.BILINEAR), \n",
    "    augmentation.TileChannels(dim=0, n_channels=3),\n",
    "#     augmentation.Normalize(  means=[0.485, 0.456, 0.406],\n",
    "#                              stds=[0.229, 0.224, 0.225]),\n",
    "#     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                      std=[0.229, 0.224, 0.225],\n",
    "#                                      inplace=False),\n",
    ")\n",
    "scripted_transforms_validation = torch.jit.script(transforms_validation)\n",
    "# scripted_transforms = transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labeled_train = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_train, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_train_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_train.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_train_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataset_labeled_val = util.dataset_simCLR(\n",
    "                                    # torch.as_tensor(X_labeled_val, device='cpu', dtype=torch.float32), \n",
    "                                    torch.as_tensor(X_labeled_val_SYT, device='cpu', dtype=torch.float32), \n",
    "                                    # torch.as_tensor(torch.zeros(X_labeled_val.shape[0]), device='cpu', dtype=torch.float32),\n",
    "                                    torch.as_tensor(torch.zeros(X_labeled_val_SYT.shape[0]), device='cpu', dtype=torch.float32),\n",
    "\n",
    "                                    n_transforms=1,\n",
    "                                    class_weights=np.array([1]),\n",
    "                                    # class_weights=np.array([1]*4)[np.random.randint(0,4, X_train.shape[0])],\n",
    "                                    transform=scripted_transforms_validation,\n",
    "                                    # DEVICE='cpu',\n",
    "                                    DEVICE='cpu',\n",
    "                                    dtype_X=torch.float32,\n",
    "                                    dtype_y=torch.int64,\n",
    "                                    \n",
    "                                    temp_uncertainty=16\n",
    "                                    )\n",
    "dataloader_labeled_train = torch.utils.data.DataLoader( dataset_labeled_train,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n",
    "dataloader_labeled_val = torch.utils.data.DataLoader( dataset_labeled_val,\n",
    "    #                                                 batch_size=int(pct_batch_size*X_train.shape[0]),\n",
    "                                                    batch_size=1024,\n",
    "                                                    shuffle=False,\n",
    "                                                    drop_last=False,\n",
    "                                                    pin_memory=True,\n",
    "                                                    num_workers=16,\n",
    "                                                    persistent_workers=True,\n",
    "                                                    # prefetch_factor=0\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch_helpers.set_device(use_GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "features_train = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_train], dim=0)\n",
    "features_val   = torch.cat([model.get_head(model.base_model(data[0][0].to(DEVICE))).detach().cpu() for data in dataloader_labeled_val], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.shape, features_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sweep of logistic regressions over C (1/L2) parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_val = [], []\n",
    "# C_toUse = np.array([1000,100,10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "C_toUse = np.array([10,1,0.1,0.01,0.001,0.0001,0.00001])\n",
    "for C in C_toUse:\n",
    "#     print(f'C = {C}')\n",
    "    logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=C)\n",
    "#     tic = time.time()\n",
    "    logreg.fit(features_train, y_labeled_train_SYT)\n",
    "    # logreg.fit(features_train, y_labeled_train)\n",
    "#     print(f'time: {time.time() - tic}')\n",
    "    acc = logreg.score(features_train, y_labeled_train_SYT)\n",
    "    # acc = logreg.score(features_train, y_labeled_train)\n",
    "    acc_train.append(acc)\n",
    "#     print(f'acc_train: {acc}')\n",
    "    acc = logreg.score(features_val, y_labeled_val_SYT)\n",
    "    # acc = logreg.score(features_val, y_labeled_val)\n",
    "    acc_val.append(acc)\n",
    "#     print(f'acc_val: {acc}')\n",
    "#     print('')\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(C_toUse, acc_train)\n",
    "plt.plot(C_toUse, acc_val)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('acc')\n",
    "plt.legend(['train', 'test']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run a sinlg logistic regression with desired parameters and check confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(solver='liblinear', max_iter=800, C=10**(1)).fit(features_train, y_labeled_train_SYT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "proba = logreg.predict_proba(features_train)\n",
    "\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_train)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_train_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('train');\n",
    "\n",
    "proba = logreg.predict_proba(features_val)\n",
    "preds = np.argmax(proba, axis=1)\n",
    "# cm = classification.confusion_matrix(preds, y_labeled_val)\n",
    "cm = classification.confusion_matrix(preds, y_labeled_val_SYT)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'))\n",
    "plt.title('val');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(key, '    ', layer.shape) for key, layer in model.state_dict().items()];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(model.state_dict()['PostHead_0.weight'].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "layer_1 = model.state_dict()['base_model.0.0.0.weight'].cpu()\n",
    "layer_2 = model.state_dict()['base_model.0.6.3.block.1.0.weight'].cpu()\n",
    "layer_3 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "# layer_4 = model.state_dict()['base_model.0.7.0.block.1.0.weight'].cpu()\n",
    "\n",
    "\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_1], dim=0), grid_shape=(5,6), kwargs_imshow={'vmax': 0.2});\n",
    "# plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_2], dim=0), grid_shape=(6,6), kwargs_imshow={'vmax': 0.2});\n",
    "plotting_helpers.plot_image_grid(torch.cat([arr for arr in layer_3], dim=0), grid_shape=(4,6), kwargs_imshow={'vmax': 0.2});\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ROIClassifier_TRAIN_20211201_JZ_supervised-comparison5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 920.76378,
   "position": {
    "height": "40px",
    "left": "1381px",
    "right": "20px",
    "top": "106px",
    "width": "500.99px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
